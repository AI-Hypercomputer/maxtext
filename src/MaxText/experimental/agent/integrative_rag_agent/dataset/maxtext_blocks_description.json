[
    {
        "block_name": "src/MaxText/layers/attention_mla.py#mla_as_linen",
        "file_path": "src/MaxText/layers/attention_mla.py",
        "code_block": "def mla_as_linen(\n    *,\n    config: Config,\n    num_query_heads: int,\n    num_kv_heads: int,\n    head_dim: int,\n    max_target_length: int,\n    mesh: Mesh,\n    attention_kernel: str,\n    inputs_q_shape: Tuple,\n    inputs_kv_shape: Tuple,\n    dtype: DType = jnp.float32,\n    weight_dtype: DType = jnp.float32,\n    max_prefill_predict_length: int = -1,\n    dropout_rate: float = 0.0,\n    kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"normal\"),\n    float32_qk_product: bool = False,  # computes logits in float32 for stability.\n    float32_logits: bool = False,  # cast logits in float32 for stability.\n    quant: Optional[Quant] = None,\n    kv_quant: Optional[KVQuant] = None,\n    attention_type: AttentionType = AttentionType.MLA,  # Default to MLA attention\n    attn_logits_soft_cap: float | None = None,\n    sliding_window_size: int | None = None,\n    use_ragged_attention: bool = False,\n    ragged_block_size: int = 256,\n    use_qk_norm: bool = False,\n    query_pre_attn_scalar: float | None = None,\n    use_bias_in_projections: bool = False,  # Set to True will enable bias in q, k, v, o projections\n    # Temperature tuning parameters used for Llama4\n    temperature_tuning: bool = False,\n    temperature_tuning_scale: float = 0.1,\n    temperature_tuning_floor_scale: float = 8192.0,\n    # Shard the query activation as the same as the key and value.\n    # TODO: Find a better sharding axis name.\n    # TODO: Further break down the Training and Inference axes for the q, k, v.\n    prefill_query_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    prefill_key_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    prefill_value_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    query_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    key_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    value_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    ep_query_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    ep_key_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    ep_value_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    input_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, EMBED),\n    ep_input_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, EMBED),\n    out_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, HEAD, D_KV),\n    ep_out_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, HEAD, D_KV),\n    prefill_input_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, EMBED),\n    decode_input_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, EMBED),\n    prefill_out_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, HEAD, D_KV),\n    decode_out_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV),\n    prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n    reshape_q: bool = False,\n    is_nope_layer: bool = False,\n    is_vision: bool = False,\n    model_mode: str = MODEL_MODE_TRAIN,\n    q_lora_rank: int = 0,\n    kv_lora_rank: int = 512,\n    qk_nope_head_dim: int = 128,\n    qk_rope_head_dim: int = 64,\n    v_head_dim: int = 128,\n    max_position_embeddings: int = 4096 * 4,\n    original_max_position_embeddings: int = 4096,\n    mscale: float = 1.0,  # scaling factor for softmax\n    rope_factor: float = 40.0,  # rotary embedding factor\n    name: str | None = None,\n):\n  \"\"\"A factory function to create an MLA as a Linen module.\n\n  This function serves as a bridge to use the NNX-based `MLA` within a\n  Linen model.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      MLA,\n      config=config,\n      num_query_heads=num_query_heads,\n      num_kv_heads=num_kv_heads,\n      head_dim=head_dim,\n      max_target_length=max_target_length,\n      mesh=mesh,\n      attention_kernel=attention_kernel,\n      inputs_q_shape=inputs_q_shape,\n      inputs_kv_shape=inputs_kv_shape,\n      dtype=dtype,\n      weight_dtype=weight_dtype,\n      max_prefill_predict_length=max_prefill_predict_length,\n      dropout_rate=dropout_rate,\n      kernel_init=kernel_init,\n      float32_qk_product=float32_qk_product,\n      float32_logits=float32_logits,\n      quant=quant,\n      kv_quant=kv_quant,\n      attention_type=attention_type,\n      attn_logits_soft_cap=attn_logits_soft_cap,\n      sliding_window_size=sliding_window_size,\n      use_ragged_attention=use_ragged_attention,\n      ragged_block_size=ragged_block_size,\n      use_qk_norm=use_qk_norm,\n      query_pre_attn_scalar=query_pre_attn_scalar,\n      use_bias_in_projections=use_bias_in_projections,\n      temperature_tuning=temperature_tuning,\n      temperature_tuning_scale=temperature_tuning_scale,\n      temperature_tuning_floor_scale=temperature_tuning_floor_scale,\n      prefill_query_axis_names=prefill_query_axis_names,\n      prefill_key_axis_names=prefill_key_axis_names,\n      prefill_value_axis_names=prefill_value_axis_names,\n      query_axis_names=query_axis_names,\n      key_axis_names=key_axis_names,\n      value_axis_names=value_axis_names,\n      ep_query_axis_names=ep_query_axis_names,\n      ep_key_axis_names=ep_key_axis_names,\n      ep_value_axis_names=ep_value_axis_names,\n      input_axis_names=input_axis_names,\n      ep_input_axis_names=ep_input_axis_names,\n      out_axis_names=out_axis_names,\n      ep_out_axis_names=ep_out_axis_names,\n      prefill_input_axis_names=prefill_input_axis_names,\n      decode_input_axis_names=decode_input_axis_names,\n      prefill_out_axis_names=prefill_out_axis_names,\n      decode_out_axis_names=decode_out_axis_names,\n      prefill_cache_axis_order=prefill_cache_axis_order,\n      ar_cache_axis_order=ar_cache_axis_order,\n      compute_axis_order=compute_axis_order,\n      reshape_q=reshape_q,\n      is_nope_layer=is_nope_layer,\n      is_vision=is_vision,\n      model_mode=model_mode,\n      q_lora_rank=q_lora_rank,\n      kv_lora_rank=kv_lora_rank,\n      qk_nope_head_dim=qk_nope_head_dim,\n      qk_rope_head_dim=qk_rope_head_dim,\n      v_head_dim=v_head_dim,\n      max_position_embeddings=max_position_embeddings,\n      original_max_position_embeddings=original_max_position_embeddings,\n      mscale=mscale,\n      rope_factor=rope_factor,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )",
        "analysis": {
            "module_type": "multi_head_latent_attention_factory",
            "purpose": "A factory function that creates a Flax Linen-compatible module from the NNX-based `MLA` (Multi-Head Latent Attention) class.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to wrap the `MLA` class.",
                "Passes all configuration parameters (e.g., `config`, `num_query_heads`, `mesh`) to the wrapper for `MLA` instantiation."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance, not a tensor. Shape is N/A."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "MLA",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "config": "The main model configuration object.",
                "num_query_heads": "The number of query heads for the attention mechanism.",
                "num_kv_heads": "The number of key/value heads for the attention mechanism.",
                "head_dim": "The dimension of each attention head.",
                "max_target_length": "The maximum sequence length for the model.",
                "mesh": "The JAX device mesh for model parallelism.",
                "attention_kernel": "Specifies the attention kernel to be used (e.g., 'dot_product').",
                "inputs_q_shape": "The shape of the query inputs, used for initialization.",
                "inputs_kv_shape": "The shape of the key/value inputs, used for initialization.",
                "kv_lora_rank": "The rank for the LoRA projection applied to keys and values.",
                "model_mode": "The operational mode of the model (e.g., 'train', 'prefill')."
            },
            "notes": [
                "This function acts as a bridge to use an NNX-defined module (`MLA`) within a Flax Linen model.",
                "It does not perform any tensor computations itself but rather configures and returns a module that does.",
                "The returned Linen module encapsulates the functionality of the `MLA` class."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_mla.py#MLA",
        "file_path": "src/MaxText/layers/attention_mla.py",
        "code_block": "class MLA(Attention):\n  \"\"\"Multi-Head Latent Attention (MLA) layer.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      num_query_heads: int,\n      num_kv_heads: int,\n      head_dim: int,\n      max_target_length: int,\n      mesh: Mesh,\n      attention_kernel: str,\n      inputs_q_shape: Tuple,\n      inputs_kv_shape: Tuple,\n      dtype: DType = jnp.float32,\n      weight_dtype: DType = jnp.float32,\n      max_prefill_predict_length: int = -1,\n      dropout_rate: float = 0.0,\n      kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"normal\"),\n      float32_qk_product: bool = False,  # computes logits in float32 for stability.\n      float32_logits: bool = False,  # cast logits in float32 for stability.\n      quant: Optional[Quant] = None,\n      kv_quant: Optional[KVQuant] = None,\n      attention_type: AttentionType = AttentionType.MLA,  # Default to MLA attention\n      attn_logits_soft_cap: float | None = None,\n      sliding_window_size: int | None = None,\n      use_ragged_attention: bool = False,\n      ragged_block_size: int = 256,\n      use_qk_norm: bool = False,\n      query_pre_attn_scalar: float | None = None,\n      use_bias_in_projections: bool = False,  # Set to True will enable bias in q, k, v, o projections\n      # Temperature tuning parameters used for Llama4\n      temperature_tuning: bool = False,\n      temperature_tuning_scale: float = 0.1,\n      temperature_tuning_floor_scale: float = 8192.0,\n      # Shard the query activation as the same as the key and value.\n      # TODO: Find a better sharding axis name.\n      # TODO: Further break down the Training and Inference axes for the q, k, v.\n      prefill_query_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      prefill_key_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      prefill_value_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      query_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      key_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      value_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      ep_query_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      ep_key_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      ep_value_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      input_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, EMBED),\n      ep_input_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, EMBED),\n      out_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, HEAD, D_KV),\n      ep_out_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, HEAD, D_KV),\n      prefill_input_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, EMBED),\n      decode_input_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, EMBED),\n      prefill_out_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, HEAD, D_KV),\n      decode_out_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV),\n      prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n      reshape_q: bool = False,\n      is_nope_layer: bool = False,\n      is_vision: bool = False,\n      model_mode: str = MODEL_MODE_TRAIN,\n      q_lora_rank: int = 0,\n      kv_lora_rank: int = 512,\n      qk_nope_head_dim: int = 128,\n      qk_rope_head_dim: int = 64,\n      v_head_dim: int = 128,\n      max_position_embeddings: int = 4096 * 4,\n      original_max_position_embeddings: int = 4096,\n      mscale: float = 1.0,  # scaling factor for softmax\n      rope_factor: float = 40.0,  # rotary embedding factor\n      name: str | None = None,\n      rngs: Optional[nnx.Rngs] = None,\n  ):\n    \"\"\"Initializes the MLA module.\n\n    Args:\n      config: The model configuration.\n      ... and other configuration parameters for MLA attention.\n      rngs: The random number generators for initialization, passed by the nnx.to_linen wrapper.\n    \"\"\"\n    base_kv_cache = config.attention != \"paged\" and config.mla_naive_kvcache\n\n    # Setting these before call to super because a field is used in super\n    self.q_lora_rank = q_lora_rank\n    self.kv_lora_rank = kv_lora_rank\n    self.qk_nope_head_dim = qk_nope_head_dim\n    self.qk_rope_head_dim = qk_rope_head_dim\n    self.v_head_dim = v_head_dim\n    self.max_position_embeddings = max_position_embeddings\n    self.original_max_position_embeddings = original_max_position_embeddings\n    self.mscale = mscale\n    self.rope_factor = rope_factor\n\n    self.qk_head_dim = self.qk_nope_head_dim + self.qk_rope_head_dim\n\n    super().__init__(\n        config=config,\n        num_query_heads=num_query_heads,\n        num_kv_heads=num_kv_heads,\n        head_dim=head_dim,\n        max_target_length=max_target_length,\n        mesh=mesh,\n        attention_kernel=attention_kernel,\n        inputs_q_shape=inputs_q_shape,\n        inputs_kv_shape=inputs_kv_shape,\n        dtype=dtype,\n        weight_dtype=weight_dtype,\n        max_prefill_predict_length=max_prefill_predict_length,\n        dropout_rate=dropout_rate,\n        kernel_init=kernel_init,\n        float32_qk_product=float32_qk_product,\n        float32_logits=float32_logits,\n        quant=quant,\n        kv_quant=kv_quant,\n        attention_type=attention_type,\n        attn_logits_soft_cap=attn_logits_soft_cap,\n        sliding_window_size=sliding_window_size,\n        use_ragged_attention=use_ragged_attention,\n        ragged_block_size=ragged_block_size,\n        use_qk_norm=use_qk_norm,\n        query_pre_attn_scalar=query_pre_attn_scalar,\n        use_bias_in_projections=use_bias_in_projections,\n        temperature_tuning=temperature_tuning,\n        temperature_tuning_scale=temperature_tuning_scale,\n        temperature_tuning_floor_scale=temperature_tuning_floor_scale,\n        prefill_query_axis_names=prefill_query_axis_names,\n        prefill_key_axis_names=prefill_key_axis_names,\n        prefill_value_axis_names=prefill_value_axis_names,\n        query_axis_names=query_axis_names,\n        key_axis_names=key_axis_names,\n        value_axis_names=value_axis_names,\n        ep_query_axis_names=ep_query_axis_names,\n        ep_key_axis_names=ep_key_axis_names,\n        ep_value_axis_names=ep_value_axis_names,\n        input_axis_names=input_axis_names,\n        ep_input_axis_names=ep_input_axis_names,\n        out_axis_names=out_axis_names,\n        ep_out_axis_names=ep_out_axis_names,\n        prefill_input_axis_names=prefill_input_axis_names,\n        decode_input_axis_names=decode_input_axis_names,\n        prefill_out_axis_names=prefill_out_axis_names,\n        decode_out_axis_names=decode_out_axis_names,\n        prefill_cache_axis_order=prefill_cache_axis_order,\n        ar_cache_axis_order=ar_cache_axis_order,\n        compute_axis_order=compute_axis_order,\n        reshape_q=reshape_q,\n        is_nope_layer=is_nope_layer,\n        is_vision=is_vision,\n        model_mode=model_mode,\n        base_kv_cache=base_kv_cache,\n        rngs=rngs,\n    )\n\n    # Module attribute names must match names previously passed to Linen for checkpointing\n    self.MlaKVCache_0 = self.init_mla_kv_caches(inputs_kv_shape) if model_mode != MODEL_MODE_TRAIN else None\n\n  def _init_projections(self, inputs_q_shape: Tuple, inputs_kv_shape: Tuple) -> None:\n    \"\"\"Initializes the MLA-specific projections.\"\"\"\n    # Assert required configuration parameters for MLA attention.\n    assert (\n        self.config.attention_type == AttentionType.MLA.value\n    ), f\"MLA requires MLA attention type {AttentionType.MLA.value}\"\n    assert self.kv_lora_rank > 0, \"KV LoRA rank must be > 0\"\n    assert self.qk_nope_head_dim > 0, \"QK NoPe head dim must be > 0\"\n    assert self.qk_rope_head_dim > 0, \"QK RoPE head dim must be > 0\"\n    assert self.v_head_dim > 0, \"V head dim must be > 0\"\n    assert self.num_query_heads == self.num_kv_heads, \"MLA requires equal number of query and kv heads\"\n    assert not self.config.fused_qkv, \"Fused QKV is not supported for MLA\"\n\n    if self.q_lora_rank == 0:\n      # Standard Q projection (without LoRA).\n      self.query = DenseGeneral(\n          in_features_shape=self.config.emb_dim,\n          out_features_shape=(self.num_query_heads, self.qk_head_dim),\n          axis=-1,\n          kernel_init=self.kernel_init,\n          kernel_axes=(\"embed\", \"q_heads\", \"kv\"),\n          dtype=self.dtype,\n          weight_dtype=self.weight_dtype,\n          quant=self.quant,\n          matmul_precision=self.config.matmul_precision,\n          rngs=self.rngs,\n      )\n    else:\n      # LoRA path for Q.\n      self.wq_a = DenseGeneral(\n          in_features_shape=self.config.emb_dim,\n          out_features_shape=self.q_lora_rank,\n          axis=-1,\n          kernel_init=self.kernel_init,\n          kernel_axes=(\"embed\", \"q_lora_up_proj\"),\n          dtype=self.dtype,\n          weight_dtype=self.weight_dtype,\n          quant=self.quant,\n          matmul_precision=self.config.matmul_precision,\n          rngs=self.rngs,\n      )\n      self.q_norm = RMSNorm(\n          num_features=self.q_lora_rank,\n          dtype=self.config.dtype,\n          weight_dtype=self.config.weight_dtype,\n          epsilon=self.config.normalization_layer_epsilon,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n      self.wq_b = DenseGeneral(\n          in_features_shape=self.q_lora_rank,\n          out_features_shape=(self.num_query_heads, self.qk_head_dim),\n          axis=-1,\n          kernel_init=self.kernel_init,\n          kernel_axes=(\"q_lora\", \"q_heads\", \"kv\"),\n          dtype=self.dtype,\n          weight_dtype=self.weight_dtype,\n          quant=self.quant,\n          matmul_precision=self.config.matmul_precision,\n          rngs=self.rngs,\n      )\n\n    # KV LoRA path.\n    self.wkv_a = DenseGeneral(\n        in_features_shape=self.config.emb_dim,\n        out_features_shape=self.kv_lora_rank + self.qk_rope_head_dim,\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"embed\", \"kv_lora_up_proj\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n    self.kv_norm = RMSNorm(\n        num_features=self.kv_lora_rank,\n        dtype=self.config.dtype,\n        weight_dtype=self.config.weight_dtype,\n        epsilon=self.config.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n    self.wkv_b = DenseGeneral(\n        in_features_shape=self.kv_lora_rank,\n        out_features_shape=(\n            self.num_query_heads,\n            (self.qk_nope_head_dim + self.v_head_dim),\n        ),\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"kv_lora\", \"kv_heads\", \"kv_head_dim\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n\n    # Set softmax scaling.\n    self.softmax_scale = self.qk_head_dim**-0.5\n    if self.max_position_embeddings > self.original_max_position_embeddings:\n      mscale = 0.1 * self.mscale * math.log(self.rope_factor) + 1.0\n      self.softmax_scale = self.softmax_scale * mscale * mscale\n\n    self.out = self.init_out_w(output_dim=inputs_q_shape[-1])\n\n    # Setup paged attention op\n    if self.config.attention == \"paged\":\n      # Set head_dim to the max of qk_head_dim and v_head_dim. The current paged\n      # attention kernel requires the head_dim to be the same for q, k, v.\n      head_dim = max(self.qk_head_dim, self.v_head_dim)\n      # Align head_dim to the pagedattn_head_dim_alignment if specified.\n      if self.config.pagedattn_head_dim_alignment > 0:\n        alignment = self.config.pagedattn_head_dim_alignment\n        head_dim = (head_dim + alignment - 1) // alignment * alignment\n      self.ds_paged_attention_op = paged_attention.PagedAttentionOp(\n          mesh=self.mesh,\n          num_pages=self.config.pagedattn_num_pages,\n          tokens_per_page=self.config.pagedattn_tokens_per_page,\n          max_pages_per_slot=(self.config.max_target_length + self.config.pagedattn_tokens_per_page - 1)\n          // self.config.pagedattn_tokens_per_page,\n          max_pages_per_prefill=(self.config.max_prefill_predict_length + self.config.pagedattn_tokens_per_page - 1)\n          // self.config.pagedattn_tokens_per_page,\n          pages_per_compute_block=self.config.pagedattn_pages_per_compute_block,\n          num_kv_heads=self.num_kv_heads,\n          kv_head_dim_size=head_dim,\n          dtype=self.dtype,\n          attn_logits_soft_cap=self.attn_logits_soft_cap,\n          rngs=self.rngs,\n      )\n\n  def mla_query_projection(self, inputs_q: Array, inputs_positions: Array, model_mode) -> Array:\n    \"\"\"Query projection for MLA, e.g. includes LoRA if q_lora_rank > 0.\"\"\"\n    # Set softmax scaling.\n    self.qk_head_dim = self.qk_nope_head_dim + self.qk_rope_head_dim\n    self.softmax_scale = self.qk_head_dim**-0.5\n    if self.max_position_embeddings > self.original_max_position_embeddings:\n      mscale = 0.1 * self.mscale * math.log(self.rope_factor) + 1.0\n      self.softmax_scale = self.softmax_scale * mscale * mscale\n\n    if self.q_lora_rank == 0:\n      q = self.query(inputs_q)\n    else:\n      # LoRA path\n      low_rank_q = self.wq_a(inputs_q)  # [B, L, q_lora_rank]\n      low_rank_q = self.q_norm(low_rank_q)  # RMSNorm on low rank\n      q = self.wq_b(low_rank_q)  # [B, L, n_heads * qk_head_dim]\n\n    # Split into non-positional and rotary parts.\n    q_nope, q_pe = jnp.split(q, [self.qk_nope_head_dim], axis=-1)\n    q_pe = self.apply_rotary_embedding(q_pe, inputs_positions=inputs_positions)\n    # Query projection is scaled by self.softmax_scale to be consistent MaxText implementation.\n    # DeepSeek v3 was doing it in attention score computation.\n    query = jnp.concatenate([q_nope, q_pe], axis=-1) * self.softmax_scale\n\n    if model_mode == MODEL_MODE_PREFILL:\n      query = nn.with_logical_constraint(query, self.prefill_query_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      query = nn.with_logical_constraint(query, self.ep_query_axis_names)\n    else:\n      query = nn.with_logical_constraint(query, self.query_axis_names)\n    return query\n\n  def mla_get_key_value(self, low_rank_main, key_rope, model_mode):\n    \"\"\"get (key,value) pair from mla\"\"\"\n    kv_out = self.wkv_b(low_rank_main)\n\n    # Split kv_out into key_nope and value parts.\n    key_nope, value = jnp.split(kv_out, [self.qk_nope_head_dim], axis=-1)\n    key_rope = jnp.broadcast_to(key_rope, (key_nope.shape[0], key_nope.shape[1], self.num_query_heads, key_rope.shape[3]))\n\n    key = jnp.concatenate([key_nope, key_rope], axis=-1)\n\n    if model_mode == MODEL_MODE_PREFILL:\n      key = nn.with_logical_constraint(key, self.prefill_key_axis_names)\n      value = nn.with_logical_constraint(value, self.prefill_value_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      key = nn.with_logical_constraint(key, self.ep_key_axis_names)\n      value = nn.with_logical_constraint(value, self.ep_value_axis_names)\n    else:\n      key = nn.with_logical_constraint(key, self.key_axis_names)\n      value = nn.with_logical_constraint(value, self.value_axis_names)\n    return key, value\n\n  def init_mla_kv_caches(self, inputs_kv_shape: Tuple):\n    \"\"\"Initializes MlaKVCache.\n\n    Args:\n      inputs_kv_shape: Key/value inputs shape for initialization.\n\n    Returns:\n      An MlaKVCache module instance.\n\n    Raises:\n      ValueError: If the configuration is invalid.\n\n    \"\"\"\n    batch_size, _, _ = inputs_kv_shape\n    # During initialization, seq_len of inputs_kv is max_target_length,\n    # which is not always correct for some functions in MlaKVCache.\n    # However, MlaKVCache internal cache shapes are based on max_prefill_length\n    # and max_target_length, not the passed seq_len.\n    # We can use a placeholder value. The correct fix might involve refactoring\n    # MlaKVCache.\n    placeholder_seq_len = 1\n\n    return kvcache.MlaKVCache(\n        max_prefill_length=self.max_prefill_predict_length,\n        max_target_length=self.max_target_length,\n        batch=batch_size,\n        key_seq_len=placeholder_seq_len,\n        value_seq_len=placeholder_seq_len,\n        key_head_size=self.kv_lora_rank,\n        value_head_size=self.qk_rope_head_dim,\n        dtype=self.dtype,\n        kv_quant=self.kv_quant,\n        prefill_cache_axis_order=self.prefill_cache_axis_order,\n        ar_cache_axis_order=self.ar_cache_axis_order,\n        model_mode=self.model_mode,\n        use_chunked_prefill=self.config.use_chunked_prefill,\n        rngs=self.rngs,\n    )\n\n  def update_mla_kv_caches(self, low_rank_main, key_rope, decoder_segment_ids, model_mode, previous_chunk=None):\n    \"\"\"Updates the MLA (Multi-Head Latent Attention) KV caches.\n\n    This method is specific to the MLA attention mechanism. It calls the\n    `mla_kv_cache_as_linen` module to update and retrieve the caches, which\n    store latent representations (`low_rank_main`) and RoPE-applied keys\n    (`key_rope`). It then reconstructs the full key and value tensors from\n    the cached components.\n\n    Args:\n      low_rank_main: The main latent component of the key.\n      key_rope: The RoPE-applied component of the key.\n      decoder_segment_ids: Segment IDs for decoder masking.\n      model_mode: The operational mode ('train', 'prefill', 'autoregressive').\n      previous_chunk: Information about previously processed chunks, for\n        chunked prefill.\n\n    Returns:\n      A list containing two elements:\n      - The prefill key-value cache, reconstructed from the MLA cache, or None.\n      - The autoregressive key-value cache, reconstructed from the MLA cache, or None.\n    \"\"\"\n\n    prefill_mla_cache, ar_mla_cache = self.MlaKVCache_0(\n        key_latent=low_rank_main,\n        key_rope=key_rope,\n        decoder_segment_ids=decoder_segment_ids,\n        model_mode=model_mode,\n        use_ragged_attention=self.use_ragged_attention,\n        previous_chunk=previous_chunk,\n    )\n\n    if prefill_mla_cache:\n      low_rank_main, key_rope, decoder_segment_ids = prefill_mla_cache\n      key, value = self.mla_get_key_value(low_rank_main, key_rope, model_mode)\n      prefill_kv_cache = key, value, decoder_segment_ids\n    else:\n      prefill_kv_cache = None\n\n    if ar_mla_cache:\n      low_rank_main, key_rope, decoder_segment_ids, lengths = ar_mla_cache\n      key, value = self.mla_get_key_value(low_rank_main, key_rope, model_mode)\n      ar_kv_cache = key, value, decoder_segment_ids, lengths\n    else:\n      ar_kv_cache = None\n    return [prefill_kv_cache, ar_kv_cache]\n\n  def mla_kv_projection(self, inputs: Array, inputs_positions: Array, decoder_segment_ids, model_mode, previous_chunk):\n    \"\"\"MLA key/value projection with integrated rotary embedding.\"\"\"\n    low_rank = self.wkv_a(inputs)\n    low_rank_main, low_rank_rope = jnp.split(low_rank, [self.kv_lora_rank], axis=-1)\n    low_rank_main = self.kv_norm(low_rank_main)\n\n    # Apply rotary embedding to key_rope.\n    key_rope = jnp.expand_dims(low_rank_rope, axis=2)\n    key_rope = self.apply_rotary_embedding(key_rope, inputs_positions=inputs_positions)\n\n    key, value = self.mla_get_key_value(low_rank_main, key_rope, model_mode)\n    cached_values = [None, None]\n    if self.config.attention != \"paged\" and model_mode != MODEL_MODE_TRAIN:\n      if self.config.mla_naive_kvcache:\n        cached_values = self.update_kv_caches(key, value, decoder_segment_ids, model_mode, previous_chunk)\n      else:\n        cached_values = self.update_mla_kv_caches(\n            low_rank_main, key_rope, decoder_segment_ids, model_mode, previous_chunk\n        )\n\n    return key, value, cached_values\n\n  def __call__(\n      self,\n      inputs_q: Array,\n      inputs_kv: Array,\n      inputs_positions: Array | None = None,\n      decoder_segment_ids: Array | None = None,\n      *,\n      model_mode: str = MODEL_MODE_TRAIN,\n      deterministic: bool = False,\n      previous_chunk: Any = None,\n      slot: Optional[int] = None,\n      page_state: Optional[page_manager.PageState] = None,\n      bidirectional_mask: Optional[Any] = None,\n  ) -> Array:\n    \"\"\"Forward pass for MLA, reusing `AttentionOp` for the actual attention.\n\n    Args:\n      inputs_q: Query input [batch, q_length, embed_dim].\n      inputs_kv: KV input   [batch, kv_length, embed_dim].\n      inputs_positions: Positions for rotary embeddings or similar.\n      decoder_segment_ids: Segment IDs for masking, if any.\n      model_mode: \"train\", \"prefill\", or \"autoregressive\".\n      deterministic: Disables dropout if set to True.\n      previous_chunk: Information about previously processed chunks for chunked prefill.\n      slot: The batch slot index for paged attention.\n      page_state: The current state of the paged attention manager.\n      bidirectional_mask: A mask for bidirectional attention, used in multimodal models.\n\n    Returns:\n      A tensor of shape [batch, length, embed_dim] containing the\n      MLA-attended outputs.\n    \"\"\"\n    if model_mode == MODEL_MODE_PREFILL:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.prefill_input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.prefill_input_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.ep_input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.ep_input_axis_names)\n    else:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.input_axis_names)\n\n    query = self.mla_query_projection(inputs_q, inputs_positions, model_mode)\n    key, value, cached_values = self.mla_kv_projection(\n        inputs_kv, inputs_positions, decoder_segment_ids, model_mode, previous_chunk\n    )\n\n    query = checkpoint_name(query, \"query_proj\")\n    key = checkpoint_name(key, \"key_proj\")\n    value = checkpoint_name(value, \"value_proj\")\n\n    if self.config.attention == \"paged\" and model_mode != MODEL_MODE_TRAIN:\n      unnormalized_out, _, exp_sum = self.ds_paged_attention_op(\n          query, key, value, decoder_segment_ids, model_mode, previous_chunk, slot=slot, page_state=page_state\n      )\n      unnormalized_out = unnormalized_out[..., : self.v_head_dim]\n      out = unnormalized_out / (exp_sum + 1e-9) if exp_sum is not None else unnormalized_out\n    else:\n      out = self.attention_op(query, key, value, decoder_segment_ids, model_mode, cached_values)\n\n    if model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      out = nn.with_logical_constraint(out, self.ep_out_axis_names)\n    else:\n      out = nn.with_logical_constraint(out, self.out_axis_names)\n\n    out = self.out_projection(out)\n    return out",
        "analysis": {
            "module_type": "multi_head_latent_attention",
            "purpose": "Implements the Multi-Head Latent Attention (MLA) mechanism, which uses a low-rank factorization for key and value projections and splits query/key heads into positional (RoPE) and non-positional parts.",
            "input": {
                "shape": "[batch_size, sequence_length, embed_dim]",
                "dtype": "jnp.float32"
            },
            "processing_steps": [
                "Apply logical constraints to input tensors based on `model_mode`.",
                "Project the query input `inputs_q` using `mla_query_projection`.",
                "Project the key/value input `inputs_kv` using `mla_kv_projection` to get key, value, and cached values.",
                "Apply checkpointing to the projected query, key, and value tensors.",
                "If using paged attention during inference, call `ds_paged_attention_op` and normalize its output.",
                "Otherwise, call the base `attention_op` with the projected tensors and cached values.",
                "Apply logical constraints to the output tensor.",
                "Apply the final output projection using `out_projection`."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, embed_dim]"
            },
            "dependencies": [
                "Attention",
                "DenseGeneral",
                "RMSNorm",
                "kvcache.MlaKVCache",
                "paged_attention.PagedAttentionOp",
                "nnx.Rngs"
            ],
            "parameters": {
                "q_lora_rank": "The rank for the LoRA-like projection of the query. If 0, a standard projection is used.",
                "kv_lora_rank": "The rank for the LoRA-like projection of the key and value.",
                "qk_nope_head_dim": "The dimension of the non-positional (NoPe) part of the query and key heads.",
                "qk_rope_head_dim": "The dimension of the rotary positional embedding (RoPE) part of the query and key heads.",
                "v_head_dim": "The dimension of the value head."
            },
            "notes": [
                "This class inherits from the base `Attention` class and overrides projection and caching logic for the MLA architecture.",
                "It uses a specific LoRA-like architecture for Q, K, and V projections.",
                "Queries and keys are split into a non-positional (NoPe) part and a rotary positional (RoPE) part.",
                "Supports different execution paths for training, prefill, and autoregressive decoding, including paged attention."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the MLA module, setting up MLA-specific parameters, calling the superclass constructor, and initializing the MLA KV cache for inference modes.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set MLA-specific parameters like LoRA ranks and head dimensions.",
                        "Calculate the combined query/key head dimension `qk_head_dim`.",
                        "Call `super().__init__` to initialize the base `Attention` class.",
                        "Initialize `MlaKVCache_0` by calling `init_mla_kv_caches` if not in training mode."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Attention",
                        "self.init_mla_kv_caches"
                    ],
                    "notes": [
                        "Many parameters are passed directly to the parent `Attention` class constructor."
                    ]
                },
                "_init_projections": {
                    "purpose": "Initializes the MLA-specific query, key, and value projection layers.",
                    "input": {
                        "shape": "inputs_q_shape: Tuple, inputs_kv_shape: Tuple",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Assert that the configuration is valid for MLA.",
                        "Initialize query projection layers (`query` or `wq_a`, `q_norm`, `wq_b`) based on `q_lora_rank`.",
                        "Initialize key/value projection layers (`wkv_a`, `kv_norm`, `wkv_b`).",
                        "Calculate and set the `softmax_scale`.",
                        "Initialize the output projection layer (`out`) by calling `init_out_w`.",
                        "If paged attention is enabled, initialize `ds_paged_attention_op`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "DenseGeneral",
                        "RMSNorm",
                        "paged_attention.PagedAttentionOp"
                    ],
                    "notes": [
                        "This method is called by the parent `Attention` class's initialization process."
                    ]
                },
                "mla_query_projection": {
                    "purpose": "Projects the input query tensor, applying LoRA (if configured) and rotary positional embeddings.",
                    "input": {
                        "shape": "inputs_q: [batch, length, embed_dim], inputs_positions: [batch, length]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Recalculate `softmax_scale`.",
                        "If `q_lora_rank > 0`, apply the LoRA path (`wq_a`, `q_norm`, `wq_b`).",
                        "Otherwise, apply the standard `query` projection.",
                        "Split the projected query into non-positional (`q_nope`) and positional (`q_pe`) parts.",
                        "Apply rotary embeddings to the `q_pe` part using `apply_rotary_embedding`.",
                        "Concatenate the parts and scale by `softmax_scale`.",
                        "Apply logical constraints based on `model_mode`."
                    ],
                    "output": {
                        "shape": "[batch, length, num_query_heads, qk_head_dim]"
                    },
                    "dependencies": [
                        "jnp.split",
                        "jnp.concatenate",
                        "self.apply_rotary_embedding"
                    ],
                    "notes": [
                        "The softmax scaling is applied directly to the query projection."
                    ]
                },
                "mla_get_key_value": {
                    "purpose": "Generates the final key and value tensors from the latent representations.",
                    "input": {
                        "shape": "low_rank_main: [batch, length, kv_lora_rank], key_rope: [batch, length, 1, qk_rope_head_dim]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Project `low_rank_main` using `wkv_b`.",
                        "Split the result into `key_nope` and `value`.",
                        "Broadcast `key_rope` to match the shape of `key_nope`.",
                        "Concatenate `key_nope` and `key_rope` to form the final key.",
                        "Apply logical constraints to the key and value based on `model_mode`."
                    ],
                    "output": {
                        "shape": "Returns a tuple (key, value) with shapes [batch, length, num_kv_heads, qk_head_dim] and [batch, length, num_kv_heads, v_head_dim] respectively."
                    },
                    "dependencies": [
                        "jnp.split",
                        "jnp.broadcast_to",
                        "jnp.concatenate"
                    ],
                    "notes": [
                        "This method reconstructs the full key and value from their factorized components."
                    ]
                },
                "init_mla_kv_caches": {
                    "purpose": "Initializes the `MlaKVCache` module for inference.",
                    "input": {
                        "shape": "inputs_kv_shape: (batch_size, seq_len, embed_dim)",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Extract `batch_size` from the input shape.",
                        "Instantiate `kvcache.MlaKVCache` with configuration parameters."
                    ],
                    "output": {
                        "shape": "An MlaKVCache module instance."
                    },
                    "dependencies": [
                        "kvcache.MlaKVCache"
                    ],
                    "notes": [
                        "Uses a placeholder sequence length of 1 during initialization, as the internal cache shapes depend on config parameters, not the input shape."
                    ]
                },
                "update_mla_kv_caches": {
                    "purpose": "Updates and retrieves the MLA key-value caches for prefill and autoregressive decoding by calling the `MlaKVCache` module and reconstructing the full key/value tensors.",
                    "input": {
                        "shape": "low_rank_main: [batch, length, kv_lora_rank], key_rope: [batch, length, 1, qk_rope_head_dim]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Call the `MlaKVCache_0` module with the latent key components.",
                        "If a prefill cache is returned, reconstruct the full key and value using `mla_get_key_value`.",
                        "If an autoregressive cache is returned, reconstruct the full key and value using `mla_get_key_value`.",
                        "Return a list containing the reconstructed prefill and autoregressive caches."
                    ],
                    "output": {
                        "shape": "A list of two items: [prefill_kv_cache, ar_kv_cache]. Each cache is a tuple of tensors or None."
                    },
                    "dependencies": [
                        "self.MlaKVCache_0",
                        "self.mla_get_key_value"
                    ],
                    "notes": [
                        "This method bridges the latent cache representation with the full key/value representation needed by the attention operation."
                    ]
                },
                "mla_kv_projection": {
                    "purpose": "Projects the input key/value tensor into its latent components, applies rotary embeddings, reconstructs the full key/value, and updates the KV cache.",
                    "input": {
                        "shape": "inputs: [batch, length, embed_dim], inputs_positions: [batch, length]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Project `inputs` using `wkv_a` to get a low-rank representation.",
                        "Split the low-rank representation into `low_rank_main` and `low_rank_rope`.",
                        "Normalize `low_rank_main` with `kv_norm`.",
                        "Apply rotary embeddings to `low_rank_rope`.",
                        "Generate the full key and value using `mla_get_key_value`.",
                        "If not training and not using paged attention, update the KV cache using `update_mla_kv_caches` or `update_kv_caches`.",
                        "Return the key, value, and cached values."
                    ],
                    "output": {
                        "shape": "A tuple `(key, value, cached_values)`."
                    },
                    "dependencies": [
                        "jnp.split",
                        "self.apply_rotary_embedding",
                        "self.mla_get_key_value",
                        "self.update_kv_caches",
                        "self.update_mla_kv_caches"
                    ],
                    "notes": [
                        "Handles the entire KV pipeline from input embeddings to final key/value tensors and caching."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass for the Multi-Head Latent Attention layer.",
                    "input": {
                        "shape": "inputs_q: [batch, q_length, embed_dim], inputs_kv: [batch, kv_length, embed_dim]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Apply logical constraints to `inputs_q` and `inputs_kv`.",
                        "Call `mla_query_projection` to get the query tensor.",
                        "Call `mla_kv_projection` to get the key, value, and cached values.",
                        "Apply checkpointing to query, key, and value.",
                        "If using paged attention, call `ds_paged_attention_op` and normalize the output.",
                        "Otherwise, call the base `attention_op`.",
                        "Apply logical constraints to the output.",
                        "Call `out_projection` to get the final output."
                    ],
                    "output": {
                        "shape": "[batch, length, embed_dim]"
                    },
                    "dependencies": [
                        "self.mla_query_projection",
                        "self.mla_kv_projection",
                        "self.ds_paged_attention_op",
                        "self.attention_op",
                        "self.out_projection",
                        "checkpoint_name"
                    ],
                    "notes": [
                        "Orchestrates the entire attention process, delegating projections to MLA-specific methods and the core attention computation to either a paged attention operator or the base class's operator."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#validate_compute_axis_order",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def validate_compute_axis_order(s: AxisIdxes) -> None:\n  valid_compute_axis_order = ((0, 1, 2, 3), (0, 2, 1, 3))\n  if s not in valid_compute_axis_order:  # currently supported compute_axis_order\n    raise ValueError(\"Invalid compute_axis_order was passed. Valid options \", valid_compute_axis_order)",
        "analysis": {
            "functionality": "This function validates whether a given compute axis order, represented as a tuple of integers, is one of the supported configurations. If the provided order is not in the predefined list of valid orders, it raises a ValueError.",
            "usage": "Call this function with a tuple of integers representing the compute axis order. For example, `validate_compute_axis_order((0, 1, 2, 3))`. The function will either complete successfully (returning None) if the order is valid, or raise a `ValueError` if it is not."
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#apply_mask_to_logits",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def apply_mask_to_logits(logits: Array, mask: Array):\n  \"\"\"Applies a floating-point mask to a set of logits.\n\n  The mask is represented as a tensor with some dtype where 0 represents true and values\n  below a large negative number (here set to\n  get_large_negative_number(logits.dtype) / 2) represent false. Applying the mask\n  leaves the logits alone in the true case and replaces them by\n  get_large_negative_number(logits.dtype) in the false case. Previously, this was\n  done by adding the logits to the mask; however, this leads to a bad fusion\n  decision in the compiler that saves the values in memory rather than\n  just the predicate. This implementation avoids that problem.\n\n  from https://github.com/google/praxis/blob/4712a6b9ee13e224b86e235ff55f7c6bab9fbab3/praxis/py_utils.py#L706\n\n  Args:\n    logits: A JTensor of logit values.\n    mask: A JTensor of mask values with the encoding described in the\n      function documentation.\n\n  Returns:\n    Masked logits.\n  \"\"\"\n  return jnp.where((mask >= DEFAULT_MASK_VALUE * 0.5), logits, DEFAULT_MASK_VALUE)",
        "analysis": {
            "module_type": "logit_masking_function",
            "purpose": "Applies a floating-point attention mask to a tensor of logits, replacing masked-out positions with a large negative value.",
            "input": {
                "shape": "The `logits` shape is arbitrary (e.g., [batch, heads, q_len, kv_len]). The `mask` shape must be broadcastable to the `logits` shape.",
                "dtype": "float32 or other floating-point types."
            },
            "processing_steps": [
                "Create a boolean condition by checking where the input `mask` tensor's values are greater than or equal to `DEFAULT_MASK_VALUE * 0.5`.",
                "Use `jnp.where` to select elements based on the condition.",
                "If the condition is true for an element, the original value from `logits` is kept.",
                "If the condition is false, the element is replaced with `DEFAULT_MASK_VALUE`."
            ],
            "output": {
                "shape": "Same as the input `logits` tensor."
            },
            "dependencies": [
                "jax.numpy.where",
                "MaxText.common_types.DEFAULT_MASK_VALUE"
            ],
            "parameters": {},
            "notes": [
                "The mask is a floating-point tensor where values close to 0.0 represent positions to keep (true), and large negative values (<= `DEFAULT_MASK_VALUE * 0.5`) represent positions to mask out (false).",
                "This implementation uses `jnp.where` instead of addition (`logits + mask`) to avoid a specific suboptimal compiler fusion decision that would otherwise save intermediate values to memory."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#validate_flash_attention_with_sinks_on_gpu",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def validate_flash_attention_with_sinks_on_gpu(sinks: Array | None) -> None:\n  \"\"\"Helper function to check for sinks with flash attention on GPU.\"\"\"\n  if sinks is not None:\n    raise ValueError(\"The flash attention with sinks is not supported on GPU yet.\")",
        "analysis": {
            "functionality": "Raises a ValueError if the 'sinks' argument is not None, indicating that flash attention with sinks is not supported on GPU.",
            "usage": "This function is a validation helper. It accepts an optional 'sinks' array. If 'sinks' is not None, it raises a ValueError. It does not return a value."
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#ChunkedCausalMask",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "class ChunkedCausalMask(splash_attention_mask._ComputableMask):  # pylint: disable=protected-access\n  \"\"\"Lazy chunked causal mask.\n\n  Attention is causal within each chunk (0, K), (K, 2K), (2K, 3K), ... tokens attend to each other but not across chunks.\n  Llama4 models use interleaved chunk attention along with global attention.\n\n  This mask class inherits from splash_attention_mask._ComputableMask and is designed to be used with Splash Attention.\n  It allows the mask logic to be computed on-the-fly or fused into the attention kernel, avoiding the memory cost of\n  materializing the full (sequence_length, sequence_length) boolean mask array, which can be prohibitive for long sequences.\n\n  Attributes:\n    chunk_size: The size of each attention chunk.\n  \"\"\"\n\n  chunk_size: int\n\n  def __init__(\n      self,\n      shape: tuple[int, int],\n      chunk_size: int,\n      shard_count: int = 1,\n  ):\n    if chunk_size <= 0:\n      raise ValueError(\"chunk_size must be positive\")\n    self.chunk_size = chunk_size\n\n    # Define the mask function for chunk attention\n    def chunked_causal_mask_function(q_ids, kv_ids):\n      \"\"\"Computes the mask logic for the given slice indices.\"\"\"\n      if q_ids.size == 0 or kv_ids.size == 0:\n        return np.empty((q_ids.shape[0], kv_ids.shape[1]), dtype=np.bool_)\n\n      # Condition 1: Same chunk\n      q_chunk = q_ids // self.chunk_size\n      kv_chunk = kv_ids // self.chunk_size\n      same_chunk = q_chunk == kv_chunk\n\n      # Condition 2: Causal\n      causal = q_ids >= kv_ids\n\n      return same_chunk & causal\n\n    # Initialize the parent ComputableMask with this function\n    super().__init__(\n        shape=shape,\n        mask_function=chunked_causal_mask_function,\n        shard_count=shard_count,\n    )\n\n  # Implement equality and hashing based on relevant attributes\n  def __eq__(self, other: object):\n    if not isinstance(other, type(self)):\n      return NotImplemented\n    # Compare shape, chunk_size, and the underlying q_sequence array\n    return (\n        self.shape == other.shape\n        and self.chunk_size == other.chunk_size\n        and np.array_equal(self.q_sequence, other.q_sequence)\n    )\n\n  def __hash__(self):\n    return hash(\n        (\n            type(self),\n            self.shape,\n            self.chunk_size,\n            self.q_sequence.tobytes() if self.q_sequence is not None else None,\n        )\n    )",
        "analysis": {
            "module_type": "chunked_causal_mask",
            "purpose": "Creates a lazy, on-the-fly computable mask for chunked causal attention, where attention is causal within fixed-size chunks but does not cross chunk boundaries, designed for use with Splash Attention.",
            "input": {
                "shape": "N/A (Parameters are passed to the constructor).",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes the object with shape, chunk_size, and shard_count.",
                "Defines an internal mask function that computes whether a query index and a key-value index are in the same chunk and if the query index is greater than or equal to the key-value index.",
                "Passes this mask function to the parent `_ComputableMask` class constructor."
            ],
            "output": {
                "shape": "An instance of the ChunkedCausalMask class, which can be used by Splash Attention kernels to compute boolean mask slices on-the-fly."
            },
            "dependencies": [
                "splash_attention_mask._ComputableMask",
                "numpy"
            ],
            "parameters": {
                "shape": "A tuple (q_sequence_length, kv_sequence_length) representing the full mask dimensions.",
                "chunk_size": "The size of each attention chunk.",
                "shard_count": "The number of shards for mask computation, passed to the parent class."
            },
            "notes": [
                "This is a 'lazy' mask, avoiding the materialization of the full boolean mask matrix in memory.",
                "The mask logic is designed to be fused into the Splash Attention kernel.",
                "Attention is allowed only if the query and key are in the same chunk AND the causal condition (q_pos >= kv_pos) is met.",
                "Implements custom `__eq__` and `__hash__` methods for comparing instances based on shape, chunk_size, and internal state."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the ChunkedCausalMask object.",
                    "input": {
                        "shape": "N/A, arguments are `shape: tuple[int, int]`, `chunk_size: int`, `shard_count: int`.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Validate that `chunk_size` is a positive integer.",
                        "Store `chunk_size` as an instance attribute.",
                        "Define a nested function `chunked_causal_mask_function` that takes query and key-value indices (q_ids, kv_ids).",
                        "Inside the nested function, calculate if indices belong to the same chunk: `(q_ids // chunk_size) == (kv_ids // chunk_size)`.",
                        "Inside the nested function, calculate the causal condition: `q_ids >= kv_ids`.",
                        "The nested function returns the logical AND of the same-chunk and causal conditions.",
                        "Call the superclass `_ComputableMask.__init__` with the provided shape, the nested mask function, and shard_count."
                    ],
                    "output": {
                        "shape": "N/A (Initializes the object instance)."
                    },
                    "dependencies": [
                        "splash_attention_mask._ComputableMask.__init__",
                        "numpy"
                    ],
                    "notes": [
                        "The core logic is encapsulated in the `chunked_causal_mask_function` which is passed to the parent class to be called by the attention kernel."
                    ]
                },
                "__eq__": {
                    "purpose": "Defines equality comparison for two ChunkedCausalMask objects.",
                    "input": {
                        "shape": "N/A, argument is another object.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if the other object is an instance of ChunkedCausalMask.",
                        "Compare `self.shape` with `other.shape`.",
                        "Compare `self.chunk_size` with `other.chunk_size`.",
                        "Compare `self.q_sequence` with `other.q_sequence` using `np.array_equal`.",
                        "Return True if all comparisons are true, otherwise False."
                    ],
                    "output": {
                        "shape": "A single boolean value."
                    },
                    "dependencies": [
                        "numpy.array_equal"
                    ],
                    "notes": [
                        "Equality depends on the mask's dimensions, chunking strategy, and the internal `q_sequence` array inherited from the parent class."
                    ]
                },
                "__hash__": {
                    "purpose": "Computes a hash for the ChunkedCausalMask object, allowing it to be used in hash-based collections like dictionaries or sets.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Create a tuple containing the object's type, shape, chunk_size, and the byte representation of the `q_sequence` array.",
                        "Compute and return the hash of this tuple."
                    ],
                    "output": {
                        "shape": "A single integer value."
                    },
                    "dependencies": [],
                    "notes": [
                        "The hash is based on the same attributes used for the equality check, ensuring that equal objects have the same hash."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#_generate_chunk_attention_mask",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def _generate_chunk_attention_mask(mask_shape: tuple[int, int], chunk_size: int, q_offset: int = 0) -> jax.Array:\n  \"\"\"Generates an explicit boolean mask for chunked causal attention.\n\n  This function computes the full boolean mask array where True indicates\n  attention is allowed based on chunked causal rules (tokens attend only\n  within the same chunk, and causally within that chunk).\n\n  Args:\n    mask_shape: The desired shape of the mask (q_seq_len, kv_seq_len).\n    chunk_size: The size of the attention chunks.\n\n  Returns:\n    A boolean mask of shape `mask_shape` where True indicates attention is\n    allowed according to chunked causal rules, and False otherwise.\n\n  Raises:\n    ValueError: If chunk_window_size is None or not positive.\n  \"\"\"\n\n  row_ids = jax.lax.broadcasted_iota(jnp.int32, mask_shape, 0) + q_offset\n  col_ids = jax.lax.broadcasted_iota(jnp.int32, mask_shape, 1)\n  if chunk_size <= 0:\n    raise ValueError(\"chunk_size must be positive\")\n\n  # chunk mask calculation\n  same_chunk = (row_ids // chunk_size) == (col_ids // chunk_size)\n  chunk_mask = same_chunk & (row_ids >= col_ids)\n  return chunk_mask",
        "analysis": {
            "functionality": "This function generates a 2D boolean attention mask for chunked causal attention. The mask allows a token to attend only to other tokens that are within the same predefined chunk and are at a preceding or current position.",
            "usage": "Call this function with a desired mask shape and a chunk size to get a JAX array representing the attention mask. For example, `_generate_chunk_attention_mask(mask_shape=(1024, 1024), chunk_size=256)` will produce a 1024x1024 boolean mask where attention is restricted to causal relationships within 256-token chunks."
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#_make_block_mask_indices",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def _make_block_mask_indices(bidirectional_mask):\n  \"\"\"Creates block mask identifying segments based on a bidirectional mask.\n\n  Args:\n    bidirectional_mask: boolean mask, e.g. [011110011010].\n\n  Returns:\n    block mask for segments, e.g. [011110022030].\n  \"\"\"\n  # Left pad 0.\n  padded_mask = jnp.pad(bidirectional_mask, [(0, 0), (1, 0)], constant_values=0)\n  boundary = padded_mask[..., 1:] > padded_mask[..., :-1]\n  numbered_boundary = jnp.cumsum(boundary, axis=-1)\n  return bidirectional_mask * numbered_boundary",
        "analysis": {
            "functionality": "Identifies contiguous segments of 'True' or '1' values in a boolean mask and assigns a unique, incrementing integer ID to each segment.",
            "usage": "Takes a boolean or integer JAX array `bidirectional_mask` (e.g., shape `[batch_size, sequence_length]`) as input. It returns an integer JAX array of the same shape, where each contiguous block of non-zero elements is replaced by a unique positive integer, and zero elements remain zero."
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#_make_bidirectional_block_mask",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def _make_bidirectional_block_mask(bidirectional_mask):\n  \"\"\"Creates bidirectional block mask from bidirectional_mask, where True corresponds to image tokens.\n  bidirectional_mask shape: [B, L]\n  bidirectional_block_mask shape: [B, L, L]\n  Examples:\n  bidirectional_mask = [[0, 1, 1, 1, 0, 0]]\n  bidirectional_block_mask = [[\n      [False, False, False, False, False, False],\n      [False,  True,  True,  True, False, False],\n      [False,  True,  True,  True, False, False],\n      [False,  True,  True,  True, False, False],\n      [False, False, False, False, False, False],\n      [False, False, False, False, False, False],\n  ]]\n  \"\"\"\n  q_block_indices = _make_block_mask_indices(bidirectional_mask)\n  kv_block_indices = q_block_indices\n  bidirectional_block_mask = (kv_block_indices[:, None, :] == q_block_indices[..., None]) & (\n      q_block_indices[..., None] > 0\n  )\n  return bidirectional_block_mask",
        "analysis": {
            "functionality": "This function transforms a 1D boolean mask indicating bidirectional segments into a 2D square attention mask. It identifies contiguous blocks of 'True' values in the input and creates a corresponding square block in the output mask where all positions are 'True', allowing for fully-connected attention within that segment.",
            "usage": "This function is used to generate a component of an attention mask, typically in multimodal models where certain tokens (e.g., from an image) need to attend to each other bidirectionally. Call this function with a JAX array of shape `[batch_size, sequence_length]` where `1`s or `True`s mark the bidirectional segments. The function returns a boolean JAX array of shape `[batch_size, sequence_length, sequence_length]` which can be combined with other attention masks."
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#attention_op_as_linen",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def attention_op_as_linen(\n    *,\n    config: Config,\n    mesh: Mesh,\n    attention_kernel: str,\n    max_target_length: int,\n    num_query_heads: int,\n    num_kv_heads: int,\n    float32_qk_product: bool = False,\n    max_prefill_predict_length: int = -1,\n    float32_logits: bool = False,\n    flash_axis_names_q: AxisNames = (BATCH, HEAD, LENGTH_NO_EXP, D_KV),\n    flash_axis_names_q_ep: AxisNames = (BATCH_NO_EXP, HEAD, LENGTH, D_KV),\n    flash_axis_names_kv: AxisNames = (BATCH, HEAD, KV_LENGTH, D_KV),\n    flash_axis_names_kv_ep: AxisNames = (BATCH_NO_EXP, HEAD, KV_LENGTH, D_KV),\n    flash_axis_names_splash_kernel: AxisNames = (HEAD, LENGTH_NO_EXP),\n    flash_axis_names_splash_kernel_ep: AxisNames = (HEAD, LENGTH),\n    prefill_cache_logical_axis_names: AxisNames = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n    cache_logical_axis_names: AxisNames = (CACHE_BATCH, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n    cache_scale_logical_axis_names: AxisNames = (\n        CACHE_SCALE_BATCH,\n        CACHE_SCALE_SEQUENCE,\n        CACHE_SCALE_HEADS,\n        CACHE_SCALE_KV,\n    ),\n    ragged_qkv_axis_names: AxisNames = (CACHE_BATCH, CACHE_HEADS, CACHE_SEQUENCE, CACHE_KV),\n    ragged_lengths_names: AxisNames = (CACHE_BATCH,),\n    compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n    key_axis_order: AxisIdxes = (2, 0, 1, 3),\n    reshape_q: bool = False,\n    dropout_rate: float = 0.0,\n    dtype: DType = jnp.float32,\n    quant: Optional[Quant] = None,\n    kv_quant: Optional[KVQuant] = None,\n    attention_type: AttentionType = AttentionType.GLOBAL,  # Default to global attention\n    attn_logits_soft_cap: float | None = None,\n    sliding_window_size: int | None = None,\n    chunk_attn_window_size: int | None = None,\n    use_ragged_attention: bool = False,\n    ragged_block_size: int = 256,\n):\n  \"\"\"A factory function to create an AttentionOp as a Linen module.\n\n  This function serves as a bridge to use the NNX-based `AttentionOp` within a\n  Linen model.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      AttentionOp,\n      config=config,\n      mesh=mesh,\n      attention_kernel=attention_kernel,\n      max_target_length=max_target_length,\n      num_query_heads=num_query_heads,\n      num_kv_heads=num_kv_heads,\n      float32_qk_product=float32_qk_product,\n      max_prefill_predict_length=max_prefill_predict_length,\n      float32_logits=float32_logits,\n      flash_axis_names_q=flash_axis_names_q,\n      flash_axis_names_q_ep=flash_axis_names_q_ep,\n      flash_axis_names_kv=flash_axis_names_kv,\n      flash_axis_names_kv_ep=flash_axis_names_kv_ep,\n      flash_axis_names_splash_kernel=flash_axis_names_splash_kernel,\n      flash_axis_names_splash_kernel_ep=flash_axis_names_splash_kernel_ep,\n      prefill_cache_logical_axis_names=prefill_cache_logical_axis_names,\n      cache_logical_axis_names=cache_logical_axis_names,\n      cache_scale_logical_axis_names=cache_scale_logical_axis_names,\n      ragged_qkv_axis_names=ragged_qkv_axis_names,\n      ragged_lengths_names=ragged_lengths_names,\n      compute_axis_order=compute_axis_order,\n      key_axis_order=key_axis_order,\n      reshape_q=reshape_q,\n      dropout_rate=dropout_rate,\n      dtype=dtype,\n      quant=quant,\n      kv_quant=kv_quant,\n      attention_type=attention_type,\n      attn_logits_soft_cap=attn_logits_soft_cap,\n      sliding_window_size=sliding_window_size,\n      chunk_attn_window_size=chunk_attn_window_size,\n      use_ragged_attention=use_ragged_attention,\n      ragged_block_size=ragged_block_size,\n      metadata_fn=variable_to_logically_partitioned,\n  )",
        "analysis": {
            "module_type": "attention_op_factory",
            "purpose": "A factory function that wraps the NNX-based `AttentionOp` class into a Flax Linen module, enabling its use within a Linen model.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Receives numerous configuration parameters for the attention operation.",
                "Calls `nnx_wrappers.to_linen` to convert the NNX `AttentionOp` class into a Linen module.",
                "Passes all received parameters to the `AttentionOp` constructor during the conversion.",
                "Returns the newly created Linen module."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance, not a tensor. Shape is N/A."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "AttentionOp",
                "variable_to_logically_partitioned",
                "max_utils.Config",
                "jax.sharding.Mesh"
            ],
            "parameters": {
                "config": "The main model configuration object.",
                "mesh": "The JAX device mesh used for model parallelism and sharding.",
                "attention_kernel": "Specifies the attention implementation to use (e.g., 'flash', 'dot_product', 'autoselected').",
                "num_query_heads": "The number of attention heads for queries.",
                "num_kv_heads": "The number of attention heads for keys and values, enabling Grouped-Query Attention (GQA).",
                "attention_type": "The type of attention mechanism, such as GLOBAL, LOCAL_SLIDING, or CHUNK.",
                "use_ragged_attention": "A boolean flag to enable ragged attention for autoregressive decoding.",
                "kv_quant": "An optional configuration for Key-Value cache quantization."
            },
            "notes": [
                "This function acts as a compatibility bridge between the Flax NNX and Linen APIs.",
                "It does not perform any computation itself but instead constructs and returns a callable Linen module.",
                "The returned module will have the same functionality and parameters as the underlying `AttentionOp` class."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#AttentionOp",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "class AttentionOp(nnx.Module):\n  \"\"\"Attention operation\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      attention_kernel: str,\n      max_target_length: int,\n      num_query_heads: int,\n      num_kv_heads: int,\n      float32_qk_product: bool = False,\n      max_prefill_predict_length: int = -1,\n      float32_logits: bool = False,\n      flash_axis_names_q: AxisNames = (BATCH, HEAD, LENGTH_NO_EXP, D_KV),\n      flash_axis_names_q_ep: AxisNames = (BATCH_NO_EXP, HEAD, LENGTH, D_KV),\n      flash_axis_names_kv: AxisNames = (BATCH, HEAD, KV_LENGTH, D_KV),\n      flash_axis_names_kv_ep: AxisNames = (BATCH_NO_EXP, HEAD, KV_LENGTH, D_KV),\n      flash_axis_names_splash_kernel: AxisNames = (HEAD, LENGTH_NO_EXP),\n      flash_axis_names_splash_kernel_ep: AxisNames = (HEAD, LENGTH),\n      prefill_cache_logical_axis_names: AxisNames = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n      cache_logical_axis_names: AxisNames = (CACHE_BATCH, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n      cache_scale_logical_axis_names: AxisNames = (\n          CACHE_SCALE_BATCH,\n          CACHE_SCALE_SEQUENCE,\n          CACHE_SCALE_HEADS,\n          CACHE_SCALE_KV,\n      ),\n      ragged_qkv_axis_names: AxisNames = (CACHE_BATCH, CACHE_HEADS, CACHE_SEQUENCE, CACHE_KV),\n      ragged_lengths_names: AxisNames = (CACHE_BATCH,),\n      compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n      key_axis_order: AxisIdxes = (2, 0, 1, 3),\n      reshape_q: bool = False,\n      dropout_rate: float = 0.0,\n      dtype: DType = jnp.float32,\n      quant: Optional[Quant] = None,\n      kv_quant: Optional[KVQuant] = None,\n      attention_type: AttentionType = AttentionType.GLOBAL,  # Default to global attention\n      attn_logits_soft_cap: float | None = None,\n      sliding_window_size: int | None = None,\n      chunk_attn_window_size: int | None = None,\n      use_ragged_attention: bool = False,\n      ragged_block_size: int = 256,\n      rngs: nnx.Rngs | None = None,\n  ):\n    \"\"\"Initializes the AttentionOp module.\n\n    Args:\n      config: The configuration for the model.\n      mesh: The device mesh.\n      attention_kernel: The attention kernel to use.\n      max_target_length: The maximum target length.\n      num_query_heads: The number of query heads.\n      num_kv_heads: The number of key/value heads.\n      float32_qk_product: Whether to compute qk_product in float32.\n      max_prefill_predict_length: The maximum prefill predict length.\n      float32_logits: Whether to compute logits in float32.\n      flash_axis_names_kv: The logical axis names for the KV cache in flash attention.\n      flash_axis_names_q: The logical axis names for the query in flash attention.\n      flash_axis_names_splash_kernel: The logical axis names for the splash attention kernel.\n      prefill_cache_logical_axis_names: The logical axis names for the prefill cache.\n      cache_logical_axis_names: The logical axis names for the cache.\n      cache_scale_logical_axis_names: The logical axis names for the cache scale.\n      ragged_qkv_axis_names: The logical axis names for ragged QKV tensors.\n      ragged_lengths_names: The logical axis names for ragged lengths.\n      compute_axis_order: The order of axes for computation.\n      key_axis_order: The order of axes for the key.\n      ... and other configuration parameters.\n      rngs: The random number generators for initialization, passed by the nnx.to_linen wrapper.\n    \"\"\"\n    self.config = config\n    self.mesh = mesh\n    self.attention_kernel = attention_kernel\n    self.max_target_length = max_target_length\n    self.num_query_heads = num_query_heads\n    self.num_kv_heads = num_kv_heads\n    self.float32_qk_product = float32_qk_product\n    self.max_prefill_predict_length = max_prefill_predict_length\n    self.float32_logits = float32_logits\n    self.flash_axis_names_q = flash_axis_names_q\n    self.flash_axis_names_q_ep = flash_axis_names_q_ep\n    self.flash_axis_names_kv = flash_axis_names_kv\n    self.flash_axis_names_kv_ep = flash_axis_names_kv_ep\n    self.flash_axis_names_splash_kernel = flash_axis_names_splash_kernel\n    self.flash_axis_names_splash_kernel_ep = flash_axis_names_splash_kernel_ep\n    self.prefill_cache_logical_axis_names = prefill_cache_logical_axis_names\n    self.cache_logical_axis_names = cache_logical_axis_names\n    self.cache_scale_logical_axis_names = cache_scale_logical_axis_names\n    self.ragged_qkv_axis_names = ragged_qkv_axis_names\n    self.ragged_lengths_names = ragged_lengths_names\n    self.compute_axis_order = compute_axis_order\n    self.key_axis_order = key_axis_order\n    self.reshape_q = reshape_q\n    self.dropout_rate = dropout_rate\n    self.dtype = dtype\n    self.quant = quant\n    self.kv_quant = kv_quant\n    self.attention_type = attention_type\n    self.attn_logits_soft_cap = attn_logits_soft_cap\n    self.sliding_window_size = sliding_window_size\n    self.chunk_attn_window_size = chunk_attn_window_size\n    self.use_ragged_attention = use_ragged_attention\n    self.ragged_block_size = ragged_block_size\n\n    def maybe_create_nnx(einsum, *args):\n      if isinstance(einsum, nn.Module):\n        return nnx_wrappers.ToNNX(einsum, rngs=rngs).lazy_init(*args)\n      return einsum\n\n    # qk_product\n    if self.kv_quant:\n      # Dummy inputs for lazy initialization\n      b = 1\n      t_prefill = self.max_prefill_predict_length\n      t_ar = 1  # Autoregressive mode has a query length of 1\n      n = self.num_query_heads\n      n_kv = self.num_kv_heads\n      d = self.config.head_dim\n      g = n // n_kv\n      s_prefill = self.max_prefill_predict_length\n      s_ar = self.max_target_length\n\n      # Dummy query/key/value shapes as before...\n      dummy_query_prefill = jnp.zeros((b, t_prefill, n_kv, g, d), dtype=self.dtype)\n      dummy_key_prefill = jnp.zeros((b, s_prefill, n_kv, d), dtype=self.dtype)\n      dummy_query_ar = jnp.zeros((b, t_ar, n_kv, g, d), dtype=self.dtype)\n      dummy_key_ar = jnp.zeros((b, s_ar, n_kv, d), dtype=self.dtype)\n\n      dummy_attn_weights_prefill = jnp.zeros((b, n_kv, g, t_prefill, s_prefill), dtype=jnp.float32)\n      dummy_value_prefill = jnp.zeros((b, s_prefill, n_kv, d), dtype=self.dtype)\n      dummy_attn_weights_ar = jnp.zeros((b, n_kv, g, t_ar, s_ar), dtype=jnp.float32)\n      dummy_value_ar = jnp.zeros((b, s_ar, n_kv, d), dtype=self.dtype)\n\n      # Prefill AqtEinsum instances\n      self.AqtEinsum_0 = maybe_create_nnx(\n          self.kv_quant.einsum_fn_with_rhs_qtensor(), \"btkgd,bskd->bkgts\", dummy_query_prefill, dummy_key_prefill\n      )\n      self.AqtEinsum_1 = maybe_create_nnx(\n          self.kv_quant.einsum_fn_with_rhs_qtensor_and_dequant(),\n          \"bkgts,bskd->btkgd\",\n          dummy_attn_weights_prefill,\n          dummy_value_prefill,\n      )\n      # Autoregressive AqtEinsum instances\n      self.AqtEinsum_2 = maybe_create_nnx(\n          self.kv_quant.einsum_fn_with_rhs_qtensor(), \"btkgd,bskd->bkgts\", dummy_query_ar, dummy_key_ar\n      )\n      self.AqtEinsum_3 = maybe_create_nnx(\n          self.kv_quant.einsum_fn_with_rhs_qtensor_and_dequant(),\n          \"bkgts,bskd->btkgd\",\n          dummy_attn_weights_ar,\n          dummy_value_ar,\n      )\n    else:\n      self.AqtEinsum_0 = jnp.einsum\n      self.AqtEinsum_1 = jnp.einsum\n      self.AqtEinsum_2 = jnp.einsum\n      self.AqtEinsum_3 = jnp.einsum\n\n  def check_attention_inputs(self, query: Array, key: Array | KVTensor, value: Array | KVTensor) -> None:\n    \"\"\"Check attention inputs.\"\"\"\n\n    assert key.ndim == value.ndim, f\"k (dim {key.ndim}), v (dim {value.ndim}) must have same rank.\"\n    assert query.shape[:-3] == key.shape[:-3] == value.shape[:-3], \"q, k, v batch dims must match.\"\n    assert key.shape[-2] == value.shape[-2], \"k, v num_kv_heads must match.\"\n    assert key.shape[-3] == value.shape[-3], \"k, v lengths must match.\"\n    assert query.shape[-1] == key.shape[-1], \"q, k depths must match.\"\n\n  def generate_attention_mask(\n      self,\n      query,\n      key,\n      decoder_segment_ids: Array | None,\n      model_mode: str,\n      previous_chunk: Any = None,\n      bidirectional_mask: Any = None,\n  ) -> Array | None:\n    \"\"\"Generates a combined attention mask for Transformer models.\n\n    This function constructs an attention mask by potentially combining\n    several types of masks based on the input parameters and model\n    configuration. The generated mask dictates which query-key pairs are\n    allowed to attend to each other.\n\n    The masking logic can enforce:\n    1.  **Sequence Separation:** Using `decoder_segment_ids`, attention is\n      confined within distinct sequences in a batch. This is crucial when\n      multiple unrelated sequences are packed together.\n    2.  **Causality:** Preventing attention to future positions. This is\n      standard for autoregressive decoding. For chunked prefill, as\n      described in the SARATHI paper [2], causality is adjusted based\n      on `previous_chunk` information.\n    3.  **Specialized Attention Patterns:** Depending on `self.attention_type`,\n      it can apply:\n      * Local Sliding Window Attention: Restricts attention to a\n          fixed-size window around each query position.\n      * Chunk Attention: Divides sequences into chunks and applies\n          masking at the chunk level.\n    4.  **Bidirectional Attention for Sub-sequences:** If `bidirectional_mask`\n      is provided (e.g., for image tokens in a multimodal model),\n      those parts of the sequence can attend bidirectionally, and this\n      mask is OR-ed with other generated masks.\n\n    The overall approach and specific masking techniques are influenced by\n    efficient attention mechanisms like those found in the Pallas MHA\n    Flash Attention reference [1].\n\n    Args:\n      query: The query tensor, typically of shape\n          `[batch_size, q_sequence_length, num_heads, head_dim]`.\n          Used primarily for deriving sequence length.\n      key: The key tensor, typically of shape\n          `[batch_size, kv_sequence_length, num_heads, head_dim]`.\n          Used primarily for deriving sequence length.\n      decoder_segment_ids: Optional `Array` of shape `[batch_size, q_sequence_length]`.\n          Identifies distinct sequences within the batch. Attention is\n          restricted to elements within the same segment ID. In autoregressive\n          mode, specific values (e.g., `common_types.DECODING_ACTIVE_SEQUENCE_INDICATOR`)\n          can mark the currently active sequence for decoding.\n      model_mode: A string (e.g., `common_types.MODEL_MODE_AUTOREGRESSIVE`,\n          `MODEL_MODE_PREFILL`) indicating the operational\n          mode. This significantly influences mask generation, particularly\n          how causality and segment separation are handled.\n      previous_chunk: Optional. Information about previously processed\n          key/value chunks, often a tensor representing the previous keys/values.\n          Used to correctly offset causal masks in chunked attention or\n          streaming scenarios. Its shape might be\n          `[batch_size, prev_kv_sequence_length, ...]`.\n      bidirectional_mask: Optional `Array` of shape `[batch_size, kv_sequence_length]`.\n          If provided, this boolean mask indicates tokens (e.g., image tokens)\n          that are allowed to attend bidirectionally. The resulting\n          block-wise bidirectional mask is combined with other masks using a\n          logical OR.\n\n    Returns:\n      An `Array` representing the attention mask, broadcastable to the shape\n      `[batch_size, num_heads, q_sequence_length, kv_sequence_length]`.\n      Positions with `0.0` allow attention, while positions with\n      `DEFAULT_MASK_VALUE` (a large negative number) prevent it.\n      Returns `None` if no masking is determined to be necessary based on\n      the inputs and configuration.\n\n    References:\n      [1] JAX Pallas MHA Flash Attention:\n          https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/ops/tpu/flash_attention.py\n      [2] SARATHI: Efficient LLM Inference by Piggybacking Decodes with\n          Chunked Prefills - ArXiv:2308.16369 (https://arxiv.org/abs/2308.16369)\n    \"\"\"\n    mask = None\n    if model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      mask = decoder_segment_ids[:, None, None, None, :] == DECODING_ACTIVE_SEQUENCE_INDICATOR\n    elif decoder_segment_ids is not None:\n      mask = decoder_segment_ids[:, :, None] == decoder_segment_ids[:, None, :]\n      mask = mask[:, None, None, :, :]\n\n    _, q_seq_len, _, _ = query.shape\n    _, kv_seq_len, _, _ = key.shape\n    next_pos = 0\n    if previous_chunk is not None:\n      next_pos = previous_chunk.shape[1]\n      if mask is not None:\n        mask = mask[:, :, :, next_pos : next_pos + q_seq_len, :]\n    elif model_mode == MODEL_MODE_AUTOREGRESSIVE and q_seq_len == 1:\n      # In autoregression, the query position is the last position in the KV sequence.\n      next_pos = kv_seq_len - 1\n\n    causal_mask = None\n    # We enforce causality except for AUTOREGRESSION\n    if model_mode != MODEL_MODE_AUTOREGRESSIVE and self.attention_type != AttentionType.FULL:\n      mask_shape = (q_seq_len, kv_seq_len)\n      # row_ids indicates the position of query\n      # col_ids indicates the position of kv\n      row_ids = jax.lax.broadcasted_iota(jnp.int32, mask_shape, 0)\n      col_ids = jax.lax.broadcasted_iota(jnp.int32, mask_shape, 1)\n      # Attention mask for chunked prefill is generated in the same way\n      # as mentioned in SARATHI - https://arxiv.org/abs/2308.16369\n      causal_mask = (col_ids <= row_ids + next_pos)[None, None, None, :, :]\n\n    output_mask = None\n    if (mask is not None) and (causal_mask is not None):\n      output_mask = jnp.logical_and(mask, causal_mask)\n    elif mask is not None:\n      output_mask = mask\n    elif causal_mask is not None:\n      output_mask = causal_mask\n\n    if self.attention_type == AttentionType.LOCAL_SLIDING and output_mask is not None:\n      if self.sliding_window_size is None:\n        raise ValueError(\"Sliding_window_size must be set if Local Sliding attention type\")\n\n      row_ids_sliding = jax.lax.broadcasted_iota(jnp.int32, (q_seq_len, 1), 0) + next_pos\n      col_ids_sliding = jax.lax.broadcasted_iota(jnp.int32, (1, kv_seq_len), 1)\n      sliding_mask = (col_ids_sliding > (row_ids_sliding - self.sliding_window_size)) & (\n          col_ids_sliding <= row_ids_sliding\n      )\n      output_mask = sliding_mask * output_mask\n    elif self.attention_type == AttentionType.CHUNK and output_mask is not None:\n      mask_shape = (q_seq_len, kv_seq_len)\n      chunk_mask = _generate_chunk_attention_mask(\n          mask_shape=(q_seq_len, kv_seq_len), chunk_size=self.chunk_attn_window_size, q_offset=next_pos\n      )\n      output_mask = chunk_mask * output_mask\n\n    if bidirectional_mask is not None:\n      image_mask = _make_bidirectional_block_mask(bidirectional_mask)\n      output_mask = output_mask | image_mask[:, None, None, ...]\n\n    return jnp.where(output_mask, 0.0, DEFAULT_MASK_VALUE) if output_mask is not None else None\n\n  def apply_attention(\n      self,\n      query: Array,\n      key: Array | KVTensor,\n      value: Array | KVTensor,\n      decoder_segment_ids: Array | None,\n      lengths: Array | None,\n      model_mode: str,\n      use_ragged_attention: bool = False,\n      previous_chunk: Any = None,\n      bidirectional_mask: Any = None,\n      sinks: Array | None = None,\n      *,\n      qk_product_einsum: Callable[..., Array],\n      wv_product_einsum: Callable[..., Array],\n  ):\n    \"\"\"Apply attention\"\"\"\n    self.check_attention_inputs(query, key, value)\n    length = query.shape[-3]\n    target_hardware = self.mesh.devices[(0,) * self.mesh.devices.ndim].platform\n\n    if use_ragged_attention and model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      if lengths is None:\n        lengths = jnp.sum(decoder_segment_ids, axis=-1)\n\n      if target_hardware == \"tpu\":\n        impl = self.tpu_ragged_attention\n      elif target_hardware == \"gpu\":\n        impl = self.gpu_ragged_attention\n      else:\n        raise NotImplementedError(target_hardware)\n      return impl(query, key, value, lengths, self.ragged_block_size)\n\n    elif (\n        self.attention_kernel == \"dot_product\"\n        or (self.attention_kernel == \"autoselected\" and model_mode == MODEL_MODE_AUTOREGRESSIVE)\n        or (self.attention_kernel == \"autoselected\" and length < 128)\n        or (self.attention_kernel == \"paged\")\n    ):\n      return self.apply_attention_dot(\n          query,\n          key,\n          value,\n          decoder_segment_ids,\n          model_mode,\n          previous_chunk,\n          bidirectional_mask=bidirectional_mask,\n          sinks=sinks,\n          qk_product_einsum=qk_product_einsum,\n          wv_product_einsum=wv_product_einsum,\n      )\n    elif self.attention_kernel in (\"flash\", \"autoselected\"):\n      if target_hardware == \"tpu\":\n        if isinstance(key, KVTensor):\n          key = key.dequant()\n        if isinstance(value, KVTensor):\n          value = value.dequant()\n\n        if model_mode == MODEL_MODE_AUTOREGRESSIVE:\n          raise ValueError(\n              \"\"\"Decode not supported with flash attention.\n                              Use `dot_product` instead.\"\"\"\n          )\n        return (\n            self.tpu_flash_attention(query, key, value, decoder_segment_ids, self.attn_logits_soft_cap, sinks),\n            None,\n            None,\n        )\n      else:\n        validate_flash_attention_with_sinks_on_gpu(sinks)\n        if model_mode == MODEL_MODE_AUTOREGRESSIVE:\n          # fallback to dot_product as pallas gpu flash attention doesn't support decode stage\n          return self.apply_attention_dot(\n              query,\n              key,\n              value,\n              decoder_segment_ids,\n              model_mode,\n              bidirectional_mask=bidirectional_mask,\n              qk_product_einsum=qk_product_einsum,\n              wv_product_einsum=wv_product_einsum,\n          )\n        else:\n          head_axis = -2\n          num_query_heads = query.shape[head_axis]\n          num_kv_heads = key.shape[head_axis]\n          if num_query_heads != num_kv_heads:\n            # Handle cases where the number of query heads is different from the number of key/value heads.\n            if num_query_heads % num_kv_heads != 0:\n              raise ValueError(\n                  f\"Number of query heads ({num_query_heads}) must be divisible by number of key/value heads ({num_kv_heads}).\"\n              )\n            # TODO Investigate if the KV copy can be eliminated. It's likely redundant.\n            q_heads_per_kv_head = num_query_heads // num_kv_heads\n\n            key = jnp.repeat(\n                key, q_heads_per_kv_head, axis=head_axis\n            )  # key shape [batch_size, kv_seq_len, num_kv_heads, head_dim]\n            value = jnp.repeat(\n                value, q_heads_per_kv_head, axis=head_axis\n            )  # value shape [batch_size, kv_seq_len, num_kv_heads, head_dim]\n          out = gpu_pallas_attention.mha(query, key, value, decoder_segment_ids, sm_scale=1.0, causal=True)\n          return out, None, None\n    elif self.attention_kernel == \"cudnn_flash_te\":\n      validate_flash_attention_with_sinks_on_gpu(sinks)\n      if isinstance(key, KVTensor):\n        key = key.dequant()\n      if isinstance(value, KVTensor):\n        value = value.dequant()\n      if model_mode == MODEL_MODE_AUTOREGRESSIVE:\n        raise ValueError(\n            \"\"\"Decode not supported with flash attention.\n                           Use `dot_product` instead.\"\"\"\n        )\n      return self.cudnn_flash_attention(query, key, value, decoder_segment_ids, model_mode), None, None\n    elif self.attention_kernel == \"cudnn_flash_jax\":\n      validate_flash_attention_with_sinks_on_gpu(sinks)\n      if isinstance(key, KVTensor):\n        key = key.dequant()\n      if isinstance(value, KVTensor):\n        value = value.dequant()\n      return *self.cudnn_jax_flash_attention(query, key, value, decoder_segment_ids, model_mode), None\n    else:\n      raise ValueError(f\"Unexpected attention kernel {self.attention_kernel=}.\")\n\n  def gpu_ragged_attention(self, q: Array, k: Array | KVTensor, v: Array | KVTensor, lengths: Array, block_size: int):\n    \"\"\"gpu ragged attention\"\"\"\n    batch_size, q_length, q_heads, head_dim = q.shape\n\n    # Reshape q to match gqa's expected shape\n    q_for_gqa = q.squeeze(axis=1)\n\n    # Define logical axis names - clearer and avoids repeated calls.\n    b = nn.logical_to_mesh_axes(self.ragged_lengths_names)\n    bsnd = nn.logical_to_mesh_axes(self.cache_logical_axis_names)\n    bnd = nn.logical_to_mesh_axes((CACHE_BATCH, CACHE_HEADS, CACHE_KV))\n    bn = nn.logical_to_mesh_axes((CACHE_BATCH, CACHE_HEADS))\n\n    @functools.partial(\n        shard_map,\n        mesh=self.mesh,\n        in_specs=(bnd, bsnd, bsnd, b, None),\n        out_specs=(bnd, bn, bn),\n        check_rep=False,\n    )\n    def wrap_ragged_attention(\n        q: Array, k: Array, v: Array, lengths: Array, block_size: int\n    ) -> Tuple[Array, Array, Array]:\n      # Use the original gqa function to get the attention output\n      \"\"\"\n      Wraps the GQA function with appropriate sharding.\n\n      Args:\n          q: Query tensor.\n          k: Key tensor.\n          v: Value tensor.\n          lengths: Sequence lengths.\n          block_size: Block size for attention.\n\n      Returns:\n          A tuple containing the output, max, and sum tensors.\n      \"\"\"\n      # Use the original gqa function to get the attention output\n      local_out, (local_sum, local_max) = gpu_pallas_decode_attention.gqa(\n          q=q,\n          k=k,\n          v=v,\n          kv_seq_len=lengths,\n          block_k=block_size,\n          sm_scale=1.0,\n          return_residuals=True,\n          normalize_output=False,\n      )\n      return local_out, local_max, local_sum\n\n    local_out, local_max, local_sum = wrap_ragged_attention(q_for_gqa, k, v, lengths, block_size)\n\n    # Reshape local_out, local_max and local_sum to match Maxtext requirements\n    local_out = local_out.reshape(batch_size, q_length, q_heads, head_dim)\n    local_max = local_max.reshape(batch_size, q_length, q_heads, 1)\n    local_sum = local_sum.reshape(batch_size, q_length, q_heads, 1)\n    return local_out, local_max, local_sum\n\n  def tpu_ragged_attention(\n      self, query: Array, key: Array | KVTensor, value: Array | KVTensor, lengths: Array, block_size: int\n  ) -> tuple[Array, Array, Array]:\n    \"\"\"Ragged Attention.\"\"\"\n    if isinstance(query, KVTensor):\n      raise TypeError(\"Ragged attention does not currently support quantized tensors.\")\n    b = nn.logical_to_mesh_axes(self.ragged_lengths_names)\n    bsnd = nn.logical_to_mesh_axes(self.cache_logical_axis_names)\n\n    @functools.partial(\n        shard_map,\n        mesh=self.mesh,\n        in_specs=(\n            bsnd,\n            bsnd,\n            bsnd,\n            b,\n            None,\n        ),\n        out_specs=bsnd,\n        check_rep=False,\n    )\n    def wrap_ragged_attention(query, key, value, lengths, block_size):\n      if query.shape[-2] == key.shape[-2]:\n        return ragged_mha(query, key, value, lengths, block_size=block_size)\n      else:\n        return ragged_gqa(query, key, value, lengths, block_size=block_size)\n\n    return wrap_ragged_attention(query, key, value, lengths, block_size)\n\n  def tpu_flash_attention(\n      self,\n      query: Array,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array | None,\n      attn_logits_soft_cap: float | None = None,\n      sinks: Array | None = None,\n  ) -> Array:\n    \"\"\"TPU Flash Attention.\"\"\"\n\n    cp_size = self.config.context_parallel_size\n    load_balanced_context_parallel = self.config.context_parallel_load_balance\n\n    # Transpose to ('batch', 'heads', 'length', 'kv')\n    query = jnp.transpose(query, axes=(0, 2, 1, 3))\n    key = jnp.transpose(key, axes=(0, 2, 1, 3))\n    value = jnp.transpose(value, axes=(0, 2, 1, 3))\n    segment_axis_names_q = None\n    segment_axis_names_kv = None\n    sink_axis_names = nn.logical_to_mesh_axes((HEAD,))\n    if decoder_segment_ids is not None:\n      if self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n        segment_axis_names_q = nn.logical_to_mesh_axes((BATCH_NO_EXP, Q_LENGTH))\n        segment_axis_names_kv = nn.logical_to_mesh_axes((BATCH_NO_EXP, KV_LENGTH))\n      else:\n        segment_axis_names_q = nn.logical_to_mesh_axes((BATCH, Q_LENGTH_NO_EXP))\n        segment_axis_names_kv = nn.logical_to_mesh_axes((BATCH, KV_LENGTH))\n\n    if self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      axis_names_splash_kernel = nn.logical_to_mesh_axes(self.flash_axis_names_splash_kernel_ep)\n      axis_names_q = nn.logical_to_mesh_axes(self.flash_axis_names_q_ep)\n      axis_names_kv = nn.logical_to_mesh_axes(self.flash_axis_names_kv_ep)\n    else:\n      axis_names_splash_kernel = nn.logical_to_mesh_axes(self.flash_axis_names_splash_kernel)\n      axis_names_q = nn.logical_to_mesh_axes(self.flash_axis_names_q)\n      axis_names_kv = nn.logical_to_mesh_axes(self.flash_axis_names_kv)\n\n    global global_block_q, global_block_kv, global_block_kv_compute, global_block_q_dkv, global_block_kv_dkv\n    global global_block_kv_dkv_compute, global_block_q_dq, global_block_kv_dq, global_use_fused_bwd_kernel\n    global global_q_layout, global_k_layout, global_v_layout\n    global_block_q = self.config.sa_block_q\n    global_block_kv = self.config.sa_block_kv\n    global_block_kv_compute = self.config.sa_block_kv_compute\n    global_block_q_dkv = self.config.sa_block_q_dkv\n    global_block_kv_dkv = self.config.sa_block_kv_dkv\n    global_block_kv_dkv_compute = self.config.sa_block_kv_dkv_compute\n    global_block_q_dq = self.config.sa_block_q_dq\n    global_block_kv_dq = self.config.sa_block_kv_dq\n    global_use_fused_bwd_kernel = self.config.sa_use_fused_bwd_kernel\n    global_q_layout = self.config.sa_q_layout\n    global_k_layout = self.config.sa_k_layout\n    global_v_layout = self.config.sa_v_layout\n\n    devices_in_data_fsdp = self.mesh.shape[\"data\"] * self.mesh.shape[\"fsdp\"]\n    assert (query.shape[0] / devices_in_data_fsdp).is_integer(), (\n        \"Batch dimension should be shardable among the devices in data and fsdp\"\n        \" axis\"\n        f\" got {query.shape[0]=}/{devices_in_data_fsdp=}\"\n    )\n\n    # create_splash_attention kernel\n    block_sizes = splash_attention_kernel.BlockSizes(\n        block_q=min(global_block_q, query.shape[2]),\n        block_kv=min(global_block_kv, key.shape[2]),\n        block_kv_compute=min(global_block_kv_compute, key.shape[2]),\n        block_q_dkv=min(global_block_q_dkv, query.shape[2]),\n        block_kv_dkv=min(global_block_kv_dkv, key.shape[2]),\n        block_kv_dkv_compute=min(global_block_kv_dkv_compute, query.shape[2]),\n        block_q_dq=None if global_use_fused_bwd_kernel else min(global_block_q_dq, query.shape[2]),\n        block_kv_dq=None if global_use_fused_bwd_kernel else min(global_block_kv_dq, query.shape[2]),\n        use_fused_bwd_kernel=global_use_fused_bwd_kernel,\n        q_layout=splash_attention_kernel.QKVLayout[global_q_layout],\n        k_layout=splash_attention_kernel.QKVLayout[global_k_layout],\n        v_layout=splash_attention_kernel.QKVLayout[global_v_layout],\n    )\n\n    mask_shape = (query.shape[2], key.shape[2])  # (q_seq_len, kv_seq_len)\n    if self.attention_type == AttentionType.FULL:\n      mask = splash_attention_mask.FullMask(mask_shape)\n    else:\n      mask = splash_attention_mask.CausalMask(shape=mask_shape)\n\n    # Create LoadBalancedCausalMask if cp and load_balancing\n    if cp_size > 1 and load_balanced_context_parallel:\n      mask = LoadBalancedCausalMask(shape=mask_shape, cp_size=cp_size)\n\n    # TODO: figure out local_sliding attention + load_balancing, default is global\n    # Apply local masking if local sliding attention is enabled.\n    if self.attention_type == AttentionType.LOCAL_SLIDING:\n      if self.sliding_window_size is None:\n        raise ValueError(\"Sliding_window_size must be set if Local Sliding attention type\")\n      mask &= splash_attention_mask.LocalMask(\n          shape=(query.shape[2], key.shape[2]),\n          window_size=(self.sliding_window_size, self.sliding_window_size),\n          offset=0,\n      )\n    elif self.attention_type == AttentionType.CHUNK:\n      if self.chunk_attn_window_size is None:\n        raise ValueError(\"chunk_attn_window_size must be set for chunk attention type\")\n\n      mask &= ChunkedCausalMask(shape=(query.shape[2], key.shape[2]), chunk_size=self.chunk_attn_window_size)\n\n    # Create multi-head mask\n    multi_head_mask = splash_attention_mask.MultiHeadMask(masks=(mask,) * query.shape[1])\n\n    # Create the splash attention kernel object separately, jit it for performance\n    @partial(\n        jax.jit,\n        static_argnames=[\n            \"multi_head_mask\",\n            \"shard_head_size\",\n        ],\n    )\n    def wrap_splash_kernel(multi_head_mask, shard_head_size=1):\n      splash_kernel = splash_attention_kernel.make_splash_mha(\n          mask=multi_head_mask,\n          head_shards=shard_head_size,  # the size of the axis if sharding over heads\n          q_seq_shards=cp_size,  # axis for sequence sharding\n          block_sizes=block_sizes,\n          attn_logits_soft_cap=attn_logits_soft_cap,\n          residual_checkpoint_name=\"context\",\n      )\n      return splash_kernel\n\n    logical_axis_rules_head = np.array(\n        [self.mesh.shape[physical_axes] for physical_axes in dict(self.config.logical_axis_rules)[HEAD]]\n    )\n    shard_head_size = np.prod(logical_axis_rules_head)\n    splash_kernel = wrap_splash_kernel(multi_head_mask, int(shard_head_size))\n    named_sharding = jax.sharding.NamedSharding(self.mesh, axis_names_splash_kernel)\n    segment_axis_names_splash_kernel = splash_kernel.manual_sharding_spec(named_sharding)\n\n    # Now call the function wrap_flash_attention which does the actual computation.\n    # The splash kernel is passed as a parameter to the function. Since we have the shard map\n    # decorating the wrap_flash_attention function, the data will be correctly sharded\n    # meaning q will be sharded over sequence aka context length but K and V will be duplicated\n    # The shardings are specified in the in_specs and out_specs of the shard_map decorator:\n    # 'segment_axis_names_q' maps to ['activation_q_length', ['context']] meaning that q is sharded over the context axis\n    #  'segment_axis_names_kv' maps to ['activation_kv_length', []] meaning that K and V are not sharded\n    # splash_kernel is sharded over (HEAD, LENGTH)\n    @functools.partial(\n        shard_map,\n        mesh=self.mesh,\n        in_specs=(\n            axis_names_q,\n            axis_names_kv,\n            axis_names_kv,\n            segment_axis_names_q,\n            segment_axis_names_kv,\n            segment_axis_names_splash_kernel,\n            None,  # no sharding for cp_size\n            None,  # no sharding for load_balanced_context_parallel\n            sink_axis_names,  # sharding align with query heads\n        ),\n        out_specs=axis_names_q,\n        check_rep=False,\n    )\n    def wrap_flash_attention(\n        query,\n        key,\n        value,\n        decoder_segment_ids_q,\n        decoder_segment_ids_kv,\n        splash_kernel,\n        cp_size,\n        load_balanced_context_parallel,\n        sinks,\n    ):\n      # If load_balanced_context_parallel is enabled, reorder the key and value tensors\n      # to ensure that they are contiguous in memory.\n      # This is necessary for the splash attention kernel to work correctly because it expects\n      # the K and V to be contiguous. Note that K and V are not sharded over the sequence aka context axis\n      # This was we get the unsharded unpermuted key and value tensors\n      if cp_size > 1 and load_balanced_context_parallel:\n        key = max_utils.reorder_sequence(tensor=key, cp_size=cp_size, seq_dim=2, to_contiguous=True)\n        value = max_utils.reorder_sequence(tensor=value, cp_size=cp_size, seq_dim=2, to_contiguous=True)\n        decoder_segment_ids_unpermuted = max_utils.reorder_sequence(\n            tensor=decoder_segment_ids_kv, cp_size=cp_size, seq_dim=1, to_contiguous=True\n        )\n\n      if decoder_segment_ids_q is not None:\n        if cp_size > 1 and load_balanced_context_parallel:\n          decoder_segment_ids_tuple = splash_attention_kernel.SegmentIds(\n              decoder_segment_ids_q, decoder_segment_ids_unpermuted\n          )\n        else:\n          # if cp=1, decoder_segment_ids_q is the same as decoder_segment_ids_kv\n          decoder_segment_ids_tuple = splash_attention_kernel.SegmentIds(decoder_segment_ids_q, decoder_segment_ids_kv)\n      else:\n        decoder_segment_ids_tuple = None\n      # TODO(ranran): remove if/else branch once b/441336842 is fixed\n      if version.parse(jax.__version__) < version.parse(\"0.7.2.dev20250824\"):\n        attention_output = jax.vmap(splash_kernel)(query, key, value, decoder_segment_ids_tuple)\n      else:\n        attention_output = jax.vmap(splash_kernel, in_axes=(0, 0, 0, 0, None))(\n            query, key, value, decoder_segment_ids_tuple, sinks\n        )\n      return attention_output\n\n    x = wrap_flash_attention(\n        query,\n        key,\n        value,\n        decoder_segment_ids,\n        decoder_segment_ids,\n        splash_kernel,\n        cp_size,\n        load_balanced_context_parallel,\n        sinks,\n    )\n\n    x = jnp.transpose(x, axes=(0, 2, 1, 3))\n\n    return x\n\n  def cudnn_flash_attention(\n      self,\n      query: Array,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array | None,\n      model_mode: str = MODEL_MODE_TRAIN,\n  ) -> Array:\n    \"\"\"CUDNN Flash Attention with Transformer Engine.\n    1. Stable API, supports GQA, SWA (only with causal masking)\n    2. Head_dim = 256 is also supported from TE-1.12 stable release with CUDNN 12.6\n    \"\"\"\n    # These imports are only meant to work in a GPU build.\n    # pylint: disable=import-outside-toplevel\n    from transformer_engine.jax.flax.transformer import DotProductAttention  # pytype: disable=import-error\n\n    _, _, _, head_dim = query.shape  # pylint: disable=unused-variable\n\n    using_context_parallelism = self.mesh.shape[\"context\"] > 1\n\n    if self.attention_type == AttentionType.LOCAL_SLIDING and using_context_parallelism:\n      raise AssertionError(\"Sliding window attention is not supported when context parallelism is enabled\")\n\n    sliding_window_size = None\n\n    if self.attention_type == AttentionType.LOCAL_SLIDING or not self.config.enable_padding_causal_mask:\n      sliding_window_size = [self.sliding_window_size, 0]\n\n    if self.attention_type == AttentionType.LOCAL_SLIDING or using_context_parallelism:\n      mask_type = \"causal\"  # SWA and Context Parallelism only work with causal masking\n      attn_mask = None\n    else:\n      # generate attn_mask\n      mask_type = \"padding_causal\"  # only padding_causal mask type can take a created mask\n      attn_mask = self.generate_attention_mask(query, key, decoder_segment_ids, model_mode)\n\n    dpa_layer = DotProductAttention(\n        head_dim=head_dim,\n        num_attention_heads=self.num_query_heads,\n        num_gqa_groups=self.num_kv_heads,\n        attn_mask_type=mask_type,  # 'no_mask', 'padding', 'causal', or 'padding_causal'\n        attn_bias_type=\"no_bias\",  # 'no_bias', 'pre_scale_bias' or 'post_scale_bias'\n        attention_dropout=self.dropout_rate,\n        dropout_rng_name=\"aqt\",\n        dtype=self.dtype,\n        float32_logits=self.float32_logits,\n        qkv_layout=\"BSHD_BSHD_BSHD\",  # 'BS3HD', 'BSHD_BS2HD' or 'BSHD_BSHD_BSHD'\n        scale_factor=1.0,\n        transpose_batch_sequence=False,\n        window_size=sliding_window_size,\n        context_parallel_causal_load_balanced=self.config.context_parallel_load_balance,\n        context_parallel_axis=\"context\",\n    )\n    return dpa_layer(query, key, value, mask=attn_mask)\n\n  def cudnn_jax_flash_attention(\n      self,\n      query: Array,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array | None,\n      model_mode: str = MODEL_MODE_TRAIN,\n  ) -> tuple[Array, Array]:\n    \"\"\"CUDNN Flash Attention with JAX SDPA API.\"\"\"\n    # These imports are only meant to work in a GPU build.\n    # pylint: disable=import-outside-toplevel\n    from jax._src.cudnn.fused_attention_stablehlo import (\n        dot_product_attention,\n        MaskType,\n    )\n\n    _, _, _, head_dim = query.shape  # pylint: disable=unused-variable\n\n    if model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      lengths = jnp.sum(decoder_segment_ids, axis=-1)\n\n      output, lse = dot_product_attention(\n          query,\n          key,\n          value,\n          q_seqlen=lengths,\n          kv_seqlen=lengths,\n          mask_type=MaskType.PADDING,\n          scale=1.0,\n          dropout_rate=self.dropout_rate,\n          qkv_layout=\"BTNH\",\n          return_residual=True,\n      )\n    else:\n      output, lse = dot_product_attention(\n          query,\n          key,\n          value,\n          mask_type=MaskType.CAUSAL,\n          scale=1.0 / math.sqrt(head_dim),\n          dropout_rate=self.dropout_rate,\n          qkv_layout=\"BTNH\",\n          return_residual=True,\n      )\n    output = checkpoint_name(output, \"context\")\n    lse = checkpoint_name(lse, \"context\")\n    return output, lse\n\n  def compute_local_attention(\n      self,\n      attn_weights: Array,\n      value: Array | KVTensor,\n      q_seq_len: int,\n      model_mode: str,\n      wv_product_einsum: Callable[..., Array],\n      sinks: Array | None = None,\n  ) -> tuple[Array, Array, Array]:\n    \"\"\"Computes the attention of a local subset of the kv cache.\n    Local attention results will need to be combined with any other local attentions and normalized\n    Based on https://github.com/google-research/google-research/blob/master/scaling_transformer_inference_efficiency/attention.py\n\n    Args:\n        attn_weights (Array): Product of query and key\n        value (Array): Current value\n        aqt_rng (PRNGKey | None): Optional rng\n\n    Returns:\n        (local_out, local_max,): where\n          local_out is local unnormalized output\n          local_max is the local max of exponentials\n          local_sum is the sum of exponentials for this chunk, divided by exp(local_max).\n    \"\"\"\n    b, n_kv, g, t, s = attn_weights.shape\n    n_q = n_kv * g\n    logits = jnp.reshape(attn_weights, (b, n_q, t, s))\n    if sinks is not None:\n      # broadcast sinks to match the attn weights dimension and combine\n      sinks_param = sinks.astype(attn_weights.dtype)  # (n_q,)\n      sinks_logits = sinks_param[jnp.newaxis, :, jnp.newaxis, jnp.newaxis]  # (1, n_q, 1, 1)\n      sinks_logits = jnp.broadcast_to(sinks_logits, (b, n_q, t, 1))\n      logits = jnp.concatenate([logits, sinks_logits], axis=-1)\n\n    # softmax\n    local_max = jnp.max(logits, axis=-1, keepdims=True)\n    local_exps_combined = jnp.exp(logits - local_max)\n    local_sum = jnp.sum(local_exps_combined, axis=-1, keepdims=True)\n\n    # reshape and transpose\n    local_exps = local_exps_combined[..., :s]\n    local_exps = jnp.reshape(local_exps, (b, n_kv, g, t, s))\n    local_max = jnp.transpose(local_max, (0, 2, 1, 3))  # (b, t, n_q, 1)\n    local_sum = jnp.transpose(local_sum, (0, 2, 1, 3))  # (b, t, n_q, 1)\n\n    local_out = self.wv_product(local_exps, value, model_mode, wv_product_einsum)\n    if model_mode == MODEL_MODE_AUTOREGRESSIVE and self.is_partition_in_decode(q_seq_len):\n      local_out = partitioning.with_sharding_constraint(local_out, (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV))\n    elif model_mode == MODEL_MODE_PREFILL:\n      local_out = partitioning.with_sharding_constraint(local_out, (BATCH, KV_LENGTH, HEAD, D_KV))\n\n    if self.reshape_q and q_seq_len == 1:\n      local_max = local_max[:, 0:1, :, :]\n      local_sum = local_sum[:, 0:1, :, :]\n      local_out = local_out[:, 0:1, :, :]\n\n    if model_mode == MODEL_MODE_AUTOREGRESSIVE and self.is_partition_in_decode(q_seq_len):\n      local_max = partitioning.with_sharding_constraint(local_max, (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV))\n      local_sum = partitioning.with_sharding_constraint(local_sum, (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV))\n      local_out = partitioning.with_sharding_constraint(local_out, (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV))\n\n    return local_out, local_max, local_sum\n\n  def is_partition_in_decode(self, seq_len):\n    return self.config.ici_context_autoregressive_parallelism > 0 and seq_len == 1\n\n  def apply_attention_dot(\n      self,\n      query: Array,\n      key: Array | KVTensor,\n      value: Array | KVTensor,\n      decoder_segment_ids: Array | None,\n      model_mode: str = MODEL_MODE_TRAIN,\n      previous_chunk: Any = None,\n      bidirectional_mask: Any = None,\n      sinks: Array | None = None,\n      *,\n      qk_product_einsum: Callable[..., Array],\n      wv_product_einsum: Callable[..., Array],\n  ):\n    \"\"\"Apply Attention.\"\"\"\n    validate_compute_axis_order(self.compute_axis_order)\n    # Casting qk_product and softmaxt computation for float32 for model stability.\n    if self.float32_qk_product:\n      if isinstance(key, KVTensor):\n        key = key.dequant()\n      query = query.astype(jnp.float32)\n      key = key.astype(jnp.float32)\n\n    # special sharding for decode\n    q_seq_len = query.shape[1]\n    prefill_qkv_sharding = (BATCH, PREFILL_LENGTH, HEAD, D_KV)\n    decode_qkv_sharding = (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV)\n    if self.is_partition_in_decode(q_seq_len):\n      query = partitioning.with_sharding_constraint(query, decode_qkv_sharding)\n      # avoid sharding scale tensor when using kv cache quantization\n      if self.kv_quant and isinstance(key, KVTensor) and isinstance(value, KVTensor):\n        key.qvalue = partitioning.with_sharding_constraint(key.qvalue, decode_qkv_sharding)\n        value.qvalue = partitioning.with_sharding_constraint(value.qvalue, decode_qkv_sharding)\n      else:\n        key = partitioning.with_sharding_constraint(key, decode_qkv_sharding)\n        value = partitioning.with_sharding_constraint(value, decode_qkv_sharding)\n    elif model_mode == MODEL_MODE_PREFILL:\n      query = partitioning.with_sharding_constraint(query, prefill_qkv_sharding)\n      # avoid sharding scale tensor when using kv cache quantization\n      if self.kv_quant and isinstance(key, KVTensor) and isinstance(value, KVTensor):\n        key.qvalue = partitioning.with_sharding_constraint(key.qvalue, prefill_qkv_sharding)\n        value.qvalue = partitioning.with_sharding_constraint(value.qvalue, prefill_qkv_sharding)\n      else:\n        key = partitioning.with_sharding_constraint(key, prefill_qkv_sharding)\n        value = partitioning.with_sharding_constraint(value, prefill_qkv_sharding)\n\n    attn_weights = self.qk_product(query, key, q_seq_len, model_mode, qk_product_einsum)\n    if self.is_partition_in_decode(q_seq_len):\n      attn_weights = partitioning.with_sharding_constraint(attn_weights, (KV_LENGTH, HEAD, None, None, None))\n    elif model_mode == MODEL_MODE_PREFILL:\n      attn_weights = partitioning.with_sharding_constraint(attn_weights, (BATCH, HEAD, None, PREFILL_LENGTH, KV_LENGTH))\n\n    if self.attn_logits_soft_cap:\n      attn_weights = jnp.tanh(attn_weights / self.attn_logits_soft_cap)\n      attn_weights = attn_weights * self.attn_logits_soft_cap\n\n    # Casting softmaxt computation for float32 for model stability.\n    if self.float32_logits:\n      attn_weights = attn_weights.astype(jnp.float32)\n    attn_mask = self.generate_attention_mask(\n        query, key, decoder_segment_ids, model_mode, previous_chunk, bidirectional_mask\n    )\n    if self.is_partition_in_decode(q_seq_len):\n      attn_mask = partitioning.with_sharding_constraint(attn_mask, (KV_LENGTH, HEAD, None, None, None))\n    elif model_mode == MODEL_MODE_PREFILL:\n      attn_mask = partitioning.with_sharding_constraint(attn_mask, (BATCH, HEAD, None, PREFILL_LENGTH, KV_LENGTH))\n    if attn_mask is not None:\n      attn_weights = apply_mask_to_logits(attn_weights, attn_mask)\n    return self.compute_local_attention(attn_weights, value, q_seq_len, model_mode, wv_product_einsum, sinks)\n\n  def qk_product(\n      self, query: Array, key: Array | KVTensor, q_seq_len: int, model_mode: str, einsum: Callable[..., Array]\n  ) -> Array:\n    \"\"\"Query-Key product.\n\n    Args:\n      query: Query projection, in shape of [b, t, n, d]\n      key: Key projection in shape of [b, s, n_kv, d]\n\n    Returns:\n      results in shape [b, n_kv, n // n_kv, t, s].\n\n    Annotations:\n      b: batch size\n      t: query length\n      s: key / value length\n      d: head / kv dimension\n      n: number of query heads\n      n_kv: number of kv heads, sometimes annotated as k\n      n // n_kv: number of group for query, sometimes annotated with g\n    \"\"\"\n    b, t, n, d = query.shape\n    n_kv = key.shape[-2]\n    assert n_kv == self.num_kv_heads\n    precision_kwargs = {\"precision\": self.config.matmul_precision} if einsum is jnp.einsum else {}\n    if model_mode == MODEL_MODE_TRAIN or self.compute_axis_order == (0, 1, 2, 3):\n      query = jnp.reshape(query, (b, t, n_kv, n // n_kv, d))\n      if self.reshape_q and q_seq_len == 1:\n        query = jnp.broadcast_to(query, (b, 2, n_kv, n // n_kv, d))\n      result = einsum(\"btkgd,bskd->bkgts\", query, key, **precision_kwargs)\n    elif self.compute_axis_order == (0, 2, 1, 3):\n      query = jnp.transpose(query, axes=self.compute_axis_order)\n      key = jax.tree.map(lambda x: jnp.transpose(x, axes=self.compute_axis_order), key)\n      query = jnp.reshape(query, (b, n_kv, n // n_kv, t, d))\n      if self.reshape_q and q_seq_len == 1:\n        query = jnp.broadcast_to(query, (b, n_kv, n // n_kv, 2, d))\n      result = einsum(\"bkgtd,bksd->bkgts\", query, key, **precision_kwargs)\n    else:\n      raise NotImplementedError(self.compute_axis_order)\n    return result\n\n  def wv_product(\n      self, attn_weights: Array, value: Array | KVTensor, model_mode: str, einsum: Callable[..., Array]\n  ) -> Array:\n    \"\"\"weighted value product.\n\n    Args:\n      attn_weights: Computed results of qk_einsum, in shape [b, n_kv, n // n_kv, t, s]\n      value: Value projection, in shape of [b, s, n_kv, d]\n\n    Returns:\n      result in shape [b, t, n, d]\n\n    Annotations:\n      b: batch size\n      t: query length\n      s: key / value length\n      d: head / kv dimension\n      n: number of query heads\n      n_kv: number of kv heads, sometimes annotated as k\n      n // n_kv: number of group for query, sometimes annotated with g\n    \"\"\"\n\n    precision_kwargs = {\"precision\": self.config.matmul_precision} if einsum is jnp.einsum else {}\n    if self.kv_quant:\n      # manually cast to bf16 to avoid the fp32 XLA ops for speedup\n      if isinstance(value, KVTensor) and self.kv_quant.dtype == jnp.float8_e4m3fn:\n        value.qvalue = value.qvalue.astype(jnp.bfloat16)\n    if model_mode == MODEL_MODE_TRAIN or self.compute_axis_order == (0, 1, 2, 3):\n      out = einsum(\"bkgts,bskd->btkgd\", attn_weights, value, **precision_kwargs)\n      b, t, n_kv, g, d = out.shape\n      result = jnp.reshape(out, (b, t, n_kv * g, d))\n    elif self.compute_axis_order == (0, 2, 1, 3):\n      value = jax.tree.map(lambda x: jnp.transpose(x, axes=self.compute_axis_order), value)\n      out = einsum(\"bkgts,bksd->bkgtd\", attn_weights, value, **precision_kwargs)\n      b, n_kv, g, t, d = out.shape\n      result = jnp.reshape(out, (b, n_kv * g, t, d))\n      result = self.reverse_transepose(result, self.compute_axis_order)\n    return result\n\n  def reverse_transepose(self, transposed_array, transpose_axis_order):\n    return jax.numpy.moveaxis(transposed_array, (0, 1, 2, 3), transpose_axis_order)\n\n  def normalize_cudnn_attention(self, local_outs, local_stats):\n    \"\"\"Normalize across two cuDNN attentions\n\n    Args:\n        local_outs (list): List of outputs entries for each cudnn attention\n          in shape [b, t, n, d].\n        local_stats (list): List of logsumexp entries for each cudnn attention\n          in shape [b, n, t].\n\n    Returns:\n        Array: Combined attention that has been normalized in shape [b, t, n, d].\n    \"\"\"\n    # reshape stat to have shape [b, n, t, 1]\n    stat0 = local_stats[0].reshape((*local_stats[0].shape, 1))\n    stat1 = local_stats[1].reshape((*local_stats[1].shape, 1))\n    global_stat = jnp.log(jnp.exp(stat0) + jnp.exp(stat1))\n    # # transpose stat to have shape [b, t, n, 1] for elemenwise multiplication\n    attn_out = local_outs[0].astype(jnp.float32) * jnp.exp(stat0 - global_stat).transpose((0, 2, 1, 3)) + local_outs[\n        1\n    ].astype(jnp.float32) * jnp.exp(stat1 - global_stat).transpose((0, 2, 1, 3))\n    return attn_out.astype(local_stats[0].dtype)\n\n  def normalize_attention(self, local_outs, local_maxes, local_sums):\n    \"\"\"Normalize across multiple localized attentions\n\n    Args:\n        local_outs (list): List of unnormalized outputs entries for each local attention\n        local_maxes (list): List of max exponentials entries for each local attention\n        local_sums (list): List of exponential sum entries for each local attention\n\n    Returns:\n        Array: Combined attention that has been normalized\n    \"\"\"\n    # Based on https://github.com/google-research/google-research/blob/master/scaling_transformer_inference_efficiency/attention.py\n    global_max = functools.reduce(jnp.maximum, local_maxes)\n    global_sum = sum(\n        (jnp.exp(local_max - global_max) * local_sum for (local_sum, local_max) in zip(local_sums, local_maxes))\n    )\n\n    attn_out = 0\n    for local_max, local_out in zip(local_maxes, local_outs):\n      local_normalizer = jnp.exp(local_max - global_max) / global_sum\n      attn_out += local_normalizer * local_out\n    return attn_out\n\n  def __call__(\n      self,\n      query,\n      key,\n      value,\n      decoder_segment_ids,\n      model_mode,\n      cached_values=None,\n      previous_chunk=None,\n      bidirectional_mask=None,\n      sinks=None,\n      slot: Optional[int] = None,\n      page_state: Optional[page_manager.PageState] = None,\n  ):\n    if cached_values is None:\n      prefill_kv_cache, ar_kv_cache = None, None\n    else:\n      prefill_kv_cache, ar_kv_cache = cached_values[0], cached_values[1]\n    if model_mode != MODEL_MODE_TRAIN:\n      assert prefill_kv_cache\n      key, value, decoder_segment_ids = prefill_kv_cache\n\n    prefill_unnormalized_output, prefill_exponentials_max, prefill_exponentials_sum = self.apply_attention(\n        query=query,\n        key=key,\n        value=value,\n        decoder_segment_ids=decoder_segment_ids,\n        lengths=None,\n        model_mode=model_mode,\n        use_ragged_attention=self.use_ragged_attention,\n        previous_chunk=previous_chunk,\n        bidirectional_mask=bidirectional_mask,\n        sinks=sinks,\n        qk_product_einsum=self.AqtEinsum_0,\n        wv_product_einsum=self.AqtEinsum_1,\n    )\n\n    # Return the \"prefill\" cache if it actually the combined prefill+ar kv cache\n    if ar_kv_cache is None:\n      if prefill_exponentials_sum is not None:\n        return prefill_unnormalized_output / prefill_exponentials_sum\n      return prefill_unnormalized_output\n\n    key, value, decoder_segment_ids, lengths = ar_kv_cache\n    ar_unnormalized_output, ar_exponentials_max, ar_exponentials_sum = self.apply_attention(\n        query=query,\n        key=key,\n        value=value,\n        decoder_segment_ids=decoder_segment_ids,\n        lengths=lengths,\n        model_mode=model_mode,\n        use_ragged_attention=self.use_ragged_attention,\n        bidirectional_mask=bidirectional_mask,\n        qk_product_einsum=self.AqtEinsum_2,\n        wv_product_einsum=self.AqtEinsum_3,\n    )\n\n    if ar_unnormalized_output is not None:\n      unnormalized_outputs = [prefill_unnormalized_output, ar_unnormalized_output]\n      exponentials_maxes = [prefill_exponentials_max, ar_exponentials_max]\n      exponentials_sums = [prefill_exponentials_sum, ar_exponentials_sum]\n      if prefill_exponentials_max is not None and prefill_exponentials_sum is None:\n        prefill_stat = prefill_exponentials_max\n        ar_stat = ar_exponentials_max\n        stats = [prefill_stat, ar_stat]\n        return self.normalize_cudnn_attention(unnormalized_outputs, stats)\n      else:\n        return self.normalize_attention(unnormalized_outputs, exponentials_maxes, exponentials_sums)\n    else:\n      return prefill_unnormalized_output / prefill_exponentials_sum",
        "analysis": {
            "module_type": "attention_operation",
            "purpose": "Encapsulates various attention mechanisms (e.g., dot-product, flash, ragged) for Transformer models, supporting different hardware (TPU, GPU) and operational modes (prefill, autoregressive).",
            "input": {
                "shape": "The primary input is via the __call__ method, which takes `query` ([batch, q_len, num_q_heads, head_dim]), `key`/`value` ([batch, kv_len, num_kv_heads, head_dim] or KVTensor), `decoder_segment_ids` ([batch, seq_len]), and other optional arguments.",
                "dtype": "Configurable, defaults to jnp.float32."
            },
            "processing_steps": [
                "In the `__call__` method, unpacks cached key-value pairs if provided.",
                "Calls `self.apply_attention` for the prefill part of the cache.",
                "If an autoregressive cache is also provided, calls `self.apply_attention` again for the autoregressive part.",
                "Combines and normalizes the results from the prefill and autoregressive attention calls using `self.normalize_attention` or `self.normalize_cudnn_attention`."
            ],
            "output": {
                "shape": "The final attention output tensor with shape [batch, q_len, num_q_heads, head_dim]."
            },
            "dependencies": [
                "flax.nnx.Module",
                "jax.sharding.Mesh",
                "MaxText.common_types.Config",
                "MaxText.inference.kvcache.KVTensor",
                "MaxText.inference.kvcache.KVQuant",
                "jax.experimental.pallas.ops.gpu.attention",
                "jax.experimental.pallas.ops.tpu.splash_attention.splash_attention_kernel",
                "MaxText.kernels.ragged_attention.ragged_gqa",
                "MaxText.kernels.ragged_attention.ragged_mha",
                "transformer_engine.jax.flax.transformer.DotProductAttention"
            ],
            "parameters": {
                "attention_kernel": "Specifies the attention implementation to use (e.g., 'dot_product', 'flash', 'autoselected').",
                "num_query_heads": "The number of attention heads for queries.",
                "num_kv_heads": "The number of attention heads for keys and values (for Grouped-Query Attention).",
                "attention_type": "The type of attention pattern to apply (e.g., GLOBAL, LOCAL_SLIDING, CHUNK).",
                "use_ragged_attention": "A boolean indicating whether to use ragged attention for autoregressive decoding.",
                "kv_quant": "Configuration for key-value cache quantization."
            },
            "notes": [
                "The class is highly configurable to support different hardware backends (TPU, GPU) and attention algorithms.",
                "It handles different operational modes like training, prefill, and autoregressive decoding.",
                "It can combine results from a prefill cache and an autoregressive cache for efficient inference.",
                "Lazy initialization of AQT einsum operations is performed in the constructor if KV quantization is enabled."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the AttentionOp module, setting up configuration and lazily initializing einsum operations for KV quantization.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store all configuration parameters as instance attributes.",
                        "Define a helper function `maybe_create_nnx` for lazy initialization.",
                        "If `kv_quant` is enabled, create dummy tensors with prefill and autoregressive shapes.",
                        "Use `maybe_create_nnx` to lazily initialize four `AqtEinsum` instances for prefill and autoregressive qk/wv products.",
                        "If `kv_quant` is disabled, set the einsum attributes to `jnp.einsum`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx_wrappers.ToNNX",
                        "jnp.einsum"
                    ],
                    "notes": [
                        "Performs lazy initialization of quantized einsum operations using dummy tensors to infer shapes."
                    ]
                },
                "__call__": {
                    "purpose": "The main entry point for executing the attention operation, potentially combining results from prefill and autoregressive caches.",
                    "input": {
                        "shape": "query: [batch, q_len, num_q_heads, head_dim], key/value: [batch, kv_len, num_kv_heads, head_dim], cached_values: Tuple[prefill_cache, ar_cache]",
                        "dtype": "Matches the module's `dtype`."
                    },
                    "processing_steps": [
                        "Unpack `cached_values` into `prefill_kv_cache` and `ar_kv_cache`.",
                        "If not in training mode, update `key`, `value`, `decoder_segment_ids` from `prefill_kv_cache`.",
                        "Call `self.apply_attention` for the prefill portion.",
                        "If `ar_kv_cache` is `None`, normalize and return the prefill output.",
                        "If `ar_kv_cache` exists, unpack it and call `self.apply_attention` for the autoregressive portion.",
                        "Combine and normalize the outputs from both calls using `self.normalize_attention` or `self.normalize_cudnn_attention`."
                    ],
                    "output": {
                        "shape": "[batch, q_len, num_q_heads, head_dim]"
                    },
                    "dependencies": [
                        "self.apply_attention",
                        "self.normalize_attention",
                        "self.normalize_cudnn_attention"
                    ],
                    "notes": [
                        "Handles the logic for combined prefill and autoregressive decoding by processing two separate KV caches and merging the results."
                    ]
                },
                "apply_attention": {
                    "purpose": "A dispatcher that selects and executes the appropriate attention implementation based on configuration and hardware.",
                    "input": {
                        "shape": "query: [batch, q_len, num_q_heads, head_dim], key/value: [batch, kv_len, num_kv_heads, head_dim]",
                        "dtype": "Matches the module's `dtype`."
                    },
                    "processing_steps": [
                        "Check input tensor shapes using `self.check_attention_inputs`.",
                        "Determine the target hardware (TPU/GPU).",
                        "If `use_ragged_attention` is true in autoregressive mode, select `tpu_ragged_attention` or `gpu_ragged_attention`.",
                        "If `attention_kernel` is 'dot_product' or 'autoselected' for decode/short sequences, call `self.apply_attention_dot`.",
                        "If `attention_kernel` is 'flash' or 'autoselected' for prefill, select TPU or GPU flash attention implementation.",
                        "If `attention_kernel` is 'cudnn_flash_te', call `self.cudnn_flash_attention`.",
                        "If `attention_kernel` is 'cudnn_flash_jax', call `self.cudnn_jax_flash_attention`."
                    ],
                    "output": {
                        "shape": "A tuple containing the attention output ([batch, q_len, num_q_heads, head_dim]) and potentially `None` or normalization statistics."
                    },
                    "dependencies": [
                        "self.check_attention_inputs",
                        "self.tpu_ragged_attention",
                        "self.gpu_ragged_attention",
                        "self.apply_attention_dot",
                        "self.tpu_flash_attention",
                        "gpu_pallas_attention.mha",
                        "self.cudnn_flash_attention",
                        "self.cudnn_jax_flash_attention"
                    ],
                    "notes": [
                        "Acts as a control flow hub to route the attention computation to the correct specialized function based on kernel type, hardware, and model mode."
                    ]
                },
                "generate_attention_mask": {
                    "purpose": "Constructs a combined attention mask based on segment IDs, causality, attention type (sliding window, chunked), and bidirectional requirements.",
                    "input": {
                        "shape": "query: [batch, q_len, ...], key: [batch, kv_len, ...], decoder_segment_ids: [batch, q_len]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Create a base mask from `decoder_segment_ids` for sequence separation.",
                        "Calculate `next_pos` offset for chunked prefill or autoregression.",
                        "If not in autoregressive mode, create a causal mask.",
                        "Combine the segment mask and causal mask.",
                        "If `attention_type` is `LOCAL_SLIDING`, apply a sliding window mask.",
                        "If `attention_type` is `CHUNK`, apply a chunked attention mask.",
                        "If `bidirectional_mask` is provided, combine it with the existing mask.",
                        "Convert the final boolean mask to a float mask using `DEFAULT_MASK_VALUE`."
                    ],
                    "output": {
                        "shape": "Broadcastable to [batch_size, num_heads, q_sequence_length, kv_sequence_length]."
                    },
                    "dependencies": [
                        "_generate_chunk_attention_mask",
                        "_make_bidirectional_block_mask",
                        "jnp.where"
                    ],
                    "notes": [
                        "Implements complex logic to combine multiple masking strategies (causal, segment, sliding window, chunked, bidirectional) into a single mask tensor."
                    ]
                },
                "apply_attention_dot": {
                    "purpose": "Implements the standard dot-product attention mechanism.",
                    "input": {
                        "shape": "query: [batch, q_len, ...], key/value: [batch, kv_len, ...]",
                        "dtype": "Matches module `dtype`."
                    },
                    "processing_steps": [
                        "Optionally cast query and key to `float32`.",
                        "Apply sharding constraints based on model mode (prefill/decode).",
                        "Compute attention weights using `self.qk_product`.",
                        "Optionally apply `attn_logits_soft_cap`.",
                        "Generate the attention mask using `self.generate_attention_mask`.",
                        "Apply the mask to the attention weights.",
                        "Compute the final output and normalization stats using `self.compute_local_attention`."
                    ],
                    "output": {
                        "shape": "A tuple `(local_out, local_max, local_sum)`, where `local_out` has shape [batch, q_len, num_q_heads, head_dim]."
                    },
                    "dependencies": [
                        "self.qk_product",
                        "self.generate_attention_mask",
                        "apply_mask_to_logits",
                        "self.compute_local_attention"
                    ],
                    "notes": [
                        "This is the fallback or standard attention implementation, used when specialized kernels are not applicable."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#LoadBalancedCausalMask",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "class LoadBalancedCausalMask(splash_attention_mask._ComputableMask):\n  \"\"\"Lazy causal mask, prevents the model from attending to future tokens.\n  Attributes:\n    offset: Offset of q start wrt kv. A positive offset shifts the bottom\n      triangle upward, a negative one shifts it downward. A negative offset\n      makes the first 'offset' rows of the attention matrix all 0s which leads\n      to undefined softmax.\n  \"\"\"\n\n  offset: int\n  shape: tuple[int, int]\n  cp_size: int\n\n  def __init__(self, shape: tuple[int, int], offset: int = 0, shard_count: int = 1, cp_size: int = 4):\n    self.offset = offset\n\n    def causal_mask_function(q_ids, kv_ids):\n      if self.offset == 0:\n        return q_ids >= kv_ids\n      else:\n        return q_ids + self.offset >= kv_ids\n\n    arr = np.arange(shape[0])\n    # we reorder the mask to be load balanced following the same approach as\n    # used to reorder the input tokens\n    out = max_utils.reorder_mask_load_balancing(arr[None, :, None, None], cp_size, 1)\n    q_sequence = out[0, :, 0, 0]\n\n    mask_function = causal_mask_function\n\n    super().__init__(\n        shape=shape,\n        mask_function=mask_function,\n        shard_count=shard_count,\n    )\n    self.q_sequence = q_sequence\n\n  def __eq__(self, other: object):\n    if not isinstance(other, type(self)):\n      return NotImplemented\n\n    return self.shape == other.shape and self.offset == other.offset and np.array_equal(self.q_sequence, other.q_sequence)\n\n  def __hash__(self):\n    return hash(\n        (\n            type(self),\n            self.shape,\n            self.offset,\n            self.q_sequence.tobytes() if self.q_sequence is not None else None,\n        )\n    )",
        "analysis": {
            "functionality": "This class creates a lazy, computable causal attention mask specifically designed for load-balanced context parallelism in Splash Attention. Instead of materializing a full boolean mask, it provides a function that computes mask values on-the-fly. It reorders the query indices to match the load-balancing permutation applied to the input tokens, ensuring correct causal masking in a distributed context.",
            "usage": "Instantiate the class with the desired mask shape and context parallelism size. The resulting object is then passed to a Splash Attention kernel, which uses it to determine which query-key pairs are allowed to attend to each other during the attention computation. It is used internally by the `tpu_flash_attention` method when `config.context_parallel_load_balance` is enabled."
        }
    },
    {
        "block_name": "src/MaxText/layers/attentions.py#L2Norm",
        "file_path": "src/MaxText/layers/attentions.py",
        "code_block": "class L2Norm(nnx.Module):\n  \"\"\"\n  Implementation of L2Norm in JAX.\n\n  Args:\n    eps: float, epsilon used for numerical stability (default value should be ok for most cases).\n  \"\"\"\n\n  eps: float = 1e-6\n  rngs: nnx.Rngs = None  # Not used in L2Norm but passed in by nnx.bridge.to_linen\n\n  def __call__(self, x):\n    return x * jax.lax.rsqrt(jnp.mean(x**2, axis=-1, keepdims=True) + self.eps)",
        "analysis": {
            "module_type": "l2_normalization",
            "purpose": "A JAX module that performs L2 normalization on an input tensor along its last dimension.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "The module is designed to be called with an input tensor, which triggers the `__call__` method."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.nnx",
                "jax",
                "jax.numpy"
            ],
            "parameters": {
                "eps": "A small float value added to the denominator for numerical stability during the reciprocal square root calculation."
            },
            "notes": [
                "The `rngs` attribute is included for compatibility with `nnx.bridge.to_linen` but is not used by the L2Norm logic itself."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Applies L2 normalization to the input tensor `x`.",
                    "input": {
                        "shape": "[..., feature_dim]",
                        "dtype": "float"
                    },
                    "processing_steps": [
                        "Square the input tensor `x` element-wise.",
                        "Compute the mean of the squared values along the last axis, keeping the dimension.",
                        "Add the epsilon value `self.eps` to the mean for numerical stability.",
                        "Calculate the reciprocal square root of the result using `jax.lax.rsqrt`.",
                        "Multiply the original input tensor `x` by the reciprocal square root to normalize it."
                    ],
                    "output": {
                        "shape": "The same shape as the input tensor `x`."
                    },
                    "dependencies": [
                        "jax.lax.rsqrt",
                        "jax.numpy.mean"
                    ],
                    "notes": [
                        "This method effectively scales the input tensor `x` such that the L2 norm of the vector along the last dimension is close to 1."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/attentions.py#l2_norm_as_linen",
        "file_path": "src/MaxText/layers/attentions.py",
        "code_block": "def l2_norm_as_linen(self, eps: float = 1e-6):\n  \"\"\"\n  Initializes the L2Norm module and returns it as a Linen module.\n\n  Args:\n    eps: float, epsilon used for numerical stability (default value should be ok for most cases).\n  \"\"\"\n  return nnx_wrappers.to_linen(L2Norm, eps=eps, metadata_fn=variable_to_logically_partitioned)",
        "analysis": {
            "module_type": "l2_norm_linen_wrapper",
            "purpose": "A factory function that initializes the NNX L2Norm module and wraps it to be used as a Flax Linen module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `L2Norm` NNX module into a Linen-compatible module.",
                "Passes the `eps` argument to the `L2Norm` constructor and `variable_to_logically_partitioned` as the `metadata_fn` during the conversion."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance, not a tensor. Shape is N/A."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "L2Norm",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "eps": "A small float value added to the denominator for numerical stability during the L2 normalization calculation."
            },
            "notes": [
                "This function serves as a bridge to allow an NNX-defined module (`L2Norm`) to be seamlessly integrated into a larger model built with the Flax Linen API."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attentions.py#attention_as_linen",
        "file_path": "src/MaxText/layers/attentions.py",
        "code_block": "def attention_as_linen(\n    *,\n    config: Config,\n    num_query_heads: int,\n    num_kv_heads: int,\n    head_dim: int,\n    max_target_length: int,\n    mesh: Mesh,\n    attention_kernel: str,\n    inputs_q_shape: Tuple,\n    inputs_kv_shape: Tuple,\n    dtype: DType = jnp.float32,\n    weight_dtype: DType = jnp.float32,\n    max_prefill_predict_length: int = -1,\n    dropout_rate: float = 0.0,\n    kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"normal\"),\n    float32_qk_product: bool = False,  # computes logits in float32 for stability.\n    float32_logits: bool = False,  # cast logits in float32 for stability.\n    quant: Optional[Quant] = None,\n    kv_quant: Optional[KVQuant] = None,\n    attention_type: AttentionType = AttentionType.GLOBAL,  # Default to global attention\n    attn_logits_soft_cap: float | None = None,\n    sliding_window_size: int | None = None,\n    use_ragged_attention: bool = False,\n    ragged_block_size: int = 256,\n    use_qk_norm: bool = False,\n    query_pre_attn_scalar: float | None = None,\n    use_bias_in_projections: bool = False,  # Set to True will enable bias in q, k, v, o projections\n    # Temperature tuning parameters used for Llama4\n    temperature_tuning: bool = False,\n    temperature_tuning_scale: float = 0.1,\n    temperature_tuning_floor_scale: float = 8192.0,\n    # Shard the query activation as the same as the key and value.\n    # TODO: Find a better sharding axis name.\n    # TODO: Further break down the Training and Inference axes for the q, k, v.\n    prefill_query_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    prefill_key_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    prefill_value_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    query_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    key_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    value_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    ep_query_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    ep_key_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    ep_value_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    input_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, EMBED),\n    ep_input_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, EMBED),\n    out_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, HEAD, D_KV),\n    ep_out_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, HEAD, D_KV),\n    prefill_input_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, EMBED),\n    decode_input_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, EMBED),\n    prefill_out_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, HEAD, D_KV),\n    decode_out_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV),\n    prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n    reshape_q: bool = False,\n    is_nope_layer: bool = False,\n    is_vision: bool = False,\n    model_mode: str = MODEL_MODE_TRAIN,\n    name: str | None = None,\n):\n  \"\"\"A factory function to create an Attention as a Linen module.\n\n  This function serves as a bridge to use the NNX-based `Attention` within a\n  Linen model.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      Attention,\n      config=config,\n      num_query_heads=num_query_heads,\n      num_kv_heads=num_kv_heads,\n      head_dim=head_dim,\n      max_target_length=max_target_length,\n      mesh=mesh,\n      attention_kernel=attention_kernel,\n      inputs_q_shape=inputs_q_shape,\n      inputs_kv_shape=inputs_kv_shape,\n      dtype=dtype,\n      weight_dtype=weight_dtype,\n      max_prefill_predict_length=max_prefill_predict_length,\n      dropout_rate=dropout_rate,\n      kernel_init=kernel_init,\n      float32_qk_product=float32_qk_product,\n      float32_logits=float32_logits,\n      quant=quant,\n      kv_quant=kv_quant,\n      attention_type=attention_type,\n      attn_logits_soft_cap=attn_logits_soft_cap,\n      sliding_window_size=sliding_window_size,\n      use_ragged_attention=use_ragged_attention,\n      ragged_block_size=ragged_block_size,\n      use_qk_norm=use_qk_norm,\n      query_pre_attn_scalar=query_pre_attn_scalar,\n      use_bias_in_projections=use_bias_in_projections,\n      temperature_tuning=temperature_tuning,\n      temperature_tuning_scale=temperature_tuning_scale,\n      temperature_tuning_floor_scale=temperature_tuning_floor_scale,\n      prefill_query_axis_names=prefill_query_axis_names,\n      prefill_key_axis_names=prefill_key_axis_names,\n      prefill_value_axis_names=prefill_value_axis_names,\n      query_axis_names=query_axis_names,\n      key_axis_names=key_axis_names,\n      value_axis_names=value_axis_names,\n      ep_query_axis_names=ep_query_axis_names,\n      ep_key_axis_names=ep_key_axis_names,\n      ep_value_axis_names=ep_value_axis_names,\n      input_axis_names=input_axis_names,\n      ep_input_axis_names=ep_input_axis_names,\n      out_axis_names=out_axis_names,\n      ep_out_axis_names=ep_out_axis_names,\n      prefill_input_axis_names=prefill_input_axis_names,\n      decode_input_axis_names=decode_input_axis_names,\n      prefill_out_axis_names=prefill_out_axis_names,\n      decode_out_axis_names=decode_out_axis_names,\n      prefill_cache_axis_order=prefill_cache_axis_order,\n      ar_cache_axis_order=ar_cache_axis_order,\n      compute_axis_order=compute_axis_order,\n      reshape_q=reshape_q,\n      is_nope_layer=is_nope_layer,\n      is_vision=is_vision,\n      model_mode=model_mode,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )",
        "analysis": {
            "module_type": "attention_factory",
            "purpose": "A factory function that creates a Flax Linen-compatible attention module by wrapping the NNX-based `Attention` class.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the NNX `Attention` class into a Flax Linen module.",
                "Passes all its configuration arguments to the `Attention` class constructor through the wrapper."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance, not a tensor. Shape is not applicable."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "Attention"
            ],
            "parameters": {
                "config": "The main configuration object for the model.",
                "num_query_heads": "Number of query attention heads.",
                "num_kv_heads": "Number of key-value attention heads.",
                "head_dim": "The dimension of each attention head.",
                "mesh": "The JAX device mesh for model parallelism.",
                "attention_kernel": "The name of the attention kernel to use (e.g., 'dot_product', 'flash').",
                "model_mode": "The operational mode of the model (e.g., 'train', 'prefill')."
            },
            "notes": [
                "This function serves as a compatibility bridge to use an NNX-defined module (`Attention`) within a Flax Linen model structure.",
                "It forwards a large number of configuration parameters for attention, sharding, and execution modes to the underlying `Attention` module."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attentions.py#Attention",
        "file_path": "src/MaxText/layers/attentions.py",
        "code_block": "class Attention(nnx.Module):\n  \"\"\"Attention Module.\n\n  This module implements multi-headed attention as described in the\n  original Transformer paper. It projects the inputs into query, key, and\n  value vectors, applies the attention mechanism, and projects the results to\n  an output vector.\n\n  Attributes:\n    config: The model configuration.\n    num_query_heads: Number of query attention heads.\n    num_kv_heads: Number of key-value attention heads.\n    head_dim: The dimension of each attention head.\n    max_target_length: Maximum sequence length.\n    mesh: The device mesh.\n    attention_kernel: The attention kernel to use (e.g., 'dot_product', 'flash').\n    inputs_q_shape: Query inputs shape for initialization, required by NNX.\n    inputs_kv_shape: Key/value inputs shape for initialization, required by NNX.\n    dtype: The data type for computation.\n    weight_dtype: The data type for weights.\n    max_prefill_predict_length: Maximum length for prefill.\n    dropout_rate: The dropout rate.\n    kernel_init: Initializer for the kernel of the dense layers.\n    float32_qk_product: If True, compute query-key product in float32.\n    float32_logits: If True, cast logits to float32 before softmax.\n    quant: Quantization configuration.\n    kv_quant: KV cache quantization configuration.\n    attention_type: The type of attention (e.g., 'global', 'local_sliding').\n    attn_logits_soft_cap: Soft cap for attention logits.\n    ... and other configuration parameters.\n  \"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      num_query_heads: int,\n      num_kv_heads: int,\n      head_dim: int,\n      max_target_length: int,\n      mesh: Mesh,\n      attention_kernel: str,\n      inputs_q_shape: Tuple,\n      inputs_kv_shape: Tuple,\n      dtype: DType = jnp.float32,\n      weight_dtype: DType = jnp.float32,\n      max_prefill_predict_length: int = -1,\n      dropout_rate: float = 0.0,\n      kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"normal\"),\n      float32_qk_product: bool = False,  # computes logits in float32 for stability.\n      float32_logits: bool = False,  # cast logits in float32 for stability.\n      quant: Optional[Quant] = None,\n      kv_quant: Optional[KVQuant] = None,\n      attention_type: AttentionType = AttentionType.GLOBAL,  # Default to global attention\n      attn_logits_soft_cap: float | None = None,\n      sliding_window_size: int | None = None,\n      use_ragged_attention: bool = False,\n      ragged_block_size: int = 256,\n      use_qk_norm: bool = False,\n      query_pre_attn_scalar: float | None = None,\n      use_bias_in_projections: bool = False,  # Set to True will enable bias in q, k, v, o projections\n      # Temperature tuning parameters used for Llama4\n      temperature_tuning: bool = False,\n      temperature_tuning_scale: float = 0.1,\n      temperature_tuning_floor_scale: float = 8192.0,\n      # Shard the query activation as the same as the key and value.\n      # TODO: Find a better sharding axis name.\n      # TODO: Further break down the Training and Inference axes for the q, k, v.\n      prefill_query_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      prefill_key_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      prefill_value_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      query_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      key_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      value_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      ep_query_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      ep_key_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      ep_value_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      input_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, EMBED),\n      ep_input_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, EMBED),\n      out_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, HEAD, D_KV),\n      ep_out_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, HEAD, D_KV),\n      prefill_input_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, EMBED),\n      decode_input_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, EMBED),\n      prefill_out_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, HEAD, D_KV),\n      decode_out_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV),\n      prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n      reshape_q: bool = False,\n      is_nope_layer: bool = False,\n      is_vision: bool = False,\n      model_mode: str = MODEL_MODE_TRAIN,\n      base_kv_cache: bool = True,\n      name: str | None = None,\n      rngs: Optional[nnx.Rngs] = None,\n  ):\n    \"\"\"Initializes the Attention module.\n\n    Attributes:\n      config: The model configuration.\n      num_query_heads: Number of query attention heads.\n      num_kv_heads: Number of key-value attention heads.\n      head_dim: The dimension of each attention head.\n      max_target_length: Maximum sequence length.\n      mesh: The device mesh.\n      attention_kernel: The attention kernel to use (e.g., 'dot_product', 'flash').\n      inputs_q_shape: Query inputs shape for initialization, required by NNX.\n      inputs_kv_shape: Key/value inputs shape for initialization, required by NNX.\n      dtype: The data type for computation.\n      weight_dtype: The data type for weights.\n      max_prefill_predict_length: Maximum length for prefill.\n      dropout_rate: The dropout rate.\n      kernel_init: Initializer for the kernel of the dense layers.\n      float32_qk_product: If True, compute query-key product in float32.\n      float32_logits: If True, cast logits to float32 before softmax.\n      quant: Quantization configuration.\n      kv_quant: KV cache quantization configuration.\n      attention_type: The type of attention (e.g., 'global', 'local_sliding').\n      attn_logits_soft_cap: Soft cap for attention logits.\n      sliding_window_size: The size of the sliding window for local attention.\n      use_ragged_attention: Whether to use ragged attention for decoding.\n      ragged_block_size: The block size for ragged attention.\n      use_qk_norm: Whether to apply normalization to query and key.\n      query_pre_attn_scalar: Scalar to apply to query before attention.\n      use_bias_in_projections: Whether to use bias in Q, K, V, and output projections.\n      temperature_tuning: Whether to use temperature tuning for attention.\n      temperature_tuning_scale: The scale for temperature tuning.\n      temperature_tuning_floor_scale: The floor scale for temperature tuning.\n      ... other configuration parameters.\n      is_nope_layer: Whether this is a \"NoPE\" (No Position-Embedding) layer.\n      is_vision: Whether this is a vision attention layer.\n      model_mode: The model's operational mode (e.g., 'train', 'prefill').\n      base_kv_cache: Whether to use base (non-MLA) kv cache, if KVCache is used\n      rngs: RNG state for initialization, passed by the nnx.to_linen wrapper.\n    \"\"\"\n\n    self.config = config\n    self.num_query_heads = num_query_heads\n    self.num_kv_heads = num_kv_heads\n    self.head_dim = head_dim\n    self.max_target_length = max_target_length\n    self.mesh = mesh\n    self.attention_kernel = attention_kernel\n    self.dtype = dtype\n    self.weight_dtype = weight_dtype\n    self.max_prefill_predict_length = max_prefill_predict_length\n    self.dropout_rate = dropout_rate\n    self.kernel_init = kernel_init\n    self.float32_qk_product = float32_qk_product\n    self.float32_logits = float32_logits\n    self.quant = quant\n    self.kv_quant = kv_quant\n    self.attention_type = attention_type\n    self.attn_logits_soft_cap = attn_logits_soft_cap\n    self.sliding_window_size = sliding_window_size\n    self.use_ragged_attention = use_ragged_attention\n    self.ragged_block_size = ragged_block_size\n    self.use_qk_norm = use_qk_norm\n    self.query_pre_attn_scalar = query_pre_attn_scalar\n    self.use_bias_in_projections = use_bias_in_projections\n    self.temperature_tuning = temperature_tuning\n    self.temperature_tuning_scale = temperature_tuning_scale\n    self.temperature_tuning_floor_scale = temperature_tuning_floor_scale\n    self.prefill_query_axis_names = prefill_query_axis_names\n    self.prefill_key_axis_names = prefill_key_axis_names\n    self.prefill_value_axis_names = prefill_value_axis_names\n    self.query_axis_names = query_axis_names\n    self.key_axis_names = key_axis_names\n    self.value_axis_names = value_axis_names\n    self.ep_query_axis_names = ep_query_axis_names\n    self.ep_key_axis_names = ep_key_axis_names\n    self.ep_value_axis_names = ep_value_axis_names\n    self.input_axis_names = input_axis_names\n    self.ep_input_axis_names = ep_input_axis_names\n    self.out_axis_names = out_axis_names\n    self.ep_out_axis_names = ep_out_axis_names\n    self.prefill_input_axis_names = prefill_input_axis_names\n    self.decode_input_axis_names = decode_input_axis_names\n    self.prefill_out_axis_names = prefill_out_axis_names\n    self.decode_out_axis_names = decode_out_axis_names\n    self.prefill_cache_axis_order = prefill_cache_axis_order\n    self.ar_cache_axis_order = ar_cache_axis_order\n    self.compute_axis_order = compute_axis_order\n    self.reshape_q = reshape_q\n    self.is_nope_layer = is_nope_layer\n    self.is_vision = is_vision\n    self.model_mode = model_mode\n    self.rngs = rngs\n\n    # Module attribute names must match names previously passed to Linen for checkpointing\n    self.KVCache_0 = (\n        self.init_kv_caches(inputs_kv_shape=inputs_kv_shape)\n        if self.model_mode != MODEL_MODE_TRAIN and base_kv_cache\n        else None\n    )\n\n    self.rotary_embedding = self.init_rotary_embedding()\n\n    self.attention_op = AttentionOp(\n        config=self.config,\n        mesh=self.mesh,\n        attention_kernel=self.attention_kernel,\n        max_target_length=self.max_target_length,\n        max_prefill_predict_length=self.max_prefill_predict_length,\n        float32_qk_product=self.float32_qk_product,\n        float32_logits=self.float32_logits,\n        quant=self.quant,\n        kv_quant=self.kv_quant,\n        num_query_heads=self.num_query_heads,\n        num_kv_heads=self.num_kv_heads,\n        dropout_rate=self.dropout_rate,\n        dtype=self.dtype,\n        compute_axis_order=self.compute_axis_order,\n        reshape_q=self.reshape_q,\n        attention_type=self.attention_type,\n        attn_logits_soft_cap=self.attn_logits_soft_cap,\n        sliding_window_size=self.sliding_window_size,\n        chunk_attn_window_size=self.config.chunk_attn_window_size,\n        use_ragged_attention=self.use_ragged_attention,\n        ragged_block_size=self.ragged_block_size,\n        rngs=self.rngs,\n    )\n    # When paged attention is enabled, paged attention op is used for all model modes except TRAIN,\n    # which uses default attention op.\n    if self.config.attention == \"paged\":\n      self.paged_attention_op = paged_attention.PagedAttentionOp(\n          mesh=self.mesh,\n          num_pages=self.config.pagedattn_num_pages,\n          tokens_per_page=self.config.pagedattn_tokens_per_page,\n          max_pages_per_slot=(self.config.max_target_length + self.config.pagedattn_tokens_per_page - 1)\n          // self.config.pagedattn_tokens_per_page,\n          max_pages_per_prefill=(self.config.max_prefill_predict_length + self.config.pagedattn_tokens_per_page - 1)\n          // self.config.pagedattn_tokens_per_page,\n          pages_per_compute_block=self.config.pagedattn_pages_per_compute_block,\n          num_kv_heads=self.num_kv_heads,\n          kv_head_dim_size=self.head_dim,\n          dtype=self.dtype,\n          attn_logits_soft_cap=self.attn_logits_soft_cap,\n          rngs=self.rngs,\n      )\n\n    self._init_projections(inputs_q_shape, inputs_kv_shape)\n\n    if self.config.attention_sink:\n      self.sinks = nnx.Param(\n          default_bias_init(self.rngs.params(), (self.config.num_query_heads,), self.weight_dtype),\n          sharding=(None,),\n      )\n    else:\n      self.sinks = None\n\n    is_llama4_decoder_block = self.config.decoder_block == DecoderBlockType.LLAMA4\n    if self.use_qk_norm and not is_llama4_decoder_block:\n      self.query_norm = RMSNorm(\n          num_features=self.head_dim,\n          dtype=self.config.dtype,\n          weight_dtype=self.config.weight_dtype,\n          epsilon=self.config.normalization_layer_epsilon,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n      self.key_norm = RMSNorm(\n          num_features=self.head_dim,\n          dtype=self.config.dtype,\n          weight_dtype=self.config.weight_dtype,\n          epsilon=self.config.normalization_layer_epsilon,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n    else:\n      self.query_norm = None\n      self.key_norm = None\n\n  def _init_projections(self, inputs_q_shape: Tuple, inputs_kv_shape: Tuple) -> None:\n    \"\"\"Initializes the query, key, value, and output projections.\"\"\"\n    if self.config.fused_qkv:\n      self.qkv_proj = self.init_qkv_w(inputs_shape=inputs_q_shape)\n    else:\n      self.query = self.init_query_w(inputs_q_shape=inputs_q_shape)\n      self.key = self.init_kv_w(inputs_kv_shape=inputs_kv_shape)\n      self.value = self.init_kv_w(inputs_kv_shape=inputs_kv_shape)\n    self.out = self.init_out_w(output_dim=inputs_q_shape[-1])\n\n  def init_query_w(self, inputs_q_shape: Tuple) -> nnx.Module:\n    \"\"\"Query projection initialization.\"\"\"\n\n    # NOTE: T5 does not explicitly rescale the attention logits by\n    #       1/sqrt(depth_kq)!  This is folded into the initializers of the\n    #       linear transformations, which is equivalent under Adafactor.\n    # We disable depth_scaling when using qk_norm or a query_pre_attn_scalar\n    # to avoid applying scaling twice.\n    if self.config.use_qk_norm or (self.query_pre_attn_scalar is not None and self.query_pre_attn_scalar != 1.0):\n      depth_scaling = 1.0\n    else:\n      depth_scaling = jnp.sqrt(self.head_dim).astype(self.dtype)\n\n    def query_init(*args):\n      # pylint: disable=no-value-for-parameter\n      return self.kernel_init(*args) / depth_scaling\n\n    kernel_axes = (\n        (None, None, None) if self.config.ici_context_autoregressive_parallelism > 1 else (\"embed\", \"q_heads\", \"kv\")\n    )\n    return DenseGeneral(\n        in_features_shape=self.convert_dense_general_inputs_shape(inputs_q_shape),\n        out_features_shape=(self.num_query_heads, self.head_dim),\n        axis=-1,\n        kernel_init=query_init,\n        kernel_axes=kernel_axes,\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        matmul_precision=self.config.matmul_precision,\n        use_bias=self.use_bias_in_projections,\n        rngs=self.rngs,\n    )\n\n  def query_projection(self, inputs_q: Array) -> Array:\n    \"\"\"Query projection.\"\"\"\n\n    return self.query(inputs_q)\n\n  def init_kv_w(self, inputs_kv_shape: Tuple) -> nnx.Module:\n    \"\"\"Initializes the key or value projection.\n\n    Args:\n      inputs_kv_shape: Key/value inputs shape for initialization.\n\n    Returns:\n      A DenseGeneral module that performs the key or value projection.\n    \"\"\"\n    if self.num_kv_heads == -1:\n      raise ValueError(\"num_kv_heads is not defined.\")\n\n    if self.num_query_heads % self.num_kv_heads != 0:\n      raise ValueError(\"Invalid num_kv_heads for GQA.\")\n\n    kernel_axes = (\n        (None, None, None)\n        if self.config.ici_context_autoregressive_parallelism > 1\n        else (\"embed\", \"kv_heads\", \"kv_head_dim\")\n    )\n\n    return DenseGeneral(\n        in_features_shape=self.convert_dense_general_inputs_shape(inputs_kv_shape),\n        out_features_shape=(self.num_kv_heads, self.head_dim),\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=kernel_axes,\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        matmul_precision=self.config.matmul_precision,\n        use_bias=self.use_bias_in_projections,\n        rngs=self.rngs,\n    )\n\n  def kv_projection(self, inputs_kv: Array, proj_name: str) -> nnx.Module:\n    \"\"\"Applies the key or value projection.\n\n    Args:\n      inputs_kv: The input tensor to project.\n      proj_name: The name of the projection (\"key\" or \"value\").\n\n    Returns:\n      The projected key or value tensor.\n\n    Raises:\n      ValueError: If `proj_name` is not one of the supported values\n        (\"key\", \"value\").\n\n    \"\"\"\n    if proj_name == \"key\":\n      return self.key(inputs_kv)\n    elif proj_name == \"value\":\n      return self.value(inputs_kv)\n    else:\n      raise ValueError(f\"proj_name must be 'key' or 'value', but got {proj_name}\")\n\n  def init_qkv_w(self, inputs_shape: Tuple) -> nnx.Module:\n    return DenseGeneral(\n        in_features_shape=self.convert_dense_general_inputs_shape(inputs_shape),\n        out_features_shape=(3, self.num_query_heads, self.head_dim),\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"embed\", \"qkv\", \"heads\", \"kv\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        matmul_precision=self.config.matmul_precision,\n        use_bias=self.use_bias_in_projections,\n        rngs=self.rngs,\n    )\n\n  def qkv_projection(self, inputs: Array, proj_name: str):\n    \"\"\"Fused QKV projection\"\"\"\n\n    qkv_proj = self.qkv_proj(inputs)\n    qkv_proj = checkpoint_name(qkv_proj, \"qkv_proj\")\n    query, key, value = qkv_proj[:, :, 0, ...], qkv_proj[:, :, 1, ...], qkv_proj[:, :, 2, ...]\n    return query, key, value\n\n  def init_out_w(self, output_dim: int) -> nnx.Module:\n    \"\"\"out projection\"\"\"\n    out_kernel_axis = (\n        (None, None, None) if self.config.ici_context_autoregressive_parallelism > 1 else (\"heads\", \"kv\", \"embed\")\n    )\n    return DenseGeneral(\n        in_features_shape=(self.num_query_heads, self.head_dim),\n        out_features_shape=output_dim,\n        axis=(-2, -1),\n        kernel_init=self.kernel_init,\n        kernel_axes=out_kernel_axis,  # trade speed with memory\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        matmul_precision=self.config.matmul_precision,\n        use_bias=self.use_bias_in_projections,\n        rngs=self.rngs,\n    )\n\n  def out_projection(self, out: Array) -> Array:\n    \"\"\"out projection\"\"\"\n\n    return self.out(out)\n\n  def convert_dense_general_inputs_shape(\n      self,\n      inputs_shape: tuple[int, ...] | None = None,\n      axis: Union[Iterable[int], int] = -1,\n  ) -> Union[Iterable[int], int]:\n    axis = canonicalize_tuple(axis)\n    return tuple(inputs_shape[ax] for ax in normalize_axes(axis, len(inputs_shape)))\n\n  def init_rotary_embedding(self):\n    \"\"\"Initializes the rotary embeddings, handling different model types.\n\n    Returns:\n      The rotary embedding module that will be used in the model.\n    \"\"\"\n    if self.config.attention_type == AttentionType.MLA.value:\n      # For MLA attention RoPE is applied to only `self.qk_rope_head_dim` portion the heads.\n      rope_embedding_dims = self.qk_rope_head_dim\n    else:\n      rope_embedding_dims = self.head_dim\n\n    rope_type = self.config.rope_type.lower()\n    rope_use_scale = self.config.rope_use_scale\n    if self.is_vision:\n      rotary_embedding = LlamaVisionRotaryEmbedding(\n          image_size=self.config.image_size_for_vit,\n          patch_size=self.config.patch_size_for_vit,\n          hidden_size=self.config.hidden_size_for_vit,\n          num_attention_heads=self.config.num_attention_heads_for_vit,\n          rope_theta=self.config.rope_theta_for_vit,\n          rngs=self.rngs,\n      )\n    elif self.config.model_name.startswith(\"llama3.1\") or rope_type.startswith(\"llama3.1\"):\n      rotary_embedding = LLaMARotaryEmbedding(\n          min_timescale=self.config.rope_min_timescale,\n          max_timescale=self.config.rope_max_timescale,\n          embedding_dims=rope_embedding_dims,\n          fprop_dtype=self.dtype,\n          use_scale=rope_use_scale,\n          rngs=self.rngs,\n      )\n    elif rope_type.startswith(\"yarn\"):\n      rotary_embedding = YarnRotaryEmbedding(\n          max_position_embeddings=self.config.max_position_embeddings,\n          original_max_position_embeddings=self.config.original_max_position_embeddings,\n          beta_fast=self.config.beta_fast,\n          beta_slow=self.config.beta_slow,\n          rope_theta=self.config.rope_max_timescale,\n          rope_factor=self.config.rope_factor,\n          embedding_dims=rope_embedding_dims,\n          fprop_dtype=self.dtype,\n          interleave=self.config.rope_interleave,\n          truncate=self.config.rope_truncate,\n          attention_scaling=self.config.rope_attention_scaling,\n          rngs=self.rngs,\n      )\n    else:\n      max_timescale = self.config.rope_max_timescale\n      # For local attention use local_rope_max_timescale if it's is positive\n      if self.attention_type == AttentionType.LOCAL_SLIDING and self.config.local_rope_max_timescale > 0:\n        max_timescale = self.config.local_rope_max_timescale\n\n      rope_linear_scaling_factor = self.config.rope_linear_scaling_factor\n      # In gemma3, linear scaling factor does not apply to local sliding layers.\n      if self.config.model_name.startswith(\"gemma3\") and self.attention_type == AttentionType.LOCAL_SLIDING:\n        rope_linear_scaling_factor = 1.0\n\n      rotary_embedding = RotaryEmbedding(\n          min_timescale=self.config.rope_min_timescale,\n          max_timescale=max_timescale,\n          embedding_dims=rope_embedding_dims,\n          fprop_dtype=self.dtype,\n          rope_linear_scaling_factor=rope_linear_scaling_factor,\n          rngs=self.rngs,\n      )\n    return rotary_embedding\n\n  def apply_rotary_embedding(self, inputs: Array, inputs_positions: Optional[Array | None] = None):\n    \"\"\"Applies rotary embeddings, handling different model types.\n\n    Args:\n      inputs: The input tensor to apply rotary embeddings to.\n      inputs_positions: The positions of the inputs.\n      name: A name for the embedding layer.\n\n    Returns:\n      The input tensor with rotary embeddings applied.\n    \"\"\"\n    return self.rotary_embedding(inputs, inputs_positions)\n\n  def init_kv_caches(self, inputs_kv_shape: Tuple):\n    \"\"\"Initializes KVCache.\n\n    Args:\n      inputs_kv_shape: Key/value inputs shape for initialization.\n\n    Returns:\n      A KVCache module instance.\n\n    \"\"\"\n    batch_size, _, _ = inputs_kv_shape\n    # During initialization, seq_len of inputs_kv is max_target_length,\n    # which is not always correct for some functions in KVCache.\n    # However, KVCache internal cache shapes are based on max_prefill_length\n    # and max_target_length, not the passed seq_len.\n    # We can use a placeholder value. The correct fix might involve refactoring\n    # KVCache.\n    placeholder_seq_len = 1\n\n    return kvcache.KVCache(\n        max_prefill_length=self.max_prefill_predict_length,\n        max_target_length=self.max_target_length,\n        batch=batch_size,\n        key_seq_len=placeholder_seq_len,\n        value_seq_len=placeholder_seq_len,\n        key_heads=self.num_kv_heads,\n        value_heads=self.num_kv_heads,\n        key_head_size=self.head_dim,\n        value_head_size=self.head_dim,\n        dtype=self.dtype,\n        kv_quant=self.kv_quant,\n        prefill_cache_axis_order=self.prefill_cache_axis_order,\n        ar_cache_axis_order=self.ar_cache_axis_order,\n        use_chunked_prefill=self.config.use_chunked_prefill,\n        model_mode=self.model_mode,\n        rngs=self.rngs,\n    )\n\n  def update_kv_caches(self, key, value, decoder_segment_ids, model_mode, previous_chunk):\n    \"\"\"Updates the KV caches for prefill and autoregressive modes.\n\n    This method uses a kvcache module to update and retrieve the key-value\n    caches based on the current operational mode.\n\n    Args:\n      key: The key tensor for the current attention computation.\n      value: The value tensor for the current attention computation.\n      decoder_segment_ids: Segment IDs for the decoder, used for masking.\n      model_mode: The operational mode ('train', 'prefill', 'autoregressive').\n      previous_chunk: Information about previously processed chunks, used for\n        chunked prefill.\n\n    Returns:\n      A list containing two elements:\n      - The prefill key-value cache, or None.\n      - The autoregressive key-value cache, or None.\n    \"\"\"\n    prefill_kv_cache, ar_kv_cache = self.KVCache_0(\n        key=key,\n        value=value,\n        decoder_segment_ids=decoder_segment_ids,\n        model_mode=model_mode,\n        use_ragged_attention=self.use_ragged_attention,\n        previous_chunk=previous_chunk,\n    )\n    return [prefill_kv_cache, ar_kv_cache]\n\n  def __call__(\n      self,\n      inputs_q: Array,\n      inputs_kv: Array,\n      inputs_positions: Array | None = None,\n      decoder_segment_ids: Array | None = None,\n      *,\n      model_mode: str = MODEL_MODE_TRAIN,\n      deterministic: bool = False,\n      previous_chunk: Any = None,\n      slot: Optional[int] = None,\n      page_state: Optional[page_manager.PageState] = None,\n      bidirectional_mask: Any = None,\n  ):\n    \"\"\"Applies Attention on the input data.\n\n    Projects the inputs into multi-headed query, key, and value vectors,\n    applies dot-product attention, and project the results to an output vector.\n\n    This method handles three modes:\n    1.  **Training**: The KV cache is ignored.\n    2.  **Prefill**: The KV cache is filled with the key-value pairs from the input sequence.\n    3.  **Autoregressive Decoding**: The KV cache is used to provide context from previous steps.\n\n    In the cache initialization call, `inputs_q` has a shape [batch, length,\n    q_features] and `inputs_kv`: [batch, length, kv_features]. During the\n    incremental decoding stage, query, key and value all have the shape [batch,\n    1, qkv_features] corresponding to a single step.\n\n    Args:\n      inputs_q: Input queries of shape `[batch, q_length, q_features]`.\n      inputs_kv: Key/values of shape `[batch, kv_length, kv_features]`.\n      inputs_positions: Input positions for rotary embeddings.\n      decoder_segment_ids: Segment IDs for masking.\n      model_mode: The operational mode ('train', 'prefill', 'autoregressive').\n      deterministic: If True, disables dropout.\n      previous_chunk: Information about previously processed chunks for chunked prefill.\n      slot: The batch slot index for paged attention.\n      page_state: The current state of the paged attention manager.\n      bidirectional_mask: A mask for bidirectional attention, used in multimodal models.\n\n    Returns:\n      output of shape `[batch, length, q_features]`.\n    \"\"\"\n    if model_mode == MODEL_MODE_PREFILL:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.prefill_input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.prefill_input_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.ep_input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.ep_input_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.input_axis_names)\n    else:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.decode_input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.decode_input_axis_names)\n\n    # apply projection.\n    if self.config.fused_qkv:\n      query, key, value = self.qkv_projection(inputs_q, proj_name=\"qkv_proj\")\n    else:\n      query = self.query_projection(inputs_q)\n      key = self.kv_projection(inputs_kv, proj_name=\"key\")\n      value = self.kv_projection(inputs_kv, proj_name=\"value\")\n\n    is_llama4_decoder_block = self.config.decoder_block == DecoderBlockType.LLAMA4\n    # NOTE: llama 4 does L2 normalization after RoPE\n    if self.use_qk_norm and not is_llama4_decoder_block:\n      query = self.query_norm(query)\n      key = self.key_norm(key)\n\n    # NOTE: is_nope_layer should be used in attention mask and also used in attention tuning\n    use_rope = not self.is_nope_layer\n    use_qk_norm = self.use_qk_norm and use_rope\n\n    if use_rope:\n      query = self.apply_rotary_embedding(query, inputs_positions=inputs_positions)\n      key = self.apply_rotary_embedding(key, inputs_positions=inputs_positions)\n\n    if use_qk_norm and is_llama4_decoder_block:\n      l2_norm = L2Norm(eps=self.config.normalization_layer_epsilon)\n      query = l2_norm(query)\n      key = l2_norm(key)\n\n    # apply query_pre_attn_scalar if it's present.\n    if self.query_pre_attn_scalar and self.query_pre_attn_scalar != 1.0:\n      query = query * self.query_pre_attn_scalar\n\n    if self.temperature_tuning and not use_rope:\n      attn_scales = (\n          jnp.log(jnp.floor((inputs_positions.astype(self.dtype) + 1.0) / self.temperature_tuning_floor_scale) + 1.0)\n          * self.temperature_tuning_scale\n          + 1.0\n      )\n      query = (query * attn_scales[:, :, jnp.newaxis, jnp.newaxis]).astype(self.dtype)\n\n    if model_mode == MODEL_MODE_PREFILL:\n      query = nn.with_logical_constraint(query, self.prefill_query_axis_names)\n      key = nn.with_logical_constraint(key, self.prefill_key_axis_names)\n      value = nn.with_logical_constraint(value, self.prefill_value_axis_names)\n    elif model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      query = nn.with_logical_constraint(query, (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV))\n      key = nn.with_logical_constraint(key, (DECODE_BATCH, DECODE_LENGTH, KV_HEAD, D_KV))\n      value = nn.with_logical_constraint(value, (DECODE_BATCH, DECODE_LENGTH, KV_HEAD, D_KV))\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      query = nn.with_logical_constraint(query, self.ep_query_axis_names)\n      key = nn.with_logical_constraint(key, self.ep_key_axis_names)\n      value = nn.with_logical_constraint(value, self.ep_value_axis_names)\n    else:\n      query = nn.with_logical_constraint(query, self.query_axis_names)\n      key = nn.with_logical_constraint(key, self.key_axis_names)\n      value = nn.with_logical_constraint(value, self.value_axis_names)\n\n    query = checkpoint_name(query, \"query_proj\")\n    key = checkpoint_name(key, \"key_proj\")\n    value = checkpoint_name(value, \"value_proj\")\n\n    assert not self.config.quantize_kvcache or self.kv_quant\n\n    if self.config.attention == \"paged\" and model_mode != MODEL_MODE_TRAIN:\n      unnormalized_out, _, exp_sum = self.paged_attention_op(\n          query, key, value, decoder_segment_ids, model_mode, previous_chunk, slot=slot, page_state=page_state\n      )\n      out = unnormalized_out / (exp_sum + 1e-9) if exp_sum is not None else unnormalized_out\n    else:\n      cached_values = [None, None]\n      if model_mode != MODEL_MODE_TRAIN:\n        cached_values = self.update_kv_caches(key, value, decoder_segment_ids, model_mode, previous_chunk)\n      out = self.attention_op(\n          query,\n          key,\n          value,\n          decoder_segment_ids,\n          model_mode,\n          cached_values,\n          previous_chunk,\n          bidirectional_mask,\n          self.sinks,\n      )\n\n    if model_mode == MODEL_MODE_PREFILL:\n      out = nn.with_logical_constraint(out, self.prefill_out_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      out = nn.with_logical_constraint(out, self.ep_out_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN:\n      out = nn.with_logical_constraint(out, self.out_axis_names)\n    else:\n      out = nn.with_logical_constraint(out, self.decode_out_axis_names)\n    out = self.out_projection(out)\n    out = checkpoint_name(out, \"out_proj\")\n    return out",
        "analysis": {
            "module_type": "multi_head_attention",
            "purpose": "Implements a multi-headed attention mechanism, projecting inputs into query, key, and value vectors, applying attention, and projecting the results to an output vector. It supports various modes like training, prefill, and autoregressive decoding, along with features like KV caching and paged attention.",
            "input": {
                "shape": "inputs_q: [batch, q_length, q_features], inputs_kv: [batch, kv_length, kv_features]",
                "dtype": "jnp.float32 or other configured dtype."
            },
            "processing_steps": [
                "The `__call__` method is the main entry point for processing."
            ],
            "output": {
                "shape": "[batch, q_length, q_features]"
            },
            "dependencies": [
                "flax.nnx.Module",
                "MaxText.layers.attention_op.AttentionOp",
                "MaxText.inference.paged_attention.PagedAttentionOp",
                "MaxText.inference.kvcache.KVCache",
                "MaxText.layers.embeddings.RotaryEmbedding",
                "MaxText.layers.linears.DenseGeneral",
                "MaxText.layers.normalizations.RMSNorm",
                "MaxText.common_types.Config"
            ],
            "parameters": {
                "config": "The main model configuration object, containing parameters like `fused_qkv`, `attention`, `use_qk_norm`, etc.",
                "num_query_heads": "Number of query attention heads.",
                "num_kv_heads": "Number of key-value attention heads (for Grouped-Query Attention).",
                "head_dim": "The dimension of each attention head.",
                "attention_kernel": "The attention kernel to use (e.g., 'dot_product', 'flash').",
                "model_mode": "The operational mode of the model, one of 'train', 'prefill', or 'autoregressive'."
            },
            "notes": [
                "This is a highly configurable module that can operate in different modes (training, prefill, autoregressive decoding).",
                "It supports standard multi-head attention, grouped-query attention (GQA), and various rotary position embedding (RoPE) types.",
                "For inference, it can utilize a Key-Value (KV) cache, including paged attention for efficiency."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Attention module, setting up configuration, projection layers, rotary embeddings, and the core attention operation module.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters as instance attributes.",
                        "Initialize KV cache (`KVCache_0`) if not in training mode.",
                        "Initialize rotary embedding (`rotary_embedding`) based on config.",
                        "Initialize the core attention operator (`attention_op` or `paged_attention_op`).",
                        "Initialize query, key, value, and output projection layers via `_init_projections`.",
                        "Initialize attention sinks if configured.",
                        "Initialize query and key normalization layers (`RMSNorm`) if configured."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "Mesh",
                        "AttentionOp",
                        "paged_attention.PagedAttentionOp",
                        "kvcache.KVCache",
                        "RotaryEmbedding",
                        "DenseGeneral",
                        "RMSNorm"
                    ],
                    "notes": [
                        "The specific sub-modules initialized (e.g., KV cache, paged attention, QK norm) depend heavily on the provided `Config` object and `model_mode`."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the multi-head attention mechanism to the input query and key/value tensors.",
                    "input": {
                        "shape": "inputs_q: [batch, q_length, q_features], inputs_kv: [batch, kv_length, kv_features]",
                        "dtype": "The class `dtype` attribute."
                    },
                    "processing_steps": [
                        "Apply logical constraints for sharding based on `model_mode`.",
                        "Project `inputs_q` and `inputs_kv` into query, key, and value tensors.",
                        "Optionally apply RMSNorm to query and key tensors.",
                        "Apply rotary position embeddings to query and key tensors if `use_rope` is true.",
                        "Optionally apply L2 normalization to query and key tensors.",
                        "Optionally scale the query tensor.",
                        "If using paged attention in inference, call `paged_attention_op`.",
                        "Otherwise, update KV cache (if in inference mode) and call `attention_op`.",
                        "Project the attention output to the final dimension using `out_projection`."
                    ],
                    "output": {
                        "shape": "[batch, q_length, q_features]"
                    },
                    "dependencies": [
                        "paged_attention.PagedAttentionOp",
                        "AttentionOp",
                        "self.update_kv_caches",
                        "self.apply_rotary_embedding",
                        "self.out_projection",
                        "L2Norm"
                    ],
                    "notes": [
                        "The execution path differs significantly based on `model_mode` ('train', 'prefill', 'autoregressive') and `config.attention` type ('paged' or other)."
                    ]
                },
                "_init_projections": {
                    "purpose": "Initializes the query, key, value, and output projection layers.",
                    "input": {
                        "shape": "inputs_q_shape: Tuple, inputs_kv_shape: Tuple",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "If `config.fused_qkv` is true, call `init_qkv_w`.",
                        "Otherwise, call `init_query_w`, `init_kv_w` for key, and `init_kv_w` for value.",
                        "Call `init_out_w` to initialize the output projection."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "self.init_qkv_w",
                        "self.init_query_w",
                        "self.init_kv_w",
                        "self.init_out_w"
                    ],
                    "notes": [
                        "This method sets up the linear transformation layers for the attention mechanism."
                    ]
                },
                "init_rotary_embedding": {
                    "purpose": "Initializes the appropriate rotary position embedding module based on the model configuration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determine the embedding dimension based on attention type.",
                        "Check `config.rope_type`, `is_vision`, and model name.",
                        "Instantiate and return the corresponding rotary embedding class (`LlamaVisionRotaryEmbedding`, `LLaMARotaryEmbedding`, `YarnRotaryEmbedding`, or `RotaryEmbedding`)."
                    ],
                    "output": {
                        "shape": "An `nnx.Module` which is a type of rotary embedding."
                    },
                    "dependencies": [
                        "LlamaVisionRotaryEmbedding",
                        "LLaMARotaryEmbedding",
                        "YarnRotaryEmbedding",
                        "RotaryEmbedding"
                    ],
                    "notes": [
                        "This method abstracts away the selection of different RoPE implementations."
                    ]
                },
                "init_kv_caches": {
                    "purpose": "Initializes the Key-Value cache for use in inference modes (prefill and autoregressive).",
                    "input": {
                        "shape": "inputs_kv_shape: A tuple representing the input key/value shape.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Extract batch size from the input shape.",
                        "Instantiate and return a `kvcache.KVCache` module with parameters derived from the main `Attention` module's config."
                    ],
                    "output": {
                        "shape": "A `kvcache.KVCache` module instance."
                    },
                    "dependencies": [
                        "kvcache.KVCache"
                    ],
                    "notes": [
                        "This is only called during initialization if `model_mode` is not 'train'."
                    ]
                },
                "update_kv_caches": {
                    "purpose": "Updates and retrieves the key-value caches for prefill and autoregressive modes.",
                    "input": {
                        "shape": "key: [batch, seq_len, num_kv_heads, head_dim], value: [batch, seq_len, num_kv_heads, head_dim]",
                        "dtype": "The class `dtype` attribute."
                    },
                    "processing_steps": [
                        "Call the initialized `KVCache_0` module with the new key, value, and other metadata."
                    ],
                    "output": {
                        "shape": "A list containing two elements: the prefill key-value cache and the autoregressive key-value cache."
                    },
                    "dependencies": [
                        "self.KVCache_0"
                    ],
                    "notes": [
                        "This method is a wrapper around the `KVCache` module's `__call__` method, used during inference."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/decoders.py#DecoderLayer",
        "file_path": "src/MaxText/layers/decoders.py",
        "code_block": "class DecoderLayer(nn.Module):\n  \"\"\"\n  Transformer decoder layer that attends to the encoder.\n  This is the core, reusable building block for both the main model's\n  decoder stack and the auxiliary MTP layers.\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n    if model_mode == MODEL_MODE_PREFILL:\n      logical_axis_names = (\"activation_batch\", \"prefill_activation_length\", \"activation_embed\")\n    else:\n      logical_axis_names = (\"activation_batch\", \"activation_length\", \"activation_embed\")\n\n    if model_mode == MODEL_MODE_PREFILL:\n      inputs = nn.with_logical_constraint(inputs, logical_axis_names)\n    else:\n      inputs = nn.with_logical_constraint(inputs, logical_axis_names)\n\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    # inputs: embedded inputs to the decoder with shape [batch, length, emb_dim]\n    lnx = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_self_attention_norm\",\n        epsilon=cfg.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n    )(inputs)\n    if model_mode == MODEL_MODE_PREFILL:\n      lnx = nn.with_logical_constraint(lnx, logical_axis_names)\n    else:\n      lnx = nn.with_logical_constraint(lnx, logical_axis_names)\n\n    attention_layer = attention_as_linen(\n        config=self.config,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention\",\n        float32_qk_product=cfg.float32_qk_product,\n        float32_logits=cfg.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        prefill_cache_axis_order=tuple(map(int, cfg.prefill_cache_axis_order.split(\",\"))),\n        ar_cache_axis_order=tuple(map(int, cfg.ar_cache_axis_order.split(\",\"))),\n        compute_axis_order=tuple(map(int, cfg.compute_axis_order.split(\",\"))),\n        reshape_q=cfg.reshape_q,\n        model_mode=self.model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=self.model_mode,\n    )\n\n    if model_mode == MODEL_MODE_PREFILL:\n      attention_lnx = nn.with_logical_constraint(attention_lnx, logical_axis_names)\n    else:\n      attention_lnx = nn.with_logical_constraint(attention_lnx, logical_axis_names)\n\n    # MLP block.\n    mlp_lnx = linears.mlp_block(\n        in_features=lnx.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp\",\n        model_mode=self.model_mode,\n        config=cfg,\n        quant=self.quant,\n    )(lnx, deterministic=deterministic)\n    if model_mode == MODEL_MODE_PREFILL:\n      mlp_lnx = nn.with_logical_constraint(mlp_lnx, logical_axis_names)\n    else:\n      mlp_lnx = nn.with_logical_constraint(mlp_lnx, logical_axis_names)\n\n    next_layer_addition = mlp_lnx + attention_lnx\n\n    next_layer_addition_dropped_out = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(\n        next_layer_addition, deterministic=deterministic\n    )\n\n    layer_output = next_layer_addition_dropped_out + inputs\n    if model_mode == MODEL_MODE_PREFILL:\n      layer_output = nn.with_logical_constraint(\n          layer_output,\n          logical_axis_names,\n      )\n    else:\n      layer_output = nn.with_logical_constraint(\n          layer_output,\n          logical_axis_names,\n      )\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    return layer_output, None if cfg.scan_layers else layer_output",
        "analysis": {
            "functionality": "The `DecoderLayer` class implements a single layer of a Transformer decoder. It performs self-attention followed by a multi-layer perceptron (MLP) block, with residual connections and layer normalization. This class is a fundamental, reusable component for building the decoder stack in a transformer model.",
            "usage": "To use this class, instantiate it with a configuration object (`Config`), a JAX mesh (`Mesh`), a model mode string, and optional quantization parameters. The instance is then called like a function with input tensors. \n\n**Inputs**:\n- `inputs`: The primary input tensor with shape `[batch_size, sequence_length, embedding_dim]`.\n- `decoder_segment_ids`: Tensor indicating different segments in the input sequence.\n- `decoder_positions`: Tensor specifying the position of each token in the sequence.\n- `deterministic`: A boolean that disables dropout when `True`.\n- `model_mode`: A string indicating the operational mode (e.g., 'prefill', 'autoregressive').\n\n**Outputs**:\n- A tuple `(layer_output, carry)`, where `layer_output` is the processed tensor with the same shape as the input, `[batch_size, sequence_length, embedding_dim]`. The `carry` is `None` if `config.scan_layers` is `True`, otherwise it is a copy of `layer_output`."
        }
    },
    {
        "block_name": "src/MaxText/layers/decoders.py#SequentialBlockDecoderLayers",
        "file_path": "src/MaxText/layers/decoders.py",
        "code_block": "class SequentialBlockDecoderLayers(nn.Module):\n  \"\"\"Sequential unscanned series of decoder layers.\"\"\"\n\n  decoder_layer: Any\n  num_decoder_layers: int\n  config: Config\n  mesh: Mesh\n  quant: Quant\n  model_mode: str\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic: bool,\n      model_mode,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n  ) -> jnp.ndarray:\n    for lyr in range(self.num_decoder_layers):\n      inputs = self.decoder_layer(\n          config=self.config, mesh=self.mesh, name=f\"layers_{lyr}\", quant=self.quant, model_mode=self.model_mode\n      )(\n          inputs,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n          slot=slot,\n          page_state=page_state,\n      )\n      if self.config.scan_layers:\n        inputs = inputs[0]  #  When scan_layers is True the decoder layers return (outputs, None).\n    if self.config.scan_layers:\n      return inputs, None  # pytype: disable=bad-return-type\n    else:\n      return inputs",
        "analysis": {
            "module_type": "sequential_decoder_layers",
            "purpose": "Applies a specified decoder layer multiple times in a sequential loop, serving as a non-scanned alternative for stacking layers.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "jnp.ndarray (configurable via config.dtype)"
            },
            "processing_steps": [
                "Iterate `num_decoder_layers` times in a for-loop.",
                "In each iteration, instantiate and call the provided `decoder_layer` with the current input tensor and other arguments.",
                "Update the input tensor with the output from the layer for the next iteration.",
                "If `config.scan_layers` is true, unpack the tuple `(outputs, None)` returned by the decoder layer.",
                "After the loop, return the final tensor, potentially wrapped in a tuple with `None` if `config.scan_layers` is true."
            ],
            "output": {
                "shape": "Returns the transformed tensor of shape [batch_size, sequence_length, hidden_dim]. If config.scan_layers is True, it returns a tuple (tensor, None)."
            },
            "dependencies": [
                "flax.linen as nn",
                "jax.numpy as jnp",
                "page_manager.PageState"
            ],
            "parameters": {
                "num_decoder_layers": "The total number of decoder layers to apply sequentially.",
                "decoder_layer": "The class of the decoder layer to be instantiated and applied in each step.",
                "config.scan_layers": "A boolean that controls the return signature to match the `nn.scan` API."
            },
            "notes": [
                "This module is used as an alternative to `nn.scan` for applying decoder layers, particularly within a pipeline parallelism setup where scanning layers per stage is disabled.",
                "The return value is conditional on `config.scan_layers` to provide a consistent interface with scanned implementations."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Executes the sequential application of decoder layers on the input tensor.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Loop from `lyr = 0` to `self.num_decoder_layers - 1`.",
                        "Instantiate `self.decoder_layer` with a unique name `f'layers_{lyr}'`.",
                        "Call the layer with `inputs`, `decoder_segment_ids`, `decoder_positions`, and other arguments.",
                        "If `self.config.scan_layers` is True, unpack the result: `inputs = result[0]`.",
                        "Return `(inputs, None)` if `self.config.scan_layers` is True, otherwise return `inputs`."
                    ],
                    "output": {
                        "shape": "Either a tensor of shape [batch_size, sequence_length, hidden_dim] or a tuple `(tensor, None)` depending on `config.scan_layers`."
                    },
                    "dependencies": [
                        "self.decoder_layer"
                    ],
                    "notes": [
                        "Passes inference-specific arguments like `slot` and `page_state` through to the underlying `decoder_layer`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/decoders.py#Decoder",
        "file_path": "src/MaxText/layers/decoders.py",
        "code_block": "class Decoder(nn.Module):\n  \"\"\"A stack of decoder layers as a part of an encoder-decoder architecture.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  quant: None | Quant = None\n  model_mode: str = MODEL_MODE_TRAIN\n\n  def setup(self):\n    \"\"\"Initialize decoder layer.\"\"\"\n    self.decoder_layer = self.get_decoder_layers()\n    self.norm_layer = self.get_norm_layer(num_features=self.config.emb_dim)\n    if self.config.using_pipeline_parallelism:\n      pipeline_stage_module = self.get_pipeline_stage_module(self.decoder_layer)\n      remat_policy = self.get_remat_policy()\n      self.pipeline_module = pipeline.Pipeline(\n          config=self.config, mesh=self.mesh, layers=pipeline_stage_module, remat_policy=remat_policy\n      )\n\n  def minimal_policy(self, with_context=False):\n    \"\"\"Helper for creating minimal checkpoint policies.\"\"\"\n    names = [\n        \"query_proj\",\n        \"value_proj\",\n        \"key_proj\",\n        \"qkv_proj\",\n        \"out_proj\",\n        \"mlpwi_0\",\n        \"mlpwi_1\",\n        \"mlpwi\",\n        \"mlpwo\",\n    ]\n    if with_context:\n      names.append(\"context\")\n    return jax.checkpoint_policies.save_only_these_names(*names)\n\n  def get_remat_policy(self):\n    \"\"\"Get remat policy\"\"\"\n    policy = None\n    cfg = self.config\n    if cfg.remat_policy != \"none\":\n      if cfg.remat_policy in (\"minimal_with_context\", \"minimal_flash\"):\n        # save all\n        if cfg.remat_policy == \"minimal_flash\":\n          max_logging.log(\"WARNING: 'minimal_flash' will be deprecated soon, please use 'minimal_with_context' instead.\")\n        policy = self.minimal_policy(with_context=True)\n      elif cfg.remat_policy == \"minimal\":\n        # save all except context\n        policy = self.minimal_policy()\n      elif cfg.remat_policy == \"save_dot_with_context_except_mlp\":\n        policy = jax.checkpoint_policies.save_only_these_names(\n            \"query_proj\",\n            \"value_proj\",\n            \"key_proj\",\n            \"qkv_proj\",\n            \"context\",\n            \"out_proj\",\n        )\n      elif cfg.remat_policy == \"save_dot_except_mlpwi\":\n        policy = jax.checkpoint_policies.save_only_these_names(\n            \"query_proj\",\n            \"value_proj\",\n            \"key_proj\",\n            \"qkv_proj\",\n            \"out_proj\",\n            \"mlpwo\",\n        )\n      elif cfg.remat_policy == \"save_dot_except_mlp\":\n        policy = jax.checkpoint_policies.save_only_these_names(\n            \"query_proj\",\n            \"value_proj\",\n            \"key_proj\",\n            \"qkv_proj\",\n            \"out_proj\",\n        )\n      elif cfg.remat_policy == \"save_qkv_proj\":\n        policy = jax.checkpoint_policies.save_only_these_names(\n            \"query_proj\",\n            \"value_proj\",\n            \"key_proj\",\n            \"qkv_proj\",\n        )\n      elif cfg.remat_policy == \"qkv_proj_offloaded\":\n        policy = jax.checkpoint_policies.save_and_offload_only_these_names(\n            names_which_can_be_saved=[],\n            names_which_can_be_offloaded=[\"query_proj\", \"value_proj\", \"key_proj\"],\n            offload_src=\"device\",\n            offload_dst=\"pinned_host\",\n        )\n      elif cfg.remat_policy == \"minimal_offloaded\":\n        # offload all except context\n        policy = jax.checkpoint_policies.save_and_offload_only_these_names(\n            names_which_can_be_saved=[],\n            names_which_can_be_offloaded=[\n                \"query_proj\",\n                \"value_proj\",\n                \"key_proj\",\n                \"qkv_proj\",\n                \"out_proj\",\n                \"mlpwi_0\",\n                \"mlpwi_1\",\n                \"mlpwi\",\n                \"mlpwo\",\n            ],\n            offload_src=\"device\",\n            offload_dst=\"pinned_host\",\n        )\n      elif cfg.remat_policy == \"custom\":\n        policy = jax.checkpoint_policies.save_and_offload_only_these_names(\n            names_which_can_be_saved=cfg.tensors_on_device,\n            names_which_can_be_offloaded=cfg.tensors_to_offload,\n            offload_src=\"device\",\n            offload_dst=\"pinned_host\",\n        )\n      elif cfg.remat_policy == \"save_out_proj\":\n        policy = jax.checkpoint_policies.save_only_these_names(\n            \"out_proj\",\n        )\n      else:\n        assert cfg.remat_policy == \"full\", \"Remat policy needs to be on list of remat policies\"\n        policy = None\n    return policy\n\n  def get_decoder_layers(self):\n    \"\"\"Retrieves a list of decoder layer classes based on the `decoder_block` config.\n\n    Returns:\n        A list containing one or more `nn.Module` classes for the decoder.\n    \"\"\"\n    match self.config.decoder_block:\n      case DecoderBlockType.DEFAULT:\n        return [DecoderLayer]\n      case DecoderBlockType.LLAMA2:\n        return [llama2.LlamaDecoderLayer]\n      case DecoderBlockType.MISTRAL:\n        # TODO(ranran): update to Mistral with sliding window attention\n        return [mistral.MistralDecoderLayer]\n      case DecoderBlockType.MIXTRAL:\n        return [mixtral.MixtralDecoderLayer]\n      case DecoderBlockType.DEEPSEEK:\n        return [deepseek.DeepSeekDenseLayer, deepseek.DeepSeekMoELayer]\n      case DecoderBlockType.GEMMA:\n        return [gemma.GemmaDecoderLayer]\n      case DecoderBlockType.GEMMA2:\n        return [gemma2.Gemma2DecoderLayer]\n      case DecoderBlockType.GEMMA3:\n        return [gemma3.Gemma3DecoderLayer]\n      case DecoderBlockType.GPT3:\n        return [gpt3.Gpt3DecoderLayer]\n      case DecoderBlockType.GPT_OSS:\n        return [gpt_oss.GptOssScannableBlock] if self.config.scan_layers else [gpt_oss.GptOssDecoderLayer]\n      case DecoderBlockType.QWEN3:\n        return [qwen3.Qwen3DecoderLayer]\n      case DecoderBlockType.QWEN3_MOE:\n        return [qwen3.Qwen3MoeDecoderLayer]\n      case DecoderBlockType.SIMPLE:\n        return [simple_layer.SimpleDecoderLayer]\n      case DecoderBlockType.SIMPLE_MLP:\n        return [simple_layer.SimpleMlpDecoderLayer]\n      case DecoderBlockType.LLAMA4:\n        return [llama4.Llama4ScannableBlock] if self.config.scan_layers else [llama4.Llama4DecoderLayer]\n      case _:\n        # Default case to handle any unknown decoder block types.\n        raise ValueError(f\"Incorrect decoder_block name {self.config.decoder_block.value=}\")\n\n  def set_remat_policy(self, block_layers, policy):\n    \"\"\"Set remat policy\"\"\"\n    RemattedBlockLayers = []\n    for block_layer in block_layers:\n      if self.config.parameter_memory_host_offload:\n        # Define parameter movement with mesh-based sharding\n        def move_to_device(variables):\n          \"\"\"Move parameters to device with proper sharding.\"\"\"\n\n          def map_fn(path, value):\n            max_logging.log(f\"models.py: Moving parameter {path} to device\")\n            return jax.device_put(value, max_utils.device_space())\n\n          return jax.tree_util.tree_map_with_path(map_fn, variables)\n\n        # Transform layer class before remat\n        block_layer = nn.map_variables(block_layer, [\"params\"], move_to_device, mutable=True)\n\n      # Apply remat policy to layer\n      layer = nn.remat(\n          block_layer,\n          prevent_cse=not self.config.scan_layers,\n          policy=policy,\n          static_argnums=(4, 5),  # Deterministic and model mode are static arguments.\n      )\n      RemattedBlockLayers.append(layer)\n    return RemattedBlockLayers\n\n  def get_norm_layer(self, num_features: int):\n    \"\"\"get normalization layer (return type inherits from nn.Module)\"\"\"\n    if self.config.decoder_block in (\n        DecoderBlockType.DEFAULT,\n        DecoderBlockType.LLAMA2,\n        DecoderBlockType.MISTRAL,\n        DecoderBlockType.MIXTRAL,\n        DecoderBlockType.DEEPSEEK,\n        DecoderBlockType.GEMMA,\n        DecoderBlockType.GEMMA2,\n        DecoderBlockType.GEMMA3,\n        DecoderBlockType.QWEN3,\n        DecoderBlockType.QWEN3_MOE,\n        DecoderBlockType.GPT_OSS,\n        DecoderBlockType.SIMPLE,\n        DecoderBlockType.SIMPLE_MLP,\n        DecoderBlockType.LLAMA4,\n    ):\n      return functools.partial(rms_norm, num_features=num_features)\n    elif self.config.decoder_block == DecoderBlockType.GPT3:\n      return functools.partial(gpt3.gpt3_layer_norm, num_features=num_features, reductions_in_fp32=False, use_bias=True)\n    else:\n      raise ValueError(f\"Incorrect decoder_block name {self.config.decoder_block.value=}\")\n\n  def scan_decoder_layers(self, cfg, decoder_layer, length, metadata_axis_name, mesh, in_axes_tuple, **kwargs):\n    \"\"\"scan decoder layers, calls `flax.linen.transforms.scan`\"\"\"\n    initializing = self.is_mutable_collection(\"params\")\n    params_spec = cfg.param_scan_axis if initializing else ScanIn(cfg.param_scan_axis)\n    cache_spec = 0\n    scan_fn = nn.scan(\n        decoder_layer,\n        variable_axes={\n            \"params\": params_spec,\n            \"cache\": cache_spec,\n            \"intermediates\": 0,\n            \"aqt\": 0,\n            \"_overwrite_with_gradient\": 0,\n        },\n        split_rngs={\n            \"params\": True,\n            \"dropout\": cfg.enable_dropout,\n        },\n        in_axes=in_axes_tuple,\n        length=length,\n        metadata_params={nn.PARTITION_NAME: metadata_axis_name},\n    )\n    return scan_fn(config=cfg, mesh=mesh, name=metadata_axis_name, quant=self.quant, model_mode=self.model_mode, **kwargs)\n\n  def get_pipeline_stage_module(self, decoder_blocks):\n    \"\"\"get pipeline stage module\"\"\"\n\n    def get_layer_to_pipeline(blocks, cfg):\n      if cfg.decoder_block == DecoderBlockType.DEEPSEEK:\n        return blocks[1]  # return the sparse block\n      else:\n        return blocks[0]\n\n    cfg = self.config\n    base_stage = get_layer_to_pipeline(decoder_blocks, cfg)\n    if cfg.set_remat_policy_on_layers_per_stage:\n      policy = self.get_remat_policy()\n      base_stage = self.set_remat_policy([base_stage], policy)[0]\n    if cfg.num_layers_per_pipeline_stage == 1:\n      stage_module = base_stage(config=cfg, mesh=self.mesh, quant=self.quant, model_mode=self.model_mode)\n    elif cfg.scan_layers_per_stage:\n      stage_module = self.scan_decoder_layers(\n          cfg,\n          base_stage,\n          cfg.num_layers_per_pipeline_stage,\n          \"layers_per_stage\",\n          self.mesh,\n          in_axes_tuple=(nn.broadcast,) * 4,\n      )\n    else:\n      stage_module = SequentialBlockDecoderLayers(\n          decoder_layer=base_stage,\n          num_decoder_layers=cfg.num_layers_per_pipeline_stage,\n          config=cfg,\n          mesh=self.mesh,\n          quant=self.quant,\n          model_mode=self.model_mode,\n      )\n    return stage_module\n\n  @nn.compact\n  def _apply_embedding(\n      self,\n      shared_embedding: nn.Module | nnx.Module,\n      decoder_input_tokens,\n      decoder_positions,\n      deterministic,\n      image_embeddings=None,\n      bidirectional_mask=None,\n  ):\n    \"\"\"Applies token and positional embeddings to the input tokens.\"\"\"\n    cfg = self.config\n\n    y = shared_embedding(decoder_input_tokens.astype(\"int32\"), model_mode=self.model_mode)\n\n    # Merge the image embeddings with the text embeddings for multimodal models\n    if image_embeddings is not None and cfg.use_multimodal:\n      if cfg.model_name in [\"gemma3-4b\", \"gemma3-12b\", \"gemma3-27b\", \"llama4-17b-16e\", \"llama4-17b-128e\"]:\n        y = multimodal_utils.merge_mm_embeddings(\n            text_embeddings=y,\n            vision_embeddings=image_embeddings,\n            mask=bidirectional_mask,\n        )\n      # TODO(hengtaoguo): Add support for other multimodal models such as Llama4, refactor if needed\n      else:\n        raise ValueError(f\"Unsupported model_name for multimodal: {cfg.model_name}\")\n\n    y = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(y, deterministic=deterministic)\n    y = y.astype(cfg.dtype)\n\n    if cfg.use_untrainable_positional_embedding:\n      y = positional_embedding_as_linen(embedding_dims=cfg.base_emb_dim)(y, decoder_positions)\n\n    if cfg.trainable_position_size > 0:\n      y += embed_as_linen(\n          num_embeddings=cfg.trainable_position_size,\n          num_features=cfg.emb_dim,\n          dtype=cfg.dtype,\n          embedding_init=nn.initializers.normal(stddev=1.0),\n          name=\"position_embedder\",\n          config=cfg,\n      )(decoder_positions, model_mode=self.model_mode)\n    return y\n\n  @nn.compact\n  def _apply_output_head(self, shared_embedding: nn.Module | nnx.Module, y, deterministic):\n    \"\"\"Applies final normalization and projects hidden states to logits.\"\"\"\n\n    cfg = self.config\n    y = self.get_norm_layer(num_features=y.shape[-1])(\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"decoder_norm\",\n        epsilon=cfg.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n        parameter_memory_host_offload=cfg.parameter_memory_host_offload,\n    )(y)\n    y = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(y, deterministic=deterministic)\n\n    # [batch, length, emb_dim] -> [batch, length, vocab_size]\n    if cfg.logits_via_embedding:\n      # Use the transpose of embedding matrix for logit transform.\n      if isinstance(shared_embedding, nnx.Module):\n        embedding_table = shared_embedding.embedding.value\n      else:\n        embedding_table = shared_embedding.variables[\"params\"][\"embedding\"]\n      if isinstance(embedding_table, nn.spmd.LogicallyPartitioned):\n        embedding_table = embedding_table.unbox()\n      attend_dtype = jnp.float32 if cfg.logits_dot_in_fp32 else cfg.dtype\n      logits = attend_on_embedding(y, embedding_table, attend_dtype, self.config)\n\n      if self.config.normalize_embedding_logits:\n        # Correctly normalize pre-softmax logits for this shared case.\n        logits = logits / jnp.sqrt(y.shape[-1])\n      if cfg.final_logits_soft_cap:\n        logits = logits / cfg.final_logits_soft_cap\n        logits = jnp.tanh(logits) * cfg.final_logits_soft_cap\n    else:\n      logits = linears.dense_general(\n          inputs_shape=y.shape,\n          out_features_shape=cfg.vocab_size,\n          weight_dtype=cfg.weight_dtype,\n          dtype=jnp.float32 if cfg.logits_dot_in_fp32 else cfg.dtype,  # for logit training stability\n          kernel_axes=(\"embed\", \"vocab\"),\n          name=\"logits_dense\",\n          matmul_precision=self.config.matmul_precision,\n          parameter_memory_host_offload=cfg.parameter_memory_host_offload,\n      )(\n          y\n      )  # We do not quantize the logits matmul.\n    if self.model_mode in (MODEL_MODE_PREFILL, MODEL_MODE_AUTOREGRESSIVE):\n      logits = nn.with_logical_constraint(logits, (None, None, \"activation_vocab\"))\n    elif cfg.num_vocab_tiling == 1:\n      logits = nn.with_logical_constraint(\n          logits, (\"activation_embed_and_logits_batch\", \"activation_length_no_exp\", \"activation_vocab\")\n      )\n\n    if self.config.cast_logits_to_fp32:\n      logits = logits.astype(jnp.float32)\n\n    return logits\n\n  @nn.compact\n  def __call__(\n      self,\n      shared_embedding: nn.Module | nnx.Module,\n      decoder_input_tokens,\n      decoder_positions,\n      decoder_segment_ids=None,\n      deterministic=False,\n      previous_chunk=None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      bidirectional_mask: None | Any = None,\n      image_embeddings: None | jnp.ndarray = None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n    assert decoder_input_tokens.ndim == 2  # [batch, len]\n\n    # [batch, length] -> [batch, length, emb_dim]\n    y = self._apply_embedding(\n        shared_embedding, decoder_input_tokens, decoder_positions, deterministic, image_embeddings, bidirectional_mask\n    )\n\n    policy = self.get_remat_policy()\n    RemattedBlockLayers = self.set_remat_policy(self.decoder_layer, policy)\n    # scan does not support kwargs in layer call, passing broadcast_args as positional arg\n    broadcast_args = (\n        decoder_segment_ids,\n        decoder_positions,\n        deterministic,\n        self.model_mode,\n    )\n    if cfg.using_pipeline_parallelism:\n      if cfg.pipeline_fsdp_ag_once:\n        partition_spec = self.pipeline_module.get_weight_sharding(\n            y, decoder_segment_ids, decoder_positions, deterministic, self.model_mode\n        )\n      else:\n        partition_spec = None  # This partition spec is only used for the fsdp_ag_once feature.\n      if cfg.decoder_block == DecoderBlockType.DEEPSEEK:\n        assert len(RemattedBlockLayers) == 2, \"Scanned layers must have a length of 2 using deepseek.\"\n        dense_layer = RemattedBlockLayers[0]\n        moe_layer = RemattedBlockLayers[1]\n        num_moe_layers = cfg.num_decoder_layers - cfg.first_num_dense_layers\n        num_moe_layers_outside_pp = num_moe_layers - self.config.pipeline_parallel_layers\n        logical_axis_rules_pp_as_dp = maxtext_utils.logical_axis_rules_pp_act_as_dp(self.config.logical_axis_rules)\n        # We chose not to pipeline the dense layers, only sparse for SPMD.\n        with self.mesh, nn.partitioning.axis_rules(logical_axis_rules_pp_as_dp):\n          y, _ = self.scan_decoder_layers(\n              cfg,\n              dense_layer,\n              cfg.first_num_dense_layers,\n              \"dense_layers\",\n              mesh,\n              in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n          )(y, *broadcast_args)\n          if num_moe_layers_outside_pp > 0:\n            y, _ = self.scan_decoder_layers(\n                cfg,\n                moe_layer,\n                num_moe_layers_outside_pp,\n                \"moe_layers\",\n                mesh,\n                in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n            )(y, *broadcast_args)\n        y = self.pipeline_module(y, *broadcast_args, partition_spec=partition_spec)\n      else:  # Not DeepSeek\n        y = self.pipeline_module(y, *broadcast_args, partition_spec=partition_spec)\n        remaining_layers = self.config.num_decoder_layers - self.config.pipeline_parallel_layers\n        if remaining_layers > 0:\n          logical_axis_rules_pp_as_dp = maxtext_utils.logical_axis_rules_pp_act_as_dp(self.config.logical_axis_rules)\n          with self.mesh, nn.partitioning.axis_rules(logical_axis_rules_pp_as_dp):\n            y, _ = self.scan_decoder_layers(\n                cfg,\n                RemattedBlockLayers[0],\n                remaining_layers,\n                \"layers_outside_pipeline\",\n                mesh,\n                in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n            )(y, *broadcast_args)\n    else:\n      if cfg.scan_layers:\n        if cfg.decoder_block == DecoderBlockType.DEEPSEEK:\n          assert len(RemattedBlockLayers) == 2, \"Scanned layers must have a length of 2 using deepseek.\"\n          layer_call_kwargs = {\n              \"page_state\": page_state,\n              \"previous_chunk\": previous_chunk,\n              \"slot\": slot,\n          }\n          dense_layer = RemattedBlockLayers[0]\n          dense_layer.__call__ = functools.partial(dense_layer.__call__, **layer_call_kwargs)\n          y, _ = self.scan_decoder_layers(\n              cfg,\n              dense_layer,\n              cfg.first_num_dense_layers,\n              \"dense_layers\",\n              mesh,\n              in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n          )(y, *broadcast_args)\n          moe_layer = RemattedBlockLayers[1]\n          moe_layer.__call__ = functools.partial(moe_layer.__call__, **layer_call_kwargs)\n          num_moe_layers = cfg.num_decoder_layers - cfg.first_num_dense_layers\n          y, _ = self.scan_decoder_layers(\n              cfg,\n              moe_layer,\n              num_moe_layers,\n              \"moe_layers\",\n              mesh,\n              in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n          )(y, *broadcast_args)\n        elif cfg.decoder_block == DecoderBlockType.GEMMA3:\n          y = self._apply_gemma3_scanned_blocks(\n              y,\n              decoder_segment_ids,\n              decoder_positions,\n              deterministic,\n              bidirectional_mask,\n              previous_chunk,\n              page_state,\n              slot,\n          )\n        else:\n          RemattedBlockLayer = RemattedBlockLayers[0]\n          scan_length = int(cfg.num_decoder_layers / cfg.inhomogeneous_layer_cycle_interval)\n          layer_kwargs = {}\n          if cfg.decoder_block == DecoderBlockType.LLAMA4:\n            layer_kwargs = {\n                \"nope_layer_interval\": self.config.nope_layer_interval,\n                \"interleave_moe_layer_step\": self.config.interleave_moe_layer_step,\n            }\n            broadcast_args += (bidirectional_mask,)\n          y, _ = self.scan_decoder_layers(\n              cfg,\n              RemattedBlockLayer,\n              scan_length,\n              \"layers\",\n              mesh,\n              in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n              **layer_kwargs,\n          )(y, *broadcast_args)\n      else:\n        if cfg.decoder_block == DecoderBlockType.DEEPSEEK:\n          assert len(RemattedBlockLayers) == 2, \"Unscanned layers must have a length of 2 using deepseek.\"\n          dense_layer = RemattedBlockLayers[0]\n          moe_layer = RemattedBlockLayers[1]\n\n          layers = [dense_layer, moe_layer]\n          layer_prefixes = [\"dense_layers\", \"moe_layers\"]\n          num_moe_layers = cfg.num_decoder_layers - cfg.first_num_dense_layers\n          num_layers_list = [cfg.first_num_dense_layers, num_moe_layers]\n          # Iterate over the two layer groups (dense and MoE) and apply layer transformation\n          for layer, num_layers, layer_prefix in zip(layers, num_layers_list, layer_prefixes):\n            for index in range(num_layers):\n              y = layer(\n                  config=cfg, mesh=mesh, name=f\"{layer_prefix}_{index}\", quant=self.quant, model_mode=self.model_mode\n              )(\n                  y,\n                  decoder_segment_ids,\n                  decoder_positions,\n                  deterministic,\n                  self.model_mode,\n                  previous_chunk=previous_chunk,\n                  page_state=page_state,\n                  slot=slot,\n              )\n        else:\n          for lyr in range(cfg.num_decoder_layers):\n            RemattedBlockLayer = RemattedBlockLayers[0]\n            layer_kwargs = {}\n            layer_call_kwargs = {}\n            if cfg.decoder_block == DecoderBlockType.GEMMA3:\n              # Gemma3 uses both global and sliding window attention depending on the layer index.\n              layer_kwargs = {\"attention_type\": gemma3.get_attention_type(layer_id=lyr)}\n              layer_call_kwargs = {\"bidirectional_mask\": bidirectional_mask}\n            if cfg.decoder_block == DecoderBlockType.LLAMA4:\n              layer_kwargs = {\n                  \"is_nope_layer\": llama4.determine_is_nope_layer(lyr, self.config.nope_layer_interval),\n                  \"is_moe_layer\": llama4.determine_is_moe_layer(lyr, self.config.interleave_moe_layer_step),\n              }\n              layer_call_kwargs = {\"bidirectional_mask\": bidirectional_mask}\n            if cfg.decoder_block == DecoderBlockType.GPT_OSS:\n              layer_kwargs = {\"attention_type\": gpt_oss.get_attention_type(layer_id=lyr)}\n            layer = RemattedBlockLayer(\n                config=cfg, mesh=mesh, name=f\"layers_{lyr}\", quant=self.quant, model_mode=self.model_mode, **layer_kwargs\n            )\n            y = layer(\n                y,\n                decoder_segment_ids,\n                decoder_positions,\n                deterministic,\n                self.model_mode,\n                previous_chunk=previous_chunk,\n                page_state=page_state,\n                slot=slot,\n                **layer_call_kwargs,\n            )\n\n    assert isinstance(y, jax.Array)\n\n    # After the final transformer layer, `y` holds the raw, un-normalized hidden state.\n    hidden_state = y\n\n    # When vocab tiling is enabled in training mode, full logits won't generate to reduce memory\n    # Instead, we keep track on the hidden states, which has smaller size compared to full logits\n    if cfg.num_vocab_tiling > 1 and self.model_mode == MODEL_MODE_TRAIN:\n      logits = None\n      self.sow('intermediates', 'hidden_states', hidden_state)\n    else:\n      logits = self._apply_output_head(shared_embedding, hidden_state, deterministic)\n\n    # The API of the Decoder is now a tuple, providing both the main output\n    # and the raw hidden state needed for auxiliary tasks.\n    return logits, hidden_state\n\n  def _apply_gemma3_scanned_blocks(\n      self,\n      y,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      bidirectional_mask,\n      previous_chunk,\n      page_state,\n      slot,\n  ):\n    \"\"\"Applies Gemma3 scanned decoder blocks, handling main scan and remainders.\"\"\"\n\n    cfg = self.config\n    mesh = self.mesh\n\n    # Define the repeating pattern length and calculate how many full blocks to scan\n    attention_pattern_length = len(gemma3.GEMMA3_ATTENTION_PATTERN)\n    scan_length = cfg.num_decoder_layers // attention_pattern_length\n\n    policy = self.get_remat_policy()\n    RemattedGemma3Block = self.set_remat_policy([gemma3.Gemma3ScannableBlock], policy)[0]\n\n    layer_call_kwargs = {\"bidirectional_mask\": bidirectional_mask}\n    layer_kwargs = {\"num_of_layers\": attention_pattern_length}\n\n    # Apply the main scan over the full blocks\n    if scan_length > 0:\n      broadcast_args = (\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          self.model_mode,\n      )\n      y, _ = self.scan_decoder_layers(\n          cfg,\n          RemattedGemma3Block,\n          scan_length,\n          \"layers\",\n          mesh,\n          in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n          **layer_kwargs,\n      )(y, *broadcast_args, **layer_call_kwargs)\n\n    # Apply any remaining layers that did not fit into a full scanned block\n    num_remaining_layers = cfg.num_decoder_layers % attention_pattern_length\n    if num_remaining_layers > 0:\n      # We name the remainder block with a 'remainder' suffix to avoid parameter name collisions\n      rem_layer_kwargs = {\"num_of_layers\": num_remaining_layers}\n      layer = RemattedGemma3Block(\n          config=cfg, mesh=mesh, quant=self.quant, model_mode=self.model_mode, name=\"layers_remainder\", **rem_layer_kwargs\n      )\n      y, _ = layer(\n          y,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          self.model_mode,\n          previous_chunk=previous_chunk,\n          page_state=page_state,\n          slot=slot,\n          **layer_call_kwargs,\n      )\n    return y",
        "analysis": {
            "module_type": "transformer_decoder",
            "purpose": "A configurable stack of transformer decoder layers that processes input tokens, applies self-attention and MLP blocks, and produces logits.",
            "input": {
                "shape": "decoder_input_tokens: [batch_size, sequence_length]",
                "dtype": "int32"
            },
            "processing_steps": [
                "Calls `_apply_embedding` to convert input tokens to embeddings, optionally merging multimodal image embeddings.",
                "Retrieves and applies a rematerialization (gradient checkpointing) policy to the decoder layers.",
                "Processes the embeddings through a series of decoder layers, handling different execution strategies like layer scanning, pipeline parallelism, or sequential execution.",
                "Calls `_apply_output_head` to apply a final normalization and project the final hidden states to vocabulary logits.",
                "Returns the final logits and the last hidden state."
            ],
            "output": {
                "shape": "A tuple of (logits, hidden_state), where logits is [batch_size, sequence_length, vocab_size] and hidden_state is [batch_size, sequence_length, emb_dim]. Logits can be None if using vocabulary tiling during training."
            },
            "dependencies": [
                "flax.linen as nn",
                "jax",
                "Config",
                "Mesh",
                "Quant",
                "DecoderLayer (and other variants like gemma3.Gemma3DecoderLayer)",
                "pipeline.Pipeline",
                "rms_norm",
                "linears.dense_general",
                "attend_on_embedding"
            ],
            "parameters": {
                "config.decoder_block": "Specifies the type of decoder layer to use (e.g., DEFAULT, LLAMA2, GEMMA3).",
                "config.num_decoder_layers": "The total number of decoder layers in the stack.",
                "config.scan_layers": "Boolean flag to determine whether to use `nn.scan` to execute the layers, which can save memory.",
                "config.using_pipeline_parallelism": "Boolean flag to enable pipeline parallelism across multiple devices.",
                "config.remat_policy": "String specifying the gradient checkpointing (rematerialization) policy to use for memory optimization.",
                "config.logits_via_embedding": "Boolean flag to tie the input token embeddings with the final output projection layer."
            },
            "notes": [
                "This module is highly configurable and supports various model architectures (e.g., Llama, Gemma, DeepSeek) by dynamically selecting the appropriate decoder layer implementation.",
                "It incorporates advanced parallelism strategies, including pipeline parallelism and scanned layers for memory efficiency.",
                "The logic for applying decoder layers is complex, with special cases for different model types and execution configurations (e.g., scanned vs. unscanned, pipelined vs. not).",
                "During training with vocabulary tiling (`config.num_vocab_tiling > 1`), it does not compute the full logits to save memory, returning `None` for logits and storing the final hidden state in Flax's 'intermediates' collection."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the decoder's components, including the specific decoder layer type, normalization layer, and the pipeline module if pipeline parallelism is enabled.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `get_decoder_layers` to select the appropriate layer class based on the config.",
                        "Calls `get_norm_layer` to get the final normalization function.",
                        "If `config.using_pipeline_parallelism` is true, initializes a `pipeline.Pipeline` module."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "get_decoder_layers",
                        "get_norm_layer",
                        "get_pipeline_stage_module",
                        "pipeline.Pipeline"
                    ],
                    "notes": [
                        "This is a standard Flax setup method called during module instantiation."
                    ]
                },
                "get_remat_policy": {
                    "purpose": "Selects and returns a JAX gradient checkpointing policy based on the `config.remat_policy` string.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks the `config.remat_policy` string against a list of known policies.",
                        "Constructs and returns the corresponding policy object from `jax.checkpoint_policies`."
                    ],
                    "output": {
                        "shape": "A JAX checkpoint policy object or None."
                    },
                    "dependencies": [
                        "jax.checkpoint_policies"
                    ],
                    "notes": [
                        "Supports a wide range of policies, including saving specific activations, offloading activations to host memory, or no checkpointing."
                    ]
                },
                "get_decoder_layers": {
                    "purpose": "Retrieves a list of decoder layer classes based on the `config.decoder_block` setting.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Uses a `match/case` statement on `self.config.decoder_block` to select the appropriate module(s)."
                    ],
                    "output": {
                        "shape": "A list containing one or more `nn.Module` classes for the decoder."
                    },
                    "dependencies": [
                        "DecoderBlockType",
                        "DecoderLayer",
                        "llama2.LlamaDecoderLayer",
                        "gemma3.Gemma3DecoderLayer",
                        "deepseek.DeepSeekDenseLayer",
                        "etc."
                    ],
                    "notes": [
                        "This method makes the Decoder class adaptable to many different transformer architectures."
                    ]
                },
                "__call__": {
                    "purpose": "Executes the forward pass of the entire decoder stack.",
                    "input": {
                        "shape": "decoder_input_tokens: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length], and other optional arguments for masking and state management.",
                        "dtype": "decoder_input_tokens: int32"
                    },
                    "processing_steps": [
                        "Calls `_apply_embedding` to get initial embeddings.",
                        "Applies the stack of decoder layers using the configured strategy (pipeline, scan, or sequential loop).",
                        "Calls `_apply_output_head` to compute logits from the final hidden state."
                    ],
                    "output": {
                        "shape": "A tuple of (logits, hidden_state), where logits is [batch_size, sequence_length, vocab_size] and hidden_state is [batch_size, sequence_length, emb_dim]."
                    },
                    "dependencies": [
                        "_apply_embedding",
                        "get_remat_policy",
                        "set_remat_policy",
                        "scan_decoder_layers",
                        "_apply_output_head"
                    ],
                    "notes": [
                        "This is the main entry point for the module, orchestrating all sub-components."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/deepseek.py#self_attention_with_norm",
        "file_path": "src/MaxText/layers/deepseek.py",
        "code_block": "def self_attention_with_norm(\n    inputs,\n    cfg,\n    mesh,\n    quant,\n    decoder_segment_ids,\n    decoder_positions,\n    deterministic,\n    model_mode,\n    previous_chunk=None,\n    page_state: None | page_manager.PageState = None,\n    slot: None | int = None,\n):\n  \"\"\"self-attention with normalization\"\"\"\n  # Normalization\n  lnx_rms = rms_norm(\n      num_features=inputs.shape[-1],\n      dtype=cfg.dtype,\n      weight_dtype=cfg.weight_dtype,\n      name=\"pre_self_attention_layer_norm\",\n      kernel_axes=(\"norm\",),\n      epsilon=cfg.normalization_layer_epsilon,\n  )\n  lnx = lnx_rms(inputs)\n  if model_mode == MODEL_MODE_PREFILL:\n    logical_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n  else:\n    logical_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n  lnx = nn.with_logical_constraint(lnx, logical_axis_names)\n\n  attention_layer = attention_mla.mla_as_linen(\n      config=cfg,\n      num_query_heads=cfg.num_query_heads,\n      num_kv_heads=cfg.num_kv_heads,\n      head_dim=cfg.head_dim,\n      max_target_length=cfg.max_target_length,\n      max_prefill_predict_length=cfg.max_prefill_predict_length,\n      attention_kernel=cfg.attention,\n      attention_type=cfg.attention_type,\n      inputs_q_shape=lnx.shape,\n      inputs_kv_shape=lnx.shape,\n      mesh=mesh,\n      dtype=cfg.dtype,\n      weight_dtype=cfg.weight_dtype,\n      dropout_rate=cfg.dropout_rate,\n      name=\"self_attention\",\n      quant=quant,\n      kv_quant=quantizations.configure_kv_quant(cfg),\n      q_lora_rank=cfg.q_lora_rank,\n      kv_lora_rank=cfg.kv_lora_rank,\n      qk_nope_head_dim=cfg.qk_nope_head_dim,\n      qk_rope_head_dim=cfg.qk_rope_head_dim,\n      v_head_dim=cfg.v_head_dim,\n      max_position_embeddings=cfg.max_position_embeddings,\n      original_max_position_embeddings=cfg.original_max_position_embeddings,\n      mscale=cfg.mscale,\n      rope_factor=cfg.rope_factor,\n      model_mode=model_mode,\n  )\n\n  attention_lnx = attention_layer(\n      lnx,\n      lnx,\n      decoder_positions,\n      decoder_segment_ids=decoder_segment_ids,\n      deterministic=deterministic,\n      model_mode=model_mode,\n      previous_chunk=previous_chunk,\n      page_state=page_state,\n      slot=slot,\n  )\n\n  attention_lnx = nn.with_logical_constraint(attention_lnx, logical_axis_names)\n  intermediate_inputs = inputs + attention_lnx\n\n  # Normalization\n  hidden_states = rms_norm(\n      num_features=intermediate_inputs.shape[-1],\n      dtype=cfg.dtype,\n      weight_dtype=cfg.weight_dtype,\n      name=\"post_self_attention_layer_norm\",\n      kernel_axes=(\"norm\",),\n      epsilon=cfg.normalization_layer_epsilon,\n  )(intermediate_inputs)\n  hidden_states = nn.with_logical_constraint(hidden_states, logical_axis_names)\n  return hidden_states, intermediate_inputs",
        "analysis": {
            "module_type": "self_attention_with_normalization",
            "purpose": "Performs self-attention on an input tensor, applying RMS normalization before and after the attention mechanism, with a residual connection.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "Configurable via cfg.dtype (e.g., float32, bfloat16)."
            },
            "processing_steps": [
                "Apply pre-attention RMS normalization to the input tensor.",
                "Apply a logical axis constraint to the normalized tensor.",
                "Initialize and execute a multi-head latent attention layer (`attention_mla.mla_as_linen`).",
                "Add the attention output to the original input tensor via a residual connection.",
                "Apply post-attention RMS normalization to the result of the residual connection.",
                "Apply a final logical axis constraint to the output.",
                "Return the final normalized hidden states and the intermediate tensor from the residual connection."
            ],
            "output": {
                "shape": "A tuple of two tensors, both with shape [batch_size, sequence_length, hidden_dim]."
            },
            "dependencies": [
                "MaxText.layers.normalizations.rms_norm",
                "MaxText.layers.attention_mla.mla_as_linen",
                "flax.linen.with_logical_constraint",
                "MaxText.inference.page_manager.PageState"
            ],
            "parameters": {
                "inputs": "The input tensor to the layer.",
                "cfg": "The model configuration object containing hyperparameters like dtype, head dimensions, dropout rate, etc.",
                "mesh": "The JAX device mesh for distributed computation.",
                "quant": "Quantization configuration for the attention layer.",
                "decoder_segment_ids": "Segment IDs used for attention masking.",
                "decoder_positions": "Positional information for the input tokens.",
                "deterministic": "A boolean flag to disable dropout during inference.",
                "model_mode": "A string indicating the operational mode (e.g., 'prefill'), which affects sharding annotations.",
                "page_state": "State object for paged attention during inference.",
                "slot": "The current slot index for paged attention."
            },
            "notes": [
                "This function implements a Pre-LN (Layer Normalization before attention) architecture.",
                "It returns two tensors: the final output of the block (`hidden_states`) and the result of the first residual connection (`intermediate_inputs`), which is intended to be passed to a subsequent MLP block.",
                "The logical axis names for sharding constraints are dynamically set based on the `model_mode`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/deepseek.py#post_process",
        "file_path": "src/MaxText/layers/deepseek.py",
        "code_block": "def post_process(cfg, layer_output, sow):\n  \"\"\"postprocessing.\"\"\"\n  if cfg.record_internal_nn_metrics:\n    sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n    sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n    sow(\n        \"intermediates\",\n        \"activation_fraction_zero\",\n        jnp.sum(layer_output == 0) / jnp.size(layer_output),\n    )\n\n  if cfg.scan_layers:\n    return layer_output, None\n  else:\n    return layer_output",
        "analysis": {
            "functionality": "This function performs post-processing on a layer's output. It optionally records activation statistics (mean, standard deviation, and fraction of zeros) and formats the output to be compatible with Flax's `nn.scan` if required.",
            "usage": "Call this function at the end of a neural network layer. Provide the configuration object `cfg`, the layer's output tensor `layer_output`, and a `sow` function for recording metrics. The function's return value is either the `layer_output` tensor directly, or a tuple `(layer_output, None)` if `cfg.scan_layers` is true."
        }
    },
    {
        "block_name": "src/MaxText/layers/deepseek.py#DeepSeekDenseLayer",
        "file_path": "src/MaxText/layers/deepseek.py",
        "code_block": "class DeepSeekDenseLayer(nn.Module):\n  \"\"\"DeepSeek-style dense layer with Multi-Head Latent Attention.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    cfg = self.config\n    if model_mode == MODEL_MODE_PREFILL:\n      logical_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n    else:\n      logical_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    inputs = nn.with_logical_constraint(inputs, logical_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n\n    hidden_states, intermediate_inputs = self_attention_with_norm(\n        inputs,\n        cfg,\n        self.mesh,\n        self.quant,\n        decoder_segment_ids,\n        decoder_positions,\n        deterministic,\n        model_mode,\n        previous_chunk,\n        page_state,\n        slot,\n    )\n    mlp_lnx = linears.mlp_block(\n        in_features=hidden_states.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp\",\n        config=cfg,\n        quant=self.quant,\n    )(hidden_states, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, logical_axis_names)\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        logical_axis_names,\n    )\n    return post_process(cfg, layer_output, self.sow)",
        "analysis": {
            "module_type": "deepseek_dense_layer",
            "purpose": "Implements a single dense decoder layer for a DeepSeek-style transformer model, consisting of a self-attention block followed by a feed-forward MLP block.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "Inferred from config (e.g., float32)"
            },
            "processing_steps": [
                "Apply logical constraints to the input tensor based on the `model_mode`.",
                "Apply a JAX checkpoint name to the input tensor for debugging and profiling.",
                "Execute the self-attention block with pre-normalization via `self_attention_with_norm`, which returns the normalized hidden states and the intermediate inputs (input + attention output).",
                "Process the normalized hidden states through an MLP block (`linears.mlp_block`).",
                "Apply a residual connection by adding the MLP output to the intermediate inputs.",
                "Apply dropout to the result of the residual connection.",
                "Apply final logical constraints to the layer output.",
                "Return the final output after passing through the `post_process` function, which may record metrics."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]"
            },
            "dependencies": [
                "nn.Module",
                "self_attention_with_norm",
                "linears.mlp_block",
                "post_process",
                "nn.Dropout",
                "jax.ad_checkpoint.checkpoint_name",
                "Config",
                "Mesh",
                "Quant"
            ],
            "parameters": {
                "config": "A Config object containing model hyperparameters like mlp_dim, mlp_activations, dropout_rate, dtype, etc.",
                "mesh": "The JAX device mesh for parallel computation.",
                "model_mode": "A string indicating the operational mode, e.g., 'prefill' or 'decode'.",
                "quant": "An optional quantization configuration object."
            },
            "notes": [
                "This layer follows a pre-normalization architecture where layer normalization is applied before both the self-attention and MLP sub-layers.",
                "The residual connections are applied after the attention and MLP blocks.",
                "The `model_mode` parameter influences the logical axis names used for tensor sharding annotations."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Performs the forward pass of the DeepSeek dense decoder layer.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim]",
                        "dtype": "Inferred from config (e.g., float32)"
                    },
                    "processing_steps": [
                        "Apply logical constraints to the input tensor based on the `model_mode`.",
                        "Apply a JAX checkpoint name to the input tensor.",
                        "Call `self_attention_with_norm` to perform self-attention with pre-normalization.",
                        "Call `linears.mlp_block` to process the output of the attention block.",
                        "Add the MLP output to the intermediate input from the attention block (residual connection).",
                        "Apply dropout.",
                        "Apply final logical constraints.",
                        "Call `post_process` before returning the final output."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "self_attention_with_norm",
                        "linears.mlp_block",
                        "post_process",
                        "nn.Dropout",
                        "jax.ad_checkpoint.checkpoint_name",
                        "page_manager.PageState"
                    ],
                    "notes": [
                        "Accepts multiple arguments for attention mechanism context, including `decoder_segment_ids` and `decoder_positions`.",
                        "Supports inference-specific state management through `previous_chunk`, `page_state`, and `slot`.",
                        "The `deterministic` flag controls whether dropout is applied."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/deepseek.py#DeepSeekMoELayer",
        "file_path": "src/MaxText/layers/deepseek.py",
        "code_block": "class DeepSeekMoELayer(nn.Module):\n  \"\"\"DeepSeek-style MoE layer with Multi-Head Latent Attention.\n  Supports dropless and dropping base on configs.\n  Uses a bias in routing instead of load balancing loss.\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    cfg = self.config\n    if model_mode == MODEL_MODE_PREFILL:\n      logical_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n    else:\n      logical_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    inputs = nn.with_logical_constraint(inputs, logical_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n\n    hidden_states, intermediate_inputs = self_attention_with_norm(\n        inputs,\n        self.config,\n        self.mesh,\n        self.quant,\n        decoder_segment_ids,\n        decoder_positions,\n        deterministic,\n        model_mode,\n        previous_chunk,\n        page_state,\n        slot,\n    )\n\n    # NOTE: the naming mismatch here is to ensure reverse compatibility with existing checkpoints.\n    # The `name` represents the weight name in JAX/checkpoints and so the class name\n    # is just for readability.\n    mlp_lnx = moe.get_routed_and_shared_moe(\n        name=\"DeepSeekMoeBlock_0\",\n        config=cfg,\n        mesh=self.mesh,\n        kernel_init=initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        quant=self.quant,\n    )(hidden_states)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, logical_axis_names)\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        logical_axis_names,\n    )\n    return post_process(cfg, layer_output, self.sow)",
        "analysis": {
            "module_type": "deepseek_moe_decoder_layer",
            "purpose": "Implements a single decoder layer for a DeepSeek-style transformer model, combining multi-head latent self-attention with a Mixture-of-Experts (MoE) block.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "config.dtype"
            },
            "processing_steps": [
                "Applies logical constraints and checkpointing to the input tensor.",
                "Performs self-attention with pre and post layer normalization.",
                "Passes the output of the attention block through a Mixture-of-Experts (MoE) layer.",
                "Adds a residual connection from the input of the MoE block.",
                "Applies dropout.",
                "Performs final post-processing, which may include recording metrics."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]"
            },
            "dependencies": [
                "flax.linen.Module",
                "self_attention_with_norm",
                "MaxText.layers.moe.get_routed_and_shared_moe",
                "post_process",
                "MaxText.common_types.Config",
                "jax.sharding.Mesh",
                "MaxText.layers.quantizations.AqtQuantization"
            ],
            "parameters": {
                "config": "A Config object containing model hyperparameters like dropout_rate, dtype, weight_dtype, and MoE configurations.",
                "mesh": "A JAX Mesh object for model parallelism.",
                "model_mode": "A string indicating the operational mode, e.g., 'prefill' or 'decode'.",
                "quant": "An optional Quant object for quantization."
            },
            "notes": [
                "This layer is a building block for a larger transformer decoder.",
                "The MoE implementation uses a bias in routing instead of a load balancing loss.",
                "The MoE block is hardcoded with the name 'DeepSeekMoeBlock_0' to ensure reverse compatibility with existing checkpoints."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Performs the forward pass of the decoder layer, applying self-attention and a Mixture-of-Experts block.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim]",
                        "dtype": "config.dtype"
                    },
                    "processing_steps": [
                        "Apply logical constraints and checkpointing to the input tensor.",
                        "Call `self_attention_with_norm` to get attention output (`hidden_states`) and the input for the second residual connection (`intermediate_inputs`).",
                        "Pass `hidden_states` through the `moe.get_routed_and_shared_moe` block to get `mlp_lnx`.",
                        "Add the MoE output to `intermediate_inputs` via a residual connection.",
                        "Apply dropout to the result.",
                        "Apply final logical constraints.",
                        "Return the output after passing through the `post_process` function."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "self_attention_with_norm",
                        "moe.get_routed_and_shared_moe",
                        "nn.with_logical_constraint",
                        "checkpoint_name",
                        "nn.Dropout",
                        "post_process",
                        "initializers.nd_dense_init"
                    ],
                    "notes": [
                        "Handles different logical axis naming for prefill vs. other modes.",
                        "Accepts optional arguments for stateful decoding (`previous_chunk`, `page_state`, `slot`)."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#_maybe_move_embedding_to_device",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def _maybe_move_embedding_to_device(embedding_table: Array, config: Config) -> Array:\n  \"\"\"Moves embedding table to device if parameter offloading is enabled.\"\"\"\n  if config.parameter_memory_host_offload:\n    max_logging.log(\"embeddings.py: Moving embedding parameter to device\")\n    return jax.device_put(embedding_table, max_utils.device_space())\n  return embedding_table",
        "analysis": {
            "functionality": "Conditionally moves an embedding table tensor to the accelerator device based on a configuration flag. If `config.parameter_memory_host_offload` is true, the function uses `jax.device_put` to transfer the array; otherwise, it returns the original array.",
            "usage": "This function is used to manage memory by offloading large parameters like embedding tables to host memory and only moving them to the device when needed for computation. Call it with the embedding table array and the model's configuration object. It returns the embedding table, which will be located on the accelerator device if offloading is enabled."
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#embed_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def embed_as_linen(\n    *,\n    num_embeddings: int,\n    num_features: int,\n    config: Config,\n    cast_input_dtype: None | DType = None,\n    dtype: DType = jnp.float32,\n    attend_dtype: None | DType = None,\n    embedding_init: Initializer = default_embed_init,\n    name: str | None = None,\n):\n  \"\"\"Initializes the Embed NNX module and returns it as a Linen module.\n\n  This function serves as a bridge to use the NNX-based `Embed` module within\n  a Linen model. It wraps the `Embed` module using `nnx.bridge.to_linen`,\n  making it compatible with the Linen API.\n\n  Args:\n    num_embeddings: The number of embeddings.\n    num_features: The number of feature dimensions for each embedding.\n    config: The model configuration.\n    cast_input_dtype: The dtype to cast the input to, if any.\n    dtype: The dtype of the embedding vectors.\n    attend_dtype: The dtype for the `attend` method.\n    embedding_init: The initializer for the embedding matrix.\n    name: The name of the Linen module.\n\n  Returns:\n    A Linen module that wraps the NNX `Embed` module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      Embed,\n      num_embeddings=num_embeddings,\n      num_features=num_features,\n      config=config,\n      cast_input_dtype=cast_input_dtype,\n      dtype=dtype,\n      attend_dtype=attend_dtype,\n      embedding_init=embedding_init,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n  )",
        "analysis": {
            "functionality": "This function is a factory that initializes the NNX-based `Embed` module and wraps it to be compatible with the Flax Linen API. It serves as a bridge, allowing an NNX module to be used within a Linen model.",
            "usage": "Call this function with embedding configuration parameters (like `num_embeddings`, `num_features`, `config`, etc.) to get a Linen-compatible embedding layer. The returned layer can then be used in a `nn.setup` method of a Linen Module. The returned module expects integer token IDs as input and outputs their corresponding embedding vectors."
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#Embed",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class Embed(nnx.Module):\n  \"\"\"A parameterized function from integers [0, n) to d-dimensional vectors.\"\"\"\n\n  def __init__(\n      self,\n      num_embeddings: int,\n      num_features: int,\n      config: Config,\n      cast_input_dtype: None | DType = None,\n      dtype: DType = jnp.float32,\n      attend_dtype: None | DType = None,\n      embedding_init: Initializer = default_embed_init,\n      *,\n      # Not used in Embed but passed in by nnx.bridge.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs,\n  ):\n    \"\"\"Initializes the Embed module.\n\n    Args:\n      num_embeddings: The number of embeddings.\n      num_features: The number of feature dimensions for each embedding.\n      config: The model configuration.\n      cast_input_dtype: The dtype to cast the input to, if any.\n      dtype: The dtype of the embedding vectors.\n      attend_dtype: The dtype for the `attend` method.\n      embedding_init: The initializer for the embedding matrix.\n      rngs: The random number generators for initialization.\n    \"\"\"\n    self.num_embeddings = num_embeddings\n    self.num_features = num_features\n    self.config = config\n    self.cast_input_dtype = cast_input_dtype\n    self.dtype = dtype\n    self.attend_dtype = attend_dtype\n\n    self.embedding = nnx.Param(\n        embedding_init(rngs.params(), (self.num_embeddings, self.num_features), self.config.weight_dtype),\n        sharding=(\"vocab\", \"embed\"),\n    )\n\n  def __call__(self, inputs: Array, model_mode: str = MODEL_MODE_TRAIN) -> Array:\n    \"\"\"Embeds the inputs along the last dimension.\n\n    Args:\n      inputs: input data, all dimensions are considered batch dimensions.\n\n    Returns:\n      Output which is embedded input data.  The output shape follows the input,\n      with an additional `num_features` dimension appended.\n    \"\"\"\n    cfg = self.config\n    if self.cast_input_dtype:\n      inputs = inputs.astype(self.cast_input_dtype)\n    if not jnp.issubdtype(inputs.dtype, jnp.integer):\n      raise ValueError(\"Input type must be an integer or unsigned integer.\")\n\n    embedding = _maybe_move_embedding_to_device(self.embedding.value, self.config)\n\n    if cfg.use_iota_embed:\n      iota = lax.iota(jnp.int32, self.num_embeddings)\n      one_hot = jnp.array(inputs[..., jnp.newaxis] == iota, dtype=self.dtype)\n      output = jnp.dot(one_hot, jnp.asarray(embedding, self.dtype))\n    else:\n      output = jnp.asarray(embedding, self.dtype)[inputs]\n\n    output_prefill_axis_names = (\"activation_embed_and_logits_batch\", \"prefill_activation_length\", \"activation_embed\")\n    output_default_axis_names = (\"activation_embed_and_logits_batch\", \"activation_length_no_exp\", \"activation_embed\")\n\n    if model_mode == MODEL_MODE_PREFILL:\n      output = nn.with_logical_constraint(output, output_prefill_axis_names)\n    else:\n      output = nn.with_logical_constraint(output, output_default_axis_names)\n    return output\n\n  def attend(self, query: Array) -> Array:\n    \"\"\"Attend over the embedding using a query array.\n\n    Args:\n      query: array with last dimension equal the feature depth `num_features` of the\n        embedding.\n\n    Returns:\n      An array with final dim `num_embeddings` corresponding to the batched\n      inner-product of the array of query vectors against each embedding.\n      Commonly used for weight-sharing between embeddings and logit transform\n      in NLP models.\n    \"\"\"\n    embedding = self.embedding.value\n    attend_dtype = self.attend_dtype if self.attend_dtype is not None else self.dtype\n    return attend_on_embedding(query, embedding, attend_dtype, self.config)",
        "analysis": {
            "module_type": "embedding_layer",
            "purpose": "A parameterized module that maps integer indices from a vocabulary to dense d-dimensional vector representations.",
            "input": {
                "shape": "[..., sequence_length]",
                "dtype": "integer"
            },
            "processing_steps": [
                "Initializes an embedding matrix of shape [num_embeddings, num_features] with a specified sharding.",
                "When called, it looks up the embedding vectors corresponding to the integer indices in the input tensor.",
                "The lookup can be performed either by direct indexing or by a one-hot encoding followed by a dot product, controlled by `config.use_iota_embed`.",
                "Applies logical constraints to the output tensor for sharding annotations, with different constraints for 'prefill' and other modes."
            ],
            "output": {
                "shape": "[..., sequence_length, num_features]"
            },
            "dependencies": [
                "nnx.Module",
                "nnx.Param",
                "nnx.Rngs",
                "jax.numpy",
                "flax.linen.nn",
                "_maybe_move_embedding_to_device",
                "attend_on_embedding"
            ],
            "parameters": {
                "num_embeddings": "The size of the vocabulary (number of possible integer inputs).",
                "num_features": "The dimensionality of the output embedding vectors.",
                "config": "The model configuration object, containing settings like `use_iota_embed`, `weight_dtype`, and `parameter_memory_host_offload`.",
                "dtype": "The data type of the embedding vectors.",
                "attend_dtype": "The data type used for the `attend` method's computation.",
                "embedding_init": "The initializer function for the embedding matrix."
            },
            "notes": [
                "The `rngs` parameter in `__init__` is a temporary requirement for compatibility with `nnx.bridge.to_linen` and is not used by the module's core logic.",
                "The module includes an `attend` method, commonly used for weight-sharing between the input embedding and the final logit transformation in language models."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Looks up the embedding vectors for the given integer input indices.",
                    "input": {
                        "shape": "[...]",
                        "dtype": "integer"
                    },
                    "processing_steps": [
                        "Optionally cast the input tensor to `cast_input_dtype`.",
                        "Validate that the input tensor is of an integer subtype.",
                        "Retrieve the embedding table, potentially moving it to the device via `_maybe_move_embedding_to_device`.",
                        "If `config.use_iota_embed` is true, compute embeddings via one-hot encoding and a dot product.",
                        "Otherwise, compute embeddings by directly indexing into the embedding table.",
                        "Apply logical constraints to the output tensor using `nn.with_logical_constraint` based on the `model_mode`."
                    ],
                    "output": {
                        "shape": "[..., num_features]"
                    },
                    "dependencies": [
                        "_maybe_move_embedding_to_device",
                        "jax.numpy",
                        "jax.lax",
                        "flax.linen.nn.with_logical_constraint"
                    ],
                    "notes": [
                        "The specific logical constraints applied depend on whether `model_mode` is `MODEL_MODE_PREFILL` or not."
                    ]
                },
                "attend": {
                    "purpose": "Computes the dot product of a query tensor with the embedding matrix, typically used for generating output logits.",
                    "input": {
                        "shape": "[..., num_features]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Retrieve the embedding table value.",
                        "Determine the appropriate data type for the attention computation (`attend_dtype` or `dtype`).",
                        "Call the `attend_on_embedding` function with the query, embedding table, and determined data type."
                    ],
                    "output": {
                        "shape": "[..., num_embeddings]"
                    },
                    "dependencies": [
                        "attend_on_embedding"
                    ],
                    "notes": [
                        "This method facilitates weight sharing between the input embedding layer and the final output layer in NLP models."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#attend_on_embedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def attend_on_embedding(query: Array, embedding_table: Array, attend_dtype: DType, config: Config) -> Array:\n  \"\"\"Attend over an embedding table using a query array.\n\n  TODO: Remove this method when Embed bridge to Linen is no longer needed\n\n  Args:\n    query: An array with a last dimension equal to the feature depth of the embedding.\n    embedding_table: The embedding table to attend over.\n    attend_dtype: The data type for the attention computation.\n    config: The model configuration, used to check for parameter offloading.\n\n  Returns:\n    An array with a final dimension equal to `num_embeddings`, corresponding to the\n    batched inner-product of the query vectors against each embedding.\n  \"\"\"\n  embedding_table = _maybe_move_embedding_to_device(embedding_table, config)\n  return jnp.dot(query, jnp.asarray(embedding_table, jnp.bfloat16).T, preferred_element_type=attend_dtype)",
        "analysis": {
            "functionality": "Computes the dot-product attention scores between a query tensor and an embedding table. This is commonly used for generating output logits in language models by comparing the final hidden state (query) with the word embeddings.",
            "usage": "To use this function, provide a `query` tensor of shape `[..., feature_dim]`, an `embedding_table` of shape `[num_embeddings, feature_dim]`, the desired computation data type `attend_dtype`, and a `config` object. The function returns an output tensor of shape `[..., num_embeddings]` containing the attention scores."
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#rotary_embedding_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def rotary_embedding_as_linen(\n    *,\n    min_timescale: int,\n    max_timescale: int,\n    embedding_dims: int = 0,\n    cast_as_fprop_dtype: bool = True,\n    fprop_dtype: DType = jnp.bfloat16,\n    name: str | None = None,\n):\n  \"\"\"Initializes the RotaryEmbedding module and returns it as a Linen module.\n\n  Args:\n    min_timescale: Start of the geometric index. Determines the periodicity of\n      the added signal.\n    max_timescale: End of the geometric index. Determines the frequency of the\n      added signal.\n    embedding_dims: Dimension of the embedding to be generated.\n    cast_as_fprop_dtype: Whether to cast the output to the fprop dtype.\n    fprop_dtype: The dtype of the output.\n    name: Name of the Linen module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      RotaryEmbedding,\n      min_timescale=min_timescale,\n      max_timescale=max_timescale,\n      embedding_dims=embedding_dims,\n      cast_as_fprop_dtype=cast_as_fprop_dtype,\n      fprop_dtype=fprop_dtype,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n  )",
        "analysis": {
            "module_type": "rotary_embedding_factory",
            "purpose": "A factory function that initializes the NNX `RotaryEmbedding` module and wraps it to be compatible with the Flax Linen API.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to wrap the `RotaryEmbedding` class.",
                "Passes configuration parameters (`min_timescale`, `max_timescale`, `embedding_dims`, etc.) to the `RotaryEmbedding` constructor during wrapping."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "RotaryEmbedding",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "min_timescale": "Start of the geometric index, which determines the periodicity of the added signal.",
                "max_timescale": "End of the geometric index, which determines the frequency of the added signal.",
                "embedding_dims": "Dimension of the embedding to be generated.",
                "cast_as_fprop_dtype": "Whether to cast the output of the returned module to the fprop dtype.",
                "fprop_dtype": "The forward propagation data type for the returned module."
            },
            "notes": [
                "This function serves as a bridge to use the NNX-based `RotaryEmbedding` module within a Flax Linen model.",
                "The returned object is a Linen module, which can then be called with input tensors."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#RotaryEmbedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class RotaryEmbedding(nnx.Module):\n  \"\"\"Rotary Position Embedding.\"\"\"\n\n  def __init__(\n      self,\n      min_timescale: int,\n      max_timescale: int,\n      embedding_dims: int = 0,\n      cast_as_fprop_dtype: bool = True,\n      fprop_dtype: DType = jnp.bfloat16,\n      # Not used in RotaryEmbedding but passed in by nnx.bridge.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rope_linear_scaling_factor: float = 1.0,\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the RotaryEmbedding module.\n\n    Args:\n      min_timescale: Start of the geometric index. Determines the periodicity of\n        the added signal.\n      max_timescale: End of the geometric index. Determines the frequency of the\n        added signal.\n      embedding_dims: Dimension of the embedding to be generated.\n      cast_as_fprop_dtype: Whether to cast the output to the fprop dtype.\n      fprop_dtype: The dtype of the output.\n      rngs: rng keys passed in by nnx.bridge.to_linen.\n    \"\"\"\n    self.min_timescale = min_timescale\n    self.max_timescale = max_timescale\n    self.embedding_dims = embedding_dims\n    self.cast_as_fprop_dtype = cast_as_fprop_dtype\n    self.fprop_dtype = fprop_dtype\n    self.rope_linear_scaling_factor = rope_linear_scaling_factor\n\n    if self.embedding_dims % 2:\n      raise ValueError(\"Embedding dim for rotary position embedding must be a multiple of 2.\")\n\n  @property\n  def timescale(self):\n    \"\"\"Returns the timescale for the rotary embedding.\"\"\"\n    half_embedding_dim = self.embedding_dims // 2\n    fraction = 2 * jnp.arange(0, half_embedding_dim) / self.embedding_dims\n    timescale = self.min_timescale * (self.max_timescale / self.min_timescale) ** fraction\n    if self.rope_linear_scaling_factor != 1.0:\n      timescale = timescale * self.rope_linear_scaling_factor\n    return timescale\n\n  def __call__(\n      self,  # pytype: disable=signature-mismatch  # overriding-parameter-count-checks\n      inputs: jax.Array,\n      position: None | jax.Array = None,\n  ) -> jax.Array:\n    \"\"\"Generates a jax.Array of sinusoids with different frequencies.\n\n    Args:\n      inputs: The input sequence on which to apply the Rotary position\n        embedding. Since rotary position embeddings are applied to query and\n        keys after projection, it is assumed of shape [B, S, N, H].\n      position: Optional position jax.Array which denotes the position of each\n        token in the sequence. This only needs to be supplied when the sequence\n        is packed. It is of shape [B, S].\n\n    Returns:\n      a jax.Array of shape [B, S, N, H] which includes the inputs together with\n      the rotary position embedding incorporated in it.\n    \"\"\"\n    assert position is not None\n    if len(inputs.shape) != 4:\n      raise ValueError(\"Input is assumed to be a rank 4 tensor of shape\" \"[batch, sequence, heads, dims].\")\n    if self.embedding_dims != inputs.shape[3]:\n      raise ValueError(\n          \"The embedding dims of the rotary position embedding\" \"must match the hidden dimension of the inputs.\"\n      )\n\n    position = position[:, :, jnp.newaxis, jnp.newaxis]\n    sinusoid_inp = position / self.timescale\n    sin = jnp.sin(sinusoid_inp).astype(inputs.dtype)\n    cos = jnp.cos(sinusoid_inp).astype(inputs.dtype)\n    first_half, second_half = jnp.split(inputs, 2, axis=-1)\n    first_part = first_half * cos - second_half * sin\n    second_part = second_half * cos + first_half * sin\n    if self.cast_as_fprop_dtype:\n      first_part = first_part.astype(self.fprop_dtype)\n      second_part = second_part.astype(self.fprop_dtype)\n    x_out = jnp.concatenate((first_part, second_part), axis=-1)\n    return x_out",
        "analysis": {
            "module_type": "rotary_position_embedding",
            "purpose": "Applies rotary position embeddings (RoPE) to an input tensor, typically used for query and key tensors in attention mechanisms, to encode positional information.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes timescale parameters and embedding dimensions.",
                "Computes a timescale vector based on min/max timescales and embedding dimensions.",
                "Applies rotary transformation to an input tensor using the computed timescale and token positions."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.nnx",
                "jax",
                "jax.numpy"
            ],
            "parameters": {
                "min_timescale": "Start of the geometric index, determining the periodicity of the signal.",
                "max_timescale": "End of the geometric index, determining the frequency of the signal.",
                "embedding_dims": "Dimension of the embedding to be generated, which must be a multiple of 2.",
                "cast_as_fprop_dtype": "A boolean indicating whether to cast the output to the fprop_dtype.",
                "fprop_dtype": "The data type for the forward pass output.",
                "rope_linear_scaling_factor": "A scaling factor applied to the timescale."
            },
            "notes": [
                "The embedding dimension must be a multiple of 2.",
                "This is an `nnx.Module`, a stateful module in Flax's NNX API.",
                "The `rngs` parameter in `__init__` is accepted for compatibility with `nnx.bridge.to_linen` but is not used."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the RotaryEmbedding module with timescale parameters and embedding dimensions.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store `min_timescale`, `max_timescale`, `embedding_dims`, `cast_as_fprop_dtype`, `fprop_dtype`, and `rope_linear_scaling_factor` as instance attributes.",
                        "Validate that `embedding_dims` is an even number."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": [
                        "The `rngs` parameter is accepted for compatibility but not used within the method."
                    ]
                },
                "timescale": {
                    "purpose": "Computes and returns the timescale vector used for generating sinusoidal embeddings.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculate `half_embedding_dim`.",
                        "Create a fraction array using `jnp.arange`.",
                        "Compute the base timescale using a geometric progression from `min_timescale` to `max_timescale`.",
                        "Optionally, scale the timescale by `rope_linear_scaling_factor`.",
                        "Return the computed timescale."
                    ],
                    "output": {
                        "shape": "[embedding_dims // 2]"
                    },
                    "dependencies": [
                        "jax.numpy"
                    ],
                    "notes": [
                        "This is a `@property`, so it's accessed like an attribute (`self.timescale`) but computed on access."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the rotary position embedding to the input tensor.",
                    "input": {
                        "shape": "inputs: [B, S, N, H], position: [B, S]",
                        "dtype": "jax.Array"
                    },
                    "processing_steps": [
                        "Assert that the `position` tensor is provided.",
                        "Validate that the input tensor has a rank of 4 and its last dimension matches `embedding_dims`.",
                        "Expand dimensions of the `position` tensor for broadcasting.",
                        "Calculate `sinusoid_inp` by dividing `position` by `self.timescale`.",
                        "Compute sine and cosine of `sinusoid_inp`.",
                        "Split the input tensor into two halves along the last dimension.",
                        "Apply the rotary transformation using the sine and cosine values.",
                        "Optionally cast the transformed parts to `self.fprop_dtype`.",
                        "Concatenate the two transformed parts back together.",
                        "Return the final output tensor."
                    ],
                    "output": {
                        "shape": "[B, S, N, H]"
                    },
                    "dependencies": [
                        "jax.numpy",
                        "self.timescale"
                    ],
                    "notes": [
                        "The method assumes the input tensor represents query or key projections in an attention mechanism.",
                        "The `position` argument is mandatory for this implementation."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#llama_rotary_embedding_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def llama_rotary_embedding_as_linen(\n    *,\n    min_timescale: int,\n    max_timescale: int,\n    embedding_dims: int = 0,\n    cast_as_fprop_dtype: bool = True,\n    fprop_dtype: DType = jnp.bfloat16,\n    use_scale: bool = True,\n    name: str | None = None,\n):\n  \"\"\"Initializes the LLaMARotaryEmbedding module and returns it as a Linen module.\n\n  Args:\n    min_timescale: Start of the geometric index. Determines the periodicity of\n      the added signal.\n    max_timescale: End of the geometric index. Determines the frequency of the\n      added signal.\n    embedding_dims: Dimension of the embedding to be generated.\n    cast_as_fprop_dtype: Whether to cast the output to the fprop dtype.\n    fprop_dtype: The dtype of the output.\n    use_scale: Whether to apply LLaMA3.1 scaling factor.\n    name: Name of the Linen module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      LLaMARotaryEmbedding,\n      min_timescale=min_timescale,\n      max_timescale=max_timescale,\n      embedding_dims=embedding_dims,\n      cast_as_fprop_dtype=cast_as_fprop_dtype,\n      fprop_dtype=fprop_dtype,\n      use_scale=use_scale,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n  )",
        "analysis": {
            "module_type": "llama_rotary_embedding_factory",
            "purpose": "A factory function that initializes and wraps the NNX-based LLaMARotaryEmbedding module, making it compatible with the Flax Linen API.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `LLaMARotaryEmbedding` NNX module into a Flax Linen module, passing along the provided configuration parameters."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance of the LLaMARotaryEmbedding."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "LLaMARotaryEmbedding",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "min_timescale": "Start of the geometric index, determining the periodicity of the signal.",
                "max_timescale": "End of the geometric index, determining the frequency of the signal.",
                "embedding_dims": "Dimension of the rotary embedding to be generated.",
                "cast_as_fprop_dtype": "Whether to cast the output of the embedding module to the fprop dtype.",
                "fprop_dtype": "The data type for the forward pass computation.",
                "use_scale": "A boolean flag to determine whether to apply the LLaMA3.1 scaling factor."
            },
            "notes": [
                "This function serves as a bridge to use the NNX-based `LLaMARotaryEmbedding` module within a Linen model."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#LLaMARotaryEmbedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class LLaMARotaryEmbedding(RotaryEmbedding):\n  \"\"\"LLaMA variant of ROPE.\"\"\"\n\n  def __init__(\n      self,\n      min_timescale: int,\n      max_timescale: int,\n      embedding_dims: int = 0,\n      cast_as_fprop_dtype: bool = True,\n      fprop_dtype: DType = jnp.bfloat16,\n      use_scale: bool = True,\n      # Not used in LLaMARotaryEmbedding but passed in by nnx.bridge.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the LLaMARotaryEmbedding module.\n\n    Args:\n      min_timescale: Start of the geometric index. Determines the periodicity of\n        the added signal.\n      max_timescale: End of the geometric index. Determines the frequency of the\n        added signal.\n      embedding_dims: Dimension of the embedding to be generated.\n      cast_as_fprop_dtype: Whether to cast the output to the fprop dtype.\n      fprop_dtype: The dtype of the output.\n      use_scale: Whether to apply LLaMA3.1 scaling factor.\n      rngs: rng keys passed in by nnx.bridge.to_linen.\n    \"\"\"\n    super().__init__(\n        min_timescale=min_timescale,\n        max_timescale=max_timescale,\n        embedding_dims=embedding_dims,\n        cast_as_fprop_dtype=cast_as_fprop_dtype,\n        fprop_dtype=fprop_dtype,\n        rngs=rngs,\n    )\n\n    # LLaMA3.1 ROPE scaling, see the original pytorch implementation:\n    # https://github.com/meta-llama/llama-models/blob/301ca3a2b3b10e94ddcd1fdd2c57e52f812e1cac/models/llama3/reference_impl/model.py#L45C5-L45C18\n    self.use_scale = use_scale\n\n  @property\n  def timescale(self):\n    half_embedding_dim = self.embedding_dims // 2\n    fraction = 2 * jnp.arange(0, half_embedding_dim) / self.embedding_dims\n    fraction = jnp.repeat(fraction, 2)\n    timescale = self.min_timescale * (self.max_timescale / self.min_timescale) ** fraction\n\n    # Apply scaling factor if enabled\n    if self.use_scale:\n      timescale = 1.0 / jax.vmap(self._apply_scaling_factor)(1.0 / timescale)\n\n    # Expand timescale dimensions for broadcasting\n    return timescale[jnp.newaxis, jnp.newaxis, jnp.newaxis, :]\n\n  def _apply_scaling_factor(self, freq):\n    \"\"\"apply scaling factor to rotary position embedding.\"\"\"\n    scale_factor = 8\n    low_freq_factor = 1\n    high_freq_factor = 4\n    old_context_len = 8192  # original llama3 length\n\n    low_freq_wavelen = old_context_len / low_freq_factor\n    high_freq_wavelen = old_context_len / high_freq_factor\n    wavelen = 2 * jnp.pi / freq\n\n    def lower_wavelen(freq):\n      return freq\n\n    def bigger_or_equal_wavelen(freq):\n      def bigger_wavelen(freq):\n        return freq / scale_factor\n\n      def equal_wavelen(freq):\n        smooth = (old_context_len / wavelen - low_freq_factor) / (high_freq_factor - low_freq_factor)\n        return (1 - smooth) * freq / scale_factor + smooth * freq\n\n      bigger_wavelen_cond = wavelen > low_freq_wavelen\n      return jax.lax.cond(bigger_wavelen_cond, bigger_wavelen, equal_wavelen, freq)\n\n    lower_wavelen_cond = wavelen < high_freq_wavelen\n    return jax.lax.cond(lower_wavelen_cond, lower_wavelen, bigger_or_equal_wavelen, freq)\n\n  def __call__(self, inputs: jax.Array, position: None | jax.Array = None) -> jax.Array:\n    \"\"\"Applies LLaMA variant of rotary position embedding.\n\n    Args:\n      inputs: The input sequence on which to apply the Rotary position\n        embedding. It is assumed of shape [B, S, N, H].\n      position: Optional position array [B, S]. Only needed when the sequence\n        is packed.\n\n    Returns:\n      A jax.Array of shape [B, S, N, H] with rotary position embeddings applied.\n    \"\"\"\n    # Ensure input is 4D\n    if len(inputs.shape) != 4:\n      raise ValueError(\"Input is assumed to be a rank 4 tensor of shape [B, S, N, H].\")\n    if self.embedding_dims != inputs.shape[3]:\n      raise ValueError(\n          \"The embedding dims of the rotary position embedding must match the hidden dimension of the inputs.\"\n      )\n\n    # Shift the inputs left and right as per LLaMA's specific behavior\n    inputs_shifted_left = jnp.concatenate([inputs[..., 1:], inputs[..., :1]], axis=-1)\n    inputs_shifted_right = jnp.concatenate([inputs[..., -1:], inputs[..., :-1]], axis=-1)\n    inputs_shifted = jax.lax.select(\n        jnp.tile(\n            jnp.mod(jnp.arange(self.embedding_dims, dtype=jnp.int32), 2),\n            inputs.shape[:-1] + (1,),\n        ),\n        inputs_shifted_right,\n        inputs_shifted_left,\n    )\n\n    # Determine positions if not provided\n    if position is None:\n      seq_length = inputs.shape[1]\n      position = jnp.arange(seq_length, dtype=jnp.float32)[jnp.newaxis, :]\n\n    # Calculate sinusoidal input\n    position = position[:, :, jnp.newaxis, jnp.newaxis]\n    sinusoid_inp = position / self.timescale\n\n    sin = jnp.sin(sinusoid_inp)\n    cos = jnp.cos(sinusoid_inp)\n\n    # Apply alternating sign\n    sign = jnp.tile(jnp.array([-1, 1]), self.embedding_dims // 2)\n\n    # Combine original inputs with sinusoidal information\n    outputs = inputs * cos + inputs_shifted * sin * sign\n\n    if self.cast_as_fprop_dtype:\n      outputs = outputs.astype(self.fprop_dtype)\n\n    return outputs",
        "analysis": {
            "module_type": "llama_rotary_embedding",
            "purpose": "Implements the LLaMA variant of Rotary Position Embedding (RoPE), which applies rotational transformations to input embeddings based on their sequence position, with an option for LLaMA3.1 frequency scaling.",
            "input": {
                "shape": "N/A (Class definition, see `__call__` method)",
                "dtype": "N/A (Class definition, see `__call__` method)"
            },
            "processing_steps": [
                "Initializes parameters for RoPE, including timescales and an optional LLaMA3.1 scaling factor.",
                "Provides a `__call__` method to apply the embedding to an input tensor."
            ],
            "output": {
                "shape": "N/A (Class definition, see `__call__` method)"
            },
            "dependencies": [
                "RotaryEmbedding",
                "jax",
                "jax.numpy"
            ],
            "parameters": {
                "min_timescale": "The starting value for the geometric sequence of timescales used to generate frequencies.",
                "max_timescale": "The ending value for the geometric sequence of timescales.",
                "embedding_dims": "The dimension of the features to which the embedding is applied; must match the last dimension of the input tensor.",
                "use_scale": "A boolean flag to enable the LLaMA3.1 frequency scaling factor, which adjusts frequencies for handling longer contexts."
            },
            "notes": [
                "This class inherits from `RotaryEmbedding` but overrides the `timescale` property and the `__call__` method to implement LLaMA-specific logic.",
                "The core difference in the `__call__` method is the use of a shifted input tensor for the transformation, rather than splitting the input tensor into two halves as in the standard RoPE implementation."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the LLaMARotaryEmbedding module.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls the parent `RotaryEmbedding` constructor with provided parameters.",
                        "Stores the `use_scale` boolean flag."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "RotaryEmbedding.__init__"
                    ],
                    "notes": []
                },
                "timescale": {
                    "purpose": "Calculates the timescale vector for generating sinusoidal embeddings, with an option for LLaMA3.1 scaling.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Generates a base timescale vector using a geometric progression from `min_timescale` to `max_timescale`.",
                        "If `use_scale` is true, applies a scaling factor to each frequency by calling `_apply_scaling_factor`.",
                        "Adds leading dimensions to the timescale tensor for broadcasting purposes."
                    ],
                    "output": {
                        "shape": "[1, 1, 1, embedding_dims]"
                    },
                    "dependencies": [
                        "jax.vmap",
                        "_apply_scaling_factor"
                    ],
                    "notes": [
                        "This is a property that overrides the parent class's implementation to provide LLaMA-specific timescale calculation."
                    ]
                },
                "_apply_scaling_factor": {
                    "purpose": "Applies the LLaMA3.1 frequency scaling factor to a single frequency value.",
                    "input": {
                        "shape": "scalar",
                        "dtype": "float"
                    },
                    "processing_steps": [
                        "Calculates the wavelength corresponding to the input frequency.",
                        "Uses `jax.lax.cond` to apply a piecewise scaling function based on whether the wavelength is smaller, larger, or within a specific range defined by `high_freq_wavelen` and `low_freq_wavelen`."
                    ],
                    "output": {
                        "shape": "scalar"
                    },
                    "dependencies": [
                        "jax.lax.cond"
                    ],
                    "notes": [
                        "This is a private helper method used by the `timescale` property to implement the context extension logic from LLaMA3.1."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the LLaMA-specific rotary position embedding to an input tensor.",
                    "input": {
                        "shape": "inputs: [B, S, N, H], position: [B, S] (optional)",
                        "dtype": "jax.Array"
                    },
                    "processing_steps": [
                        "Validates that the input tensor is 4-dimensional and its last dimension matches `embedding_dims`.",
                        "Creates a `inputs_shifted` tensor by swapping adjacent feature pairs in the last dimension of the input.",
                        "Generates position indices from 0 to S-1 if not provided.",
                        "Calculates sine and cosine embeddings using the `timescale` property and positions.",
                        "Applies the rotary transformation using the formula: `outputs = inputs * cos + inputs_shifted * sin * sign`.",
                        "Optionally casts the output tensor to `fprop_dtype`."
                    ],
                    "output": {
                        "shape": "[B, S, N, H]"
                    },
                    "dependencies": [
                        "jnp.concatenate",
                        "jax.lax.select",
                        "jnp.sin",
                        "jnp.cos"
                    ],
                    "notes": [
                        "The transformation formula is distinct from the standard RoPE implementation in the parent class, which splits the tensor in half."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#yarn_rotary_embedding_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def yarn_rotary_embedding_as_linen(\n    *,\n    embedding_dims: int,\n    max_position_embeddings: int = 4096 * 4,\n    original_max_position_embeddings: int = 4096,\n    beta_fast: float = 32,\n    beta_slow: float = 1,\n    rope_theta: float = 10000.0,\n    rope_factor: float = 40,\n    cast_as_fprop_dtype: bool = True,\n    fprop_dtype: DType = jnp.bfloat16,\n    name: str | None = None,\n    interleave: bool = True,\n    truncate: bool = True,\n    attention_scaling: bool = False,\n):\n  \"\"\"Initializes the YarnRotaryEmbedding module and returns it as a Linen module.\n\n  Args:\n    embedding_dims: The dimension of the embeddings.\n    max_position_embeddings: The maximum number of positions.\n    original_max_position_embeddings: The original maximum number of positions.\n    beta_fast: The fast beta parameter for YaRN.\n    beta_slow: The slow beta parameter for YaRN.\n    rope_theta: The base for the rotary frequencies.\n    rope_factor: The scaling factor for RoPE.\n    cast_as_fprop_dtype: Whether to cast the output to `fprop_dtype`.\n    fprop_dtype: The forward pass dtype.\n    name: The name of the module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      YarnRotaryEmbedding,\n      embedding_dims=embedding_dims,\n      max_position_embeddings=max_position_embeddings,\n      original_max_position_embeddings=original_max_position_embeddings,\n      beta_fast=beta_fast,\n      beta_slow=beta_slow,\n      rope_theta=rope_theta,\n      rope_factor=rope_factor,\n      cast_as_fprop_dtype=cast_as_fprop_dtype,\n      fprop_dtype=fprop_dtype,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n      interleave=interleave,\n      truncate=truncate,\n      attention_scaling=attention_scaling,\n  )",
        "analysis": {
            "module_type": "yarn_rotary_embedding_factory",
            "purpose": "Initializes a `YarnRotaryEmbedding` NNX module and wraps it to be compatible with the Flax Linen API.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to wrap the `YarnRotaryEmbedding` class.",
                "Passes all configuration arguments to the `YarnRotaryEmbedding` constructor.",
                "Specifies `variable_to_logically_partitioned` as the metadata function for parameter handling."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "YarnRotaryEmbedding",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "embedding_dims": "The dimension of the embeddings.",
                "max_position_embeddings": "The maximum number of positions the model can handle.",
                "original_max_position_embeddings": "The original maximum sequence length for which the base frequencies were defined.",
                "beta_fast": "The fast beta parameter for YaRN interpolation.",
                "beta_slow": "The slow beta parameter for YaRN interpolation.",
                "rope_theta": "The base for the rotary frequencies.",
                "rope_factor": "The scaling factor for RoPE.",
                "interleave": "Whether the complex representation is interleaved or concatenated.",
                "truncate": "Whether to floor/ceil the correction range for YaRN.",
                "attention_scaling": "Whether to apply an additional scaling factor to the output."
            },
            "notes": [
                "This function acts as a factory or bridge, making an NNX-based module (`YarnRotaryEmbedding`) usable within a Flax Linen model.",
                "The returned object is a Flax Linen module, not a tensor.",
                "The actual tensor processing logic is contained within the `YarnRotaryEmbedding` class that this function instantiates."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#YarnRotaryEmbedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class YarnRotaryEmbedding(nnx.Module):\n  \"\"\"Yarn rotary embedding.\n\n  Based on https://arxiv.org/abs/2309.00071\n  This implementation uses DeepSeek-v3 PyTorch as reference\n  https://github.com/deepseek-ai/DeepSeek-V3/blob/2f7b80eecebf3d1c84da5a0d465f6639ea175012/inference/model.py#L294\n\n  Attributes:\n    embedding_dims: Dimension of the embedding to be generated.\n    max_position_embeddings: The maximum sequence length that will be encountered.\n    original_max_position_embeddings: The sequence length for which the base frequencies were defined.\n    beta_fast: Lower bound parameter for correction.\n    beta_slow: Upper bound parameter for correction.\n    rope_theta: The base theta value for the frequency computation.\n    rope_factor: Factor applied to adjust the frequencies.\n    cast_as_fprop_dtype: Whether to cast the output to `fprop_dtype`.\n    fprop_dtype: The forward pass dtype.\n    rope_interleave: Whether complex representation is interleaved or concatenated.\n    rope_truncate: Whether or not to floor lower bound and ceil upper bound for correction range.\n    rope_attention_scaling: Whether or not to scale the rotary embedding output.\n    rngs: rng keys passed in by nnx.bridge.to_linen.\n  \"\"\"\n\n  def __init__(\n      self,\n      embedding_dims: int,\n      max_position_embeddings: int = 4096 * 4,\n      original_max_position_embeddings: int = 4096,\n      beta_fast: float = 32,\n      beta_slow: float = 1,\n      rope_theta: float = 10000.0,\n      rope_factor: float = 40,\n      cast_as_fprop_dtype: bool = True,\n      fprop_dtype: DType = jnp.bfloat16,\n      interleave=True,\n      truncate=True,\n      attention_scaling=False,\n      # Not used in YarnRotaryEmbedding but passed in by nnx.bridge.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the YarnRotaryEmbedding module.\"\"\"\n    self.embedding_dims = embedding_dims\n    self.max_position_embeddings = max_position_embeddings\n    self.original_max_position_embeddings = original_max_position_embeddings\n    self.beta_fast = beta_fast\n    self.beta_slow = beta_slow\n    self.rope_theta = rope_theta\n    self.rope_factor = rope_factor\n    self.cast_as_fprop_dtype = cast_as_fprop_dtype\n    self.fprop_dtype = fprop_dtype\n    self.interleave = interleave\n    self.truncate = truncate\n    self.attention_scaling = attention_scaling\n\n    if self.embedding_dims % 2:\n      raise ValueError(\"Embedding dim for rotary position embedding must be a multiple of 2.\")\n\n  @property\n  def freqs_cis(self):\n    \"\"\"Frequencies for rotary embedding.\"\"\"\n    half_dim = self.embedding_dims // 2\n    # Compute base frequencies for each (even-indexed) dimension.\n    # (Note: We use jnp.arange with float32 for precision.)\n    freqs = 1.0 / (self.rope_theta ** (2.0 * jnp.arange(0, half_dim, dtype=jnp.float32) / self.embedding_dims))\n\n    low, high = self._find_correction_range(\n        self.beta_fast,\n        self.beta_slow,\n        self.embedding_dims,\n        self.rope_theta,\n        self.original_max_position_embeddings,\n        self.truncate,\n    )\n    smooth = 1 - self._linear_ramp_factor(low, high, half_dim)\n    # The corrected frequency is a weighted mix of the scaled and base values.\n    freqs = freqs / self.rope_factor * (1 - smooth) + freqs * smooth\n\n    # Precompute frequencies for all positions by taking the outer product.\n    t = jnp.arange(self.max_position_embeddings, dtype=jnp.float32)  # shape [max_position_embeddings]\n    # This gives a [max_position_embeddings, half_dim] tensor with rows as time steps.\n    freqs = jnp.outer(t, freqs)\n\n    # Compute the complex \u201ccis\u201d values: exp(i * theta).\n    return jnp.exp(1j * freqs)  # shape [max_position_embeddings, half_dim]\n\n  def _find_correction_dim(self, num_rotations: float, dim: int, base: float, max_position_embeddings: int) -> float:\n    \"\"\"Compute the correction dimension for a given number of rotations.\"\"\"\n    return dim * math.log(max_position_embeddings / (num_rotations * 2 * math.pi)) / (2 * math.log(base))\n\n  def _find_correction_range(\n      self, low_rot: float, high_rot: float, dim: int, base: float, max_position_embeddings: int, truncate: bool\n  ):\n    \"\"\"Computes the range of correction dimensions for rotary positional embeddings.\n\n    Args:\n        low_rot (float): Lower bound for the number of rotations.\n        high_rot (float): Upper bound for the number of rotations.\n        dim (int): Dimensionality of the embedding space.\n        base (float): Base value for the exponential computation.\n        max_position_embeddings (int): Maximum sequence length.\n        truncate (bool): Whether to floor lower bound and ceil upper bound.\n\n    Returns:\n        tuple[int, int]: The range of correction dimensions (low, high), clamped to valid indices.\n    \"\"\"\n    low = self._find_correction_dim(low_rot, dim, base, max_position_embeddings)\n    high = self._find_correction_dim(high_rot, dim, base, max_position_embeddings)\n    if truncate:\n      low = math.floor(low)\n      high = math.ceil(high)\n    low = max(low, 0)\n    high = min(high, dim - 1)\n    return low, high\n\n  def _linear_ramp_factor(self, min_val: float, max_val: float, dim: int) -> Array:\n    \"\"\"Computes a linear ramp over the dimension.\n\n    Returns a jax.Array of shape (dim,) with values between 0 and 1.\n    \"\"\"\n    if min_val == max_val:\n      max_val += 0.001  # Avoid division by zero.\n    linear_func = (jnp.arange(dim, dtype=jnp.float32) - min_val) / (max_val - min_val)\n    return jnp.clip(linear_func, 0, 1)\n\n  def __call__(self, inputs: Array, position: None | Array = None) -> Array:\n    \"\"\"Applies the rotary positional embedding using the precomputed complex frequencies.\n\n    Args:\n      inputs: jax.Array of shape [B, S, N, H]. (H must equal self.embedding_dims.)\n      position: jax.Array of shape [B, S] with integer positions (indexes into precomputed freqs).\n\n    Returns:\n      jax.Array of shape [B, S, N, H] with the rotary embedding applied.\n    \"\"\"\n    if len(inputs.shape) != 4:\n      raise ValueError(\"Input is assumed to be a rank 4 tensor of shape [batch, sequence, heads, dims].\")\n    if self.embedding_dims != inputs.shape[3]:\n      raise ValueError(\n          \"The embedding dims of the rotary position embedding must match the hidden dimension of the inputs.\"\n      )\n\n    # Determine positions if not provided\n    if position is None:\n      seq_length = inputs.shape[1]\n      position = jnp.arange(seq_length, dtype=jnp.int32)[jnp.newaxis, :]\n    else:\n      position = position.astype(jnp.int32)\n\n    # Lookup the precomputed frequencies using the position indices.\n    # self.freqs_cis has shape [max_position_embeddings, half_dim] so we use jnp.take along axis 0.\n    # After indexing, shape becomes [B, S, half_dim]; we then add an axis for the heads.\n    freqs = jnp.take(self.freqs_cis, position, axis=0)  # shape: [B, S, half_dim]\n    freqs = freqs[:, :, jnp.newaxis, :]  # shape: [B, S, 1, half_dim]\n\n    if self.interleave:\n      # Inputs with interleaved format [real1, img1, real2, img2, ...] at last dimension\n      # Convert the last dimension into a complex representation.\n      # First reshape so that each pair of numbers represents the real and imaginary parts.\n      B, S, N, H = inputs.shape\n      half_dim = H // 2\n      inputs_reshaped = inputs.reshape(B, S, N, half_dim, 2)\n      first_half, second_half = inputs_reshaped[..., 0], inputs_reshaped[..., 1]\n    else:\n      # Inputs with concatenated format [real1, real2, ..., img1, img2, ...] at last dimension\n      first_half, second_half = jnp.split(inputs, 2, axis=-1)\n\n    inputs_complex = first_half + 1j * second_half  # shape: [B, S, N, half_dim]\n    # Apply the rotary transformation via complex multiplication.\n    rotated = inputs_complex * freqs  # shape: [B, S, N, half_dim]\n    # Convert the complex result back to a real tensor.\n    # Split the complex number into its real and imaginary parts.\n    # [real1, real2, ..., img1, img2, ...]\n    output = jnp.concatenate([jnp.real(rotated), jnp.imag(rotated)], axis=-1)\n\n    if self.attention_scaling:\n      attention_scaling = 1.0 if self.rope_factor <= 1 else (0.1 * math.log(self.rope_factor) + 1.0)\n      output = output * attention_scaling\n\n    if self.cast_as_fprop_dtype:\n      output = output.astype(self.fprop_dtype)\n    return output",
        "analysis": {
            "module_type": "yarn_rotary_positional_embedding",
            "purpose": "Implements the YaRN (Yet another RoPE extensioN) method to apply rotary positional embeddings, designed to extend the context window of transformer models by modifying RoPE frequencies.",
            "input": {
                "shape": "[batch_size, sequence_length, num_heads, head_dim]",
                "dtype": "float"
            },
            "processing_steps": [
                "The `__call__` method is invoked to apply the embedding to an input tensor."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, num_heads, head_dim]"
            },
            "dependencies": [
                "nnx.Module",
                "jax.numpy"
            ],
            "parameters": {
                "embedding_dims": "Dimension of the embedding to be generated.",
                "max_position_embeddings": "The maximum sequence length that will be encountered.",
                "original_max_position_embeddings": "The sequence length for which the base frequencies were defined.",
                "beta_fast": "Lower bound parameter for YaRN correction.",
                "beta_slow": "Upper bound parameter for YaRN correction.",
                "rope_theta": "The base theta value for the frequency computation.",
                "rope_factor": "Factor applied to adjust the frequencies.",
                "interleave": "Whether the complex representation of the input is interleaved or concatenated.",
                "truncate": "Whether to floor the lower bound and ceil the upper bound for the correction range.",
                "attention_scaling": "Whether to scale the rotary embedding output."
            },
            "notes": [
                "This implementation is based on the paper 'YaRN: Efficient Context Window Extension of Large Language Models' (https://arxiv.org/abs/2309.00071).",
                "It uses the DeepSeek-v3 PyTorch implementation as a reference.",
                "The core logic involves pre-computing modified complex frequencies in the `freqs_cis` property and applying them via complex multiplication in the `__call__` method."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the YarnRotaryEmbedding module with its configuration parameters.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Assigns configuration parameters (e.g., embedding_dims, rope_theta, rope_factor) to instance attributes.",
                        "Validates that `embedding_dims` is a multiple of 2."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": [
                        "The `rngs` parameter is accepted for compatibility with `nnx.bridge.to_linen` but is not used."
                    ]
                },
                "freqs_cis": {
                    "purpose": "A property that precomputes the complex sinusoidal frequencies for all positions up to `max_position_embeddings`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Compute base frequencies based on `rope_theta`.",
                        "Call `_find_correction_range` to determine the dimension range for YaRN correction.",
                        "Call `_linear_ramp_factor` to create a smoothing factor.",
                        "Apply the YaRN correction to the base frequencies, mixing scaled and base values.",
                        "Compute frequencies for all positions using an outer product with a time vector.",
                        "Convert frequencies to complex 'cis' values using `jnp.exp(1j * freqs)`."
                    ],
                    "output": {
                        "shape": "[max_position_embeddings, embedding_dims // 2]"
                    },
                    "dependencies": [
                        "_find_correction_range",
                        "_linear_ramp_factor",
                        "jnp.outer",
                        "jnp.exp"
                    ],
                    "notes": [
                        "This property is computed on-demand and its result is implicitly cached for subsequent calls."
                    ]
                },
                "_find_correction_dim": {
                    "purpose": "A helper function to compute the correction dimension for a given number of rotations, as part of the YaRN algorithm.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "float, int"
                    },
                    "processing_steps": [
                        "Applies a mathematical formula involving logarithms to calculate the dimension."
                    ],
                    "output": {
                        "shape": "scalar"
                    },
                    "dependencies": [
                        "math.log"
                    ],
                    "notes": [
                        "This is an internal implementation detail of the YaRN scaling logic."
                    ]
                },
                "_find_correction_range": {
                    "purpose": "Computes the range of dimensions over which the YaRN correction is applied.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "float, int, bool"
                    },
                    "processing_steps": [
                        "Calls `_find_correction_dim` for both the low and high rotation bounds.",
                        "Optionally truncates the results (floor for low, ceil for high).",
                        "Clamps the range to be within valid dimension indices `[0, dim - 1]`."
                    ],
                    "output": {
                        "shape": "tuple[int, int]"
                    },
                    "dependencies": [
                        "_find_correction_dim",
                        "math.floor",
                        "math.ceil"
                    ],
                    "notes": [
                        "This defines the start and end dimensions for the linear ramp used in frequency correction."
                    ]
                },
                "_linear_ramp_factor": {
                    "purpose": "Computes a linear ramp from 0 to 1 over a specified range within the embedding dimensions.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "float, int"
                    },
                    "processing_steps": [
                        "Calculates a linear function over the range `[min_val, max_val]` for `dim` steps.",
                        "Clips the result to be between 0 and 1."
                    ],
                    "output": {
                        "shape": "[dim]"
                    },
                    "dependencies": [
                        "jnp.arange",
                        "jnp.clip"
                    ],
                    "notes": [
                        "This creates the `smooth` factor used to interpolate between scaled and original frequencies in `freqs_cis`."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the precomputed rotary positional embeddings to an input tensor.",
                    "input": {
                        "shape": "[batch_size, sequence_length, num_heads, head_dim]",
                        "dtype": "float"
                    },
                    "processing_steps": [
                        "Validate input tensor shape.",
                        "Generate position indices if not provided.",
                        "Look up the precomputed complex frequencies (`freqs_cis`) using the position indices.",
                        "Reshape the input tensor's last dimension into a complex representation, handling both `interleaved` and `concatenated` formats.",
                        "Apply the rotary transformation via complex multiplication.",
                        "Convert the complex result back to a real tensor by concatenating real and imaginary parts.",
                        "Optionally apply attention scaling.",
                        "Optionally cast the final output to `fprop_dtype`."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, num_heads, head_dim]"
                    },
                    "dependencies": [
                        "self.freqs_cis",
                        "jnp.take",
                        "jnp.split",
                        "jnp.concatenate",
                        "jnp.real",
                        "jnp.imag"
                    ],
                    "notes": [
                        "This is the main forward pass method of the module."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#positional_embedding_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def positional_embedding_as_linen(*, embedding_dims: int, max_wavelength: int = _MAX_WAVELENGTH):\n  \"\"\"Initializes the PositionalEmbedding module and returns it as a Linen module.\n\n  Args:\n    embedding_dims: The dimension of the embeddings.\n    max_wavelength: The maximum wavelength for the sinusoidal positional embeddings.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      PositionalEmbedding,\n      embedding_dims=embedding_dims,\n      max_wavelength=max_wavelength,\n      metadata_fn=variable_to_logically_partitioned,\n  )",
        "analysis": {
            "module_type": "positional_embedding_factory",
            "purpose": "Initializes the `PositionalEmbedding` NNX module and returns it as a Flax Linen module, acting as a bridge between NNX and Linen.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to wrap the `PositionalEmbedding` class.",
                "Passes `embedding_dims`, `max_wavelength`, and `metadata_fn` to the wrapper function."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "PositionalEmbedding",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "embedding_dims": "The dimension of the embeddings.",
                "max_wavelength": "The maximum wavelength for the sinusoidal positional embeddings."
            },
            "notes": [
                "This function is a factory that creates and configures a Linen-compatible version of the `PositionalEmbedding` module."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#PositionalEmbedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class PositionalEmbedding(nnx.Module):\n  \"\"\"A layer that adds sinusoidal positional embeddings to the input.\n\n  Attributes:\n    embedding_dims: The dimension of the embeddings.\n    max_wavelength: The maximum wavelength for the sinusoidal positional embeddings.\n    rngs: RNG state passed in by nnx.bridge.to_linen, not used in this module.\n  \"\"\"\n\n  embedding_dims: int\n  max_wavelength: int = _MAX_WAVELENGTH\n\n  rngs: nnx.Rngs = None  # Not used in PositionalEmbedding but passed in by nnx.bridge.to_linen\n\n  def __call__(\n      self,  # pytype: disable=signature-mismatch  # overriding-parameter-count-checks\n      input_embedding: jax.Array,\n      position: jax.Array,\n  ) -> jax.Array:\n    num_timescales = self.embedding_dims // 2\n    log_timescale_increment = jnp.log(float(self.max_wavelength)) / jnp.maximum(\n        jnp.asarray(num_timescales, dtype=jnp.float32) - 1, 1\n    )\n    inv_timescales = jnp.exp(jnp.arange(num_timescales, dtype=jnp.float32) * -log_timescale_increment)\n    position = position[:, :, jnp.newaxis]\n    inv_timescales = inv_timescales[jnp.newaxis, jnp.newaxis, :]\n    scaled_time = position * inv_timescales\n    signal = jnp.concatenate([jnp.sin(scaled_time), jnp.cos(scaled_time)], axis=-1)\n    # signal = jnp.pad(signal, [[0, jnp.mod(self.embedding_dims, 2)]])\n    position_embedding = signal.astype(jnp.float32)\n    return input_embedding + position_embedding",
        "analysis": {
            "module_type": "positional_embedding",
            "purpose": "A layer that adds sinusoidal positional embeddings to an input tensor.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "The main processing occurs within the __call__ method."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.nnx.Module",
                "jax",
                "jax.numpy"
            ],
            "parameters": {
                "embedding_dims": "The dimension of the embeddings.",
                "max_wavelength": "The maximum wavelength for the sinusoidal positional embeddings."
            },
            "notes": [
                "This module implements the sinusoidal positional encoding from the 'Attention Is All You Need' paper.",
                "The `rngs` attribute is included for compatibility with `nnx.bridge.to_linen` but is not used by this module."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Calculates sinusoidal positional embeddings based on token positions and adds them to the input embeddings.",
                    "input": {
                        "shape": "input_embedding: [batch_size, sequence_length, embedding_dims], position: [batch_size, sequence_length]",
                        "dtype": "jax.Array"
                    },
                    "processing_steps": [
                        "Calculate inverse timescales based on `embedding_dims` and `max_wavelength`.",
                        "Expand dimensions of `position` and `inv_timescales` for broadcasting.",
                        "Compute `scaled_time` by multiplying `position` with `inv_timescales`.",
                        "Generate a `signal` by concatenating the sine and cosine of `scaled_time`.",
                        "Cast the signal to float32 to create the `position_embedding`.",
                        "Return the element-wise sum of `input_embedding` and `position_embedding`."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, embedding_dims]"
                    },
                    "dependencies": [
                        "jax.numpy"
                    ],
                    "notes": [
                        "The positional embeddings are generated on-the-fly during the forward pass."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#llama_vision_rotary_embedding_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def llama_vision_rotary_embedding_as_linen(\n    *,\n    image_size: int,\n    patch_size: int,\n    hidden_size: int,\n    num_attention_heads: int,\n    rope_theta: float = 10000.0,\n    cast_as_fprop_dtype: bool = True,\n    fprop_dtype: DType = jnp.bfloat16,\n    name: str | None = None,\n):\n  \"\"\"Initializes the LlamaVisionRotaryEmbedding module and returns it as a Linen module.\n\n  Args:\n    image_size: The size of the input image.\n    patch_size: The size of the image patches.\n    hidden_size: The size of the hidden dimension.\n    num_attention_heads: The number of attention heads.\n    rope_theta: The base theta value for the frequency computation.\n    cast_as_fprop_dtype: Whether to cast the output to the fprop dtype.\n    fprop_dtype: The dtype of the output.\n    name: The name of the Linen module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      LlamaVisionRotaryEmbedding,\n      image_size=image_size,\n      patch_size=patch_size,\n      hidden_size=hidden_size,\n      num_attention_heads=num_attention_heads,\n      rope_theta=rope_theta,\n      cast_as_fprop_dtype=cast_as_fprop_dtype,\n      fprop_dtype=fprop_dtype,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n  )",
        "analysis": {
            "module_type": "vision_rotary_embedding_factory",
            "purpose": "A factory function that initializes a `LlamaVisionRotaryEmbedding` NNX module and wraps it as a Flax Linen module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to wrap the `LlamaVisionRotaryEmbedding` class.",
                "Passes all configuration arguments to the `LlamaVisionRotaryEmbedding` constructor.",
                "Specifies `variable_to_logically_partitioned` as the metadata function for parameter handling."
            ],
            "output": {
                "shape": "A Flax Linen module instance that wraps the `LlamaVisionRotaryEmbedding` NNX module."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "LlamaVisionRotaryEmbedding",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "image_size": "The size of the input image.",
                "patch_size": "The size of the image patches.",
                "hidden_size": "The size of the hidden dimension.",
                "num_attention_heads": "The number of attention heads.",
                "rope_theta": "The base theta value for the frequency computation.",
                "cast_as_fprop_dtype": "Whether to cast the output to the fprop dtype.",
                "fprop_dtype": "The data type for the forward pass."
            },
            "notes": [
                "This function serves as a bridge to use the NNX-based `LlamaVisionRotaryEmbedding` module within a Linen model.",
                "The returned object is a Linen module compatible with the Linen API."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#LlamaVisionRotaryEmbedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class LlamaVisionRotaryEmbedding(nnx.Module):\n  \"\"\"Rotary position embedding for Llama4 vision encoder.\n\n  Based on Pytorch Reference\n  https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama4/modeling_llama4.py\n  This implementation follows the Llama4 vision encoder's rotary embedding approach,\n  which uses 2D coordinates (x, y) to generate rotary position embeddings.\n\n  Attributes:\n    image_size: int size of the input image\n    patch_size: int size of the image patches\n    hidden_size: int size of the hidden dimension\n    num_attention_heads: int number of attention heads\n    rope_theta: float = 10000.0 base theta value for the frequency computation\n    cast_as_fprop_dtype: bool = True whether to cast the output to the fprop dtype\n    fprop_dtype: DType = jnp.bfloat16 the dtype of the output\n    rngs: RNG state passed in by nnx.bridge.to_linen, not used in this module.\n  Returns:\n    jax.Array of shape [batch_size_times_tiles, num_patches_incl_cls, num_heads, head_dim]\n    where vision rotary position embeddings are applied.\n  \"\"\"\n\n  image_size: int\n  patch_size: int\n  hidden_size: int\n  num_attention_heads: int\n  rope_theta: float = 10000.0\n  cast_as_fprop_dtype: bool = True\n  fprop_dtype: DType = jnp.bfloat16\n  # Not used in LlamaVisionRotaryEmbedding but passed in by nnx.bridge.to_linen.\n  # TODO: Remove when bridge no longer needed\n  rngs: nnx.Rngs = None\n\n  @property\n  def freqs_cis(self):\n    \"\"\"Frequencies for rotary embedding.\"\"\"\n    idx = self.image_size // self.patch_size\n    img_idx = jnp.arange(idx**2, dtype=jnp.int32).reshape(idx**2, 1)\n    img_idx = jnp.concatenate([img_idx, img_idx[:1]], axis=0)\n    img_idx = img_idx.at[-1, -1].set(-2)  # ID_CLS_TOKEN\n\n    # Get 2D coordinates\n    frequencies_x = img_idx % idx  # x coordinates\n    frequencies_y = img_idx // idx  # y coordinates\n\n    # Compute frequency dimensions\n    freq_dim = self.hidden_size // self.num_attention_heads // 2\n    rope_freq = 1.0 / (self.rope_theta ** (jnp.arange(0, freq_dim, 2)[: (freq_dim // 2)].astype(jnp.float32) / freq_dim))\n\n    # Compute frequencies for x and y coordinates\n    freqs_x = (frequencies_x + 1)[..., None] * rope_freq[None, None, :]\n    freqs_y = (frequencies_y + 1)[..., None] * rope_freq[None, None, :]\n\n    # Interleave x and y frequencies\n    freqs_x = jnp.repeat(freqs_x, 2, axis=-1)\n    freqs_y = jnp.repeat(freqs_y, 2, axis=-1)\n\n    # Combine frequencies\n    freqs = jnp.concatenate([freqs_x, freqs_y], axis=-1).astype(jnp.float32)\n    freqs = freqs[..., ::2]\n\n    # Mask out invalid positions\n    freqs = jnp.where(img_idx.reshape(-1, 1, 1) < 0, 0, freqs)\n    # Convert to complex representation\n    return jnp.exp(1j * freqs)\n\n  def __call__(self, inputs: Array, position: None | Array = None) -> Array:\n    \"\"\"Applies rotary embeddings to the input tensor for Llama4 vision encoder.\n\n    Args:\n      inputs: Input tensor of shape [batch_size_times_tiles, num_patches_incl_cls, num_heads, head_dim]\n\n    Returns:\n      Tensor with rotary embeddings applied, maintaining the same shape as input.\n    \"\"\"\n    if len(inputs.shape) != 4:\n      raise ValueError(\n          \"\"\"Input is assumed to be a rank 4 tensor of shape [batch_size_times_tiles, num_patches_incl_cls, \n          num_heads, head_dim].\"\"\"\n      )\n\n    # Reshape inputs to complex representation\n    B, S, N, H = inputs.shape\n    half_dim = H // 2\n\n    # Convert the last dimension into a complex representation.\n    # First reshape so that each pair of numbers represents the real and imaginary parts.\n    inputs_reshaped = inputs.reshape(B, S, N, half_dim, 2)\n    inputs_complex = inputs_reshaped[..., 0] + 1j * inputs_reshaped[..., 1]\n\n    # Reshape freqs_ci for broadcasting\n    freqs_ci = self.freqs_cis[jnp.newaxis, :, :, :]\n\n    # Apply rotary transformation\n    rotated = inputs_complex * freqs_ci\n\n    # Convert the complex result back to a real tensor.\n    # Split the complex number into its real and imaginary parts.\n    rotated_real = jnp.stack([jnp.real(rotated), jnp.imag(rotated)], axis=-1)\n    output = rotated_real.reshape(B, S, N, H)\n\n    if self.cast_as_fprop_dtype:\n      output = output.astype(self.fprop_dtype)\n\n    return output",
        "analysis": {
            "module_type": "llama_vision_rotary_embedding",
            "purpose": "Implements rotary position embeddings for a vision encoder, generating embeddings based on 2D image patch coordinates.",
            "input": {
                "shape": "See the `__call__` method.",
                "dtype": "See the `__call__` method."
            },
            "processing_steps": [
                "Initializes parameters such as image size, patch size, and hidden size.",
                "The `__call__` method applies the rotary embeddings to an input tensor."
            ],
            "output": {
                "shape": "See the `__call__` method."
            },
            "dependencies": [
                "flax.nnx",
                "jax.numpy"
            ],
            "parameters": {
                "image_size": "The size of the input image.",
                "patch_size": "The size of the image patches.",
                "hidden_size": "The size of the hidden dimension.",
                "num_attention_heads": "The number of attention heads.",
                "rope_theta": "The base theta value for the frequency computation.",
                "cast_as_fprop_dtype": "Whether to cast the output to the fprop dtype.",
                "fprop_dtype": "The dtype of the output if casting is enabled."
            },
            "notes": [
                "This implementation is based on the Llama4 vision encoder's rotary embedding approach.",
                "It uses 2D coordinates (x, y) derived from patch indices to generate position embeddings.",
                "A special CLS token is handled by setting its index to -2 and masking its frequency contribution to zero."
            ],
            "methods": {
                "freqs_cis": {
                    "purpose": "Pre-computes the complex-valued frequencies (cisoids) for the rotary position embeddings based on 2D patch coordinates.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculate the number of patches per side of the image.",
                        "Generate a 1D index for each patch and append a special index for the CLS token.",
                        "Convert the 1D indices into 2D coordinates (x, y).",
                        "Compute the base frequency dimensions using `rope_theta`.",
                        "Calculate frequencies for x and y coordinates separately.",
                        "Interleave and concatenate x and y frequencies to form a combined frequency tensor.",
                        "Mask out invalid positions (e.g., the CLS token) by setting their frequencies to zero.",
                        "Convert the final frequencies to a complex representation using `jnp.exp(1j * freqs)`."
                    ],
                    "output": {
                        "shape": "[num_patches_incl_cls, 1, head_dim / 2]"
                    },
                    "dependencies": [
                        "jax.numpy"
                    ],
                    "notes": [
                        "This is a cached property, computed once and reused on subsequent calls."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the pre-computed rotary position embeddings to an input tensor.",
                    "input": {
                        "shape": "[batch_size_times_tiles, num_patches_incl_cls, num_heads, head_dim]",
                        "dtype": "float"
                    },
                    "processing_steps": [
                        "Validate that the input tensor has 4 dimensions.",
                        "Reshape the last dimension of the input into a complex representation (real and imaginary parts).",
                        "Retrieve the pre-computed complex frequencies from `self.freqs_cis`.",
                        "Broadcast and multiply the complex input with the complex frequencies.",
                        "Convert the resulting complex tensor back to a real tensor by stacking its real and imaginary parts.",
                        "Reshape the output back to the original input shape.",
                        "Optionally cast the output to `fprop_dtype`."
                    ],
                    "output": {
                        "shape": "[batch_size_times_tiles, num_patches_incl_cls, num_heads, head_dim]"
                    },
                    "dependencies": [
                        "self.freqs_cis"
                    ],
                    "notes": [
                        "The `position` argument is defined but not used in this implementation."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/encoders.py#VisionEncoder",
        "file_path": "src/MaxText/layers/encoders.py",
        "code_block": "class VisionEncoder(nn.Module):\n  \"\"\"Vision encoder to encode images into soft tokens.\"\"\"\n\n  config: Config\n  mesh: Mesh\n\n  def setup(self):\n    self.vision_encoder_layer = self.get_vision_encoder_layers()\n\n  def get_vision_encoder_layers(self):\n    \"\"\"Get vision encoder layers specific to the model, classes of nn.Module type.\"\"\"\n    if self.config.model_name in [\"gemma3-4b\", \"gemma3-12b\", \"gemma3-27b\"]:\n      from MaxText.layers import gemma3  # pylint: disable=import-outside-toplevel\n\n      return [gemma3.gemma3visionencoder_as_linen, gemma3.visionembedder_as_linen]\n    elif self.config.model_name in [\"llama4-17b-16e\", \"llama4-17b-128e\"]:\n      from MaxText.layers import llama4  # pylint: disable=import-outside-toplevel\n\n      return [llama4.Llama4VisionModel, llama4.Llama4MultiModalProjector]\n    else:\n      raise ValueError(f\"No VisionEncoder implemented for {self.config.model_name} yet\")\n\n  @nn.compact\n  def __call__(self, input_images, deterministic=False):\n    cfg = self.config\n    mesh = self.mesh\n    # vision encoder output, frozen params in many cases\n    embeddings = self.vision_encoder_layer[0](config=cfg, mesh=mesh)(input_images, deterministic=deterministic)\n    if cfg.freeze_vision_encoder_params:\n      embeddings = jax.lax.stop_gradient(embeddings)\n\n    if len(self.vision_encoder_layer) > 1:\n      # vision embedder / projection layer, not frozen in most cases, trained / finetuned together with main model\n      embeddings = self.vision_encoder_layer[1](config=cfg, mesh=mesh)(embeddings)\n    return embeddings",
        "analysis": {
            "module_type": "vision_encoder",
            "purpose": "A Flax module that encodes input images into a sequence of soft token embeddings using a model-specific vision model and an optional projection layer.",
            "input": {
                "shape": "[batch_size, height, width, channels]",
                "dtype": "float32"
            },
            "processing_steps": [
                "In the `setup` method, dynamically selects the appropriate vision encoder and projector layer classes based on `config.model_name`.",
                "In the `__call__` method, passes the input images through the selected vision encoder layer.",
                "Optionally stops the gradient flow from the vision encoder's output if `config.freeze_vision_encoder_params` is true.",
                "If a second layer (projector) is specified, passes the embeddings through it.",
                "Returns the final image embeddings."
            ],
            "output": {
                "shape": "[batch_size, num_tokens, embedding_dim]"
            },
            "dependencies": [
                "flax.linen.Module",
                "jax.lax.stop_gradient",
                "MaxText.common_types.Config",
                "jax.sharding.Mesh",
                "MaxText.layers.gemma3",
                "MaxText.layers.llama4"
            ],
            "parameters": {
                "config": "Configuration object containing model settings, specifically `model_name` to select the correct layers and `freeze_vision_encoder_params` to control gradient flow.",
                "mesh": "The JAX sharding mesh for distributed computation."
            },
            "notes": [
                "This class acts as a factory and wrapper, dynamically choosing the underlying vision model implementation based on the configuration.",
                "It supports a two-stage encoding process: a base vision model followed by a multimodal projector."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the `vision_encoder_layer` attribute by calling `get_vision_encoder_layers`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `self.get_vision_encoder_layers()` to retrieve the appropriate layer classes.",
                        "Assigns the returned list of classes to `self.vision_encoder_layer`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "self.get_vision_encoder_layers"
                    ],
                    "notes": [
                        "This is a standard Flax method for deferred initialization of module parameters and submodules."
                    ]
                },
                "get_vision_encoder_layers": {
                    "purpose": "Selects and returns the appropriate vision encoder and projector layer classes based on the model name in the configuration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Reads `self.config.model_name`.",
                        "Performs a conditional check to find a matching model name (e.g., 'gemma3-...' or 'llama4-...').",
                        "Dynamically imports the required module (e.g., `MaxText.layers.gemma3`).",
                        "Returns a list of the corresponding vision model and projector layer classes.",
                        "Raises a ValueError if no implementation is found for the given model name."
                    ],
                    "output": {
                        "shape": "A list of one or two nn.Module classes."
                    },
                    "dependencies": [
                        "MaxText.layers.gemma3",
                        "MaxText.layers.llama4"
                    ],
                    "notes": [
                        "Uses dynamic, inline imports to avoid loading unnecessary large modules."
                    ]
                },
                "__call__": {
                    "purpose": "Processes a batch of input images through the selected vision layers to produce embeddings.",
                    "input": {
                        "shape": "[batch_size, height, width, channels]",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Instantiates and calls the primary vision encoder layer (`self.vision_encoder_layer[0]`) with the input images.",
                        "If `config.freeze_vision_encoder_params` is true, applies `jax.lax.stop_gradient` to the resulting embeddings.",
                        "If a projector layer exists (`len(self.vision_encoder_layer) > 1`), instantiates and calls it with the embeddings.",
                        "Returns the final embeddings."
                    ],
                    "output": {
                        "shape": "[batch_size, num_tokens, embedding_dim]"
                    },
                    "dependencies": [
                        "jax.lax.stop_gradient"
                    ],
                    "notes": [
                        "The `deterministic` flag is passed to the underlying vision encoder, which typically controls behaviors like dropout."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma.py#GemmaDecoderLayer",
        "file_path": "src/MaxText/layers/gemma.py",
        "code_block": "class GemmaDecoderLayer(nn.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_manager=None,\n      page_state=None,\n      slot=None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    # inputs: embedded inputs to the decoder with shape [batch, length, emb_dim]\n    lnx = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_self_attention_norm\",\n        kernel_axes=(\"norm\",),\n    )(inputs)\n\n    lnx = nn.with_logical_constraint(lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    attention_layer = attention_as_linen(\n        config=cfg,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention\",\n        float32_qk_product=cfg.float32_qk_product,\n        float32_logits=cfg.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        use_ragged_attention=cfg.use_ragged_attention,\n        ragged_block_size=cfg.ragged_block_size,\n        model_mode=model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n\n    attention_lnx = nn.with_logical_constraint(\n        attention_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n    attention_lnx += inputs\n    residual = attention_lnx\n    attn_output = rms_norm(\n        num_features=attention_lnx.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_ffw_norm\",\n        kernel_axes=(\"norm\",),\n    )(attention_lnx)\n\n    # MLP block.\n    mlp_lnx = mlp_block(\n        in_features=attn_output.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp\",\n        config=cfg,\n        quant=self.quant,\n    )(attn_output, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    next_layer_addition = mlp_lnx + residual\n\n    next_layer_addition_dropped_out = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(\n        next_layer_addition, deterministic=deterministic\n    )\n\n    layer_output = next_layer_addition_dropped_out\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"),\n    )\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "gemma_decoder_layer",
            "purpose": "Implements a single layer of a Gemma-style Transformer decoder, including self-attention and a feed-forward network with residual connections.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes the decoder layer with configuration, mesh, and optional quantization settings."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.linen.Module as nn",
                "MaxText.common_types.Config",
                "jax.sharding.Mesh",
                "MaxText.layers.quantizations.AqtQuantization as Quant",
                "MaxText.layers.normalizations.rms_norm",
                "MaxText.layers.attentions.attention_as_linen",
                "MaxText.layers.linears.mlp_block"
            ],
            "parameters": {
                "config": "A configuration object containing model hyperparameters like dimensions, dropout rates, and activation functions.",
                "mesh": "The JAX device mesh for distributed training and inference.",
                "model_mode": "A string indicating the operational mode, e.g., 'train', 'prefill', 'autoregressive'.",
                "quant": "Optional quantization configuration for weights and activations."
            },
            "notes": [
                "This is a standard Transformer decoder layer using pre-normalization (RMSNorm is applied before the attention and MLP sub-layers).",
                "It contains two residual connections: one after the self-attention block and another after the MLP block."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Performs the forward pass of the decoder layer, applying self-attention and a feed-forward network.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim], decoder_segment_ids: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length]",
                        "dtype": "Depends on config.dtype (e.g., float32, bfloat16)."
                    },
                    "processing_steps": [
                        "Apply RMS normalization to the input tensor.",
                        "Perform self-attention on the normalized input.",
                        "Add the original input to the attention output (first residual connection).",
                        "Apply a second RMS normalization to the result of the first residual connection.",
                        "Pass the result through a multi-layer perceptron (MLP) block.",
                        "Add the result of the first residual connection to the MLP output (second residual connection).",
                        "Apply dropout to the final result.",
                        "Optionally record internal activation metrics if `config.record_internal_nn_metrics` is true.",
                        "Return the final layer output."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]. If config.scan_layers is True, returns a tuple (output, None)."
                    },
                    "dependencies": [
                        "rms_norm",
                        "attention_as_linen",
                        "mlp_block",
                        "nn.Dropout",
                        "jax.numpy as jnp"
                    ],
                    "notes": [
                        "The return signature changes based on the `config.scan_layers` flag to be compatible with `flax.linen.scan`.",
                        "The arguments `previous_chunk`, `page_manager`, `page_state`, and `slot` are passed but not used in the main logic shown, suggesting they are for specific inference modes not detailed here."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma2.py#Gemma2DecoderLayer",
        "file_path": "src/MaxText/layers/gemma2.py",
        "code_block": "class Gemma2DecoderLayer(nn.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n    if model_mode == MODEL_MODE_PREFILL:\n      activation_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n    else:\n      activation_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n    inputs = nn.with_logical_constraint(inputs, activation_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    # inputs: embedded inputs to the decoder with shape [batch, length, emb_dim]\n    lnx = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_self_attention_norm_local\",\n        kernel_axes=(\"norm\",),\n    )(inputs)\n    lnx = nn.with_logical_constraint(lnx, activation_axis_names)\n\n    attention_layer = attention_as_linen(\n        config=cfg,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention_local\",\n        float32_qk_product=cfg.float32_qk_product,\n        float32_logits=cfg.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        attention_type=AttentionType.LOCAL_SLIDING,\n        sliding_window_size=cfg.sliding_window_size,\n        attn_logits_soft_cap=cfg.attn_logits_soft_cap,\n        model_mode=model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n    if cfg.use_post_attn_norm:\n      attention_lnx = rms_norm(\n          num_features=attention_lnx.shape[-1],\n          dtype=cfg.dtype,\n          weight_dtype=cfg.weight_dtype,\n          name=\"post_self_attention_norm_local\",\n          kernel_axes=(\"norm\",),\n      )(attention_lnx)\n    attention_lnx = nn.with_logical_constraint(attention_lnx, activation_axis_names)\n    attention_lnx += inputs\n    residual = attention_lnx\n\n    attn_output = rms_norm(\n        num_features=attention_lnx.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_ffw_norm_local\",\n        kernel_axes=(\"norm\",),\n    )(attention_lnx)\n\n    # MLP block.\n    mlp_lnx = mlp_block(\n        in_features=attn_output.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp_local\",\n        model_mode=model_mode,\n        config=cfg,\n        quant=self.quant,\n    )(attn_output, deterministic=deterministic)\n\n    if cfg.use_post_ffw_norm:\n      mlp_lnx = rms_norm(\n          num_features=mlp_lnx.shape[-1],\n          dtype=cfg.dtype,\n          weight_dtype=cfg.weight_dtype,\n          name=\"post_ffw_norm_local\",\n          kernel_axes=(\"norm\",),\n      )(mlp_lnx)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, activation_axis_names)\n\n    next_layer_addition = mlp_lnx + residual\n\n    next_layer_addition_dropped_out = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(\n        next_layer_addition, deterministic=deterministic\n    )\n\n    layer_output = next_layer_addition_dropped_out\n    layer_output = nn.with_logical_constraint(layer_output, activation_axis_names)\n\n    ### global part\n    inputs = nn.with_logical_constraint(layer_output, activation_axis_names)\n    # inputs: embedded inputs to the decoder with shape [batch, length, emb_dim]\n    lnx = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_self_attention_norm_global\",\n        kernel_axes=(\"norm\",),\n    )(inputs)\n    lnx = nn.with_logical_constraint(lnx, activation_axis_names)\n\n    attention_layer = attention_as_linen(\n        config=cfg,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention_global\",\n        float32_qk_product=True,\n        float32_logits=True,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        attention_type=AttentionType.GLOBAL,\n        attn_logits_soft_cap=cfg.attn_logits_soft_cap,\n        model_mode=model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n    if cfg.use_post_attn_norm:\n      attention_lnx = rms_norm(\n          num_features=attention_lnx.shape[-1],\n          dtype=cfg.dtype,\n          weight_dtype=cfg.weight_dtype,\n          name=\"post_self_attention_norm_global\",\n          kernel_axes=(\"norm\",),\n      )(attention_lnx)\n\n    attention_lnx = nn.with_logical_constraint(attention_lnx, activation_axis_names)\n    attention_lnx += inputs\n    residual = attention_lnx\n\n    attn_output = rms_norm(\n        num_features=attention_lnx.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_ffw_norm_global\",\n        kernel_axes=(\"norm\",),\n    )(attention_lnx)\n\n    # MLP block.\n    mlp_lnx = mlp_block(\n        in_features=attn_output.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp_global\",\n        model_mode=model_mode,\n        config=cfg,\n        quant=self.quant,\n    )(attn_output, deterministic=deterministic)\n    if cfg.use_post_ffw_norm:\n      mlp_lnx = rms_norm(\n          num_features=mlp_lnx.shape[-1],\n          dtype=cfg.dtype,\n          weight_dtype=cfg.weight_dtype,\n          name=\"post_ffw_norm_global\",\n          kernel_axes=(\"norm\",),\n      )(mlp_lnx)\n\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, activation_axis_names)\n\n    next_layer_addition = mlp_lnx + residual\n\n    next_layer_addition_dropped_out = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(\n        next_layer_addition, deterministic=deterministic\n    )\n\n    layer_output = next_layer_addition_dropped_out\n    layer_output = nn.with_logical_constraint(layer_output, activation_axis_names)\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "gemma2_decoder_layer",
            "purpose": "Implements a single transformer decoder layer for the Gemma2 model, which uniquely consists of a local (sliding window) attention block followed by a global attention block.",
            "input": {
                "shape": "[batch_size, sequence_length, embedding_dim]",
                "dtype": "float32 or bfloat16 (specified by config.dtype)"
            },
            "processing_steps": [
                "The `__call__` method is invoked to process the input through the layer."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, embedding_dim]"
            },
            "dependencies": [
                "flax.linen.Module",
                "MaxText.common_types.Config",
                "jax.sharding.Mesh",
                "MaxText.layers.quantizations.AqtQuantization"
            ],
            "parameters": {
                "config": "The model configuration object containing hyperparameters like dtype, dropout_rate, mlp_dim, attention type, etc.",
                "mesh": "The JAX device mesh for distributed computation.",
                "model_mode": "Specifies the operational mode, e.g., 'prefill' or 'decode'.",
                "quant": "Optional quantization configuration for the layer."
            },
            "notes": [
                "This layer is specific to the Gemma2 architecture.",
                "It processes inputs through two sequential blocks: a local attention block and a global attention block.",
                "Each block is a standard transformer sub-layer containing self-attention and an MLP, with residual connections and layer normalization."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Processes an input tensor through one complete Gemma2 decoder layer, applying both local and global attention mechanisms.",
                    "input": {
                        "shape": "{'inputs': '[batch_size, sequence_length, embedding_dim]', 'decoder_segment_ids': '[batch_size, sequence_length]', 'decoder_positions': '[batch_size, sequence_length]'}",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Apply RMS normalization to the input for the local attention block.",
                        "Perform local sliding window self-attention.",
                        "Optionally apply post-attention RMS normalization.",
                        "Add a residual connection from the original input.",
                        "Apply RMS normalization before the local MLP block.",
                        "Pass the result through the local MLP block.",
                        "Optionally apply post-MLP RMS normalization.",
                        "Add a residual connection from the attention block's output.",
                        "Apply dropout to the output of the local block.",
                        "Apply RMS normalization to the local block's output for the global attention block.",
                        "Perform global self-attention.",
                        "Optionally apply post-attention RMS normalization.",
                        "Add a residual connection from the local block's output.",
                        "Apply RMS normalization before the global MLP block.",
                        "Pass the result through the global MLP block.",
                        "Optionally apply post-MLP RMS normalization.",
                        "Add a residual connection from the global attention block's output.",
                        "Apply dropout to the final result.",
                        "Optionally record internal activation metrics if `config.record_internal_nn_metrics` is true.",
                        "Return the final layer output."
                    ],
                    "output": {
                        "shape": "Returns `[batch_size, sequence_length, embedding_dim]`. If `config.scan_layers` is true, returns a tuple `(output, None)`."
                    },
                    "dependencies": [
                        "rms_norm",
                        "attention_as_linen",
                        "mlp_block",
                        "nn.Dropout",
                        "quantizations.configure_kv_quant",
                        "jax.ad_checkpoint.checkpoint_name",
                        "flax.linen.with_logical_constraint"
                    ],
                    "notes": [
                        "The `model_mode` argument ('prefill' or 'decode') determines the logical axis names for tensor sharding.",
                        "The use of post-attention and post-MLP normalization is conditional on `config.use_post_attn_norm` and `config.use_post_ffw_norm` respectively."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#get_attention_type",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def get_attention_type(layer_id):\n  layer_id %= len(GEMMA3_ATTENTION_PATTERN)\n  return GEMMA3_ATTENTION_PATTERN[layer_id]",
        "analysis": {
            "module_type": "attention_type_selector",
            "purpose": "Determines the attention type for a given layer ID based on a predefined repeating pattern.",
            "input": {
                "shape": "scalar",
                "dtype": "int"
            },
            "processing_steps": [
                "Calculate the layer ID modulo the length of the `GEMMA3_ATTENTION_PATTERN` constant.",
                "Return the `AttentionType` enum from `GEMMA3_ATTENTION_PATTERN` at the resulting index."
            ],
            "output": {
                "shape": "scalar (AttentionType enum)"
            },
            "dependencies": [
                "GEMMA3_ATTENTION_PATTERN",
                "MaxText.common_types.AttentionType"
            ],
            "parameters": {},
            "notes": [
                "The function implements a repeating pattern of attention types with a cycle length of 6, as defined by `GEMMA3_ATTENTION_PATTERN`.",
                "The pattern consists of five `LOCAL_SLIDING` attention types followed by one `GLOBAL` attention type."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#get_query_pre_attn_scalar",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def get_query_pre_attn_scalar(config) -> float:\n  \"\"\"Returns the scalar to multiply the query by before attention.\"\"\"\n  if config.model_name in [\"gemma3-4b\", \"gemma3-12b\"]:\n    return config.head_dim**-0.5\n  elif config.model_name == \"gemma3-27b\":\n    return (config.base_emb_dim // config.base_num_query_heads) ** -0.5\n  else:\n    raise ValueError(f\"Unsupported model name: {config.model_name}\")",
        "analysis": {
            "module_type": "query_pre_attention_scalar_getter",
            "purpose": "Calculates and returns a model-specific scalar used to scale the query tensor before the attention computation.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check the `config.model_name` attribute.",
                "If the model is 'gemma3-4b' or 'gemma3-12b', calculate the scalar as `config.head_dim ** -0.5`.",
                "If the model is 'gemma3-27b', calculate the scalar as `(config.base_emb_dim // config.base_num_query_heads) ** -0.5`.",
                "If the model name is not supported, raise a ValueError.",
                "Return the calculated float scalar."
            ],
            "output": {
                "shape": "scalar"
            },
            "dependencies": [],
            "parameters": {
                "config": "A configuration object containing model-specific parameters like `model_name`, `head_dim`, `base_emb_dim`, and `base_num_query_heads`."
            },
            "notes": [
                "The function implements a conditional logic based on the model name to apply different scaling factors.",
                "This scaling is a form of normalization for the query tensor in the attention mechanism, similar to the standard scaled dot-product attention."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Gemma3DecoderLayer",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Gemma3DecoderLayer(nn.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n  attention_type: AttentionType = AttentionType.LOCAL_SLIDING\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=None,\n      bidirectional_mask=None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    # inputs: embedded inputs to the decoder with shape [batch, length, emb_dim]\n    lnx = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_self_attention_norm\",\n        kernel_axes=(\"norm\",),\n    )(inputs)\n\n    lnx = nn.with_logical_constraint(lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    query_pre_attn_scalar = get_query_pre_attn_scalar(cfg)\n\n    attention_layer = attention_as_linen(\n        config=cfg,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention\",\n        float32_qk_product=cfg.float32_qk_product,\n        float32_logits=cfg.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        attention_type=self.attention_type,\n        sliding_window_size=cfg.sliding_window_size,\n        attn_logits_soft_cap=cfg.attn_logits_soft_cap,\n        use_qk_norm=True,  # Gemma 3 models use query, key normalizations\n        query_pre_attn_scalar=query_pre_attn_scalar,\n        model_mode=model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n        bidirectional_mask=bidirectional_mask,\n    )\n    if cfg.use_post_attn_norm:\n      attention_lnx = rms_norm(\n          num_features=attention_lnx.shape[-1],\n          dtype=cfg.dtype,\n          weight_dtype=cfg.weight_dtype,\n          name=\"post_self_attention_norm\",\n          kernel_axes=(\"norm\",),\n      )(attention_lnx)\n\n    attention_lnx = nn.with_logical_constraint(\n        attention_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n    attention_lnx += inputs\n    residual = attention_lnx\n\n    attn_output = rms_norm(\n        num_features=attention_lnx.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_ffw_norm\",\n        kernel_axes=(\"norm\",),\n    )(attention_lnx)\n\n    # MLP block.\n    mlp_lnx = mlp_block(\n        in_features=attn_output.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp\",\n        config=cfg,\n        quant=self.quant,\n    )(attn_output, deterministic=deterministic)\n\n    if cfg.use_post_ffw_norm:\n      mlp_lnx = rms_norm(\n          num_features=mlp_lnx.shape[-1],\n          dtype=cfg.dtype,\n          weight_dtype=cfg.weight_dtype,\n          name=\"post_ffw_norm\",\n          kernel_axes=(\"norm\",),\n      )(mlp_lnx)\n\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    next_layer_addition = mlp_lnx + residual\n    next_layer_addition_dropped_out = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(\n        next_layer_addition, deterministic=deterministic\n    )\n\n    layer_output = next_layer_addition_dropped_out\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"),\n    )\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "gemma3_decoder_layer",
            "purpose": "Implements a single transformer decoder layer for a Gemma 3 model, including self-attention and a feed-forward network with residual connections.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "N/A"
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.linen.Module",
                "MaxText.common_types.Config",
                "jax.sharding.Mesh",
                "MaxText.layers.quantizations.AqtQuantization",
                "MaxText.layers.normalizations.rms_norm",
                "MaxText.layers.attentions.attention_as_linen",
                "MaxText.layers.linears.mlp_block",
                "MaxText.layers.gemma3.get_query_pre_attn_scalar"
            ],
            "parameters": {
                "config": "The main configuration object containing model hyperparameters like dimensions, dropout rates, and feature flags (e.g., use_post_attn_norm).",
                "mesh": "The JAX device mesh used for model parallelism and sharding.",
                "model_mode": "A string indicating the operational mode, e.g., 'train' or 'decode'.",
                "quant": "Optional quantization configuration for the layer's weights.",
                "attention_type": "The type of attention mechanism to use, defaulting to LOCAL_SLIDING."
            },
            "notes": [
                "This layer follows a pre-normalization architecture, applying RMSNorm before the self-attention and MLP blocks.",
                "It contains two residual connections: one after the attention block and one after the MLP block.",
                "The specific attention behavior (e.g., local sliding vs. global) is determined by the `attention_type` parameter provided during initialization."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Executes the forward pass of the decoder layer, applying self-attention and a feed-forward network with residual connections.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "Inferred from config.dtype (e.g., float32, bfloat16)"
                    },
                    "processing_steps": [
                        "Apply pre-attention RMS normalization to the input tensor.",
                        "Calculate a query pre-attention scalar using `get_query_pre_attn_scalar`.",
                        "Apply the self-attention mechanism using `attention_as_linen`.",
                        "Optionally, apply post-attention RMS normalization if `config.use_post_attn_norm` is true.",
                        "Add the original input to the attention output (first residual connection).",
                        "Apply pre-feed-forward RMS normalization to the result of the first residual connection.",
                        "Pass the result through a multi-layer perceptron block (`mlp_block`).",
                        "Optionally, apply post-feed-forward RMS normalization if `config.use_post_ffw_norm` is true.",
                        "Add the result from the first residual connection to the MLP output (second residual connection).",
                        "Apply dropout to the final output.",
                        "Optionally, record internal activation metrics if `config.record_internal_nn_metrics` is true."
                    ],
                    "output": {
                        "shape": "Returns a tensor of shape [batch_size, sequence_length, hidden_dim]. If `config.scan_layers` is true, it returns a tuple `(tensor, None)`."
                    },
                    "dependencies": [
                        "rms_norm",
                        "get_query_pre_attn_scalar",
                        "attention_as_linen",
                        "mlp_block",
                        "flax.linen.Dropout"
                    ],
                    "notes": [
                        "The method's return signature changes based on the `config.scan_layers` flag to support JAX's `scan` transformation.",
                        "It handles various operational modes (e.g., 'train', 'decode') which are passed down to the attention layer.",
                        "Uses a model-specific scalar (`query_pre_attn_scalar`) to scale the query tensor before attention computation, a feature of Gemma 3 models."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Gemma3ScannableBlock",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Gemma3ScannableBlock(nn.Module):\n  \"\"\"A repeatable block of Gemma3 decoder layers.\n\n    This block applies multiple decoder layers sequentially, using the attention\n    pattern defined by GEMMA3_ATTENTION_PATTERN. It's designed to be\n    used with `nn.scan` for efficient compilation.\n\n  Attributes:\n    config: Config, MaxText model config\n    mesh: Mesh, JAX device mesh (used for sharding)\n    quant: None | Quant, quantization config\n    num_of_layers: int, number of decoder layers in the block\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n  num_of_layers: int = 1\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      slot=None,\n      page_state=None,\n      previous_chunk=None,\n      bidirectional_mask=None,\n  ):\n\n    cfg = self.config\n    mesh = self.mesh\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    y = inputs\n    for layer_id in range(self.num_of_layers):\n      attention_type = get_attention_type(layer_id)\n      layer = Gemma3DecoderLayer(\n          config=cfg,\n          mesh=mesh,\n          model_mode=model_mode,\n          name=f\"layers_{layer_id}\",\n          quant=self.quant,\n          attention_type=attention_type,\n      )\n      y = layer(\n          y,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n          previous_chunk=previous_chunk,\n          page_state=page_state,\n          slot=slot,\n          bidirectional_mask=bidirectional_mask,\n      )\n      if cfg.scan_layers:\n        y = y[0]\n    if cfg.scan_layers:\n      return y, None\n    else:\n      return y",
        "analysis": {
            "module_type": "gemma3_scannable_block",
            "purpose": "Applies a sequence of Gemma3 decoder layers, designed for efficient compilation with `nn.scan` by grouping multiple layers into a single scannable unit.",
            "input": {
                "shape": "inputs: [batch_size, sequence_length, embedding_dim]",
                "dtype": "float (determined by config.dtype)"
            },
            "processing_steps": [
                "Apply a logical constraint and a gradient checkpoint name to the input tensor.",
                "Initialize an output tensor `y` with the input tensor.",
                "Iterate `num_of_layers` times, processing the tensor `y` through a `Gemma3DecoderLayer` in each iteration.",
                "In each iteration, determine the attention type for the layer using the `get_attention_type` helper function.",
                "If `config.scan_layers` is true, unpack the output from the layer to update `y`.",
                "After the loop, return the final tensor `y`, potentially in a tuple with `None` if `config.scan_layers` is true."
            ],
            "output": {
                "shape": "If `config.scan_layers` is true, returns a tuple `(output_tensor, None)` where `output_tensor` has shape `[batch_size, sequence_length, embedding_dim]`. Otherwise, returns just the `output_tensor` with the same shape."
            },
            "dependencies": [
                "Gemma3DecoderLayer",
                "get_attention_type",
                "flax.linen.Module",
                "jax.ad_checkpoint.checkpoint_name"
            ],
            "parameters": {
                "config": "The MaxText model configuration object.",
                "mesh": "The JAX device mesh for sharding.",
                "model_mode": "A string indicating the operational mode (e.g., 'train', 'decode').",
                "quant": "Optional quantization configuration.",
                "num_of_layers": "The number of decoder layers to apply sequentially within this block."
            },
            "notes": [
                "This module is specifically designed to be used with `nn.scan` for performance, which influences its return signature when `config.scan_layers` is enabled.",
                "The attention mechanism used in each internal `Gemma3DecoderLayer` follows a cyclical pattern defined by `GEMMA3_ATTENTION_PATTERN`."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Processes the input tensor through a series of `num_of_layers` Gemma3 decoder layers.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, embedding_dim], decoder_segment_ids: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length]",
                        "dtype": "float for `inputs`, integer for others."
                    },
                    "processing_steps": [
                        "Apply logical constraints and a gradient checkpoint name to the input tensor.",
                        "Loop from 0 to `self.num_of_layers`.",
                        "For each layer index, get the specific attention type via `get_attention_type`.",
                        "Create and call a `Gemma3DecoderLayer` with the current hidden state and other inputs.",
                        "If `config.scan_layers` is true, update the hidden state with the first element of the layer's output tuple.",
                        "After the loop, return the final hidden state."
                    ],
                    "output": {
                        "shape": "If `config.scan_layers` is true, returns `(tensor, None)` where tensor has shape `[batch_size, sequence_length, embedding_dim]`. Otherwise, returns just the tensor."
                    },
                    "dependencies": [
                        "Gemma3DecoderLayer",
                        "get_attention_type"
                    ],
                    "notes": [
                        "The method's return signature is conditional on the `config.scan_layers` flag to support `nn.scan`'s requirement for a `(carry, output)` tuple."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#_posemb_sincos_2d",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def _posemb_sincos_2d(\n    h: int,\n    w: int,\n    *,\n    width: int,\n    temperature: float = 10_000.0,\n    precision: str = \"default\",\n    dtype: jnp.dtype = jnp.float32,\n):\n  \"\"\"Follows the MoCo v3 logic.\"\"\"\n  y, x = jnp.mgrid[:h, :w]  # pylint: disable=unpacking-non-sequence\n\n  assert width % 4 == 0, \"Width must be mult of 4 for sincos posemb\"\n  omega = jnp.arange(width // 4) / (width // 4 - 1)\n  omega = 1.0 / (temperature**omega)\n  y = jnp.einsum(\"m,d->md\", y.flatten(), omega, precision=precision)\n  x = jnp.einsum(\"m,d->md\", x.flatten(), omega, precision=precision)\n  pe = jnp.concatenate([jnp.sin(x), jnp.cos(x), jnp.sin(y), jnp.cos(y)], axis=1)\n  return jnp.asarray(pe, dtype)[None, :, :]",
        "analysis": {
            "module_type": "sinusoidal_positional_embedding_2d",
            "purpose": "Generates a 2D sinusoidal positional embedding for a grid of a given height and width, following the logic from MoCo v3.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Create 2D coordinate grids for y and x positions using `jnp.mgrid`.",
                "Calculate angular frequencies (`omega`) based on the embedding `width` and `temperature`.",
                "Apply the frequencies to the flattened y and x coordinates using `jnp.einsum`.",
                "Compute the sine and cosine of the scaled x and y coordinates.",
                "Concatenate the sin(x), cos(x), sin(y), and cos(y) components along the feature dimension.",
                "Add a leading batch dimension to the final embedding tensor and cast to the specified dtype."
            ],
            "output": {
                "shape": "[1, h * w, width]"
            },
            "dependencies": [
                "jax.numpy"
            ],
            "parameters": {
                "h": "The height of the 2D grid.",
                "w": "The width of the 2D grid.",
                "width": "The dimension of the output positional embedding.",
                "temperature": "The temperature parameter for scaling the frequencies.",
                "precision": "The precision for the `jnp.einsum` operation.",
                "dtype": "The data type of the output tensor."
            },
            "notes": [
                "The implementation follows the logic from MoCo v3.",
                "The `width` parameter must be a multiple of 4.",
                "This is a standalone helper function that takes scalar integers for dimensions, not tensors."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#MlpBlockViT",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class MlpBlockViT(nnx.Module):\n  \"\"\"NNX version of Transformer MLP / feed-forward block.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      block_id: int,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.config = config\n    self.block_id = block_id\n    self.rngs = rngs\n\n    self.Dense_0 = DenseGeneral(\n        in_features_shape=self.config.hidden_size_for_vit,\n        out_features_shape=self.config.intermediate_size_for_vit,\n        dtype=self.config.dtype_mm,\n        use_bias=True,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n    self.Dropout_0 = nnx.Dropout(rate=self.config.dropout_rate)\n    self.Dense_1 = DenseGeneral(\n        in_features_shape=self.config.intermediate_size_for_vit,\n        out_features_shape=self.config.hidden_size_for_vit,\n        dtype=self.config.dtype_mm,\n        use_bias=True,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n\n  def __call__(self, x: jax.Array, deterministic: bool = True) -> jax.Array:\n    \"\"\"Applies the Transformer MlpBlock module.\"\"\"\n    x = self.Dense_0(x)\n    x = nnx.gelu(x)\n    x = self.Dropout_0(x, deterministic=deterministic)\n    x = self.Dense_1(x)\n    return x",
        "analysis": {
            "module_type": "vision_transformer_mlp_block",
            "purpose": "Implements a standard two-layer feed-forward network (MLP block) for a Vision Transformer using the Flax NNX API.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes a DenseGeneral layer to project to an intermediate size.",
                "Initializes an nnx.Dropout layer.",
                "Initializes a second DenseGeneral layer to project back to the original hidden size."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx.Module",
                "DenseGeneral",
                "nnx.Dropout",
                "nnx.Rngs",
                "Config"
            ],
            "parameters": {
                "config.hidden_size_for_vit": "The input and output feature dimension of the MLP block.",
                "config.intermediate_size_for_vit": "The feature dimension of the intermediate layer in the MLP block.",
                "config.dtype_mm": "The data type for matrix multiplication operations.",
                "config.dropout_rate": "The dropout rate applied after the GELU activation.",
                "config.matmul_precision": "The precision for matrix multiplication operations."
            },
            "notes": [
                "This is the NNX (Flax's new API) version of a standard transformer feed-forward block."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Applies the MLP transformation to an input tensor.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_size_for_vit]",
                        "dtype": "Matches config.dtype_mm"
                    },
                    "processing_steps": [
                        "Apply the first dense layer (self.Dense_0).",
                        "Apply the GELU activation function (nnx.gelu).",
                        "Apply dropout (self.Dropout_0).",
                        "Apply the second dense layer (self.Dense_1)."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_size_for_vit]"
                    },
                    "dependencies": [
                        "self.Dense_0",
                        "nnx.gelu",
                        "self.Dropout_0",
                        "self.Dense_1"
                    ],
                    "notes": [
                        "The `deterministic` parameter controls whether dropout is applied. If True, dropout is skipped."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Encoder1DBlock",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Encoder1DBlock(nnx.Module):\n  \"\"\"Single transformer encoder block (MHSA + MLP).\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      block_id: int,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.block_id = block_id\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n    self.seq_len = (self.config.image_size_for_vit // self.config.patch_size_for_vit) ** 2\n\n    self.LayerNorm_0 = nnx.LayerNorm(num_features=self.config.hidden_size_for_vit, rngs=self.rngs)\n    self.MultiHeadDotProductAttention_0 = Attention(\n        config=self.config,\n        num_query_heads=self.config.num_attention_heads_for_vit,\n        num_kv_heads=self.config.num_attention_heads_for_vit,\n        head_dim=self.config.hidden_size_for_vit // self.config.num_attention_heads_for_vit,\n        max_target_length=self.seq_len,\n        mesh=self.mesh,\n        attention_kernel=\"dot_product\",\n        inputs_q_shape=(self.config.per_device_batch_size, self.seq_len, self.config.hidden_size_for_vit),\n        inputs_kv_shape=(self.config.per_device_batch_size, self.seq_len, self.config.hidden_size_for_vit),\n        dropout_rate=0,\n        is_nope_layer=True,\n        use_bias_in_projections=True,\n        attention_type=AttentionType.FULL,\n        use_qk_norm=False,\n        query_pre_attn_scalar=1 / (self.config.hidden_size_for_vit // self.config.num_attention_heads_for_vit) ** 0.5,\n        model_mode=\"train\",\n        is_vision=True,\n        rngs=self.rngs,\n    )\n    self.LayerNorm_1 = nnx.LayerNorm(num_features=self.config.hidden_size_for_vit, rngs=self.rngs)\n    self.MlpBlockViT_0 = MlpBlockViT(\n        block_id=self.block_id,\n        config=self.config,\n        rngs=self.rngs,\n    )\n    self.Dropout_0 = nnx.Dropout(self.config.dropout_rate, rngs=self.rngs)\n\n  def __call__(self, x: jax.Array, deterministic: bool = False) -> jax.Array:\n    y = self.LayerNorm_0(x)\n\n    y = self.MultiHeadDotProductAttention_0(inputs_q=y, inputs_kv=y, deterministic=deterministic)\n    y = self.Dropout_0(y, deterministic=deterministic)\n    x = x + y\n\n    y = self.LayerNorm_1(x)\n    y = self.MlpBlockViT_0(y, deterministic=deterministic)\n    y = self.Dropout_0(y, deterministic=deterministic)\n    x = x + y\n    return x",
        "analysis": {
            "functionality": "The `Encoder1DBlock` class implements a single Vision Transformer (ViT) encoder block. It consists of a multi-head self-attention (MHSA) layer followed by a multi-layer perceptron (MLP) block, each preceded by layer normalization and followed by a residual connection.",
            "usage": "Instantiate the class with a configuration object, a JAX mesh, a block ID, and NNX RNGs. Call the instance with an input tensor of shape `[batch_size, sequence_length, hidden_dim]` to perform the forward pass. The output will have the same shape as the input."
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Encoder",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Encoder(nnx.Module):\n  \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n\n    for lyr in range(self.config.num_hidden_layers_for_vit):\n      layer_name = f\"encoderblock_{lyr}\"\n      layer = Encoder1DBlock(\n          block_id=lyr,\n          config=self.config,\n          mesh=self.mesh,\n          rngs=self.rngs,\n      )\n      setattr(self, layer_name, layer)\n    self.encoder_norm = nnx.LayerNorm(num_features=self.config.hidden_size_for_vit, rngs=self.rngs)\n\n  def __call__(self, x: jax.Array, deterministic: bool = True) -> jax.Array:\n    # TODO(aireenmei, hengtaoguo): add if-scan branch to enable scan support for vision encoder\n    for lyr in range(self.config.num_hidden_layers_for_vit):\n      x = getattr(self, f\"encoderblock_{lyr}\")(x, deterministic=deterministic)\n    x = self.encoder_norm(x)\n    return x",
        "analysis": {
            "module_type": "vision_transformer_encoder",
            "purpose": "A stack of Transformer encoder blocks designed to process sequences of embeddings, typically for vision tasks.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes `config.num_hidden_layers_for_vit` instances of `Encoder1DBlock` in a loop.",
                "Initializes a final `nnx.LayerNorm` layer."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx.Module",
                "Encoder1DBlock",
                "nnx.LayerNorm",
                "jax.sharding.Mesh",
                "MaxText.common_types.Config"
            ],
            "parameters": {
                "config.num_hidden_layers_for_vit": "The number of `Encoder1DBlock` layers to create and stack.",
                "config.hidden_size_for_vit": "The feature dimension for the input, output, and normalization layers."
            },
            "notes": [
                "This class acts as a container that sequentially applies multiple identical encoder blocks.",
                "The layers are created dynamically in the constructor using `setattr`."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Encoder by creating a specified number of Encoder1DBlock layers and a final LayerNorm.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Iterate from 0 to `config.num_hidden_layers_for_vit`.",
                        "Instantiate an `Encoder1DBlock` for each layer index.",
                        "Assign the created layer to the module using `setattr` with a name like 'encoderblock_{lyr}'.",
                        "Instantiate a final `nnx.LayerNorm` layer as `self.encoder_norm`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Encoder1DBlock",
                        "nnx.LayerNorm"
                    ],
                    "notes": [
                        "This constructor takes a configuration object, a JAX mesh, and random number generators as input."
                    ]
                },
                "__call__": {
                    "purpose": "Sequentially applies each encoder block to the input tensor, followed by a final layer normalization.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_size_for_vit]",
                        "dtype": "jax.Array"
                    },
                    "processing_steps": [
                        "Iterate from 0 to `config.num_hidden_layers_for_vit`.",
                        "Retrieve the corresponding encoder block using `getattr`.",
                        "Apply the retrieved encoder block to the input tensor `x`.",
                        "Apply the final `encoder_norm` layer to the result."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_size_for_vit]"
                    },
                    "dependencies": [],
                    "notes": [
                        "The `deterministic` flag is passed to each `Encoder1DBlock` to control behaviors like dropout.",
                        "A TODO comment indicates a plan to add support for `jax.lax.scan` for potential performance improvements."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Einsum",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Einsum(nnx.Module):\n  \"\"\"Einsum is a convenience module for parameterized tensor multiplication.\"\"\"\n\n  def __init__(\n      self,\n      shape: tuple[int, ...],\n      initializer: nnx.initializers.Initializer = nnx.initializers.normal(),\n      dtype: jnp.dtype | None = None,\n      precision: str = \"default\",\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.precision = precision\n    self.w = nnx.Param(initializer(rngs.params(), shape, dtype))\n\n  def __call__(self, eqn: str, x: jax.Array) -> jax.Array:\n    return jnp.einsum(eqn, x, self.w, precision=self.precision)",
        "analysis": {
            "module_type": "parameterized_einsum",
            "purpose": "A convenience module for performing parameterized tensor multiplication using an einsum operation.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes a learnable weight parameter `w` with a given shape.",
                "When called, performs a `jnp.einsum` operation between an input tensor `x` and the internal weight `w`."
            ],
            "output": {
                "shape": "The output shape is determined by the einsum equation string and the input tensor shapes."
            },
            "dependencies": [
                "nnx.Module",
                "nnx.Param",
                "nnx.initializers.Initializer",
                "jnp.einsum"
            ],
            "parameters": {
                "shape": "The shape of the learnable weight tensor `w`.",
                "initializer": "The initializer function for the weight tensor.",
                "dtype": "The data type of the weight tensor.",
                "precision": "The precision for the `jnp.einsum` operation."
            },
            "notes": [
                "This module encapsulates a single learnable weight tensor `w` which is used in the einsum operation."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the module by creating a learnable weight parameter `w`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores the `precision` attribute.",
                        "Initializes a learnable weight tensor `self.w` using `nnx.Param` with the specified `shape`, `dtype`, and `initializer`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.Param",
                        "nnx.initializers.Initializer"
                    ],
                    "notes": [
                        "The `rngs` argument is required to initialize the parameter."
                    ]
                },
                "__call__": {
                    "purpose": "Performs a tensor multiplication between the input `x` and the internal weight `w` using the provided einsum equation.",
                    "input": {
                        "shape": "Any shape compatible with the provided `eqn` string and the internal weight's shape.",
                        "dtype": "jax.Array dtype (e.g., float32)"
                    },
                    "processing_steps": [
                        "Calls `jnp.einsum` with the equation string, the input tensor `x`, the internal weight tensor `self.w`, and the specified precision."
                    ],
                    "output": {
                        "shape": "Depends on the `eqn` string and the shapes of the input and weight tensors."
                    },
                    "dependencies": [
                        "jnp.einsum"
                    ],
                    "notes": [
                        "The first argument `eqn` is a string that defines the einsum operation."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#VisionEmbedder",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class VisionEmbedder(nnx.Module):\n  \"\"\"Projects image embeddings to the embedding space of the text encoder.\"\"\"\n\n  def __init__(self, config: Config, mesh: Mesh, *, rngs: nnx.Rngs):\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n\n    self.mm_soft_embedding_norm = RMSNorm(\n        num_features=self.config.hidden_size_for_vit,\n        dtype=self.config.dtype_mm,\n        weight_dtype=self.config.weight_dtype,\n        epsilon=self.config.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n    self.mm_input_projection = Einsum(\n        shape=(self.config.hidden_size_for_vit, self.config.emb_dim),\n        precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n\n  def __call__(self, x: jax.Array, eqn: str = \"...tm,md->...td\") -> jax.Array:\n    x = self.mm_soft_embedding_norm(x)\n    x = self.mm_input_projection(eqn, x)\n    return x",
        "analysis": {
            "module_type": "vision_embedder",
            "purpose": "Projects image embeddings from the vision transformer's embedding space to the text encoder's embedding space by applying normalization and a linear projection.",
            "input": {
                "shape": "[..., sequence_length, hidden_size_for_vit]",
                "dtype": "config.dtype_mm"
            },
            "processing_steps": [
                "Apply RMS normalization to the input tensor using `mm_soft_embedding_norm`.",
                "Project the normalized tensor to a new dimension using the `mm_input_projection` (Einsum) layer."
            ],
            "output": {
                "shape": "[..., sequence_length, emb_dim]"
            },
            "dependencies": [
                "nnx.Module",
                "RMSNorm",
                "Einsum",
                "Config"
            ],
            "parameters": {
                "config.hidden_size_for_vit": "The feature dimension of the input image embeddings.",
                "config.emb_dim": "The target embedding dimension, matching the text encoder's embedding space.",
                "config.dtype_mm": "The data type for the module's computations.",
                "config.weight_dtype": "The data type for the weights of the normalization layer.",
                "config.matmul_precision": "The precision for the einsum operation in the projection layer."
            },
            "notes": [
                "This class is an `nnx.Module`.",
                "The projection is implemented using a flexible `Einsum` module, which performs a parameterized tensor multiplication."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the VisionEmbedder module, setting up the normalization and projection layers.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Instantiate an `RMSNorm` layer as `self.mm_soft_embedding_norm`.",
                        "Instantiate an `Einsum` layer as `self.mm_input_projection` with a weight shape of `(config.hidden_size_for_vit, config.emb_dim)`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "Mesh",
                        "nnx.Rngs",
                        "RMSNorm",
                        "Einsum"
                    ],
                    "notes": [
                        "The layers are configured using parameters from the `Config` object."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass by applying normalization and projection to the input tensor.",
                    "input": {
                        "shape": "[..., sequence_length, hidden_size_for_vit]",
                        "dtype": "jax.Array"
                    },
                    "processing_steps": [
                        "Call `self.mm_soft_embedding_norm` on the input tensor `x`.",
                        "Call `self.mm_input_projection` on the normalized tensor with the specified einsum equation."
                    ],
                    "output": {
                        "shape": "[..., sequence_length, emb_dim]"
                    },
                    "dependencies": [
                        "jax.Array"
                    ],
                    "notes": [
                        "The `einsum` equation `eqn` defaults to '...tm,md->...td', which maps the last dimension from `hidden_size_for_vit` to `emb_dim`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#visionembedder_as_linen",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def visionembedder_as_linen(\n    config: Config,\n    mesh: Mesh,\n):\n  \"\"\"Creates a VisionEmbedder module.\"\"\"\n  return nnx_wrappers.to_linen(\n      VisionEmbedder,\n      config,\n      mesh=mesh,\n      name=\"VisionEmbedder_0\",\n      abstract_init=False,\n      metadata_fn=variable_to_logically_partitioned,\n  )",
        "analysis": {
            "module_type": "linen_vision_embedder_wrapper",
            "purpose": "A factory function that wraps the NNX `VisionEmbedder` module into a Flax Linen-compatible module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `VisionEmbedder` NNX module into a Flax Linen module.",
                "Passes the configuration, mesh, name, and a metadata function for logical partitioning to the wrapper."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance of the VisionEmbedder."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "VisionEmbedder",
                "variable_to_logically_partitioned",
                "Config",
                "jax.sharding.Mesh"
            ],
            "parameters": {
                "config": "The model and training configuration object.",
                "mesh": "The JAX device mesh for model parallelism."
            },
            "notes": [
                "This function serves as a bridge to use an NNX-defined module (`VisionEmbedder`) within a Linen-based model architecture.",
                "The `metadata_fn` is used to apply logical partitioning rules to the module's parameters for distributed training."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#VisionExit",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class VisionExit(nnx.Module):\n  \"\"\"The vision exit layer.\n\n  Possibly downsample the soft tokens to a required output length.\n\n  Attributes:\n    output_length: The embed will be spatially avg-pooled to this output length.\n  \"\"\"\n\n  def __init__(self, output_length: int = 256, *, rngs: nnx.Rngs):\n    self.output_length = output_length\n    self.rngs = rngs\n\n  def __call__(self, x):\n    cur_length = x.shape[1]\n    if cur_length == self.output_length:\n      return x\n    cur_width = int(cur_length**0.5)\n    assert cur_width**2 == cur_length\n    output_width = int(self.output_length**0.5)\n    assert output_width**2 == self.output_length, f\"Cannot pool {x.shape=} to {self.output_length}=!\"\n    batch_size = x.shape[0]\n    embed_dim = x.shape[-1]\n    x = jnp.reshape(x, (batch_size, cur_width, cur_width, embed_dim))\n    assert not cur_width % output_width, f\"{cur_width=} {output_width=}\"\n    window = cur_width // output_width\n    window_shape = (window, window)\n    x = nnx.avg_pool(x, window_shape=window_shape, strides=window_shape)\n    batch_size, height, width, embed_dim = x.shape\n    return jnp.reshape(x, (batch_size, height * width, embed_dim))",
        "analysis": {
            "module_type": "vision_exit_layer",
            "purpose": "Downsamples a sequence of vision tokens to a specified output length by reshaping them into a 2D grid and applying average pooling.",
            "input": {
                "shape": "[batch_size, sequence_length, embed_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Checks if the input sequence length matches the target output length, returning the input if they are equal.",
                "Reshapes the input tensor from [batch_size, sequence_length, embed_dim] to a 4D tensor [batch_size, width, height, embed_dim], assuming the sequence length is a perfect square.",
                "Calculates the window size for average pooling based on the ratio of the input width to the target output width.",
                "Applies 2D average pooling using `nnx.avg_pool` to downsample the feature map.",
                "Reshapes the pooled tensor back to the shape [batch_size, output_length, embed_dim]."
            ],
            "output": {
                "shape": "[batch_size, output_length, embed_dim]"
            },
            "dependencies": [
                "flax.nnx",
                "jax.numpy"
            ],
            "parameters": {
                "output_length": "The target sequence length for the output tokens after pooling. Must be a perfect square."
            },
            "notes": [
                "This module assumes that both the input `sequence_length` and the target `output_length` are perfect squares, as it treats the sequence as a 2D grid.",
                "It includes assertions to validate that the lengths are perfect squares and that the input grid width is divisible by the output grid width."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the VisionExit module with a target output length.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Sets the `output_length` attribute.",
                        "Stores the `rngs` object required by the `nnx.Module` base class."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "flax.nnx.Rngs"
                    ],
                    "notes": [
                        "The `rngs` parameter is required by the `nnx.Module` base class but is not used in the logic of this specific module."
                    ]
                },
                "__call__": {
                    "purpose": "Applies spatial average pooling to the input tensor to match the target output length.",
                    "input": {
                        "shape": "[batch_size, sequence_length, embed_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Return input directly if its length already matches `output_length`.",
                        "Reshape the input from a 1D sequence to a 2D grid: [B, L, D] -> [B, H, W, D].",
                        "Calculate the window size for pooling based on the ratio of current width to output width.",
                        "Apply 2D average pooling using `nnx.avg_pool`.",
                        "Reshape the pooled output back to a 1D sequence: [B, H', W', D] -> [B, L', D]."
                    ],
                    "output": {
                        "shape": "[batch_size, output_length, embed_dim]"
                    },
                    "dependencies": [
                        "jax.numpy.reshape",
                        "flax.nnx.avg_pool"
                    ],
                    "notes": [
                        "Asserts that both input and output lengths are perfect squares.",
                        "Asserts that the input grid width is perfectly divisible by the output grid width."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#vision_exit_as_linen",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def vision_exit_as_linen(x: jax.Array, output_length: int) -> jax.Array:\n  \"\"\"A wrapper to use VisionExit as a function.\"\"\"\n  return nnx.bridge.to_linen(VisionExit, output_length=output_length)(x)",
        "analysis": {
            "functionality": "Wraps the `VisionExit` NNX module, converting it to a Flax Linen module on-the-fly and applying it to an input tensor to downsample its sequence length.",
            "usage": "Call the function with a JAX array `x` of shape `[batch_size, sequence_length, embed_dim]` and an integer `output_length`. It returns a JAX array of shape `[batch_size, output_length, embed_dim]`."
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Gemma3VisionEncoderLayer",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Gemma3VisionEncoderLayer(nnx.Module):\n  \"\"\"gemma 3 vision encoder layer\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n\n    self.embedding = nnx.Conv(\n        in_features=self.config.num_channels_for_vit,\n        out_features=self.config.hidden_size_for_vit,\n        kernel_size=(self.config.patch_size_for_vit, self.config.patch_size_for_vit),\n        strides=self.config.conv_stride_for_vit,\n        padding=\"VALID\",\n        precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n    self.pos_embedding = self._get_posemb(\n        self.config.posemb_type_for_vit,\n        seqshape=(\n            self.config.image_size_for_vit // self.config.patch_size_for_vit,\n            self.config.image_size_for_vit // self.config.patch_size_for_vit,\n        ),\n        width=self.config.hidden_size_for_vit,\n        dtype=self.config.dtype_mm,\n    )\n    self.Dropout_0 = nnx.Dropout(self.config.dropout_rate, rngs=self.rngs)\n    self.Transformer = Encoder(\n        config=self.config,\n        mesh=self.mesh,\n        rngs=self.rngs,\n    )\n    self.VisionExit = VisionExit(output_length=256, rngs=self.rngs)\n\n  def _get_posemb(\n      self,\n      typ: str,\n      *,\n      seqshape: tuple[int, int],\n      width: int,\n      dtype: jnp.dtype = jnp.float32,\n  ):\n    \"\"\"Returns the position embedding.\"\"\"\n    if typ == \"learn\":\n      shape = (1, seqshape[0] * seqshape[1], width)\n      initializer = nnx.initializers.normal(stddev=1 / (width**0.5))\n      return nnx.Param(initializer(self.rngs.params(), shape, dtype))\n    elif typ == \"sincos2d\":\n      return _posemb_sincos_2d(*seqshape, width=width, dtype=dtype, precision=self.config.matmul_precision)\n    else:\n      raise ValueError(f\"Unknown posemb type: {typ}\")\n\n  def __call__(self, inputs, deterministic, train=False):\n    \"\"\"ViT model that transforms image inputs to image embeddings.\n    Args:\n      inputs: jnp.array shaped [B, N, H, W, C], e.g. [4, 1, 896, 896, 3]\n    Returns:\n      jnp.array for image embeddings, shaped [B, N, P, D], e.g. [4, 1, 256, 1152]\n    \"\"\"\n    # currently only supports N=1, the inputs shape is [B, H, W, C]\n    if len(inputs.shape) == 4:\n      inputs = inputs[:, None, :]\n    b, n, h, w, c = inputs.shape\n    x = jnp.reshape(inputs, [b * n, h, w, c])\n    # Gemma3 uses conv2d with stride 14 and kernel size 14 to extract patches.\n    x = self.embedding(x)\n    bn, h, w, c = x.shape\n    x = jnp.reshape(x, [bn, h * w, c])\n\n    x = self.pos_embedding + x\n    x = self.Dropout_0(x)\n\n    # Transformer encoder to extract image features.\n    x = self.Transformer(x, deterministic=deterministic)\n\n    # Gemma3 use a vision exit layer to downsample the soft tokens to a required output length.\n    x = self.VisionExit(x)\n    bn, l, c = x.shape\n    x = jnp.reshape(x, [b, n, l, c])\n    return x",
        "analysis": {
            "module_type": "gemma3_vision_encoder_layer",
            "purpose": "A Vision Transformer (ViT) model that transforms image inputs into a sequence of image embeddings.",
            "input": {
                "shape": "[batch_size, num_images, height, width, channels] or [batch_size, height, width, channels]",
                "dtype": "A JAX float type (e.g., float32)."
            },
            "processing_steps": [
                "Extracts image patches using a convolutional layer.",
                "Adds positional embeddings to the patches.",
                "Applies dropout for regularization.",
                "Processes the patch sequence through a Transformer encoder.",
                "Downsamples the output sequence to a fixed length using a vision exit layer."
            ],
            "output": {
                "shape": "[batch_size, num_images, output_length, hidden_dim]"
            },
            "dependencies": [
                "nnx.Module",
                "nnx.Conv",
                "nnx.Dropout",
                "Encoder",
                "VisionExit",
                "_posemb_sincos_2d",
                "Config",
                "Mesh",
                "nnx.Rngs"
            ],
            "parameters": {
                "config": "The model and training configuration object, providing parameters like hidden sizes, patch sizes, and dropout rates.",
                "mesh": "The JAX device mesh used for model parallelism and sharding.",
                "rngs": "Flax NNX random number generators for stochastic operations like dropout and initializations."
            },
            "notes": [
                "This class implements a Vision Transformer (ViT) style encoder specifically for the Gemma 3 architecture.",
                "The positional embedding can be either learned or a fixed sincos2d type, controlled by `config.posemb_type_for_vit`."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Gemma3VisionEncoderLayer, setting up the patch embedding, positional embedding, Transformer encoder, and vision exit layers.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initializes a convolutional layer (`nnx.Conv`) for patch embedding.",
                        "Generates positional embeddings using the `_get_posemb` helper method.",
                        "Initializes a dropout layer (`nnx.Dropout`).",
                        "Initializes the main `Encoder` module.",
                        "Initializes the `VisionExit` module for output downsampling."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.Conv",
                        "nnx.Dropout",
                        "Encoder",
                        "VisionExit",
                        "self._get_posemb"
                    ],
                    "notes": [
                        "All sub-modules are configured using parameters from the `Config` object."
                    ]
                },
                "_get_posemb": {
                    "purpose": "Generates and returns the positional embedding based on the specified type ('learn' or 'sincos2d').",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "If type is 'learn', creates a learnable `nnx.Param` with a normal initializer.",
                        "If type is 'sincos2d', calls the `_posemb_sincos_2d` function to generate fixed embeddings.",
                        "If type is unknown, raises a ValueError."
                    ],
                    "output": {
                        "shape": "[1, seqshape[0] * seqshape[1], width]"
                    },
                    "dependencies": [
                        "nnx.Param",
                        "nnx.initializers.normal",
                        "_posemb_sincos_2d"
                    ],
                    "notes": [
                        "This is a helper method called during the layer's initialization."
                    ]
                },
                "__call__": {
                    "purpose": "Processes a batch of images through the ViT encoder to produce image embeddings.",
                    "input": {
                        "shape": "[B, N, H, W, C] or [B, H, W, C]",
                        "dtype": "A JAX float type."
                    },
                    "processing_steps": [
                        "Reshape input to a standard 4D tensor [B*N, H, W, C].",
                        "Apply the convolutional embedding layer to extract patches.",
                        "Reshape the patched output into a sequence of shape [B*N, num_patches, hidden_dim].",
                        "Add the positional embedding to the patch embeddings.",
                        "Apply dropout.",
                        "Pass the sequence through the `Transformer` encoder.",
                        "Downsample the output sequence using the `VisionExit` layer.",
                        "Reshape the final output back to [B, N, output_length, hidden_dim]."
                    ],
                    "output": {
                        "shape": "[B, N, P, D] (e.g., [4, 1, 256, 1152])"
                    },
                    "dependencies": [
                        "jnp.reshape",
                        "self.embedding",
                        "self.pos_embedding",
                        "self.Dropout_0",
                        "self.Transformer",
                        "self.VisionExit"
                    ],
                    "notes": [
                        "The method internally handles reshaping for cases where the input has 4 dimensions (assuming N=1).",
                        "The `deterministic` flag is passed to the Transformer encoder and dropout layers to control their behavior during training vs. inference."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#gemma3visionencoder_as_linen",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def gemma3visionencoder_as_linen(\n    config: Config,\n    mesh: Mesh,\n):\n  \"\"\"Creates a Gemma3VisionEncoder module.\"\"\"\n  module = nnx_wrappers.to_linen(\n      Gemma3VisionEncoderLayer,\n      config=config,\n      mesh=mesh,\n      name=\"Gemma3VisionEncoderLayer_0\",\n      abstract_init=False,\n      metadata_fn=variable_to_logically_partitioned,\n  )\n  return module",
        "analysis": {
            "module_type": "vision_encoder_factory",
            "purpose": "A factory function that wraps the NNX `Gemma3VisionEncoderLayer` class into a Flax Linen module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `Gemma3VisionEncoderLayer` NNX module into a Flax Linen module.",
                "Passes the configuration, mesh, a specific name, and a metadata function for logical partitioning to the conversion wrapper."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance, shape is not applicable."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "Gemma3VisionEncoderLayer",
                "variable_to_logically_partitioned",
                "MaxText.common_types.Config",
                "jax.sharding.Mesh"
            ],
            "parameters": {
                "config": "A `Config` object containing model hyperparameters for the vision encoder.",
                "mesh": "A JAX `Mesh` object for device parallelism and sharding."
            },
            "notes": [
                "This function serves as a bridge to make an NNX-defined module compatible with a Flax Linen-based model architecture.",
                "The `abstract_init=False` argument indicates that the module's parameters are initialized immediately upon creation rather than during a separate initialization step.",
                "The `metadata_fn` is used to apply logical partitioning rules to the module's variables for sharding purposes."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt3.py#Gpt3LayerNorm",
        "file_path": "src/MaxText/layers/gpt3.py",
        "code_block": "class Gpt3LayerNorm(nnx.Module):\n  \"\"\"GPT3 Layer normalization operating on the last axis of the input data.\"\"\"\n\n  def __init__(\n      self,\n      num_features: int,\n      epsilon: float = 1e-6,\n      dtype: Any = jnp.float32,\n      weight_dtype: Any = jnp.float32,\n      kernel_axes: tuple[None | str, ...] = (),\n      scale_init: Initializer = nn.initializers.zeros,\n      use_bias: bool = True,\n      reductions_in_fp32: bool = False,\n      parameter_memory_host_offload: bool = False,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.epsilon = epsilon\n    self.dtype = dtype\n    self.weight_dtype = weight_dtype\n    self.kernel_axes = kernel_axes\n    self.scale_init = scale_init\n    self.use_bias = use_bias\n    self.reductions_in_fp32 = reductions_in_fp32\n    self.parameter_memory_host_offload = parameter_memory_host_offload\n\n    self.scale = nnx.Param(\n        self.scale_init(rngs.params(), (num_features,), self.weight_dtype),\n        sharding=self.kernel_axes,\n    )\n    if self.use_bias:\n      self.bias = nnx.Param(\n          initializers.default_bias_init(rngs.params(), (num_features,), self.weight_dtype), sharding=self.kernel_axes\n      )\n    else:\n      self.bias = None\n\n  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Applies layer normalization on the input.\"\"\"\n    if self.reductions_in_fp32:\n      x = jnp.asarray(x, jnp.float32)\n    mean = jnp.mean(x, axis=[-1], keepdims=True)\n    var = jnp.mean(jnp.square(x - mean), axis=[-1], keepdims=True)\n    normed_inputs = (x - mean) * lax.rsqrt(var + self.epsilon)\n    if self.reductions_in_fp32:\n      normed_inputs = normed_inputs.astype(self.dtype)\n\n    scale = self.scale.value\n    # Move scale to device if parameter offloading is enabled\n    if self.parameter_memory_host_offload:\n      max_logging.log(\"gpt3.py: Moving scale parameter to device\")\n      scale = jax.device_put(scale, max_utils.device_space())\n\n    scale = jnp.asarray(scale, self.dtype)\n    output = normed_inputs * (scale + 1)\n\n    if self.bias is not None:\n      bias = self.bias.value\n      bias = jnp.asarray(bias, self.dtype)\n      output += bias\n    return output",
        "analysis": {
            "module_type": "gpt3_layer_norm",
            "purpose": "A GPT-3 specific layer normalization module that normalizes the input tensor along its last axis.",
            "input": {
                "shape": "[..., num_features]",
                "dtype": "The dtype specified during initialization (e.g., jnp.float32)."
            },
            "processing_steps": [
                "The module is called with an input tensor `x`.",
                "It calculates the mean and variance of `x` along the last dimension.",
                "It normalizes `x` using the calculated mean and variance.",
                "It applies a learnable scale and optional bias to the normalized tensor.",
                "The final transformed tensor is returned."
            ],
            "output": {
                "shape": "Same as the input shape, [..., num_features]."
            },
            "dependencies": [
                "flax.nnx.Module",
                "jax.numpy",
                "jax.lax",
                "MaxText.layers.initializers"
            ],
            "parameters": {
                "num_features": "The number of features in the input tensor, i.e., the size of the last dimension.",
                "epsilon": "A small float added to variance for numerical stability.",
                "dtype": "The data type of the computation.",
                "weight_dtype": "The data type of the learnable parameters (scale and bias).",
                "use_bias": "A boolean indicating whether to include a learnable bias parameter.",
                "reductions_in_fp32": "If True, the mean and variance calculations are performed in float32 for higher precision.",
                "parameter_memory_host_offload": "If True, parameters are moved from host memory to the device during the forward pass."
            },
            "notes": [
                "This implementation is specific to GPT-3, notably applying the scale as `(scale + 1)` rather than the standard `scale`.",
                "The `scale` parameter is initialized to zeros by default, making the initial transformation an identity operation on the normalized inputs."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the layer's learnable parameters, `scale` and an optional `bias`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration arguments as attributes.",
                        "Initialize the 'scale' parameter using `nnx.Param` and the provided `scale_init`.",
                        "If `use_bias` is True, initialize the 'bias' parameter using `nnx.Param`.",
                        "If `use_bias` is False, set `self.bias` to None."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.Param",
                        "nnx.Rngs",
                        "initializers.default_bias_init"
                    ],
                    "notes": [
                        "Requires an `nnx.Rngs` object for parameter initialization."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the layer normalization transformation to an input tensor.",
                    "input": {
                        "shape": "[..., num_features]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Optionally cast input to float32 if `reductions_in_fp32` is True.",
                        "Calculate mean and variance along the last axis.",
                        "Normalize the input tensor using `(x - mean) * lax.rsqrt(var + self.epsilon)`.",
                        "Optionally cast the normalized tensor back to the original `self.dtype`.",
                        "Retrieve the `scale` parameter, optionally moving it to the device.",
                        "Apply the learnable scale: `normed_inputs * (scale + 1)`.",
                        "If `bias` exists, retrieve it and add it to the output.",
                        "Return the final tensor."
                    ],
                    "output": {
                        "shape": "Same as the input shape, [..., num_features]."
                    },
                    "dependencies": [
                        "jax.numpy",
                        "jax.lax",
                        "max_logging",
                        "max_utils"
                    ],
                    "notes": [
                        "The normalization is performed over the last axis (`axis=[-1]`) of the input tensor."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt3.py#gpt3_layer_norm",
        "file_path": "src/MaxText/layers/gpt3.py",
        "code_block": "def gpt3_layer_norm(\n    *,\n    num_features: int,\n    epsilon: float = 1e-6,\n    dtype: Any = jnp.float32,\n    weight_dtype: Any = jnp.float32,\n    kernel_axes: tuple[None | str, ...] = (),\n    scale_init: Initializer = nn.initializers.zeros,\n    use_bias: bool = True,\n    reductions_in_fp32: bool = False,\n    parameter_memory_host_offload: bool = False,\n    name: None | str = None,\n):\n  \"\"\"Initializes the gpt3_layer_norm module.\n\n  Args:\n    num_features: the number of features.\n    epsilon: the epsilon for the layer norm.\n    dtype: the dtype of the computation (default: float32).\n    weight_dtype: the dtype of the weights (default: float32).\n    kernel_axes: logical axes for partitioning the kernel.\n    scale_init: initializer for the scale.\n    use_bias: whether to add bias in linear transformation.\n    reductions_in_fp32: whether to do reductions in fp32.\n    parameter_memory_host_offload: Determines whether to offload params to host\n    name: name passed to the ToLinen Module\n  \"\"\"\n\n  module = nnx_wrappers.to_linen(\n      Gpt3LayerNorm,\n      num_features=num_features,\n      epsilon=epsilon,\n      dtype=dtype,\n      weight_dtype=weight_dtype,\n      kernel_axes=kernel_axes,\n      scale_init=scale_init,\n      use_bias=use_bias,\n      reductions_in_fp32=reductions_in_fp32,\n      parameter_memory_host_offload=parameter_memory_host_offload,\n      name=name,\n      metadata_fn=initializers.variable_to_logically_partitioned,\n  )\n  return module",
        "analysis": {
            "module_type": "gpt3_layer_norm_factory",
            "purpose": "A factory function that initializes and wraps the `Gpt3LayerNorm` NNX module into a Flax Linen compatible module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `Gpt3LayerNorm` NNX module into a Flax Linen module.",
                "Passes all configuration arguments (`num_features`, `epsilon`, etc.) to the `Gpt3LayerNorm` constructor via the wrapper.",
                "Returns the created Linen module."
            ],
            "output": {
                "shape": "A Flax Linen module instance of the wrapped `Gpt3LayerNorm`."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "Gpt3LayerNorm",
                "initializers.variable_to_logically_partitioned"
            ],
            "parameters": {
                "num_features": "The number of features in the input tensor, which corresponds to the size of the learnable scale and bias parameters.",
                "epsilon": "A small float added to the variance to avoid division by zero for numerical stability.",
                "dtype": "The data type for the computation.",
                "weight_dtype": "The data type for the layer's weights (scale and bias).",
                "reductions_in_fp32": "If True, the mean and variance calculations are performed in `float32` for improved numerical stability, with the input being cast before and the output cast back after.",
                "parameter_memory_host_offload": "Determines whether to offload parameters to host memory to save device memory."
            },
            "notes": [
                "This function serves as a bridge, making the NNX-defined `Gpt3LayerNorm` class usable within a Flax Linen model structure.",
                "The actual layer normalization logic is implemented within the `Gpt3LayerNorm` class, which this function instantiates."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt3.py#Gpt3MultiHeadAttention",
        "file_path": "src/MaxText/layers/gpt3.py",
        "code_block": "class Gpt3MultiHeadAttention(nn.Module):\n  \"\"\"Multi-head attention in gpt3.\n\n  Attributes:\n    num_heads: number of attention heads. Features (i.e. inputs_q.shape[-1])\n      should be divisible by the number of heads.\n    head_dim: dimension of each head.\n    max_target_length: maximum length of output\n    max_prefill_predict_length: size of the maximum prefill\n    mesh: device mesh\n    dtype: the dtype of the computation.\n    dropout_rate: dropout rate\n    kernel_init: initializer for the kernel of the Dense layers.\n    float32_qk_product: bool, if True then compute logits via float32 qk_product to avoid\n      numerical issues with bfloat16.\n    float32_logits: bool, if True then cast logits to float32 before softmax to avoid\n      numerical issues with bfloat16.\n    fused_qkv: whether to fuse query, key and value into one projection.\n    quant: Quant, stores quantization config, defaults to None implying no quantization.\n    use_bias: whether to add bias in linear transformation.\n  \"\"\"\n\n  config: Config\n  num_heads: int\n  head_dim: int\n  max_target_length: int\n  max_prefill_predict_length: int\n  mesh: Mesh\n  attention_kernel: str\n  dtype: DType = jnp.float32\n  weight_dtype: DType = jnp.float32\n  dropout_rate: float = 0.0\n  kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"normal\")\n  float32_qk_product: bool = False  # computes logits in float32 for stability.\n  float32_logits: bool = True  # cast logits in float32 for stability.\n  fused_qkv: bool = True\n  quant: None | Quant = None\n  kv_quant: None | KVQuant = None\n  use_bias: bool = True\n\n  input_axis_names: AxisNames = (BATCH, LENGTH, EMBED)\n  query_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV)\n  key_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV)\n  value_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV)\n  out_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV)\n\n  def qkv_projection(self, inputs: Array, proj_name: str):\n    \"\"\"Fused QKV projection\"\"\"\n\n    qkv_proj = dense_general(\n        inputs_shape=inputs.shape,\n        out_features_shape=(3, self.num_heads, self.head_dim),\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"embed\", \"qkv\", \"heads\", \"kv\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        name=proj_name,\n        quant=self.quant,\n        use_bias=self.use_bias,\n        matmul_precision=self.config.matmul_precision,\n    )(inputs)\n    qkv_proj = checkpoint_name(qkv_proj, \"qkv_proj\")\n    query, key, value = qkv_proj[:, :, 0, ...], qkv_proj[:, :, 1, ...], qkv_proj[:, :, 2, ...]\n    return query, key, value\n\n  def projection(self, inputs: Array, proj_name: str) -> Array:\n    \"\"\"individual projection for one of q, k and v.\"\"\"\n    proj = dense_general(\n        inputs_shape=inputs.shape,\n        out_features_shape=(self.num_heads, self.head_dim),\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"embed\", \"heads\", \"kv\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        name=proj_name,\n        quant=self.quant,\n        use_bias=self.use_bias,\n        matmul_precision=self.config.matmul_precision,\n    )(inputs)\n    return proj\n\n  def out_projection(self, output_dim: int, out: Array) -> Array:\n    \"\"\"output projection\"\"\"\n    out_proj = dense_general(\n        inputs_shape=out.shape,\n        out_features_shape=output_dim,\n        axis=(-2, -1),\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"heads\", \"kv\", \"embed\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        name=\"out\",\n        quant=self.quant,\n        use_bias=self.use_bias,\n        matmul_precision=self.config.matmul_precision,\n    )(out)\n    return out_proj\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs_q: Array,\n      decoder_segment_ids: Array | None = None,\n      *,\n      model_mode: str = MODEL_MODE_TRAIN,\n      deterministic: bool = False,\n  ):\n    inputs_q = nn.with_logical_constraint(inputs_q, self.input_axis_names)\n    if self.fused_qkv:\n      query, key, value = self.qkv_projection(inputs_q, proj_name=\"qkv_proj\")\n    else:\n      query = self.projection(inputs_q, proj_name=\"query\")\n      key = self.projection(inputs_q, proj_name=\"key\")\n      value = self.projection(inputs_q, proj_name=\"value\")\n\n    depth_scaling = jnp.sqrt(self.head_dim).astype(self.dtype)\n    query /= depth_scaling\n\n    # annotate with sharding constraint.\n    query = nn.with_logical_constraint(query, self.query_axis_names)\n    query = checkpoint_name(query, \"query_proj\")\n    key = nn.with_logical_constraint(key, self.key_axis_names)\n    key = checkpoint_name(key, \"key_proj\")\n    value = nn.with_logical_constraint(value, self.value_axis_names)\n    value = checkpoint_name(value, \"value_proj\")\n\n    attention_op = attention_op_as_linen(\n        config=self.config,\n        mesh=self.mesh,\n        attention_kernel=self.attention_kernel,\n        max_target_length=self.max_target_length,\n        float32_qk_product=self.float32_qk_product,\n        float32_logits=self.float32_logits,\n        quant=self.quant,\n        kv_quant=self.kv_quant,\n        num_query_heads=self.num_heads,\n        num_kv_heads=self.num_heads,\n        dtype=self.dtype,\n    )\n\n    out = attention_op(query, key, value, decoder_segment_ids, model_mode)\n\n    out = nn.with_logical_constraint(out, self.out_axis_names)\n\n    # apply output projection,  output dim is set to the input dim.\n    out = self.out_projection(inputs_q.shape[-1], out)\n    out = checkpoint_name(out, \"out_proj\")\n    return out",
        "analysis": {
            "functionality": "The `Gpt3MultiHeadAttention` class implements a multi-head self-attention mechanism, a core component of GPT-3 style transformer models. It projects an input sequence into query, key, and value tensors, computes scaled dot-product attention, and then projects the result back to the original input dimension. The class supports both fused and separate QKV projections and allows for different underlying attention kernel implementations.",
            "usage": "To use this class, instantiate it with a configuration object (`Config`), a device mesh, and other parameters like the number of heads and head dimension. Then, call the instance with an input tensor of shape `[batch_size, sequence_length, embedding_dim]`. The module will return an output tensor of the same shape, representing the contextually enriched representation of the input sequence."
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt3.py#Gpt3DecoderLayer",
        "file_path": "src/MaxText/layers/gpt3.py",
        "code_block": "class Gpt3DecoderLayer(nn.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: models.Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    lnx = gpt3_layer_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        name=\"pre_self_attention_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n        reductions_in_fp32=False,\n        use_bias=True,\n    )(inputs)\n\n    lnx = nn.with_logical_constraint(lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    # Self-attention block\n    assert (\n        cfg.num_query_heads == cfg.num_kv_heads\n    ), f\"{cfg.num_query_heads=} should be the same as {cfg.num_kv_heads=} in gpt3\"\n    attention_layer = Gpt3MultiHeadAttention(\n        config=cfg,\n        num_heads=cfg.num_query_heads,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        mesh=mesh,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention\",\n        float32_qk_product=cfg.float32_qk_product,\n        float32_logits=cfg.float32_logits,\n        fused_qkv=cfg.fused_qkv,\n        use_bias=True,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n    )\n\n    attention_lnx = attention_layer(\n        lnx, decoder_segment_ids=decoder_segment_ids, model_mode=model_mode, deterministic=deterministic\n    )\n\n    attention_lnx = nn.with_logical_constraint(\n        attention_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n    attention_lnx += inputs\n\n    # MLP block.\n    mlp_lnx = mlp_block(\n        in_features=attention_lnx.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp\",\n        use_bias=True,\n        use_pre_norm=True,\n        config=cfg,\n        quant=self.quant,\n    )(attention_lnx, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    layer_output = attention_lnx + mlp_lnx\n\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"),\n    )\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "gpt3_decoder_layer",
            "purpose": "Implements a single layer of a GPT-3 style transformer decoder, consisting of a self-attention block and a feed-forward MLP block.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "The `__call__` method executes the forward pass of the decoder layer."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.linen.Module",
                "maxtext.layers.models.Config",
                "jax.sharding.Mesh",
                "maxtext.layers.quantizations.AqtQuantization",
                "Gpt3MultiHeadAttention",
                "gpt3_layer_norm",
                "mlp_block"
            ],
            "parameters": {
                "config": "A configuration object containing model hyperparameters like dimensions, dropout rates, and data types.",
                "mesh": "The JAX device mesh for distributed computation.",
                "model_mode": "A string indicating the operational mode (e.g., 'train', 'decode').",
                "quant": "An optional quantization configuration object."
            },
            "notes": [
                "This module uses a pre-layer normalization architecture.",
                "It includes two residual connections: one after the self-attention block and one after the MLP block.",
                "The implementation asserts that the number of query heads equals the number of key-value heads, which is characteristic of the original GPT-3 architecture."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Performs the forward pass for a single GPT-3 decoder layer.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, embedding_dim]",
                        "dtype": "Depends on config.dtype (e.g., float32, bfloat16)."
                    },
                    "processing_steps": [
                        "Apply pre-attention layer normalization to the input tensor.",
                        "Pass the normalized tensor through the Gpt3MultiHeadAttention block.",
                        "Add the original input tensor to the attention output (first residual connection).",
                        "Pass the result through an MLP block.",
                        "Add the output of the attention block to the MLP output (second residual connection).",
                        "Apply dropout to the final output.",
                        "Optionally record internal activation metrics.",
                        "Return the final layer output."
                    ],
                    "output": {
                        "shape": "If config.scan_layers is False: [batch_size, sequence_length, embedding_dim]. If True: a tuple of ([batch_size, sequence_length, embedding_dim], None)."
                    },
                    "dependencies": [
                        "gpt3_layer_norm",
                        "Gpt3MultiHeadAttention",
                        "mlp_block",
                        "flax.linen.Dropout",
                        "jax.numpy"
                    ],
                    "notes": [
                        "The arguments `decoder_positions`, `previous_chunk`, `page_state`, and `slot` are present in the method signature for interface compatibility but are not used in this implementation.",
                        "The return value's structure depends on the `config.scan_layers` flag to support efficient layer stacking via `flax.linen.scan`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt_oss.py#get_attention_type",
        "file_path": "src/MaxText/layers/gpt_oss.py",
        "code_block": "def get_attention_type(layer_id):\n  \"\"\"Get attention type based on layer ID.\"\"\"\n  layer_id %= len(GPT_OSS_ATTENTION_PATTERN)\n  return GPT_OSS_ATTENTION_PATTERN[layer_id]",
        "analysis": {
            "module_type": "attention_type_selector",
            "purpose": "Determines the attention type for a given layer ID by cycling through a predefined pattern.",
            "input": {
                "shape": "N/A",
                "dtype": "int"
            },
            "processing_steps": [
                "Calculate an index by taking the input 'layer_id' modulo the length of the 'GPT_OSS_ATTENTION_PATTERN' constant.",
                "Return the attention type from 'GPT_OSS_ATTENTION_PATTERN' at the calculated index."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "GPT_OSS_ATTENTION_PATTERN"
            ],
            "parameters": {},
            "notes": [
                "The function implements a cyclic pattern for attention types.",
                "The pattern defined by 'GPT_OSS_ATTENTION_PATTERN' alternates between 'attentions.AttentionType.LOCAL_SLIDING' and 'attentions.AttentionType.GLOBAL'."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt_oss.py#GptOssDecoderLayer",
        "file_path": "src/MaxText/layers/gpt_oss.py",
        "code_block": "class GptOssDecoderLayer(nn.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: models.Config\n  mesh: Mesh\n  model_mode: str\n  attention_type: AttentionType\n  quant: Optional[Quant] = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    lnx_rms = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=jnp.float32,\n        name=\"pre_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )\n    lnx = lnx_rms(inputs)\n\n    lnx = nn.with_logical_constraint(lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    # Self-attention block\n    attention_layer = attention_as_linen(\n        config=cfg,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"GptOssAttention\",\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        use_bias_in_projections=cfg.attention_bias,\n        attention_type=self.attention_type,\n        sliding_window_size=cfg.sliding_window_size,\n        query_pre_attn_scalar=(cfg.head_dim**-0.5),\n        model_mode=model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n\n    attention_lnx = nn.with_logical_constraint(\n        attention_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n    intermediate_inputs = inputs + attention_lnx\n\n    # Fully Connected\n    hidden_states = rms_norm(\n        num_features=intermediate_inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=jnp.float32,\n        name=\"post_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )(intermediate_inputs)\n    hidden_states = nn.with_logical_constraint(\n        hidden_states, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n\n    load_balance_loss = None\n    mlp_lnx, load_balance_loss = moe.get_routed_moe(\n        name=\"GptOssMlp\",\n        config=cfg,\n        num_experts=cfg.num_experts,\n        num_experts_per_tok=cfg.num_experts_per_tok,\n        mesh=mesh,\n        kernel_init=initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        intermediate_dim=cfg.mlp_dim,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        quant=self.quant,\n    )(hidden_states)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"),\n    )\n\n    if load_balance_loss is not None:\n      self.sow(\"intermediates\", \"moe_lb_loss\", load_balance_loss)\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "transformer_decoder_layer",
            "purpose": "Implements a single transformer decoder layer for a GPT-style model, including self-attention and a Mixture-of-Experts (MoE) MLP block.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "Configurable via `config.dtype` (e.g., float32, bfloat16)."
            },
            "processing_steps": [
                "Apply pre-attention RMS normalization to the input tensor.",
                "Apply a self-attention mechanism (`GptOssAttention`).",
                "Add the attention output to the original input (first residual connection).",
                "Apply post-attention RMS normalization.",
                "Pass the result through a Mixture-of-Experts (MoE) MLP layer (`GptOssMlp`).",
                "Add the MLP output to the intermediate input (second residual connection).",
                "Apply dropout to the final output.",
                "Optionally record MoE load balancing loss and other activation metrics."
            ],
            "output": {
                "shape": "Returns a tensor of shape [batch_size, sequence_length, hidden_dim]. If `config.scan_layers` is true, returns a tuple `(tensor, None)`."
            },
            "dependencies": [
                "flax.linen.Module",
                "maxtext.layers.normalizations.rms_norm",
                "maxtext.layers.attentions.attention_as_linen",
                "maxtext.layers.moe.get_routed_moe",
                "maxtext.layers.models.Config",
                "jax.sharding.Mesh"
            ],
            "parameters": {
                "config": "The main configuration object containing model hyperparameters like `dtype`, `num_query_heads`, `num_kv_heads`, `head_dim`, `mlp_dim`, `num_experts`, etc.",
                "mesh": "The JAX device mesh for model parallelism and sharding.",
                "attention_type": "Specifies the type of attention mechanism to use (e.g., LOCAL_SLIDING, GLOBAL).",
                "quant": "Optional quantization configuration for weights and activations."
            },
            "notes": [
                "This layer uses two residual connections: one after the self-attention block and another after the MLP block.",
                "It employs RMSNorm for layer normalization before both the attention and MLP blocks.",
                "The MLP block is a Mixture-of-Experts (MoE) layer, which may compute and record a load balancing loss.",
                "The output format depends on the `config.scan_layers` flag, returning either the tensor directly or a tuple for use with `nn.scan`."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Processes the input tensor through one complete decoder layer, including self-attention and an MoE MLP.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim], decoder_segment_ids: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length]",
                        "dtype": "Configurable via `config.dtype`."
                    },
                    "processing_steps": [
                        "Apply pre-attention RMS normalization to the input tensor.",
                        "Apply the self-attention mechanism to the normalized input.",
                        "Add the attention output to the original input via a residual connection.",
                        "Apply post-attention RMS normalization to the result of the first residual connection.",
                        "Pass the normalized state through a Mixture-of-Experts (MoE) MLP layer.",
                        "Add the MLP output to the intermediate input via a second residual connection.",
                        "Apply dropout to the final output.",
                        "Optionally record MoE load balancing loss and other internal metrics using `self.sow`."
                    ],
                    "output": {
                        "shape": "Returns a tensor of shape [batch_size, sequence_length, hidden_dim]. If `config.scan_layers` is true, returns a tuple `(tensor, None)`."
                    },
                    "dependencies": [
                        "rms_norm",
                        "attention_as_linen",
                        "moe.get_routed_moe",
                        "nn.Dropout"
                    ],
                    "notes": [
                        "The `deterministic` flag controls whether dropout is applied.",
                        "The `model_mode` parameter is passed down to the attention layer to control its behavior during different phases like prefill or decode."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt_oss.py#GptOssScannableBlock",
        "file_path": "src/MaxText/layers/gpt_oss.py",
        "code_block": "class GptOssScannableBlock(nn.Module):\n  \"\"\"A repeatable block of GPT OSS decoder layers.\n\n    This block applies multiple decoder layers sequentially, using the attention\n    pattern defined by GPT_OSS_ATTENTION_PATTERN. It's designed to be\n    used with `nn.scan` for efficient compilation.\n\n  Attributes:\n    config: Config, MaxText model config\n    mesh: Mesh, JAX device mesh (used for sharding)\n    num_of_layers: int, number of decoder layers in the block\n    quant: Optional[Quant], quantization config\n  \"\"\"\n\n  config: models.Config\n  mesh: Mesh\n  model_mode: str\n  quant: Optional[Quant] = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n  ):\n\n    cfg = self.config\n    mesh = self.mesh\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    y = inputs\n    for layer_id in range(cfg.inhomogeneous_layer_cycle_interval):\n      attention_type = get_attention_type(layer_id)\n      layer = GptOssDecoderLayer(\n          config=cfg,\n          mesh=mesh,\n          model_mode=model_mode,\n          name=f\"layers_{layer_id}\",\n          attention_type=attention_type,\n          quant=self.quant,\n      )\n      y = layer(\n          y,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n      )\n      if cfg.scan_layers:\n        y = y[0]\n    if cfg.scan_layers:\n      return y, None\n    else:\n      return y",
        "analysis": {
            "functionality": "The `GptOssScannableBlock` class is a Flax module that encapsulates a sequence of `GptOssDecoderLayer`s. It is designed to be a repeatable unit within a larger transformer model, optimized for use with `nn.scan` for efficient JAX compilation. The block iteratively applies decoder layers, with each layer's attention mechanism determined by a predefined pattern.",
            "usage": "This class is instantiated with a model configuration, a JAX mesh for distributed computation, a model mode string, and an optional quantization configuration. The forward pass (`__call__` method) takes input tensors (`inputs`, `decoder_segment_ids`, `decoder_positions`), a `deterministic` flag for dropout, and the `model_mode`. It processes the inputs through its sequence of decoder layers and returns the output tensor. The return signature varies based on the `config.scan_layers` flag to support `nn.scan`."
        }
    },
    {
        "block_name": "src/MaxText/layers/initializers.py#nd_dense_init",
        "file_path": "src/MaxText/layers/initializers.py",
        "code_block": "def nd_dense_init(scale, mode, distribution):\n  \"\"\"Creates a variance-scaling initializer with dynamic in/out axes.\n\n  This function is a factory that returns an initializer function. The returned\n  function is a wrapper around `jax.nn.initializers.variance_scaling` that\n  allows the `in_axis` and `out_axis` to be specified at call time, rather\n  than at creation time.\n\n  Args:\n    scale: The scaling factor for the variance.\n    mode: The mode for variance scaling ('fan_in', 'fan_out', 'fan_avg').\n    distribution: The distribution to sample from ('normal', 'uniform', etc.).\n\n  Returns:\n    A function that takes a PRNG key, shape, dtype, in_axis, and out_axis,\n    and returns an initialized array.\n  \"\"\"\n\n  def init_fn(key, shape, dtype, in_axis, out_axis):\n    \"\"\"Initializes an array using variance scaling with specified axes.\"\"\"\n    fn = jax.nn.initializers.variance_scaling(scale, mode, distribution, in_axis, out_axis)\n    return fn(key, shape, dtype)\n\n  return init_fn",
        "analysis": {
            "module_type": "initializer_factory",
            "purpose": "A factory function that creates a variance-scaling initializer where the input and output axes can be specified dynamically at call time.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Defines an inner function `init_fn` that captures the `scale`, `mode`, and `distribution` arguments.",
                "The inner function `init_fn` creates a `jax.nn.initializers.variance_scaling` initializer using the captured arguments and the dynamically provided `in_axis` and `out_axis`.",
                "The inner function then calls the newly created JAX initializer with a PRNG key, shape, and dtype to generate and return an initialized array.",
                "Returns the `init_fn` function."
            ],
            "output": {
                "shape": "Returns a callable function. The returned function produces an array of the shape specified by its `shape` argument."
            },
            "dependencies": [
                "jax.nn.initializers.variance_scaling"
            ],
            "parameters": {
                "scale": "The scaling factor for the variance.",
                "mode": "The mode for variance scaling ('fan_in', 'fan_out', 'fan_avg').",
                "distribution": "The distribution to sample from ('normal', 'uniform', etc.)."
            },
            "notes": [
                "This function allows for deferred specification of `in_axis` and `out_axis`, making the returned initializer more flexible for N-dimensional tensors compared to creating `jax.nn.initializers.variance_scaling` directly."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/initializers.py#variable_to_logically_partitioned",
        "file_path": "src/MaxText/layers/initializers.py",
        "code_block": "def variable_to_logically_partitioned(variable: nnx.VariableState):\n  \"\"\"Wraps an NNX variable's value in `nn.LogicallyPartitioned`.\n\n  This function inspects the metadata of an `nnx.VariableState` object. If\n  sharding information ('sharding' or 'sharding_names') is present, it wraps\n  the variable's value in `nn.LogicallyPartitioned` to apply the specified\n  sharding constraints.\n\n  It handles special cases for `aqt_tensor.QTensor` and variables of type\n  `_overwrite_with_gradient` by returning their values directly without\n  wrapping.\n\n  Args:\n    variable: The `nnx.VariableState` object to process.\n\n  Returns:\n    The variable's value, potentially wrapped in `nn.LogicallyPartitioned`.\n  \"\"\"\n  if isinstance(variable.value, aqt_tensor.QTensor):\n    return variable.value\n\n  if variable.type.__name__ == \"_overwrite_with_gradient\":\n    return variable.value\n\n  metadata = variable.get_metadata()\n  if \"sharding\" in metadata or \"sharding_names\" in metadata:\n    if \"sharding_names\" in metadata:\n      sharding_names = metadata[\"sharding_names\"]\n    else:\n      sharding_names = metadata[\"sharding\"]\n    return nn.LogicallyPartitioned(  # type: ignore[wrong-keyword-args]\n        variable.value,\n        sharding_names,  # type: ignore[arg-type]\n        mesh=metadata.get(\"mesh\"),\n        rules=metadata.get(\"rules\"),\n    )\n  else:\n    return variable.value",
        "analysis": {
            "functionality": "Wraps an NNX variable's value in `nn.LogicallyPartitioned` if sharding metadata is present, applying sharding constraints, while bypassing certain special variable types.",
            "usage": "Call this function with an `nnx.VariableState` object. It inspects the variable's metadata for 'sharding' or 'sharding_names' keys. If found, it returns the variable's value wrapped in `nn.LogicallyPartitioned`. If the value is an `aqt_tensor.QTensor` or of type `_overwrite_with_gradient`, or if no sharding info is present, it returns the original value."
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#_convert_to_activation_function",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def _convert_to_activation_function(fn_or_string: str | Callable[..., Any]) -> Callable[..., Any]:\n  \"\"\"Convert a string to an activation function.\"\"\"\n  if fn_or_string == \"linear\":\n    return lambda x: x\n  elif isinstance(fn_or_string, str):\n    return getattr(nn, fn_or_string)\n  elif callable(fn_or_string):\n    return fn_or_string\n  else:\n    raise ValueError(\n        f\"\"\"Don't know how to convert {fn_or_string}\n                         to an activation function\"\"\"\n    )",
        "analysis": {
            "functionality": "This function converts a given string or a callable into a callable activation function. It handles three cases: if the input is the string 'linear', it returns an identity function; if it's another string, it retrieves the corresponding function from `flax.linen.nn`; if it's already a callable, it returns it as is. It raises a ValueError for unsupported input types.",
            "usage": "Call this function with a string representing an activation function name (e.g., 'relu', 'gelu', 'linear') or an existing callable function. The function returns a callable that can be applied to a tensor. For example, `activation = _convert_to_activation_function('relu')` followed by `output = activation(input_tensor)`."
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#normalize_axes",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def normalize_axes(axes: Iterable[int], ndim: int) -> tuple[int, ...]:\n  # A tuple by convention. len(axes_tuple) then also gives the rank efficiently.\n  return tuple(ax if ax >= 0 else ndim + ax for ax in axes)",
        "analysis": {
            "module_type": "axis_normalization_utility",
            "purpose": "Converts an iterable of axis indices, including negative ones, into a tuple of their positive equivalents based on the total number of dimensions.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Iterate through the input `axes` iterable.",
                "For each axis `ax`, if it is negative, compute its positive equivalent as `ndim + ax`.",
                "Collect all normalized axes into a new tuple."
            ],
            "output": {
                "shape": "A tuple of positive integers with the same length as the input `axes`."
            },
            "dependencies": [],
            "parameters": {},
            "notes": [
                "This is a common utility function for handling array axis indexing where negative values count from the end.",
                "The function returns a tuple by convention, which allows for efficient rank calculation via `len()`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#canonicalize_tuple",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def canonicalize_tuple(x):\n  if isinstance(x, Iterable):\n    return tuple(x)\n  else:\n    return (x,)",
        "analysis": {
            "functionality": "Ensures the input is converted to a tuple. If the input is an iterable, it's converted to a tuple; otherwise, it's wrapped in a single-element tuple.",
            "usage": "Call this function with a single argument `x`. It returns a tuple representation of `x`. For example, `canonicalize_tuple([1, 2])` returns `(1, 2)`, and `canonicalize_tuple(1)` returns `(1,)`."
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#_compute_dot_general",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def _compute_dot_general(inputs, kernel, kernel_axes, axis, contract_ind, matmul_precision, quant):\n  \"\"\"Computes a dot_general operation that may be quantized.\"\"\"\n  dot_general = lax.dot_general\n  matmul_precision = lax.Precision(matmul_precision)\n  if quant:\n    dot_general_cls = quant.dot_general_cls(mesh_axes=kernel_axes)\n    dot_general = dot_general_cls()\n    return dot_general(inputs, kernel, ((axis, contract_ind), ((), ())), precision=None)\n  return dot_general(inputs, kernel, ((axis, contract_ind), ((), ())), precision=matmul_precision)",
        "analysis": {
            "functionality": "This function computes a `lax.dot_general` operation, which is a generalized matrix multiplication. It can perform either a standard or a quantized dot product based on the `quant` parameter.",
            "usage": "Call this function with two input tensors (`inputs`, `kernel`), axes information (`kernel_axes`, `axis`, `contract_ind`), and configuration for precision (`matmul_precision`) and optional quantization (`quant`). It returns the result of the dot product. If `quant` is provided, a quantized version of the operation is used; otherwise, a standard `lax.dot_general` is performed with the specified precision."
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#_compute_dot_general_nnx",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def _compute_dot_general_nnx(\n    inputs, kernel, axis, contract_ind, matmul_precision, quant_dot_general: nnx_wrappers.ToNNX | None, initializing: bool\n):\n  \"\"\"Computes a dot_general operation that may be quantized.\"\"\"\n  dot_general = lax.dot_general\n  matmul_precision = lax.Precision(matmul_precision)\n  if quant_dot_general is not None:\n    if initializing:\n      quant_dot_general.lazy_init(inputs, kernel, ((axis, contract_ind), ((), ())), precision=None)\n    return quant_dot_general(inputs, kernel, ((axis, contract_ind), ((), ())), precision=None, mutable=[\"aqt\"])\n  return dot_general(inputs, kernel, ((axis, contract_ind), ((), ())), precision=matmul_precision)",
        "analysis": {
            "functionality": "Computes a `dot_general` operation, which is a generalized matrix multiplication. It can optionally perform a quantized version of this operation if a quantization module is provided.",
            "usage": "This is an internal helper function. To use it, provide two input tensors (`inputs`, `kernel`), contraction dimensions (`axis`, `contract_ind`), and a precision level (`matmul_precision`). Optionally, pass a `quant_dot_general` module to perform quantization and a boolean `initializing` flag for the first-run setup of the quantization module. The function returns the result of the dot product."
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#DenseGeneral",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "class DenseGeneral(nnx.Module):\n  \"\"\"A linear transformation with flexible axes.\"\"\"\n\n  def __init__(\n      self,\n      in_features_shape: Iterable[int] | int,\n      out_features_shape: Iterable[int] | int,\n      axis: Iterable[int] | int = -1,\n      weight_dtype: DType = jnp.float32,\n      dtype: DType = jnp.float32,\n      kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n      kernel_axes: tuple[None | str, ...] = (),\n      quant: None | Quant = None,\n      use_bias: bool = False,\n      matmul_precision: str = \"default\",\n      parameter_memory_host_offload: bool = False,\n      *,  # Following arguments are keyword-only\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the DenseGeneral module.\n\n    Args:\n      in_features_shape: tuple with numbers of input features for axes specified in\n        'axis'.\n      out_features_shape: tuple with numbers of output features.\n      axis: tuple with axes to apply the transformation on.\n      weight_dtype: the dtype of the weights (default: float32).\n      dtype: the dtype of the computation (default: float32).\n      kernel_init: initializer function for the weight matrix.\n      kernel_axes: logical axes for partitioning the kernel.\n      quant: quantization config, defaults to None implying no quantization.\n      use_bias: whether to add bias in linear transformation.\n      matmul_precision: Precision for matrix multiplication.\n      parameter_memory_host_offload: Determines whether to offload params to host\n      rngs: RNG state for initialization in nnx.\n    \"\"\"\n    self.in_features_shape = canonicalize_tuple(in_features_shape)\n    self.out_features_shape = canonicalize_tuple(out_features_shape)\n    self.axis = canonicalize_tuple(axis)\n    self.weight_dtype = weight_dtype\n    self.dtype = dtype\n    self.kernel_init = kernel_init\n    self.kernel_axes = kernel_axes\n    self.quant = quant\n    self.use_bias = use_bias\n    self.matmul_precision = matmul_precision\n    self.parameter_memory_host_offload = parameter_memory_host_offload\n\n    # Parameter initialization\n    kernel_shape = self.in_features_shape + self.out_features_shape\n    kernel_in_axis = np.arange(len(self.axis))\n    kernel_out_axis = np.arange(len(self.axis), len(self.axis) + len(self.out_features_shape))\n\n    if not quantizations.in_serve_mode(self.quant):\n      self.kernel = nnx.Param(\n          self.kernel_init(\n              rngs.params(),\n              kernel_shape,\n              self.weight_dtype,\n              kernel_in_axis,\n              kernel_out_axis,\n          ),\n          sharding=self.kernel_axes,\n      )\n\n    if self.use_bias:\n      bias_axes = self.kernel_axes[-len(self.out_features_shape) :]\n      bias_shape = kernel_shape[-len(self.out_features_shape) :]\n      self.bias = nnx.Param(\n          default_bias_init(rngs.params(), bias_shape, self.weight_dtype),\n          sharding=bias_axes,\n      )\n    else:\n      self.bias = None\n\n    if quant:\n      dot_general_cls = quant.dot_general_cls(mesh_axes=kernel_axes)\n      dot_general_linen = dot_general_cls()\n      quant_dot_general = nnx_wrappers.ToNNX(dot_general_linen, rngs=rngs)\n      self._quant_dot_general_name = f\"{type(dot_general_linen).__name__}_0\"\n      setattr(self, self._quant_dot_general_name, quant_dot_general)\n      dummy_inputs = jnp.zeros((1, *self.in_features_shape), dtype=self.dtype)\n      self(dummy_inputs, _initializing=True)\n    else:\n      self._quant_dot_general_name = None\n\n  @property\n  def quant_dot_general(self) -> nnx_wrappers.ToNNX | None:\n    if self._quant_dot_general_name is None:\n      return None\n    return getattr(self, self._quant_dot_general_name)\n\n  def __call__(self, inputs: Array, _initializing: bool = False) -> Array:\n    \"\"\"Applies a linear transformation to the inputs along multiple dimensions.\n\n    Args:\n      inputs: The nd-array to be transformed.\n\n    Returns:\n      The transformed input.\n    \"\"\"\n    inputs = jnp.asarray(inputs, self.dtype)\n    norm_axis = normalize_axes(self.axis, inputs.ndim)\n\n    for i, ax in enumerate(norm_axis):\n      if inputs.shape[ax] != self.in_features_shape[i]:\n        raise ValueError(\n            f\"Input dimension {inputs.shape[ax]} at axis {ax} \"\n            f\"does not match expected input feature size {self.in_features_shape[i]}\"\n        )\n\n    if quantizations.in_serve_mode(self.quant):\n      kernel_shape = self.in_features_shape + self.out_features_shape\n      kernel = jnp.zeros(kernel_shape, dtype=self.dtype)\n    else:\n      kernel = self.kernel[...]\n      # Move logit_dense kernel to device if parameter offloading is enabled\n      if self.parameter_memory_host_offload:\n        max_logging.log(\"linear.py: Moving parameter logits_dense kernel to device\")\n        kernel = jax.device_put(kernel, max_utils.device_space())\n      kernel = jnp.asarray(kernel, self.dtype)\n\n    contract_ind = tuple(range(0, len(self.axis)))\n    output = _compute_dot_general_nnx(\n        inputs,\n        kernel,\n        norm_axis,\n        contract_ind,\n        self.matmul_precision,\n        self.quant_dot_general,\n        _initializing,\n    )\n\n    if self.bias is not None:\n      bias = jnp.asarray(self.bias[...], self.dtype)\n      output += bias\n    return output",
        "analysis": {
            "module_type": "dense_general",
            "purpose": "A flexible linear transformation layer that applies a dot-product with a kernel along specified axes of an input tensor.",
            "input": {
                "shape": "N/A (Handled by the __call__ method)",
                "dtype": "N/A (Handled by the __call__ method)"
            },
            "processing_steps": [
                "Initializes kernel and optional bias parameters based on input/output feature shapes.",
                "Sets up an optional quantized dot-general operator if a quantization configuration is provided.",
                "In the forward pass, performs a dot-product between the input tensor and the kernel along specified axes.",
                "Adds the bias term to the result if enabled."
            ],
            "output": {
                "shape": "N/A (Handled by the __call__ method)"
            },
            "dependencies": [
                "nnx.Module",
                "nnx.Param",
                "nnx.Rngs",
                "jnp",
                "quantizations",
                "nnx_wrappers",
                "_compute_dot_general_nnx",
                "canonicalize_tuple",
                "normalize_axes"
            ],
            "parameters": {
                "in_features_shape": "The shape of the input features to be contracted.",
                "out_features_shape": "The shape of the output features.",
                "axis": "The axis or axes of the input tensor to apply the linear transformation on.",
                "quant": "Quantization configuration object, if any.",
                "use_bias": "A boolean indicating whether to add a bias term.",
                "matmul_precision": "The precision for the underlying matrix multiplication.",
                "parameter_memory_host_offload": "If True, parameters are stored in host memory and moved to the device only when needed."
            },
            "notes": [
                "This is a generalized version of a standard dense (fully connected) layer.",
                "It supports quantization through the `quant` parameter.",
                "Parameter initialization happens in the `__init__` method."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the DenseGeneral module, creating the kernel, optional bias, and quantization-related submodules.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Canonicalizes `in_features_shape`, `out_features_shape`, and `axis` arguments to tuples.",
                        "Calculates the kernel shape from the input and output feature shapes.",
                        "Initializes the kernel weight as an `nnx.Param` using the provided `kernel_init` function, unless in quantization serving mode.",
                        "Initializes the bias as an `nnx.Param` if `use_bias` is True.",
                        "If `quant` is provided, it initializes a quantized dot-general module and performs a lazy initialization with a dummy input."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "canonicalize_tuple",
                        "np.arange",
                        "quantizations.in_serve_mode",
                        "nnx.Param",
                        "default_bias_init",
                        "nnx_wrappers.ToNNX"
                    ],
                    "notes": [
                        "This constructor requires an `nnx.Rngs` object for parameter initialization."
                    ]
                },
                "quant_dot_general": {
                    "purpose": "A property to access the quantized dot-general operator if it exists.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks if the internal name for the quantization module is set.",
                        "If set, returns the module using `getattr`.",
                        "Otherwise, returns None."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": [
                        "This provides a clean interface to the optional quantization submodule."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the linear transformation to the input tensor.",
                    "input": {
                        "shape": "[..., in_feature_dim_1, ..., in_feature_dim_N, ...]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Casts the input tensor to the module's computation `dtype`.",
                        "Normalizes the `axis` parameter relative to the input tensor's dimensions.",
                        "Validates that the input tensor's shape along the specified axes matches `in_features_shape`.",
                        "Retrieves the kernel, potentially moving it from host to device memory if offloading is enabled.",
                        "Calls `_compute_dot_general_nnx` to perform the core dot-product operation, which may be quantized.",
                        "Adds the bias to the output if `use_bias` is enabled.",
                        "Returns the transformed tensor."
                    ],
                    "output": {
                        "shape": "[..., out_feature_dim_1, ..., out_feature_dim_M, ...]"
                    },
                    "dependencies": [
                        "jnp.asarray",
                        "normalize_axes",
                        "quantizations.in_serve_mode",
                        "jax.device_put",
                        "_compute_dot_general_nnx"
                    ],
                    "notes": [
                        "The output shape is the same as the input shape, except that the dimensions specified by `axis` are replaced by the `out_features_shape`.",
                        "The `_initializing` argument is a private flag used during setup to lazily initialize the quantization module."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#dense_general",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def dense_general(\n    *,\n    inputs_shape: tuple[int, ...] | None = None,\n    in_features_shape: tuple[int, ...] | int | None = None,\n    out_features_shape: Iterable[int] | int,\n    axis: Iterable[int] | int = -1,\n    weight_dtype: DType = jnp.float32,\n    dtype: DType = jnp.float32,\n    kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n    kernel_axes: tuple[None | str, ...] = (),\n    quant: None | Quant = None,\n    use_bias: bool = False,\n    matmul_precision: str = \"default\",\n    parameter_memory_host_offload: bool = False,\n    name: None | str = None,\n):\n  \"\"\"Creates a DenseGeneral Linen module using nnx.bridge.to_linen.\n\n  Args:\n    inputs_shape: tuple with the shape of the inputs\n    in_features_shape: tuple with numbers of input features for axes specified in\n      'axis'.\n    out_features_shape: tuple with numbers of output features.\n    axis: tuple with axes to apply the transformation on.\n    weight_dtype: the dtype of the weights (default: float32).\n    dtype: the dtype of the computation (default: float32).\n    kernel_init: initializer function for the weight matrix.\n    kernel_axes: logical axes for partitioning the kernel.\n    quant: quantization config, defaults to None implying no quantization.\n    use_bias: whether to add bias in linear transformation.\n    matmul_precision: Precision for matrix multiplication.\n    parameter_memory_host_offload: Determines whether to offload params to host\n    name: name passed to the ToLinen Module\n  \"\"\"\n  if not (inputs_shape is not None) ^ (in_features_shape is not None):\n    raise ValueError(\"Exactly one of inputs_shape or in_features must be specified.\")\n\n  if inputs_shape is not None:\n    axis = canonicalize_tuple(axis)\n    in_features_shape = tuple(inputs_shape[ax] for ax in normalize_axes(axis, len(inputs_shape)))\n  else:\n    assert in_features_shape is not None\n  module = nnx_wrappers.to_linen(\n      DenseGeneral,\n      in_features_shape=in_features_shape,\n      out_features_shape=out_features_shape,\n      axis=axis,\n      weight_dtype=weight_dtype,\n      dtype=dtype,\n      kernel_init=kernel_init,\n      kernel_axes=kernel_axes,\n      quant=quant,\n      use_bias=use_bias,\n      matmul_precision=matmul_precision,\n      parameter_memory_host_offload=parameter_memory_host_offload,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )\n  return module",
        "analysis": {
            "module_type": "dense_general_factory",
            "purpose": "A factory function that creates a Flax Linen `DenseGeneral` module by wrapping the corresponding `flax.nnx` module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Validate that exactly one of `inputs_shape` or `in_features_shape` is provided.",
                "If `inputs_shape` is provided, calculate `in_features_shape` using the `axis` parameter.",
                "Call `nnx_wrappers.to_linen` to create a Linen-compatible module from the `DenseGeneral` nnx.Module.",
                "Pass all configuration arguments (features, dtypes, initializers, quantization, etc.) to the wrapper.",
                "Return the newly created Flax Linen module."
            ],
            "output": {
                "shape": "Returns a Flax Linen module. The module's output shape will be the input tensor's shape with dimensions specified by 'axis' replaced by 'out_features_shape'."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "DenseGeneral",
                "canonicalize_tuple",
                "normalize_axes",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "inputs_shape": "The shape of the input tensor, used to infer `in_features_shape`.",
                "in_features_shape": "The shape of the input features along the specified `axis`.",
                "out_features_shape": "The desired shape of the output features.",
                "axis": "The axis or axes of the input tensor to apply the linear transformation on.",
                "quant": "Configuration for quantization, or None for no quantization.",
                "use_bias": "A boolean indicating whether to add a bias term to the output."
            },
            "notes": [
                "This function is a factory that returns a module; it does not perform the computation itself.",
                "It serves as a bridge to make the `nnx.Module` `DenseGeneral` usable within a Flax Linen model definition."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#Dropout",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "class Dropout(nnx.Dropout):\n  \"\"\"Forked nnx.Dropout that is easier to use with bridge\"\"\"\n\n  def __init__(  # pylint: disable=super-init-not-called\n      self,\n      rate: float,\n      *,\n      broadcast_dims: Sequence[int] = (),\n      deterministic: bool = False,\n      rng_collection: str = \"dropout\",\n      rngs: nnx.Rngs | None = None,\n  ):\n    self.rate = rate\n    self.broadcast_dims = broadcast_dims\n    self.deterministic = deterministic\n    self.rng_collection = rng_collection\n\n    if isinstance(rngs, nnx.Rngs):\n      self.rngs = rngs.fork() if hasattr(type(rngs), \"fork\") else rngs\n    else:\n      raise TypeError(f\"rngs must be a Rngs, RngStream or None, but got {type(rngs)}.\")",
        "analysis": {
            "module_type": "dropout_layer",
            "purpose": "A modified version of flax.nnx.Dropout designed for easier integration with the nnx.bridge, which handles its own RNG forking during initialization.",
            "input": {
                "shape": "Inherited from nnx.Dropout, typically [batch_size, ..., feature_dim].",
                "dtype": "Inherited from nnx.Dropout, typically a float type like float32."
            },
            "processing_steps": [
                "The __call__ method, inherited from nnx.Dropout, applies dropout to the input tensor.",
                "If not in deterministic mode, randomly sets a fraction of input units to 0 at each update during training time.",
                "Scales the remaining units by 1 / (1 - rate)."
            ],
            "output": {
                "shape": "Same as input shape."
            },
            "dependencies": [
                "flax.nnx.Dropout",
                "flax.nnx.Rngs"
            ],
            "parameters": {
                "rate": "The dropout probability; the fraction of the input units to drop.",
                "deterministic": "If True, dropout is not applied, and the module acts as an identity function.",
                "rng_collection": "The name of the RNG collection to use for generating dropout masks.",
                "rngs": "The RNG state manager from which to fork a new RNG stream for this layer."
            },
            "notes": [
                "This class intentionally does not call the `super().__init__()` of `nnx.Dropout`.",
                "It manually forks the provided `rngs` object to create a dedicated RNG stream for the dropout operation.",
                "The core dropout logic in the `__call__` method is inherited from the parent `flax.nnx.Dropout` class."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Dropout layer's parameters and forks the RNG stream.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Assigns `rate`, `broadcast_dims`, `deterministic`, and `rng_collection` to instance attributes.",
                        "Checks if the `rngs` argument is an instance of `nnx.Rngs`.",
                        "Forks the provided `rngs` object to create a new, independent RNG stream for this layer and assigns it to `self.rngs`.",
                        "Raises a TypeError if `rngs` is not of the expected type."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "flax.nnx.Rngs"
                    ],
                    "notes": [
                        "This method intentionally avoids calling `super().__init__()`.",
                        "The primary purpose of this custom `__init__` is to manage RNG forking in a way that is compatible with the project's bridging mechanism."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#MlpBlock",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "class MlpBlock(nnx.Module):\n  \"\"\"Transformer MLP / feed-forward block.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      in_features: int,\n      intermediate_dim: int = 2048,\n      activations: Sequence[str | Callable[..., Any]] = (\"relu\",),\n      kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n      intermediate_dropout_rate: float = 0.1,\n      dtype: Any = jnp.float32,\n      weight_dtype: Any = jnp.float32,\n      use_bias: bool = False,\n      use_pre_norm: bool = False,\n      quant: None | Quant = None,\n      model_mode: None | str = None,\n      *,\n      rngs: nnx.Rngs,\n  ) -> None:\n    \"\"\"A MlpBlock module.\n\n    Args:\n      config: Config object containing model parameters.\n      in_features: Number of input features.\n      intermediate_dim: Shared dimension of hidden layers.\n      activations: Type of activations for each layer.  Each element is either\n        'linear', a string function name in flax.linen, or a function.\n      kernel_init: Kernel function, passed to the dense layers.\n      deterministic: Whether the dropout layers should be deterministic.\n      intermediate_dropout_rate: Dropout rate used after the intermediate layers.\n      dtype: computation data type for the dense layer.\n      weight_dtype: weight data type for the dense layer.\n      use_bias: whether to add bias in all feedforward layers.\n      use_pre_norm: whether to add pre layer norm in mlp layers.\n      quant: Optional quantization config, no quantization if None.\n    \"\"\"\n    self.config = config\n    self.in_features = in_features\n    self.intermediate_dim = intermediate_dim\n    self.activations = activations\n    self.kernel_init = kernel_init\n    self.intermediate_dropout_rate = intermediate_dropout_rate\n    self.dtype = dtype\n    self.weight_dtype = weight_dtype\n    self.use_bias = use_bias\n    self.use_pre_norm = use_pre_norm\n    self.quant = quant\n    self.model_mode = model_mode\n\n    if self.use_pre_norm:\n      self.mlp_layer_norm = self.get_norm_layer(num_features=in_features)(\n          dtype=config.dtype,\n          weight_dtype=config.weight_dtype,\n          kernel_axes=(\"norm\",),\n          epsilon=config.normalization_layer_epsilon,\n          rngs=rngs,\n      )\n    else:\n      self.mlp_layer_norm = None\n\n    if config.fused_mlp:\n      self.wi = DenseGeneral(\n          in_features_shape=in_features,\n          out_features_shape=(len(self.activations), self.intermediate_dim),\n          dtype=self.dtype,\n          weight_dtype=self.weight_dtype,\n          kernel_init=self.kernel_init,\n          kernel_axes=(\"embed\", \"num_activations\", \"mlp\"),\n          quant=self.quant,\n          use_bias=self.use_bias,\n          matmul_precision=self.config.matmul_precision,\n          rngs=rngs,\n      )\n    else:\n      for idx in range(len(self.activations)):\n        dense_name = \"wi\" if len(self.activations) == 1 else f\"wi_{idx}\"\n        module = DenseGeneral(\n            in_features_shape=in_features,\n            out_features_shape=self.intermediate_dim,\n            dtype=self.dtype,\n            weight_dtype=self.weight_dtype,\n            kernel_init=self.kernel_init,\n            kernel_axes=(\"embed\", \"mlp\"),\n            quant=self.quant,\n            use_bias=self.use_bias,\n            matmul_precision=self.config.matmul_precision,\n            rngs=rngs,\n        )\n        setattr(self, dense_name, module)\n    self.dropout = Dropout(rate=self.intermediate_dropout_rate, broadcast_dims=(-2,), rngs=rngs)\n    self.wo = DenseGeneral(\n        in_features_shape=self.intermediate_dim,\n        out_features_shape=in_features,\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"mlp\", \"embed\"),\n        quant=self.quant,\n        use_bias=self.use_bias,\n        matmul_precision=self.config.matmul_precision,\n        rngs=rngs,\n    )\n\n  def get_norm_layer(self, num_features: int):\n    \"\"\"get normalization layer.\"\"\"\n    if self.config.decoder_block in (\n        DecoderBlockType.DEFAULT,\n        DecoderBlockType.LLAMA2,\n        DecoderBlockType.MISTRAL,\n        DecoderBlockType.MIXTRAL,\n        DecoderBlockType.GEMMA,\n        DecoderBlockType.GEMMA2,\n        DecoderBlockType.GEMMA3,\n        DecoderBlockType.QWEN3,\n        DecoderBlockType.DEEPSEEK,\n        DecoderBlockType.LLAMA4,\n    ):\n      return functools.partial(normalizations.RMSNorm, num_features=num_features)\n    elif self.config.decoder_block == DecoderBlockType.GPT3:\n      from MaxText.layers import gpt3  # pylint: disable=import-outside-toplevel\n\n      return functools.partial(\n          gpt3.Gpt3LayerNorm, num_features=num_features, reductions_in_fp32=False, use_bias=self.use_bias\n      )\n    else:\n      raise ValueError(f\"Incorrect decoder_block name {self.config.decoder_block.value=}\")\n\n  def __call__(self, inputs, decode: bool = False, deterministic: bool = False):\n    \"\"\"Applies Transformer MlpBlock module.\"\"\"\n    cfg = self.config\n\n    if self.mlp_layer_norm is not None:\n      inputs = self.mlp_layer_norm(inputs)\n\n    # Iterate over specified MLP input activation functions.\n    # e.g. ('relu',) or ('gelu', 'linear') for gated-gelu.\n    activations = []\n    if cfg.fused_mlp:\n      x = self.wi(inputs)\n      x = checkpoint_name(x, \"mlpwi\")\n      for idx, act_fn in enumerate(self.activations):\n        y = _convert_to_activation_function(act_fn)(x[:, :, idx, ...])\n        activations.append(y)\n    else:\n      for idx, act_fn in enumerate(self.activations):\n        dense_name = \"wi\" if len(self.activations) == 1 else f\"wi_{idx}\"\n        module = getattr(self, dense_name)\n        x = module(inputs)\n        x = checkpoint_name(x, \"mlp\" + dense_name)\n        if cfg.activations_in_float32:\n          x = x.astype(jnp.float32)\n        x = _convert_to_activation_function(act_fn)(x)\n        activations.append(x)\n\n    # Take elementwise product of above intermediate activations.\n    x = functools.reduce(operator.mul, activations).astype(self.dtype)\n    # Apply dropout and final dense output projection.\n    x = self.dropout(x, deterministic=deterministic)  # Broadcast along length.\n    if self.model_mode == MODEL_MODE_PREFILL:\n      x = nn.with_logical_constraint(x, (\"activation_batch\", \"prefill_activation_length\", \"activation_mlp\"))\n    else:\n      x = nn.with_logical_constraint(x, (\"activation_batch\", \"activation_length\", \"activation_mlp\"))\n    output = self.wo(x)\n\n    output = checkpoint_name(output, \"mlpwo\")\n    return output",
        "analysis": {
            "module_type": "transformer_mlp_block",
            "purpose": "Implements a standard feed-forward network (FFN) block, also known as an MLP block, used within a transformer layer.",
            "input": {
                "shape": "[batch_size, sequence_length, in_features]",
                "dtype": "jnp.float32 (configurable via the 'dtype' parameter)"
            },
            "processing_steps": [
                "Optionally apply pre-layer normalization to the input tensor if `use_pre_norm` is True.",
                "Project the input tensor to an intermediate dimension using one or more dense layers (`wi`).",
                "Apply a sequence of activation functions to the intermediate representations.",
                "Multiply the outputs of the activation functions element-wise (for gated activations).",
                "Apply dropout for regularization.",
                "Project the result back to the original input dimension using a final dense layer (`wo`)."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, in_features]"
            },
            "dependencies": [
                "nnx.Module",
                "DenseGeneral",
                "Dropout",
                "normalizations.RMSNorm",
                "gpt3.Gpt3LayerNorm",
                "_convert_to_activation_function",
                "jax.ad_checkpoint.checkpoint_name"
            ],
            "parameters": {
                "intermediate_dim": "The dimensionality of the hidden layer in the MLP.",
                "activations": "A sequence of activation functions to be applied. If more than one, their outputs are multiplied, enabling gated mechanisms like SwiGLU.",
                "config.fused_mlp": "A boolean from the config that determines whether to use a single dense layer for all activations (fused) or separate layers.",
                "use_pre_norm": "A boolean indicating whether to apply layer normalization before the MLP block.",
                "intermediate_dropout_rate": "The dropout rate applied after the activation functions."
            },
            "notes": [
                "Supports both standard and 'fused' MLP implementations, controlled by `config.fused_mlp`.",
                "Can function as a gated MLP (like SwiGLU) by providing multiple activation functions in the `activations` sequence.",
                "The choice of normalization layer (if `use_pre_norm` is True) is determined by `config.decoder_block`."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the MLP block's layers, including dense projections, optional normalization, and dropout.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters.",
                        "Conditionally initialize a normalization layer (`mlp_layer_norm`) via `get_norm_layer` if `use_pre_norm` is True.",
                        "Initialize input projection layer(s) (`wi`) as either a single fused `DenseGeneral` layer or multiple separate ones based on `config.fused_mlp`.",
                        "Initialize the `Dropout` layer.",
                        "Initialize the output projection layer (`wo`) using `DenseGeneral`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "DenseGeneral",
                        "Dropout",
                        "self.get_norm_layer"
                    ],
                    "notes": [
                        "The initialization logic for the input projection (`wi`) differs based on the `config.fused_mlp` flag."
                    ]
                },
                "get_norm_layer": {
                    "purpose": "Selects and returns the appropriate normalization layer constructor based on the model's decoder block type.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check the `self.config.decoder_block` value.",
                        "Return a partially applied `normalizations.RMSNorm` for most common decoder types.",
                        "Return a partially applied `gpt3.Gpt3LayerNorm` for the GPT3 decoder type.",
                        "Raise a ValueError for unsupported decoder types."
                    ],
                    "output": {
                        "shape": "A callable class constructor (functools.partial object)."
                    },
                    "dependencies": [
                        "functools.partial",
                        "normalizations.RMSNorm",
                        "gpt3.Gpt3LayerNorm",
                        "DecoderBlockType"
                    ],
                    "notes": [
                        "Returns a callable (a partially initialized class) rather than an instantiated layer."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the MLP block.",
                    "input": {
                        "shape": "[batch_size, sequence_length, in_features]",
                        "dtype": "jnp.float32 (configurable)"
                    },
                    "processing_steps": [
                        "Apply `mlp_layer_norm` to inputs if it exists.",
                        "Apply the `wi` dense layer(s) to the input.",
                        "Apply each activation function from `self.activations` to the intermediate tensor(s).",
                        "Element-wise multiply the results of the activation functions.",
                        "Apply dropout to the result.",
                        "Apply logical constraints for tensor partitioning using `nn.with_logical_constraint`.",
                        "Apply the `wo` output dense layer.",
                        "Apply a gradient checkpoint name to the output.",
                        "Return the final output tensor."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, in_features]"
                    },
                    "dependencies": [
                        "_convert_to_activation_function",
                        "functools.reduce",
                        "operator.mul",
                        "jax.ad_checkpoint.checkpoint_name",
                        "flax.linen.with_logical_constraint"
                    ],
                    "notes": [
                        "The logic for applying the `wi` layer and activations branches based on whether `config.fused_mlp` is enabled."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#mlp_block",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def mlp_block(\n    *,\n    config: Config,\n    in_features: int,\n    intermediate_dim: int = 2048,\n    activations: Sequence[str | Callable[..., Any]] = (\"relu\",),\n    kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n    intermediate_dropout_rate: float = 0.1,\n    dtype: Any = jnp.float32,\n    weight_dtype: Any = jnp.float32,\n    use_bias: bool = False,\n    use_pre_norm: bool = False,\n    quant: None | Quant = None,\n    model_mode: None | str = None,\n    name: None | str = None,\n):\n  \"\"\"Creates a MlpBlock Linen module using nnx.bridge.to_linen.\"\"\"\n  module = nnx_wrappers.to_linen(\n      MlpBlock,\n      config=config,\n      in_features=in_features,\n      intermediate_dim=intermediate_dim,\n      activations=activations,\n      kernel_init=kernel_init,\n      intermediate_dropout_rate=intermediate_dropout_rate,\n      dtype=dtype,\n      weight_dtype=weight_dtype,\n      use_bias=use_bias,\n      use_pre_norm=use_pre_norm,\n      quant=quant,\n      model_mode=model_mode,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )\n  return module",
        "analysis": {
            "module_type": "mlp_block_factory",
            "purpose": "A factory function that creates a Flax Linen `MlpBlock` module by wrapping the corresponding NNX `MlpBlock` class.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `MlpBlock` NNX module into a Flax Linen module.",
                "Passes all configuration parameters (in_features, intermediate_dim, activations, etc.) to the `MlpBlock` constructor via the wrapper."
            ],
            "output": {
                "shape": "Returns a `flax.linen.Module` instance. When called, this module will typically return a tensor of the same shape as its input, e.g., `[batch_size, sequence_length, in_features]`."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "MlpBlock",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "config": "The main configuration object for the model, containing parameters like dtype, matmul_precision, etc.",
                "in_features": "The number of input features to the MLP block.",
                "intermediate_dim": "The shared dimension of the hidden layers within the MLP block.",
                "activations": "A sequence of activation functions to be applied in the MLP.",
                "quant": "Optional quantization configuration for the dense layers within the MLP block."
            },
            "notes": [
                "This function acts as a bridge between the NNX module definition (`MlpBlock`) and the Flax Linen API.",
                "The `abstract_init=False` argument indicates that the module's parameters are initialized immediately upon creation."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/llama2.py#LlamaDecoderLayer",
        "file_path": "src/MaxText/layers/llama2.py",
        "code_block": "class LlamaDecoderLayer(nn.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      previous_chunk=None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n\n    if model_mode == MODEL_MODE_PREFILL:\n      activation_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n    else:\n      activation_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n    inputs = nn.with_logical_constraint(inputs, activation_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    lnx_rms = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )\n    lnx = lnx_rms(inputs)\n\n    lnx = nn.with_logical_constraint(lnx, activation_axis_names)\n\n    # Self-attention block\n    attention_layer = attention_as_linen(\n        config=cfg,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention\",\n        float32_qk_product=cfg.float32_qk_product,\n        float32_logits=cfg.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        prefill_cache_axis_order=tuple(map(int, cfg.prefill_cache_axis_order.split(\",\"))),\n        ar_cache_axis_order=tuple(map(int, cfg.ar_cache_axis_order.split(\",\"))),\n        compute_axis_order=tuple(map(int, cfg.compute_axis_order.split(\",\"))),\n        reshape_q=cfg.reshape_q,\n        use_ragged_attention=cfg.use_ragged_attention,\n        ragged_block_size=cfg.ragged_block_size,\n        model_mode=model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n        slot=slot,\n        page_state=page_state,\n        previous_chunk=previous_chunk,\n    )\n\n    attention_lnx = nn.with_logical_constraint(attention_lnx, activation_axis_names)\n    intermediate_inputs = inputs + attention_lnx\n\n    # Fully Connected\n    hidden_states = rms_norm(\n        num_features=intermediate_inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"post_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )(intermediate_inputs)\n    hidden_states = nn.with_logical_constraint(hidden_states, activation_axis_names)\n\n    # MLP block.\n    mlp_lnx = mlp_block(\n        in_features=hidden_states.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp\",\n        config=cfg,\n        quant=self.quant,\n        model_mode=model_mode,\n    )(hidden_states, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, activation_axis_names)\n\n    layer_output = mlp_lnx + intermediate_inputs\n\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n\n    layer_output = nn.with_logical_constraint(layer_output, activation_axis_names)\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "llama_decoder_layer",
            "purpose": "Implements a single transformer decoder layer for a Llama-style model, consisting of a self-attention block and a feed-forward MLP block with pre-normalization (RMSNorm) and residual connections.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes class attributes such as config, mesh, model_mode, and quant."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nn.Module",
                "rms_norm",
                "attention_as_linen",
                "mlp_block",
                "Config",
                "Mesh",
                "Quant",
                "page_manager.PageState"
            ],
            "parameters": {
                "config": "A Config object containing model hyperparameters like dimensions, dropout rates, and layer types.",
                "mesh": "A JAX sharding Mesh for distributed computation.",
                "model_mode": "A string indicating the operational mode, e.g., 'prefill' or 'generate'.",
                "quant": "Optional quantization configuration for weights and activations."
            },
            "notes": [
                "This layer uses a pre-normalization architecture where RMSNorm is applied before the self-attention and MLP blocks.",
                "It includes two residual connections: one after the attention block and one after the MLP block.",
                "The behavior, particularly logical axis naming for sharding, changes based on the `model_mode`."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Executes the forward pass for the decoder layer, applying self-attention and MLP transformations.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim], decoder_positions: [batch_size, sequence_length], decoder_segment_ids: [batch_size, sequence_length]",
                        "dtype": "Defined by config.dtype"
                    },
                    "processing_steps": [
                        "Apply logical constraints to the input tensor based on `model_mode`.",
                        "Apply pre-attention RMS normalization to the input.",
                        "Perform self-attention using the `attention_as_linen` module.",
                        "Add the output of the attention block to the original input (first residual connection).",
                        "Apply post-attention RMS normalization to the result of the first residual connection.",
                        "Pass the result through an `mlp_block`.",
                        "Add the output of the MLP block to the result of the first residual connection (second residual connection).",
                        "Apply dropout.",
                        "Optionally record internal metrics if `config.record_internal_nn_metrics` is true.",
                        "Return the final tensor."
                    ],
                    "output": {
                        "shape": "If config.scan_layers is False: [batch_size, sequence_length, hidden_dim]. If True: a tuple of ([batch_size, sequence_length, hidden_dim], None)."
                    },
                    "dependencies": [
                        "rms_norm",
                        "attention_as_linen",
                        "mlp_block",
                        "nn.with_logical_constraint",
                        "nn.Dropout",
                        "checkpoint_name"
                    ],
                    "notes": [
                        "Accepts additional arguments like `slot`, `page_state`, and `previous_chunk` to support advanced inference techniques like paged attention.",
                        "The `deterministic` flag controls whether dropout is applied.",
                        "The return signature changes based on the `config.scan_layers` flag, which is used for optimizing training via `nn.scan`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4UnfoldConvolution",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4UnfoldConvolution(nn.Module):\n  \"\"\"implementation of Llama4UnfoldConvolution for Llama4 Multi modal model.\n\n  This module extracts patches from input images and projects them to hidden dimension.\n\n  Attributes:\n    config: Config containing model parameters\n  \"\"\"\n\n  config: Config\n\n  def setup(self):\n    \"\"\"\n    Initialize Llama4UnfoldConvolution\n    \"\"\"\n    cfg = self.config\n    # Linear projection layer using dense_general.\n    # patches sent to dense_general with shape:\n    # [batch_size, num_patches, num_channels * patch_size * patch_size]\n    self.linear = linears.dense_general(\n        in_features_shape=(cfg.num_channels_for_vit * cfg.patch_size_for_vit * cfg.patch_size_for_vit),\n        out_features_shape=cfg.hidden_size_for_vit,\n        dtype=cfg.dtype_mm,\n        name=\"vit_unfold_linear\",\n        use_bias=False,\n        matmul_precision=cfg.matmul_precision,\n    )\n\n  def __call__(self, inputs: Array) -> Array:\n    \"\"\"Extract patches and project to hidden dimension.\n\n    Args:\n      inputs: Input tensor of shape [batch_size, channels, img, img]\n\n    Returns:\n      Tensor of shape [batch_size, num_patches*num_patches, hidden_size]\n    \"\"\"\n    cfg = self.config\n    # Extract patches using conv_general_dilated_patches\n    batch_size, num_channels, img, _ = inputs.shape\n    num_patches = (img // cfg.patch_size_for_vit) ** 2\n\n    # Extract patches using conv_general_dilated_patches\n    patches = lax.conv_general_dilated_patches(\n        inputs,\n        filter_shape=[cfg.patch_size_for_vit, cfg.patch_size_for_vit],\n        window_strides=[cfg.patch_size_for_vit, cfg.patch_size_for_vit],\n        padding=\"VALID\",\n        dimension_numbers=(\"NCHW\", \"HWIO\", \"NCHW\"),\n    )\n\n    # reshape patches to [batch_size, num_patches, num_channels * patch_size * patch_size]\n    patches = patches.reshape(batch_size, num_channels * cfg.patch_size_for_vit * cfg.patch_size_for_vit, num_patches)\n    # After transpose, patches shape:\n    # [batch_size, num_patches, num_channels * patch_size * patch_size]\n    patches = patches.transpose(0, 2, 1)\n\n    # Project patches to hidden dimension using dense_general\n    hidden_states = self.linear(patches)\n\n    return hidden_states",
        "analysis": {
            "module_type": "unfold_convolution_projection",
            "purpose": "Extracts non-overlapping patches from an input image tensor and projects them into a specified hidden dimension.",
            "input": {
                "shape": "[batch_size, channels, img_height, img_width]",
                "dtype": "Inferred from config.dtype_mm"
            },
            "processing_steps": [
                "Extracts patches from the input tensor using `lax.conv_general_dilated_patches`.",
                "Reshapes and transposes the patches to group patch features.",
                "Applies a linear projection to the flattened patches to map them to the hidden dimension."
            ],
            "output": {
                "shape": "[batch_size, num_patches, hidden_size_for_vit]"
            },
            "dependencies": [
                "flax.linen.nn",
                "jax.lax",
                "MaxText.layers.linears"
            ],
            "parameters": {
                "config.patch_size_for_vit": "The height and width of the square patches to be extracted.",
                "config.num_channels_for_vit": "The number of channels in the input image.",
                "config.hidden_size_for_vit": "The dimensionality of the output feature space.",
                "config.dtype_mm": "The data type for the multi-modal computations.",
                "config.matmul_precision": "The precision for the matrix multiplication in the linear layer."
            },
            "notes": [
                "This module effectively implements the patch embedding step common in Vision Transformers (ViT).",
                "The patch extraction is performed using a convolution operation with a stride equal to the patch size, ensuring no overlap between patches."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the linear projection layer used to embed the image patches.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initializes a `dense_general` linear layer with input features corresponding to the flattened patch size and output features corresponding to the vision hidden size."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "MaxText.layers.linears.dense_general"
                    ],
                    "notes": [
                        "The linear layer is configured with `use_bias=False`."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of extracting and projecting image patches.",
                    "input": {
                        "shape": "[batch_size, channels, img, img]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Calculates the number of patches based on image and patch size.",
                        "Extracts patches using `lax.conv_general_dilated_patches`.",
                        "Reshapes the patches tensor to [batch_size, flattened_patch_dim, num_patches].",
                        "Transposes the patches tensor to [batch_size, num_patches, flattened_patch_dim].",
                        "Applies the pre-configured linear layer (`self.linear`) to project patches to the hidden dimension."
                    ],
                    "output": {
                        "shape": "[batch_size, num_patches, hidden_size_for_vit]"
                    },
                    "dependencies": [
                        "jax.lax.conv_general_dilated_patches"
                    ],
                    "notes": [
                        "The `dimension_numbers` argument in `conv_general_dilated_patches` is set to `(\"NCHW\", \"HWIO\", \"NCHW\")`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#pixel_shuffle",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "def pixel_shuffle(input_tensor: Array, shuffle_ratio: float) -> Array:\n  \"\"\"Apply pixel shuffle operation to the input tensor.\"\"\"\n  batch_size, num_patches, channels = input_tensor.shape\n  patch_size = int(math.sqrt(num_patches))\n\n  # Reshape to [batch_size, patch_size, patch_size, channels]\n  input_tensor = input_tensor.reshape(batch_size, patch_size, patch_size, -1)\n  batch_size, height, width, channels = input_tensor.shape\n\n  # Reshape to [batch_size, height, width * shuffle_ratio, channels / shuffle_ratio]\n  reshaped_tensor = input_tensor.reshape(batch_size, height, int(width * shuffle_ratio), int(channels / shuffle_ratio))\n\n  # Transpose to [batch_size, width * shuffle_ratio, height, channels / shuffle_ratio]\n  reshaped_tensor = reshaped_tensor.transpose(0, 2, 1, 3)\n\n  # Reshape to [batch_size, height * shuffle_ratio, width * shuffle_ratio, channels / (shuffle_ratio^2)]\n  reshaped_tensor = reshaped_tensor.reshape(\n      batch_size, int(height * shuffle_ratio), int(width * shuffle_ratio), int(channels / (shuffle_ratio**2))\n  )\n\n  # Transpose to [batch_size, width * shuffle_ratio, height * shuffle_ratio, channels / (shuffle_ratio^2)]\n  reshaped_tensor = reshaped_tensor.transpose(0, 2, 1, 3)\n\n  # Reshape back to [batch_size, num_patches, channels]\n  output_tensor = reshaped_tensor.reshape(batch_size, -1, reshaped_tensor.shape[-1])\n  return output_tensor",
        "analysis": {
            "module_type": "pixel_shuffle",
            "purpose": "Rearranges elements of a 3D tensor by moving data from the channel dimension to spatial dimensions, effectively upsampling the spatial resolution.",
            "input": {
                "shape": "[batch_size, num_patches, channels]",
                "dtype": "Array (e.g., jax.numpy.ndarray of float type)"
            },
            "processing_steps": [
                "Calculate the side length of a square grid of patches from the `num_patches` dimension.",
                "Reshape the 3D input tensor into a 4D tensor representing a spatial grid: [batch_size, patch_size, patch_size, channels].",
                "Perform a sequence of reshape and transpose operations to upscale the spatial dimensions by `shuffle_ratio` and downscale the channel dimension by `shuffle_ratio**2`.",
                "Reshape the resulting 4D tensor back into a 3D tensor."
            ],
            "output": {
                "shape": "[batch_size, num_patches * (shuffle_ratio**2), channels / (shuffle_ratio**2)]"
            },
            "dependencies": [
                "jax.numpy",
                "math"
            ],
            "parameters": {
                "shuffle_ratio": "The upscaling factor for the spatial dimensions. The channel dimension will be downscaled by this factor squared."
            },
            "notes": [
                "The `num_patches` dimension of the input tensor must be a perfect square.",
                "The `channels` dimension must be divisible by `shuffle_ratio` squared.",
                "This function is a common operation in image super-resolution models."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionMLP",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionMLP(nn.Module):\n  \"\"\"MLP block for Llama4EncoderLayer.\n\n  Attributes:\n    config: Config containing model parameters\n  \"\"\"\n\n  config: Config\n\n  def setup(self):\n    cfg = self.config\n    self.fc1 = linears.dense_general(\n        in_features_shape=cfg.hidden_size_for_vit,\n        out_features_shape=cfg.intermediate_size_for_vit,\n        dtype=cfg.dtype_mm,\n        name=\"vit_encoder_layer_mlp_fc1\",\n        use_bias=True,\n        matmul_precision=cfg.matmul_precision,\n    )\n    self.fc2 = linears.dense_general(\n        in_features_shape=cfg.intermediate_size_for_vit,\n        out_features_shape=cfg.hidden_size_for_vit,\n        dtype=cfg.dtype_mm,\n        name=\"vit_encoder_layer_mlp_fc2\",\n        use_bias=True,\n        matmul_precision=cfg.matmul_precision,\n    )\n\n  def __call__(self, hidden_states: Array) -> Array:\n    \"\"\"Apply MLP transformation to hidden states.\n\n    Args:\n      hidden_states: Input tensor\n      deterministic: If True, disables dropout during inference\n    \"\"\"\n    hidden_states = self.fc1(hidden_states)\n    hidden_states = nn.gelu(hidden_states, approximate=False)\n\n    hidden_states = self.fc2(hidden_states)\n\n    return hidden_states",
        "analysis": {
            "functionality": "The `Llama4VisionMLP` class defines a two-layer Multi-Layer Perceptron (MLP) block, a standard component in Transformer architectures. It takes an input tensor, projects it to a higher-dimensional space using a first linear layer, applies a GELU activation function, and then projects it back to the original dimension with a second linear layer.",
            "usage": "This class is intended to be used as a feed-forward network within a larger model, specifically the `Llama4VisionEncoderLayer`. To use it, instantiate the class with a configuration object and then call the instance with an input tensor of shape `[batch_size, sequence_length, hidden_size_for_vit]`. It will return a tensor of the same shape."
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionMLP2",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionMLP2(nn.Module):\n  \"\"\"MLP block for Llama4VisionPixelShuffleMLP.\n\n  Attributes:\n    config: Config containing model parameters\n  \"\"\"\n\n  config: Config\n\n  def setup(self):\n    \"\"\"\n    Initialize Llama4VisionMLP2\n    \"\"\"\n    cfg = self.config\n    self.fc1 = linears.dense_general(\n        in_features_shape=cfg.intermediate_size_for_vit,\n        out_features_shape=cfg.projector_input_dim_for_vit,\n        dtype=cfg.dtype_mm,\n        name=\"vit_pixel_shuffle_mlp_fc1\",\n        use_bias=False,\n        matmul_precision=cfg.matmul_precision,\n    )\n    self.fc2 = linears.dense_general(\n        in_features_shape=cfg.projector_input_dim_for_vit,\n        out_features_shape=cfg.projector_output_dim_for_vit,\n        dtype=cfg.dtype_mm,\n        name=\"vit_pixel_shuffle_mlp_fc2\",\n        use_bias=False,\n        matmul_precision=cfg.matmul_precision,\n    )\n    self.dropout = nn.Dropout(rate=cfg.projector_dropout_for_vit)\n\n  def __call__(self, hidden_states: Array, deterministic: bool = False) -> Array:\n    \"\"\"Apply MLP transformation to hidden states.\n\n    Args:\n      hidden_states: Input tensor\n      deterministic: If True, disables dropout during inference\n    \"\"\"\n    # First linear layer with GELU activation\n    hidden_states = self.fc1(hidden_states)\n    hidden_states = nn.gelu(hidden_states, approximate=False)\n\n    # Apply dropout\n    # in pytorch it's using default Dropout Rate of 0.5\n    hidden_states = self.dropout(hidden_states, deterministic=deterministic)\n\n    # Second linear layer with GELU activation\n    hidden_states = self.fc2(hidden_states)\n    hidden_states = nn.gelu(hidden_states, approximate=False)\n\n    return hidden_states",
        "analysis": {
            "module_type": "vision_mlp",
            "purpose": "A two-layer Multi-Layer Perceptron (MLP) with GELU activations and dropout, designed for transforming vision features.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.linen.Module",
                "maxtext.layers.linears.dense_general",
                "flax.linen.Dropout",
                "flax.linen.gelu"
            ],
            "parameters": {
                "intermediate_size_for_vit": "The input feature dimension for the first linear layer.",
                "projector_input_dim_for_vit": "The intermediate feature dimension, which is the output of the first linear layer and input to the second.",
                "projector_output_dim_for_vit": "The final output feature dimension from the second linear layer.",
                "projector_dropout_for_vit": "The dropout rate to apply between the two linear layers.",
                "dtype_mm": "The data type for the multi-modal layers.",
                "matmul_precision": "The precision setting for matrix multiplication operations."
            },
            "notes": [
                "This module is a component of the `Llama4VisionPixelShuffleMLP`.",
                "The linear layers (`fc1`, `fc2`) are configured without a bias term.",
                "The GELU activation function is non-approximate."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the two dense linear layers and the dropout layer based on the model configuration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initialize the first dense layer (`self.fc1`) using `linears.dense_general`.",
                        "Initialize the second dense layer (`self.fc2`) using `linears.dense_general`.",
                        "Initialize the dropout layer (`self.dropout`) using `nn.Dropout`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "linears.dense_general",
                        "nn.Dropout"
                    ],
                    "notes": [
                        "All layer parameters (dimensions, dtype, precision) are read from the `config` attribute."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the MLP transformation to the input hidden states.",
                    "input": {
                        "shape": "[batch_size, sequence_length, config.intermediate_size_for_vit]",
                        "dtype": "Corresponds to `config.dtype_mm`"
                    },
                    "processing_steps": [
                        "Apply the first linear transformation (`self.fc1`) to the input `hidden_states`.",
                        "Apply a GELU activation function.",
                        "Apply dropout, controlled by the `deterministic` flag.",
                        "Apply the second linear transformation (`self.fc2`).",
                        "Apply a second GELU activation function."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, config.projector_output_dim_for_vit]"
                    },
                    "dependencies": [
                        "self.fc1",
                        "self.fc2",
                        "self.dropout",
                        "nn.gelu"
                    ],
                    "notes": [
                        "The `deterministic` flag, when `True`, disables the dropout layer, which is standard practice during inference."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionPixelShuffleMLP",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionPixelShuffleMLP(nn.Module):\n  \"\"\"Implementation of Llama4VisionPixelShuffleMLP for Llama4 Multi modal model.\n\n  This module applies pixel shuffle operation and MLP to encoded patches.\n\n  Attributes:\n    config: Config containing model parameters\n  \"\"\"\n\n  config: Config\n\n  def setup(self):\n    cfg = self.config\n    self.pixel_shuffle_ratio = cfg.pixel_shuffle_ratio_for_vit\n    self.pixel_shuffle_mlp = Llama4VisionMLP2(cfg)\n\n  def __call__(self, encoded_patches: Array, deterministic: bool = False) -> Array:\n    \"\"\"Apply pixel shuffle and MLP to encoded patches.\n\n    Args:\n      encoded_patches: Input tensor of shape [batch_size, num_patches, hidden_size]\n      deterministic: If True, disables dropout during inference\n\n    Returns:\n      Tensor of shape [batch_size, num_patches, hidden_size]\n    \"\"\"\n    # Apply pixel shuffle operation\n    encoded_patches = pixel_shuffle(encoded_patches, self.pixel_shuffle_ratio)\n\n    # Apply MLP transformation\n    result = self.pixel_shuffle_mlp(encoded_patches, deterministic=deterministic)\n\n    return result",
        "analysis": {
            "module_type": "pixel_shuffle_mlp",
            "purpose": "Applies a pixel shuffle operation followed by an MLP transformation to encoded image patches.",
            "input": {
                "shape": "[batch_size, num_patches, hidden_size]",
                "dtype": "Array"
            },
            "processing_steps": [
                "Initializes the pixel shuffle ratio and the `Llama4VisionMLP2` module in the `setup` method.",
                "In the `__call__` method, applies the `pixel_shuffle` function to the input `encoded_patches`.",
                "Passes the result of the pixel shuffle through the initialized `Llama4VisionMLP2` module."
            ],
            "output": {
                "shape": "[batch_size, num_patches, projector_output_dim_for_vit]"
            },
            "dependencies": [
                "flax.linen.Module",
                "Llama4VisionMLP2",
                "pixel_shuffle"
            ],
            "parameters": {
                "pixel_shuffle_ratio_for_vit": "The ratio used for the pixel shuffle operation, which rearranges patch data."
            },
            "notes": [
                "This is a Flax Linen module designed for a multi-modal Llama4 model.",
                "The `deterministic` flag in the `__call__` method is passed down to the internal MLP to control dropout."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the module's components, including the pixel shuffle ratio and the internal MLP.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Retrieves `pixel_shuffle_ratio_for_vit` from the config object.",
                        "Instantiates `Llama4VisionMLP2` and assigns it to `self.pixel_shuffle_mlp`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Llama4VisionMLP2"
                    ],
                    "notes": [
                        "This is a standard Flax setup method called during module initialization."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the pixel shuffle operation and MLP transformation to the input tensor.",
                    "input": {
                        "shape": "[batch_size, num_patches, hidden_size]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Calls the `pixel_shuffle` function on the `encoded_patches`.",
                        "Passes the shuffled patches to the `self.pixel_shuffle_mlp` module."
                    ],
                    "output": {
                        "shape": "[batch_size, num_patches, projector_output_dim_for_vit]"
                    },
                    "dependencies": [
                        "pixel_shuffle",
                        "Llama4VisionMLP2"
                    ],
                    "notes": [
                        "The `deterministic` argument controls whether dropout is enabled in the subsequent MLP layer."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4MultiModalProjector",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4MultiModalProjector(nn.Module):\n  \"\"\"Implementation of Llama4MultiModalProjector for Llama4 Multi modal model.\n\n  This module projects vision features to text hidden dimension.\n\n  Attributes:\n    config: Config containing model parameters\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n\n  def setup(self):\n    cfg = self.config\n    self.linear = linears.dense_general(\n        in_features_shape=cfg.vision_output_dim_for_vit,\n        out_features_shape=cfg.base_emb_dim,\n        dtype=cfg.dtype_mm,\n        name=\"vit_multi_modal_projector\",\n        use_bias=False,\n        matmul_precision=cfg.matmul_precision,\n    )\n\n  def __call__(self, image_features: Array) -> Array:\n    \"\"\"Project image features to text hidden dimension.\n\n    Args:\n      image_features: Input tensor of shape [batch_size, num_patches, (pixel_shuffle_ratio**2), vision_output_dim]\n\n    Returns:\n      Tensor of shape [batch_size, num_patches, (pixel_shuffle_ratio**2), vision_hidden_size]\n    \"\"\"\n    b, t, c, d = image_features.shape\n    image_features = image_features.reshape(b * t, c, d)\n    hidden_states = self.linear(image_features)\n    _, c, d = hidden_states.shape\n    hidden_states = hidden_states.reshape(b, t, c, d)\n    return hidden_states",
        "analysis": {
            "module_type": "multi_modal_projector",
            "purpose": "Projects vision features from the vision encoder's output dimension to the text model's hidden dimension.",
            "input": {
                "shape": "[batch_size, num_patches, (pixel_shuffle_ratio**2), vision_output_dim]",
                "dtype": "Inferred from config.dtype_mm"
            },
            "processing_steps": [
                "Reshape the 4D input tensor `image_features` into a 3D tensor of shape [batch_size * num_patches, (pixel_shuffle_ratio**2), vision_output_dim].",
                "Apply a linear transformation using `self.linear` to project the features.",
                "Reshape the resulting 3D tensor back to a 4D tensor of shape [batch_size, num_patches, (pixel_shuffle_ratio**2), base_emb_dim]."
            ],
            "output": {
                "shape": "[batch_size, num_patches, (pixel_shuffle_ratio**2), base_emb_dim]"
            },
            "dependencies": [
                "flax.linen.Module",
                "maxtext.layers.linears.dense_general",
                "Config",
                "Mesh"
            ],
            "parameters": {
                "config.vision_output_dim_for_vit": "The input feature dimension from the vision model.",
                "config.base_emb_dim": "The target output feature dimension, corresponding to the text model's embedding dimension.",
                "config.dtype_mm": "The data type for the multi-modal projection layer.",
                "config.matmul_precision": "The precision for the matrix multiplication in the linear layer."
            },
            "notes": [
                "This module acts as a bridge between the vision and language components of a multi-modal model.",
                "The linear projection is applied without a bias term."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the linear projection layer based on the model configuration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Instantiates a linear layer using `linears.dense_general` with parameters from the `config` object."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "linears.dense_general"
                    ],
                    "notes": [
                        "The layer is named 'vit_multi_modal_projector' and is configured without a bias."
                    ]
                },
                "__call__": {
                    "purpose": "Executes the forward pass, projecting image features to the text hidden dimension.",
                    "input": {
                        "shape": "[batch_size, num_patches, (pixel_shuffle_ratio**2), vision_output_dim]",
                        "dtype": "Inferred from config.dtype_mm"
                    },
                    "processing_steps": [
                        "Unpack the shape of the input `image_features`.",
                        "Reshape the input tensor to combine the first two dimensions.",
                        "Apply the `self.linear` transformation.",
                        "Reshape the output tensor back to its original 4D structure with the new feature dimension."
                    ],
                    "output": {
                        "shape": "[batch_size, num_patches, (pixel_shuffle_ratio**2), base_emb_dim]"
                    },
                    "dependencies": [
                        "self.linear"
                    ],
                    "notes": [
                        "The reshaping allows for efficient application of the linear layer across all patches in the batch."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#determine_is_nope_layer",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "def determine_is_nope_layer(layer_id: int, nope_layer_interval: int) -> bool:\n  \"\"\"\n  Determines whether the given layer at `layer_id` should use RoPE or not (NoPE).\n\n  Args:\n    layer_id: The index of the layer.\n    nope_layer_interval: The interval at which layers should use NoPE.\n\n  Returns:\n    True if the layer should use NoPE, False otherwise.\n  \"\"\"\n  return nope_layer_interval is not None and nope_layer_interval > 0 and (layer_id + 1) % nope_layer_interval == 0",
        "analysis": {
            "functionality": "Determines if a given layer should be a NoPE (No Position Embedding) layer based on its index and a specified interval.",
            "usage": "Call the function with an integer `layer_id` (the layer's index) and an integer `nope_layer_interval`. It returns `True` if the layer index (plus one) is a multiple of a positive `nope_layer_interval`, and `False` otherwise."
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#determine_is_moe_layer",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "def determine_is_moe_layer(layer_id: int, interleave_moe_layer_step: int) -> bool:\n  \"\"\"\n  Determines whether the given layer at `layer_id` is MoE layer.\n\n  This function implements a striding pattern. For example:\n  - If moe_layer_stride is 1, all layers are MoE layers.\n  - If moe_layer_stride is 2, layers with index 1, 3, 5, ... are MoE layers.\n\n  Args:\n    layer_id: The 0-based index of the layer being checked.\n    interleave_moe_layer_step: The interval or stride for placing MoE layers.\n\n  Returns:\n    True if the layer is MoE layer, False otherwise.\n  \"\"\"\n  return (\n      interleave_moe_layer_step is not None\n      and interleave_moe_layer_step > 0\n      and (layer_id + 1) % interleave_moe_layer_step == 0\n  )",
        "analysis": {
            "module_type": "layer_type_determiner",
            "purpose": "Determines if a given layer, identified by its index, should be a Mixture-of-Experts (MoE) layer based on a specified stride or interval.",
            "input": {
                "shape": "N/A",
                "dtype": "int"
            },
            "processing_steps": [
                "Check if `interleave_moe_layer_step` is not None and is greater than 0.",
                "Check if `(layer_id + 1)` is perfectly divisible by `interleave_moe_layer_step` using the modulo operator.",
                "Return `True` if all conditions are met, otherwise `False`."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [],
            "parameters": {
                "layer_id": "The 0-based index of the layer being checked.",
                "interleave_moe_layer_step": "The interval or stride for placing MoE layers."
            },
            "notes": [
                "The function implements a striding pattern. For example, if `interleave_moe_layer_step` is 2, layers with indices 1, 3, 5, etc., are designated as MoE layers.",
                "The check uses `layer_id + 1`, effectively applying a 1-based logic to the 0-indexed layer ID for the stride calculation."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4DecoderLayer",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4DecoderLayer(nn.Module):\n  \"\"\"Transformer decoder layer for Llama4.\n\n  Attributes:\n    config: Config, MaxText model config\n    mesh: Mesh, JAX device mesh (used for sharding)\n    quant: None | Quant, quantization config\n    is_nope_layer: bool, whether to use RoPE or not on this layer\n    is_moe_layer: bool, whether this layer operates as a MoE layer\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n  is_nope_layer: bool = False\n  is_moe_layer: bool = False\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      bidirectional_mask=None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      previous_chunk=None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n\n    assert cfg.num_experts >= 1, \"Expected the Llama4 config to have `num_experts > 1`.\"\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    lnx_rms = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )\n    lnx = lnx_rms(inputs)\n\n    lnx = nn.with_logical_constraint(lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    # Instead of scaling the query values in the checkpoint conversion (`llama_or_mistral_ckpt`)\n    # we'll do it dynamically in the forward pass of Attention\n    query_pre_attn_scalar = cfg.head_dim**-0.5\n\n    # Self-attention block\n    attention_layer = attentions.attention_as_linen(\n        config=cfg,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention\",\n        float32_qk_product=cfg.float32_qk_product,\n        float32_logits=cfg.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        prefill_cache_axis_order=tuple(map(int, cfg.prefill_cache_axis_order.split(\",\"))),\n        ar_cache_axis_order=tuple(map(int, cfg.ar_cache_axis_order.split(\",\"))),\n        compute_axis_order=tuple(map(int, cfg.compute_axis_order.split(\",\"))),\n        reshape_q=cfg.reshape_q,\n        use_ragged_attention=cfg.use_ragged_attention,\n        ragged_block_size=cfg.ragged_block_size,\n        is_nope_layer=self.is_nope_layer,\n        use_qk_norm=cfg.use_qk_norm,\n        query_pre_attn_scalar=query_pre_attn_scalar,\n        temperature_tuning=cfg.temperature_tuning,\n        temperature_tuning_scale=0.1,\n        temperature_tuning_floor_scale=8192.0,\n        # note: chunk_attn_window_size is set in the config\n        attention_type=AttentionType.GLOBAL if self.is_nope_layer else AttentionType.CHUNK,\n        model_mode=model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n        slot=slot,\n        page_state=page_state,\n        previous_chunk=previous_chunk,\n        bidirectional_mask=bidirectional_mask,\n    )\n\n    attention_lnx = nn.with_logical_constraint(\n        attention_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n    intermediate_inputs = inputs + attention_lnx\n\n    # Fully Connected\n    hidden_states = rms_norm(\n        num_features=intermediate_inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"post_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )(intermediate_inputs)\n    hidden_states = nn.with_logical_constraint(\n        hidden_states, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n\n    load_balance_loss = None\n    if self.is_moe_layer:\n      # NOTE: the naming mismatch here is to ensure reverse compatibility with existing checkpoints.\n      # The `name` represents the weight name in JAX/checkpoints and so the class name\n      # is just for readability.\n      mlp_lnx = moe.get_routed_and_shared_moe(\n          name=\"Llama4MoEBlock_0\",\n          config=cfg,\n          mesh=self.mesh,\n          kernel_init=initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n          kernel_axes=(\"embed\", None),\n          dtype=cfg.dtype,\n          weight_dtype=cfg.weight_dtype,\n          quant=self.quant,\n      )(hidden_states)\n    else:\n      mlp_lnx = mlp_block(\n          in_features=hidden_states.shape[-1],\n          intermediate_dim=cfg.mlp_dim,\n          activations=cfg.mlp_activations,\n          intermediate_dropout_rate=cfg.dropout_rate,\n          dtype=cfg.dtype,\n          weight_dtype=cfg.weight_dtype,\n          name=\"mlp\",\n          config=cfg,\n          quant=self.quant,\n      )(hidden_states, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    layer_output = mlp_lnx + intermediate_inputs\n\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"),\n    )\n\n    # NOTE: this is only needed for dropping MoE\n    if load_balance_loss is not None:\n      self.sow(\"intermediates\", \"moe_lb_loss\", load_balance_loss)\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "functionality": "The `Llama4DecoderLayer` class implements a single transformer decoder layer for the Llama4 model. It processes an input tensor through a self-attention mechanism followed by a feed-forward network, which can be either a standard Multi-Layer Perceptron (MLP) or a Mixture-of-Experts (MoE) block. The layer includes pre-normalization (RMSNorm) and residual connections.",
            "usage": "This class is intended to be used as a building block within a larger transformer model. To use it, instantiate the class with a configuration object (`Config`), a JAX device mesh (`Mesh`), and flags to specify if it's a NoPE (No Position Embedding) or MoE layer. The forward pass is executed by calling the instance with an input tensor of shape `[batch_size, sequence_length, hidden_dim]`, along with segment IDs and positions. The output is a tensor of the same shape."
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4ScannableBlock",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4ScannableBlock(nn.Module):\n  '''\n  A repeatable block given nope_layer_interval and interleave_moe_layer_step\n\n  Attributes:\n    config: Config, MaxText model config\n    mesh: Mesh, JAX device mesh (used for sharding)\n    quant: None | Quant, quantization config\n    nope_layer_interval: int, the interval at which layers should use NoPE.\n    interleave_moe_layer_step: int, the interval or stride for placing MoE layers.\n  \"\"\"\n  '''\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n  nope_layer_interval: int = 1\n  interleave_moe_layer_step: int = 1\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      bidirectional_mask=None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      previous_chunk=None,\n  ):\n\n    cfg = self.config\n    mesh = self.mesh\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    y = inputs\n    for layer_id in range(cfg.inhomogeneous_layer_cycle_interval):\n      nope_layer = determine_is_nope_layer(layer_id, self.nope_layer_interval)\n      moe_layer = determine_is_moe_layer(layer_id, self.interleave_moe_layer_step)\n      layer = Llama4DecoderLayer(\n          config=cfg,\n          mesh=mesh,\n          name=f\"layers_{layer_id}\",\n          quant=self.quant,\n          model_mode=model_mode,\n          is_nope_layer=nope_layer,\n          is_moe_layer=moe_layer,\n      )\n      y = layer(\n          y,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n          previous_chunk=previous_chunk,\n          page_state=page_state,\n          slot=slot,\n          bidirectional_mask=bidirectional_mask,\n      )\n      if cfg.scan_layers:\n        y = y[0]\n    if cfg.scan_layers:\n      return y, None\n    else:\n      return y",
        "analysis": {
            "module_type": "llama4_scannable_block",
            "purpose": "A container module that sequentially applies a configured number of `Llama4DecoderLayer` instances, designed to be a repeatable unit, potentially for use with `nn.scan`.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes a loop to create and apply decoder layers.",
                "The number of layers within the block is determined by `config.inhomogeneous_layer_cycle_interval`."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.linen.Module",
                "Llama4DecoderLayer",
                "determine_is_nope_layer",
                "determine_is_moe_layer"
            ],
            "parameters": {
                "nope_layer_interval": "The interval at which layers should disable RoPE (NoPE).",
                "interleave_moe_layer_step": "The interval or stride for placing Mixture-of-Experts (MoE) layers.",
                "inhomogeneous_layer_cycle_interval": "The number of decoder layers to apply within this block.",
                "scan_layers": "A boolean flag from the config indicating if the block is used with `nn.scan`, which affects the return signature."
            },
            "notes": [
                "This block dynamically configures each internal `Llama4DecoderLayer` as a standard, NoPE, or MoE layer based on its index within the loop."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Processes an input tensor through a sequence of `Llama4DecoderLayer`s.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Apply logical constraint and checkpoint name to the input tensor.",
                        "Loop `config.inhomogeneous_layer_cycle_interval` times.",
                        "Inside the loop, call `determine_is_nope_layer` and `determine_is_moe_layer` to configure the current layer.",
                        "Instantiate and apply a `Llama4DecoderLayer` to the hidden state.",
                        "Update the hidden state with the output of the layer."
                    ],
                    "output": {
                        "shape": "If `config.scan_layers` is true, returns `([batch_size, sequence_length, hidden_dim], None)`. Otherwise, returns `[batch_size, sequence_length, hidden_dim]`."
                    },
                    "dependencies": [
                        "Llama4DecoderLayer",
                        "determine_is_nope_layer",
                        "determine_is_moe_layer",
                        "jax.ad_checkpoint.checkpoint_name"
                    ],
                    "notes": [
                        "The method's return signature is conditional on the `config.scan_layers` flag to ensure compatibility with Flax's `nn.scan` primitive."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionEncoderLayer",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionEncoderLayer(nn.Module):\n  \"\"\"Transformer encoder layer for Llama4 vision model.\"\"\"\n\n  config: Config\n  mesh: Mesh\n\n  @nn.compact\n  def __call__(\n      self,\n      hidden_states: Array,\n      deterministic: bool = False,\n  ) -> Array:\n    \"\"\"Forward pass of the vision encoder layer.\n\n    Args:\n      hidden_states: Input hidden states\n      deterministic: Whether to use deterministic mode\n\n    Returns:\n      Output hidden states\n    \"\"\"\n    # Self Attention\n    residual = hidden_states\n\n    # Input layer norm\n    hidden_states = nn.LayerNorm(name=\"input_layer_norm\", epsilon=1e-5)(hidden_states)\n\n    # Self attention\n    attention_layer = attentions.attention_as_linen(\n        config=self.config,\n        num_query_heads=self.config.num_attention_heads_for_vit,\n        num_kv_heads=self.config.num_attention_heads_for_vit,\n        head_dim=self.config.hidden_size_for_vit // self.config.num_attention_heads_for_vit,\n        max_target_length=(self.config.image_size_for_vit // self.config.patch_size_for_vit) ** 2 + 1,\n        attention_kernel=\"dot_product\",\n        inputs_q_shape=hidden_states.shape,\n        inputs_kv_shape=hidden_states.shape,\n        float32_qk_product=self.config.float32_qk_product,\n        float32_logits=self.config.float32_logits,\n        mesh=self.mesh,\n        dropout_rate=0,\n        name=\"self_attention_vision\",\n        attention_type=AttentionType.FULL,\n        is_nope_layer=False,\n        use_bias_in_projections=True,\n        is_vision=True,\n        use_qk_norm=False,\n        query_pre_attn_scalar=1 / math.sqrt(self.config.hidden_size_for_vit // self.config.num_attention_heads_for_vit),\n        # The vision encoder processes an image in a single forward pass to produce\n        # embeddings. It doesn't have the concept of \"prefill\" and \"autoregressive\"\n        # steps that a text decoder has. Therefore, it doesn't need a KV cache for\n        # its self-attention mechanism.\n        model_mode=MODEL_MODE_TRAIN,\n    )\n\n    hidden_states = attention_layer(\n        inputs_q=hidden_states,\n        inputs_kv=hidden_states,\n        deterministic=deterministic,\n    )\n\n    hidden_states = residual + hidden_states\n\n    residual = hidden_states\n\n    # Post attention layer norm\n    hidden_states = nn.LayerNorm(name=\"post_attention_layer_norm\", epsilon=1e-5)(hidden_states)\n\n    # MLP\n    mlp = Llama4VisionMLP(self.config)\n    hidden_states = mlp(hidden_states)\n\n    hidden_states = residual + hidden_states\n\n    return hidden_states",
        "analysis": {
            "functionality": "The `Llama4VisionEncoderLayer` class implements a single transformer encoder layer for a vision model. It processes input embeddings through a sequence of self-attention and a multi-layer perceptron (MLP), each preceded by layer normalization and followed by a residual connection.",
            "usage": "This class is intended to be used as a building block within a larger vision encoder. To use it, instantiate the class with a configuration object and a JAX mesh, then call the instance with an input tensor of shape `[batch_size, num_patches, hidden_dim]`. The `deterministic` flag can be set to `True` during inference to disable dropout. The class returns a tensor of the same shape as the input."
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionEncoder",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionEncoder(nn.Module):\n  \"\"\"Transformer encoder consisting of multiple Llama4VisionEncoderLayer layers.\n\n  This encoder is based on the PyTorch reference implementation and uses multiple\n  encoder layers to process vision input.\n\n  Attributes:\n    config: Config containing model parameters\n    mesh: Mesh, JAX device mesh (used for sharding)\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n\n  @nn.compact\n  def __call__(\n      self,\n      hidden_states: Array,\n      deterministic: bool = False,\n  ) -> Array:\n    \"\"\"Forward pass of the vision encoder.\n\n    Args:\n      hidden_states: Input hidden states\n      deterministic: Whether to use deterministic mode (disables dropout)\n\n    Returns:\n      Final hidden states\n    \"\"\"\n    cfg = self.config\n\n    # Iterate through encoder layers (non-scan version)\n    for layer_idx in range(cfg.num_hidden_layers_for_vit):\n\n      # TODO\uff1a add scan version\n      layer = Llama4VisionEncoderLayer(config=cfg, mesh=self.mesh, name=f\"layers_{layer_idx}\")\n\n      hidden_states = layer(\n          hidden_states=hidden_states,\n          deterministic=deterministic,\n      )\n\n    return hidden_states",
        "analysis": {
            "module_type": "vision_transformer_encoder",
            "purpose": "A transformer encoder that processes vision input features by applying a stack of Llama4VisionEncoderLayer layers.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "The `__call__` method iterates `config.num_hidden_layers_for_vit` times, sequentially applying a `Llama4VisionEncoderLayer` to the input hidden states."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.linen.Module",
                "Llama4VisionEncoderLayer"
            ],
            "parameters": {
                "config": "Configuration object containing model parameters, such as `num_hidden_layers_for_vit`.",
                "mesh": "The JAX device mesh used for sharding model parameters and activations."
            },
            "notes": [
                "This encoder is based on a PyTorch reference implementation.",
                "The code includes a TODO comment to add a JAX `scan` version for potentially better performance."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Performs the forward pass of the vision encoder by sequentially applying multiple encoder layers.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "Array (typically float32 or bfloat16)"
                    },
                    "processing_steps": [
                        "Iterate from `layer_idx` 0 to `config.num_hidden_layers_for_vit - 1`.",
                        "Instantiate a `Llama4VisionEncoderLayer` for the current `layer_idx`.",
                        "Pass the `hidden_states` through the instantiated layer.",
                        "Update `hidden_states` with the output of the layer.",
                        "Return the final `hidden_states` after the loop."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "Llama4VisionEncoderLayer"
                    ],
                    "notes": [
                        "The `deterministic` argument controls whether dropout is enabled within the sub-layers."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionModel",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionModel(nn.Module):\n  \"\"\"Llama4 vision model for processing image inputs.\n\n  This model extracts patches from input image tiles and processes them\n  through Llama4VisionEncoder and other vision-specific layers.\n\n  Attributes:\n    config: Config containing model parameters\n    mesh: Mesh, JAX device mesh (used for sharding)\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n\n  def setup(self):\n    self.scale = self.config.hidden_size_for_vit**-0.5\n    self.num_patches = (self.config.tile_size_for_vit // self.config.patch_size_for_vit) ** 2 + 1\n    self.class_embedding = self.param(\n        \"class_embedding\",\n        nn.initializers.normal(stddev=self.scale, dtype=self.config.dtype_mm),\n        (self.config.hidden_size_for_vit,),\n    )\n    self.positional_embedding_vlm = self.param(\n        \"positional_embedding_vlm\",\n        nn.initializers.normal(stddev=self.scale, dtype=self.config.dtype_mm),\n        (self.num_patches, self.config.hidden_size_for_vit),\n    )\n\n  @nn.compact\n  def __call__(\n      self,\n      pixel_values: Array,\n      output_attentions: None | bool = None,\n      output_hidden_states: None | bool = None,\n      return_dict: None | bool = None,\n      deterministic: None | bool = False,\n  ) -> Array:\n    \"\"\"Forward pass of the Llama4 vision model.\n\n    Args:\n      inputs: Input tensor of shape [batch_size, num_tiles, num_channels_for_vit, tile_size_for_vit, tile_size_for_vit]\n      deterministic: Whether to use deterministic mode (disables dropout)\n\n    Returns:\n      Final hidden states from the vision encoder of shape [batch_size, num_tiles, num_patches, vision_output_dim_for_vit]\n    \"\"\"\n    cfg = self.config\n    mesh = self.mesh\n\n    b, t, c, h, w = pixel_values.shape\n    pixel_values = jnp.reshape(pixel_values, [b * t, c, h, w])\n\n    # Unfold convolution to extract patches\n    hidden_states = Llama4UnfoldConvolution(config=cfg)(pixel_values)\n\n    # Add class embedding to the beginning of the sequence\n    class_embedding_expanded = jnp.expand_dims(jnp.expand_dims(self.class_embedding, axis=0), axis=0)\n    class_embedding = jnp.broadcast_to(class_embedding_expanded, (hidden_states.shape[0], 1, cfg.hidden_size_for_vit))\n    hidden_states = jnp.concatenate([class_embedding, hidden_states], axis=1)\n\n    # Add positional embedding\n    hidden_states += self.positional_embedding_vlm\n\n    # Transformation layers\n    hidden_states = nn.LayerNorm(name=\"layernorm_pre\")(hidden_states)\n    hidden_states = Llama4VisionEncoder(config=cfg, mesh=mesh)(hidden_states)\n    hidden_states = nn.LayerNorm(name=\"layernorm_post\")(hidden_states)\n    hidden_states = hidden_states[:, :-1, :]\n\n    hidden_states = Llama4VisionPixelShuffleMLP(config=cfg)(hidden_states)\n\n    # Reshape hidden states\n    _, patch_num, patch_dim = hidden_states.shape\n    hidden_states = jnp.reshape(hidden_states, [b, t, patch_num, patch_dim])\n\n    return hidden_states",
        "analysis": {
            "module_type": "llama4_vision_model",
            "purpose": "Processes tiled image inputs by extracting patches, encoding them through a vision transformer, and applying further transformations to produce image feature embeddings.",
            "input": {
                "shape": "[batch_size, num_tiles, num_channels_for_vit, tile_size_for_vit, tile_size_for_vit]",
                "dtype": "float"
            },
            "processing_steps": [
                "Reshape the 5D input tensor into a 4D tensor by merging the batch and tile dimensions.",
                "Extract and project image patches using `Llama4UnfoldConvolution`.",
                "Prepend a learnable class embedding to the sequence of patch embeddings.",
                "Add positional embeddings to the sequence.",
                "Apply a pre-encoder Layer Normalization.",
                "Process the sequence through the `Llama4VisionEncoder`.",
                "Apply a post-encoder Layer Normalization.",
                "Remove the class token embedding from the sequence.",
                "Process the sequence through `Llama4VisionPixelShuffleMLP`.",
                "Reshape the final hidden states to reintroduce the tile dimension."
            ],
            "output": {
                "shape": "[batch_size, num_tiles, num_patches, vision_output_dim_for_vit]"
            },
            "dependencies": [
                "Llama4UnfoldConvolution",
                "Llama4VisionEncoder",
                "Llama4VisionPixelShuffleMLP",
                "flax.linen.nn",
                "jax.numpy"
            ],
            "parameters": {
                "config": "A configuration object containing model parameters such as hidden sizes, patch sizes, tile sizes, and data types.",
                "mesh": "A JAX device mesh used for model sharding and parallel computation."
            },
            "notes": [
                "This model is designed to handle images that are split into multiple tiles.",
                "A learnable class embedding is prepended to the sequence of image patches before the encoder and is removed after the encoder."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the model's learnable parameters, including the class embedding and positional embeddings.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculate the initialization scale based on `config.hidden_size_for_vit`.",
                        "Calculate the number of patches based on `config.tile_size_for_vit` and `config.patch_size_for_vit`.",
                        "Initialize the `class_embedding` parameter.",
                        "Initialize the `positional_embedding_vlm` parameter."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "flax.linen.nn.initializers"
                    ],
                    "notes": [
                        "Embeddings are initialized using a normal distribution with a standard deviation scaled by the inverse square root of the vision hidden size."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the vision model, transforming image pixels into feature embeddings.",
                    "input": {
                        "shape": "[batch_size, num_tiles, num_channels_for_vit, tile_size_for_vit, tile_size_for_vit]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Reshape the 5D input `pixel_values` into a 4D tensor by combining batch and tile dimensions.",
                        "Call `Llama4UnfoldConvolution` to extract and project image patches.",
                        "Prepend the learnable `class_embedding` to the sequence of patch embeddings.",
                        "Add the `positional_embedding_vlm` to the sequence.",
                        "Apply a pre-encoder LayerNorm (`layernorm_pre`).",
                        "Pass the sequence through the `Llama4VisionEncoder`.",
                        "Apply a post-encoder LayerNorm (`layernorm_post`).",
                        "Remove the class token embedding from the sequence.",
                        "Pass the result through `Llama4VisionPixelShuffleMLP`.",
                        "Reshape the final hidden states to reintroduce the tile dimension."
                    ],
                    "output": {
                        "shape": "[batch_size, num_tiles, num_patches, vision_output_dim_for_vit]"
                    },
                    "dependencies": [
                        "Llama4UnfoldConvolution",
                        "Llama4VisionEncoder",
                        "Llama4VisionPixelShuffleMLP",
                        "jax.numpy"
                    ],
                    "notes": [
                        "The `deterministic` parameter controls dropout behavior in submodules.",
                        "The class token is used during the encoder stage but is discarded before the final projection layers."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/mistral.py#MistralDecoderLayer",
        "file_path": "src/MaxText/layers/mistral.py",
        "code_block": "class MistralDecoderLayer(nn.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: models.Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    lnx_rms = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )\n    lnx = lnx_rms(inputs)\n\n    lnx = nn.with_logical_constraint(lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    # Self-attention block\n    attention_layer = attention_as_linen(\n        config=cfg,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention\",\n        float32_qk_product=cfg.float32_qk_product,\n        float32_logits=cfg.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        prefill_cache_axis_order=tuple(map(int, cfg.prefill_cache_axis_order.split(\",\"))),\n        ar_cache_axis_order=tuple(map(int, cfg.ar_cache_axis_order.split(\",\"))),\n        compute_axis_order=tuple(map(int, cfg.compute_axis_order.split(\",\"))),\n        model_mode=model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n        previous_chunk=previous_chunk,\n    )\n\n    attention_lnx = nn.with_logical_constraint(\n        attention_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n    intermediate_inputs = inputs + attention_lnx\n\n    # Fully Connected\n    hidden_states = rms_norm(\n        num_features=intermediate_inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"post_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )(intermediate_inputs)\n    hidden_states = nn.with_logical_constraint(\n        hidden_states, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n\n    mlp_lnx = mlp_block(\n        in_features=hidden_states.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp\",\n        config=cfg,\n        quant=self.quant,\n    )(hidden_states, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"),\n    )\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "mistral_decoder_layer",
            "purpose": "Implements a single decoder layer for a Mistral-style transformer model, consisting of a self-attention block and a feed-forward MLP block.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "The `__call__` method executes the forward pass of the decoder layer."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.linen.Module",
                "MaxText.layers.models.Config",
                "jax.sharding.Mesh",
                "MaxText.layers.quantizations.AqtQuantization",
                "MaxText.layers.normalizations.rms_norm",
                "MaxText.layers.attentions.attention_as_linen",
                "MaxText.layers.linears.mlp_block"
            ],
            "parameters": {
                "config": "Configuration object containing model hyperparameters like dimensions, dropout rates, and activation functions.",
                "mesh": "The JAX sharding mesh for distributed computation.",
                "model_mode": "A string indicating the operational mode, e.g., 'train', 'prefill', or 'autoregressive'.",
                "quant": "Optional quantization configuration for weights and activations."
            },
            "notes": [
                "This class represents a standard transformer decoder layer with pre-normalization (RMSNorm), two residual connections, and optional gradient checkpointing.",
                "It is designed to be stackable to form a complete transformer decoder."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Processes an input tensor through one complete transformer decoder layer, including self-attention and an MLP.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "Depends on config.dtype (e.g., float32, bfloat16)"
                    },
                    "processing_steps": [
                        "Apply pre-attention RMS normalization to the input tensor.",
                        "Pass the normalized tensor through a self-attention layer (`attention_as_linen`).",
                        "Add the attention output to the original input (first residual connection).",
                        "Apply post-attention RMS normalization to the result of the first residual connection.",
                        "Pass the result through an MLP block (`mlp_block`).",
                        "Add the MLP output to the result of the first residual connection (second residual connection).",
                        "Apply dropout to the final output.",
                        "Optionally record internal activation metrics.",
                        "Return the final layer output."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "rms_norm",
                        "attention_as_linen",
                        "mlp_block",
                        "nn.Dropout",
                        "nn.with_logical_constraint",
                        "jax.ad_checkpoint.checkpoint_name"
                    ],
                    "notes": [
                        "The `deterministic` flag controls whether dropout is active.",
                        "If `config.scan_layers` is true, the method returns a tuple `(output, None)` for compatibility with `flax.linen.scan`.",
                        "The `model_mode` parameter is passed to the attention layer to control its behavior (e.g., caching in autoregressive mode)."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/mixtral.py#MixtralDecoderLayer",
        "file_path": "src/MaxText/layers/mixtral.py",
        "code_block": "class MixtralDecoderLayer(nn.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: models.Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    lnx_rms = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )\n    lnx = lnx_rms(inputs)\n\n    lnx = nn.with_logical_constraint(lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    # Self-attention block\n    attention_layer = attention_as_linen(\n        config=cfg,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention\",\n        float32_qk_product=cfg.float32_qk_product,\n        float32_logits=cfg.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        prefill_cache_axis_order=tuple(map(int, cfg.prefill_cache_axis_order.split(\",\"))),\n        ar_cache_axis_order=tuple(map(int, cfg.ar_cache_axis_order.split(\",\"))),\n        compute_axis_order=tuple(map(int, cfg.compute_axis_order.split(\",\"))),\n        model_mode=model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n        previous_chunk=previous_chunk,\n    )\n\n    attention_lnx = nn.with_logical_constraint(\n        attention_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n    intermediate_inputs = inputs + attention_lnx\n\n    # Fully Connected\n    hidden_states = rms_norm(\n        num_features=intermediate_inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"post_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )(intermediate_inputs)\n    hidden_states = nn.with_logical_constraint(\n        hidden_states, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n\n    load_balance_loss = None\n    # NOTE: the naming mismatch here is to ensure reverse compatibility with existing checkpoints.\n    # The `name` represents the weight name in JAX/checkpoints and so the class name\n    # is just for readability.\n    mlp_lnx, load_balance_loss = moe.get_routed_moe(\n        name=\"MoeBlock_0\",\n        config=cfg,\n        num_experts=cfg.num_experts,\n        num_experts_per_tok=cfg.num_experts_per_tok,\n        mesh=mesh,\n        kernel_init=initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        intermediate_dim=cfg.mlp_dim,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        quant=self.quant,\n    )(hidden_states)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"),\n    )\n\n    if load_balance_loss is not None:\n      self.sow(\"intermediates\", \"moe_lb_loss\", load_balance_loss)\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "mixtral_decoder_layer",
            "purpose": "Implements a single decoder layer for a Mixtral-style transformer model, including self-attention and a Mixture-of-Experts (MoE) feed-forward block.",
            "input": {
                "shape": "N/A (Handled by the '__call__' method)",
                "dtype": "N/A (Handled by the '__call__' method)"
            },
            "processing_steps": [
                "Initializes a self-attention layer (`attention_as_linen`).",
                "Initializes a Mixture-of-Experts layer (`moe.get_routed_moe`).",
                "Applies pre-attention RMS normalization.",
                "Performs self-attention on the normalized input.",
                "Adds the attention output to the original input (residual connection).",
                "Applies post-attention RMS normalization.",
                "Passes the result through the Mixture-of-Experts (MoE) block.",
                "Adds the MoE output to the intermediate input (second residual connection).",
                "Applies dropout.",
                "Returns the final layer output."
            ],
            "output": {
                "shape": "N/A (Handled by the '__call__' method)"
            },
            "dependencies": [
                "flax.linen.Module",
                "MaxText.layers.models.Config",
                "jax.sharding.Mesh",
                "MaxText.layers.normalizations.rms_norm",
                "MaxText.layers.attentions.attention_as_linen",
                "MaxText.layers.moe.get_routed_moe",
                "MaxText.layers.quantizations.AqtQuantization"
            ],
            "parameters": {
                "config": "Configuration object containing model hyperparameters like dimensions, dropout rates, and number of experts.",
                "mesh": "JAX sharding mesh for distributed computation.",
                "model_mode": "A string indicating the operational mode (e.g., 'train', 'prefill', 'autoregressive').",
                "quant": "Optional quantization configuration for weights and activations."
            },
            "notes": [
                "This layer uses two residual connections: one after the self-attention block and another after the MoE block.",
                "It utilizes RMSNorm for layer normalization before both the attention and MoE blocks.",
                "The feed-forward network is a Mixture-of-Experts (MoE) layer, which can generate a load balancing loss that is collected via `self.sow`.",
                "The return signature of the `__call__` method changes based on `config.scan_layers` to support `flax.linen.scan`."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Applies one complete Mixtral decoder layer transformation, including self-attention and a Mixture-of-Experts block, to the input sequence.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim]",
                        "dtype": "Depends on config.dtype (e.g., float32, bfloat16)"
                    },
                    "processing_steps": [
                        "Apply RMS normalization to the input tensor (`pre_self_attention_layer_norm`).",
                        "Execute the self-attention mechanism via `attention_as_linen`.",
                        "Add the attention output to the original input (first residual connection).",
                        "Apply another RMS normalization (`post_self_attention_layer_norm`).",
                        "Process the result through a Mixture-of-Experts (MoE) block using `moe.get_routed_moe`, potentially generating a load balance loss.",
                        "Add the MoE output to the intermediate tensor (second residual connection).",
                        "Apply dropout to the final output.",
                        "Optionally log intermediate metrics and the MoE load balancing loss using `self.sow`."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "jax.ad_checkpoint.checkpoint_name",
                        "flax.linen.with_logical_constraint",
                        "MaxText.layers.normalizations.rms_norm",
                        "MaxText.layers.attentions.attention_as_linen",
                        "MaxText.layers.moe.get_routed_moe",
                        "flax.linen.Dropout",
                        "jax.numpy"
                    ],
                    "notes": [
                        "The method handles different execution paths based on `model_mode` (e.g., 'train', 'prefill', 'autoregressive') which is passed down to the attention layer.",
                        "It uses `self.sow` to collect intermediate values like the MoE load balancing loss and other metrics for logging or auxiliary loss calculation.",
                        "The return value is a tuple `(layer_output, None)` when `cfg.scan_layers` is enabled, to be compatible with `flax.linen.scan`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/models.py#TransformerLinenPure",
        "file_path": "src/MaxText/layers/models.py",
        "code_block": "class TransformerLinenPure(nn.Module):\n  \"\"\"An autoregressive transformer model.\"\"\"\n\n  # Make new attributes required, so that all Transformer dependencies (train, decode,\n  # compile, etc) will error instead of silently use defaults.\n  # pylint: disable=attribute-defined-outside-init\n  config: Config\n  mesh: Mesh\n  quant: Quant\n  # Possible model_mode values can be found in MaxText.common_types.\n  # We generally use MaxText.common_types.MODEL_MODE_TRAIN or\n  # MaxText.common_types.MODEL_MODE_PREFILL for initializations here.\n  # TODO: Make model_mode required after confirming no users are affected.\n  model_mode: str = MODEL_MODE_TRAIN  # May be different than the model_mode passed to __call__\n  # pylint: enable=attribute-defined-outside-init\n\n  def init(self, *args, model_mode: str = MODEL_MODE_TRAIN, **kwargs):\n    \"\"\"Initializes the model.\"\"\"\n    module = self.clone(model_mode=model_mode)\n    return nn.Module.init(module, *args, **kwargs)\n\n  def apply(self, *args, model_mode: str = MODEL_MODE_TRAIN, **kwargs):\n    \"\"\"Applies the model.\"\"\"\n    module = self.clone(model_mode=model_mode)\n    return nn.Module.apply(module, *args, **kwargs)\n\n  def setup(self):\n    \"\"\"Initialize shared_embedding & decoder layers.\"\"\"\n\n    cfg = self.config\n    mesh = self.mesh\n    self.shared_embedding = embed_as_linen(\n        num_embeddings=cfg.vocab_size,\n        num_features=cfg.emb_dim,\n        dtype=cfg.dtype,\n        attend_dtype=jnp.float32 if cfg.logits_dot_in_fp32 else cfg.dtype,  # for logit training stability\n        embedding_init=nn.initializers.normal(stddev=1.0),\n        name=\"token_embedder\",\n        config=cfg,\n    )\n    self.vision_encoder = VisionEncoder(config=cfg, mesh=mesh) if cfg.use_multimodal else None\n    self.decoder = Decoder(config=cfg, mesh=mesh, quant=self.quant, model_mode=self.model_mode)\n    # If MTP is enabled via config, set up the MTP block.\n    if self.config.mtp_num_layers > 0:\n      # Get the list of layer blueprints for the current model.\n      layer_types = self.decoder.get_decoder_layers()\n      # For MTP, we use the DecoderLayer blueprint to ensure architectural consistency.\n      # By convention, this is the last layer in the list.\n      mtp_layer = layer_types[-1]\n      self.mtp_block = MultiTokenPredictionBlock(\n          config=self.config, mesh=self.mesh, name=\"mtp_block\", transformer_layer_module=mtp_layer, decoder=self.decoder\n      )\n\n  def logits_from_hidden_states(self, hidden_states, deterministic):\n    \"\"\"\n    Compute logits from hidden states (wrapping decoder._apply_output_head).\n    This function is only used for vocabulary tiling.\n    \"\"\"\n    logits = self.decoder._apply_output_head(\n        shared_embedding=self.shared_embedding,\n        y=hidden_states,\n        deterministic=deterministic,\n    )\n    return logits\n\n  def __call__(\n      self,\n      decoder_input_tokens: jnp.ndarray,\n      decoder_positions: jnp.ndarray,\n      decoder_segment_ids=None,\n      encoder_images: None | jnp.ndarray = None,\n      enable_dropout=True,\n      previous_chunk=None,\n      true_length: None | int = None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      decoder_target_tokens: None | jnp.ndarray = None,\n      decoder_target_mask: None | jnp.ndarray = None,\n      nnx_method=None,\n  ):\n    \"\"\"Applies Transformer decoder-branch on encoded-input and target.\n\n    Args:\n      true_length: (Optional) Prompt length before padding\n      slot: (Optional) An integer representing the decode batch index selected\n        for this request.\n    \"\"\"\n\n    if decoder_segment_ids is not None and self.model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      raise ValueError(\n          f\"During autoregressive decoding we assume the tokens are in the active sequence\"\n          f\" which is always {DECODING_ACTIVE_SEQUENCE_INDICATOR}.\"\n      )\n\n    bidirectional_mask = None\n    image_embeddings = None\n    if self.config.use_multimodal and encoder_images is not None:\n      image_embeddings = self.vision_encoder(input_images=encoder_images, deterministic=not enable_dropout)\n\n      if self.config.decoder_block == DecoderBlockType.GEMMA3:\n        bidirectional_mask = decoder_input_tokens == multimodal_utils.GEMMA_TOKEN_PLACEHOLDER\n      elif self.config.decoder_block == DecoderBlockType.LLAMA4:\n        bidirectional_mask = decoder_input_tokens == multimodal_utils.LLAMA4_PATCH_TOKEN\n\n    logits, hidden_state = self.decoder(\n        shared_embedding=self.shared_embedding,\n        decoder_input_tokens=decoder_input_tokens,\n        decoder_positions=decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=not enable_dropout,\n        previous_chunk=previous_chunk,\n        slot=slot,\n        page_state=page_state,\n        bidirectional_mask=bidirectional_mask,\n        image_embeddings=image_embeddings,\n    )\n\n    # If we are initializing the model AND MTP is enabled, we must create\n    # dummy target tensors. This allows Flax to trace the MTPBlock and create\n    # all its necessary parameters, without requiring the main training pipeline\n    # to be aware of this initialization detail.\n    if self.is_initializing() and self.config.mtp_num_layers > 0:\n      if decoder_target_tokens is None:\n        dummy_shape = decoder_input_tokens.shape\n        decoder_target_tokens = jnp.ones(dummy_shape, dtype=jnp.int32)\n        decoder_target_mask = jnp.ones(dummy_shape, dtype=jnp.int32)\n        decoder_segment_ids = jnp.ones(dummy_shape, dtype=jnp.int32)\n\n    # The Multi-Token Prediction (MTP) block functions as a \"side-car\" to the main\n    # model, active only during training. It computes an auxiliary loss based on\n    # predicting multiple future tokens, as described in the DeepSeek-V3 paper.\n    # To ensure architectural consistency, it uses two key components from the parent Transformer:\n    #   1. The same `DecoderLayer` blueprint for its internal transformer blocks.\n    #   2. The `shared_embedding` for both embedding future tokens and for its final\n    #      logit projection.\n    # Its only effect is to \"sow\" these losses; it does not alter the primary logits output.\n    if self.config.mtp_num_layers > 0:\n      self.mtp_block(\n          shared_embedding=self.shared_embedding,\n          main_hidden_state=hidden_state,\n          input_ids=decoder_input_tokens,\n          target_ids=decoder_target_tokens,\n          target_mask=decoder_target_mask,\n          position_ids=decoder_positions,\n          decoder_segment_ids=decoder_segment_ids,\n          deterministic=not enable_dropout,\n      )\n\n    return logits",
        "analysis": {
            "module_type": "autoregressive_transformer",
            "purpose": "An autoregressive transformer model implemented in pure Flax Linen, capable of handling text and multimodal inputs.",
            "input": {
                "shape": "See the `__call__` method.",
                "dtype": "See the `__call__` method."
            },
            "processing_steps": [
                "Initializes shared embedding, a decoder, an optional vision encoder, and an optional Multi-Token Prediction (MTP) block during `setup`.",
                "In the forward pass (`__call__`), it optionally processes images with the vision encoder.",
                "Passes token embeddings, positions, and optional image embeddings to the decoder to compute logits and hidden states.",
                "Optionally, passes the hidden states and target information to the MTP block to compute an auxiliary loss.",
                "Returns the final logits."
            ],
            "output": {
                "shape": "See the `__call__` method."
            },
            "dependencies": [
                "flax.linen.Module",
                "MaxText.layers.decoders.Decoder",
                "MaxText.layers.encoders.VisionEncoder",
                "MaxText.layers.multi_token_prediction.MultiTokenPredictionBlock",
                "MaxText.layers.embeddings.embed_as_linen",
                "MaxText.common_types.Config",
                "jax.sharding.Mesh",
                "MaxText.layers.quantizations.AqtQuantization"
            ],
            "parameters": {
                "config": "A `Config` object containing model hyperparameters and settings.",
                "mesh": "The JAX sharding mesh for device parallelism.",
                "quant": "The quantization configuration object.",
                "model_mode": "The operational mode of the model (e.g., 'train', 'prefill', 'autoregressive')."
            },
            "notes": [
                "This is a pure Flax Linen implementation of a transformer model.",
                "It supports multimodality through an optional `VisionEncoder`.",
                "It supports Multi-Token Prediction (MTP) for auxiliary loss calculation during training via an optional `MultiTokenPredictionBlock`."
            ],
            "methods": {
                "init": {
                    "purpose": "Initializes the model, cloning it with a specific `model_mode`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Clones the module with the provided `model_mode`.",
                        "Calls the parent `nn.Module.init` method."
                    ],
                    "output": {
                        "shape": "Initialized model parameters (Flax PyTree)."
                    },
                    "dependencies": [
                        "flax.linen.Module"
                    ],
                    "notes": [
                        "This method allows for initializing the model in a specific mode (e.g., 'train') which can differ from the mode used during `apply`."
                    ]
                },
                "apply": {
                    "purpose": "Applies the model, cloning it with a specific `model_mode`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Clones the module with the provided `model_mode`.",
                        "Calls the parent `nn.Module.apply` method."
                    ],
                    "output": {
                        "shape": "Depends on the method called, typically logits tensor."
                    },
                    "dependencies": [
                        "flax.linen.Module"
                    ],
                    "notes": [
                        "This method allows for applying the model in a specific mode (e.g., 'autoregressive') which can differ from the mode used during initialization."
                    ]
                },
                "setup": {
                    "purpose": "Initializes the shared embedding, decoder, optional vision encoder, and optional Multi-Token Prediction (MTP) block.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initializes the shared token embedding layer using `embed_as_linen`.",
                        "Initializes the `VisionEncoder` if `config.use_multimodal` is true.",
                        "Initializes the `Decoder` module.",
                        "Initializes the `MultiTokenPredictionBlock` if `config.mtp_num_layers` > 0."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "embed_as_linen",
                        "VisionEncoder",
                        "Decoder",
                        "MultiTokenPredictionBlock"
                    ],
                    "notes": [
                        "This is a standard Flax method called once during model initialization to create submodules."
                    ]
                },
                "logits_from_hidden_states": {
                    "purpose": "Computes logits from hidden states by calling the decoder's output head.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Calls the `decoder._apply_output_head` method with the shared embedding and hidden states."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, vocab_size]"
                    },
                    "dependencies": [
                        "Decoder"
                    ],
                    "notes": [
                        "The docstring states this function is specifically used for vocabulary tiling."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the Transformer model to compute logits from input tokens and optional images.",
                    "input": {
                        "shape": "decoder_input_tokens: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length], encoder_images: [batch_size, num_patches, patch_dim] (optional)",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "If multimodal, process `encoder_images` with `vision_encoder` to get `image_embeddings`.",
                        "If multimodal, create a `bidirectional_mask` based on special token placeholders.",
                        "Pass inputs and embeddings to the `decoder` to get `logits` and `hidden_state`.",
                        "If Multi-Token Prediction (MTP) is enabled, call the `mtp_block` with the hidden state and target tokens to compute an auxiliary loss (sown via Flax).",
                        "Return the primary `logits`."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, vocab_size]"
                    },
                    "dependencies": [
                        "VisionEncoder",
                        "Decoder",
                        "MultiTokenPredictionBlock"
                    ],
                    "notes": [
                        "The MTP block computes an auxiliary loss but does not modify the returned logits.",
                        "Raises a ValueError if `decoder_segment_ids` are provided in autoregressive mode.",
                        "During initialization with MTP enabled, it creates dummy target tensors to ensure the MTP block parameters are created."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/models.py#transformer_as_linen",
        "file_path": "src/MaxText/layers/models.py",
        "code_block": "def transformer_as_linen(\n    config: Config,\n    mesh: Mesh,\n    quant: Quant,\n    model_mode: str = MODEL_MODE_TRAIN,\n    *,\n    name: str | None = None,\n) -> nnx_wrappers.ToLinen | TransformerLinenPure:\n  \"\"\"Constructs a Transformer model as a Linen or NNX module.\n\n  This function returns an autoregressive Transformer model as either a Linen module\n  or an NNX-wrapped module, depending on the `config.enable_nnx` flag. The returned module\n  is suitable for training, evaluation, or decoding.\n\n  If `config.enable_nnx` is True, returns a `TransformerLinen` that wraps the NNX-style\n  Transformer for integration with NNX-specific APIs and workflows.\n  Otherwise, returns a pure Flax Linen implementation (`TransformerLinenPure`).\n\n  Args:\n    config (Config): The configuration object specifying model hyperparameters and options.\n    mesh (Mesh): The JAX sharding mesh for device partitioning.\n    quant (Quant): The quantization module or configuration to use.\n    model_mode (str, optional): The operational mode for the model, e.g.\n      training, prefill, or autoregressive. Defaults to `MODEL_MODE_TRAIN`.\n    name (str, optional): Optional module name for Linen/NNX construction.\n\n  Returns:\n    nnx_wrappers.ToLinen | TransformerLinenPure:\n      A constructed Transformer model compatible with the specified framework (Linen or NNX).\n  \"\"\"\n  if config.enable_nnx:\n    return TransformerLinen(\n        Transformer,\n        args=(),\n        kwargs=nn.FrozenDict(\n            {\n                \"mesh\": mesh,\n                \"config\": config,\n                \"quant\": quant,\n                \"model_mode\": model_mode,\n            }\n        ),\n        metadata_fn=initializers.variable_to_logically_partitioned,\n        name=name,\n    )\n  else:\n    return TransformerLinenPure(config, mesh, quant, model_mode=model_mode, name=name)",
        "analysis": {
            "module_type": "transformer_factory",
            "purpose": "Constructs and returns an autoregressive Transformer model, choosing between a pure Flax Linen implementation (TransformerLinenPure) and an NNX-wrapped Linen implementation (TransformerLinen) based on a configuration flag.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check the `config.enable_nnx` flag.",
                "If true, instantiate and return `TransformerLinen`, which wraps the NNX `Transformer` module.",
                "If false, instantiate and return the pure Flax `TransformerLinenPure` module."
            ],
            "output": {
                "shape": "An uninitialized Flax Linen module (either `TransformerLinen` or `TransformerLinenPure`)."
            },
            "dependencies": [
                "Config",
                "Mesh",
                "Quant",
                "TransformerLinen",
                "TransformerLinenPure",
                "Transformer",
                "nn.FrozenDict",
                "initializers.variable_to_logically_partitioned"
            ],
            "parameters": {
                "config.enable_nnx": "A boolean flag that determines whether to return the NNX-based model (`TransformerLinen`) or the pure Linen model (`TransformerLinenPure`).",
                "model_mode": "Specifies the operational mode for the model, such as training, prefill, or autoregressive decoding."
            },
            "notes": [
                "This function acts as a factory, abstracting away the choice between the two underlying Transformer implementations.",
                "The returned module is a class instance ready for initialization or application within a Flax/NNX workflow."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/models.py#TransformerLinen",
        "file_path": "src/MaxText/layers/models.py",
        "code_block": "class TransformerLinen(nnx_wrappers.ToLinen):\n  \"\"\"Transformer model as a linen module.\"\"\"\n\n  def init(self, *args, model_mode: str = MODEL_MODE_TRAIN, **kwargs):\n    \"\"\"Initializes the model.\"\"\"\n    model_kwargs = self.kwargs.copy({\"model_mode\": model_mode})  # type: ignore[wrong-arg-types]\n    module = self.clone(kwargs=model_kwargs)\n    return nnx_wrappers.ToLinen.init(module, *args, **kwargs)\n\n  def apply(self, *args, model_mode: str = MODEL_MODE_TRAIN, **kwargs):\n    \"\"\"Applies the model.\"\"\"\n    model_kwargs = self.kwargs.copy({\"model_mode\": model_mode})  # type: ignore[wrong-arg-types]\n    module = self.clone(kwargs=model_kwargs)\n    return nnx_wrappers.ToLinen.apply(module, *args, **kwargs)",
        "analysis": {
            "module_type": "nnx_to_linen_transformer_wrapper",
            "purpose": "A wrapper class that allows an NNX-based Transformer model to be used as a Flax Linen module, specifically overriding the `init` and `apply` methods to handle the `model_mode` parameter.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Inherits from nnx_wrappers.ToLinen to bridge an NNX module with the Linen API.",
                "Overrides the `init` and `apply` methods to inject or update the `model_mode` parameter before calling the parent class's implementation."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx_wrappers.ToLinen",
                "Transformer"
            ],
            "parameters": {
                "model_mode": "A string indicating the operational mode of the model (e.g., 'train', 'prefill', 'autoregressive'). This parameter is dynamically handled by the `init` and `apply` methods."
            },
            "notes": [
                "This class is part of a compatibility layer to use NNX-defined models within a Flax Linen-based ecosystem.",
                "The use of `self.clone(kwargs=...)` creates a new instance of the module with updated configuration for each `init` or `apply` call, ensuring the correct `model_mode` is used."
            ],
            "methods": {
                "init": {
                    "purpose": "Initializes the wrapped NNX Transformer model, allowing the `model_mode` to be specified or overridden during initialization.",
                    "input": {
                        "shape": "Accepts variable positional and keyword arguments (`*args`, `**kwargs`) that are passed to the underlying model's initialization logic.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Copies the base keyword arguments (`self.kwargs`).",
                        "Updates the copied arguments with the provided `model_mode`.",
                        "Clones the module instance with the new arguments.",
                        "Calls the parent `nnx_wrappers.ToLinen.init` method with the cloned module."
                    ],
                    "output": {
                        "shape": "Returns the initialized variables (parameters and state) of the Linen module, typically in a dictionary-like structure.",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "nnx_wrappers.ToLinen.init"
                    ],
                    "notes": [
                        "The default `model_mode` is `MODEL_MODE_TRAIN`."
                    ]
                },
                "apply": {
                    "purpose": "Applies the wrapped NNX Transformer model for a forward pass, allowing the `model_mode` to be specified or overridden.",
                    "input": {
                        "shape": "Accepts variable positional and keyword arguments (`*args`, `**kwargs`) that are passed to the underlying model's forward pass.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Copies the base keyword arguments (`self.kwargs`).",
                        "Updates the copied arguments with the provided `model_mode`.",
                        "Clones the module instance with the new arguments.",
                        "Calls the parent `nnx_wrappers.ToLinen.apply` method with the cloned module."
                    ],
                    "output": {
                        "shape": "Returns the output of the underlying model's forward pass (e.g., logits).",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "nnx_wrappers.ToLinen.apply"
                    ],
                    "notes": [
                        "The default `model_mode` is `MODEL_MODE_TRAIN`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/models.py#Transformer",
        "file_path": "src/MaxText/layers/models.py",
        "code_block": "class Transformer(nnx.Module):\n  \"\"\"An autoregressive transformer model.\"\"\"\n\n  # Make new attributes required, so that all Transformer dependencies (train, decode,\n  # compile, etc) will error instead of silently use defaults.\n  # pylint: disable=attribute-defined-outside-init\n  def __init__(self, config: Config, mesh: Mesh, quant: Quant, *, model_mode: str = MODEL_MODE_TRAIN, rngs: nnx.Rngs):\n    \"\"\"Initialize shared_embedding & decoder layers.\"\"\"\n    self.config = config\n    self.mesh = mesh\n    self.quant = quant\n    self.model_mode = model_mode\n\n    cfg = self.config\n    mesh = self.mesh\n    self.token_embedder = Embed(\n        num_embeddings=cfg.vocab_size,\n        num_features=cfg.emb_dim,\n        dtype=cfg.dtype,\n        attend_dtype=jnp.float32 if cfg.logits_dot_in_fp32 else cfg.dtype,  # for logit training stability\n        embedding_init=nn.initializers.normal(stddev=1.0),\n        config=cfg,\n        rngs=rngs,\n    )\n    self.vision_encoder = VisionEncoder(config=cfg, mesh=mesh) if cfg.use_multimodal else None\n\n    decoder_linen = Decoder(config=cfg, mesh=mesh, quant=self.quant, model_mode=self.model_mode)\n    self.decoder = nnx_wrappers.ToNNX(decoder_linen, rngs=rngs)\n    self.hidden_states = None\n    if self.model_mode == MODEL_MODE_PREFILL:\n      seq_len = cfg.max_prefill_predict_length\n    elif self.model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      seq_len = 1\n    else:\n      seq_len = cfg.max_target_length\n\n    batch_size = cfg.micro_batch_size_to_train_on\n    dummy_decoder_input_tokens = jnp.ones((batch_size, seq_len), dtype=jnp.int32)\n    dummy_decoder_positions = jnp.ones((batch_size, seq_len), dtype=jnp.int32)\n\n    self.decoder.lazy_init(\n        shared_embedding=self.token_embedder,\n        decoder_input_tokens=dummy_decoder_input_tokens,\n        decoder_positions=dummy_decoder_positions,\n    )\n\n    # If MTP is enabled via config, set up the MTP block.\n    if self.config.mtp_num_layers > 0:\n      # Get the list of layer blueprints for the current model.\n      layer_types = self.decoder.get_decoder_layers()\n      # For MTP, we use the DecoderLayer blueprint to ensure architectural consistency.\n      # By convention, this is the last layer in the list.\n      mtp_layer = layer_types[-1]\n      mtp_block_linen = MultiTokenPredictionBlock(\n          config=self.config, mesh=self.mesh, name=\"mtp_block\", transformer_layer_module=mtp_layer, decoder=self.decoder\n      )\n      self.mtp_block = nnx_wrappers.ToNNX(mtp_block_linen, rngs=rngs)\n\n      self.mtp_block.lazy_init(\n          shared_embedding=self.token_embedder,\n          main_hidden_state=jnp.ones((1, 1, self.config.emb_dim), dtype=self.config.dtype),\n          input_ids=jnp.ones((1, 1), dtype=jnp.int32),\n          target_ids=jnp.ones((1, 1), dtype=jnp.int32),\n          target_mask=jnp.ones((1, 1), dtype=jnp.int32),\n          position_ids=jnp.ones((1, 1), dtype=jnp.int32),\n          decoder_segment_ids=jnp.ones((1, 1), dtype=jnp.int32),\n          deterministic=True,\n      )\n\n  def no_op(self, *args, **kwargs):\n    \"\"\"A no-op method to allow the model to be used in a lazy context.\"\"\"\n    return\n\n  def init_cache(self, cache_size: int, batch_size: int, dtype=jnp.float32):\n    \"\"\"Initializes the KV cache for the Transformer.\n\n    Args:\n      cache_size: The maximum size of the KV cache.\n      batch_size: The batch size for which the cache is initialized.\n      dtype: Data type for the cache. Defaults to `jnp.float32`.\n\n    Returns:\n      True if the cache is successfully initialized.\n    \"\"\"\n    return True\n\n  def __call__(\n      self,\n      decoder_input_tokens: jnp.ndarray,\n      decoder_positions: jnp.ndarray,\n      decoder_segment_ids=None,\n      cache=None,\n      encoder_images: jax.Array | None = None,\n      enable_dropout=True,\n      previous_chunk=None,\n      true_length: int | None = None,\n      slot: int | None = None,\n      page_state: page_manager.PageState | None = None,\n      decoder_target_tokens: jax.Array | None = None,\n      decoder_target_mask: jax.Array | None = None,\n  ):\n    \"\"\"Applies the Zero-1 FSDP wrapped Transformer model.\n\n    This method handles the all-gather operation for model weights before\n    applying the underlying Transformer model, and then releases them.\n\n    Args:\n      decoder_input_tokens: Input tokens for the decoder.\n      decoder_positions: Positional encodings for the decoder inputs.\n      decoder_segment_ids: Segment IDs for the decoder inputs (optional).\n      encoder_images: Encoder images for multimodal models (optional).\n      enable_dropout: Whether to enable dropout. Defaults to True.\n      previous_chunk: Previous chunk for incremental decoding (optional).\n      true_length: True length of the prompt before padding (optional).\n      slot: An integer representing the decode batch index selected for this request (optional).\n      page_state: Page state for paged attention (optional).\n      partition_spec: Partition specification for FSDP all-gather.\n      decoder_target_tokens: Target tokens for the decoder (optional, used in MTP).\n      decoder_target_mask: Target mask for the decoder (optional, used in MTP).\n      nnx_method: Method to call on the NNX module (optional).\n\n    Returns:\n      Logits from the Transformer model.\n    \"\"\"\n    if decoder_segment_ids is not None and self.model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      raise ValueError(\n          f\"During autoregressive decoding we assume the tokens are in the active sequence\"\n          f\" which is always {DECODING_ACTIVE_SEQUENCE_INDICATOR}.\"\n      )\n\n    bidirectional_mask = None\n    image_embeddings = None\n    if self.config.use_multimodal and encoder_images is not None:\n      image_embeddings = self.vision_encoder(input_images=encoder_images, deterministic=not enable_dropout)\n\n      if self.config.decoder_block == DecoderBlockType.GEMMA3:\n        bidirectional_mask = decoder_input_tokens == multimodal_utils.GEMMA_TOKEN_PLACEHOLDER\n      elif self.config.decoder_block == DecoderBlockType.LLAMA4:\n        bidirectional_mask = decoder_input_tokens == multimodal_utils.LLAMA4_PATCH_TOKEN\n\n    logits, hidden_state = self.decoder(\n        shared_embedding=self.token_embedder,\n        decoder_input_tokens=decoder_input_tokens,\n        decoder_positions=decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=not enable_dropout,\n        previous_chunk=previous_chunk,\n        slot=slot,\n        page_state=page_state,\n        bidirectional_mask=bidirectional_mask,\n        image_embeddings=image_embeddings,\n    )\n\n    # Materialize hidden state when vocab tiling is enabled\n    if self.config.num_vocab_tiling > 1:\n      self.hidden_states = hidden_state\n\n    # If we are initializing the model AND MTP is enabled, we must create\n    # dummy target tensors. This allows Flax to trace the MTPBlock and create\n    # all its necessary parameters, without requiring the main training pipeline\n    # to be aware of this initialization detail.\n    # if self.is_initializing() and self.config.mtp_num_layers > 0:\n    #   if decoder_target_tokens is None:\n    #     dummy_shape = decoder_input_tokens.shape\n    #     decoder_target_tokens = jnp.ones(dummy_shape, dtype=jnp.int32)\n    #     decoder_target_mask = jnp.ones(dummy_shape, dtype=jnp.int32)\n    #     decoder_segment_ids = jnp.ones(dummy_shape, dtype=jnp.int32)\n\n    # The Multi-Token Prediction (MTP) block functions as a \"side-car\" to the main\n    # model, active only during training. It computes an auxiliary loss based on\n    # predicting multiple future tokens, as described in the DeepSeek-V3 paper.\n    # To ensure architectural consistency, it uses two key components from the parent Transformer:\n    #   1. The same `DecoderLayer` blueprint for its internal transformer blocks.\n    #   2. The `shared_embedding` for both embedding future tokens and for its final\n    #      logit projection.\n    # Its only effect is to \"sow\" these losses; it does not alter the primary logits output.\n    if self.config.mtp_num_layers > 0:\n      self.mtp_block(\n          shared_embedding=self.token_embedder,\n          main_hidden_state=hidden_state,\n          input_ids=decoder_input_tokens,\n          target_ids=decoder_target_tokens,\n          target_mask=decoder_target_mask,\n          position_ids=decoder_positions,\n          decoder_segment_ids=decoder_segment_ids,\n          deterministic=not enable_dropout,\n      )\n\n    return logits",
        "analysis": {
            "module_type": "autoregressive_transformer",
            "purpose": "An autoregressive transformer model implemented using Flax NNX, capable of handling text, optional image inputs (multimodal), and multi-token prediction (MTP) for auxiliary loss.",
            "input": {
                "shape": "See the `__call__` method.",
                "dtype": "See the `__call__` method."
            },
            "processing_steps": [
                "Initializes `token_embedder` using the `Embed` class.",
                "Initializes `vision_encoder` using `VisionEncoder` if `config.use_multimodal` is true.",
                "Wraps a `Decoder` linen module into an NNX module `self.decoder`.",
                "Performs lazy initialization of `self.decoder` with dummy input tensors.",
                "Initializes and lazy-initializes `mtp_block` using `MultiTokenPredictionBlock` if `config.mtp_num_layers > 0`."
            ],
            "output": {
                "shape": "See the `__call__` method."
            },
            "dependencies": [
                "flax.nnx",
                "MaxText.layers.embeddings.Embed",
                "MaxText.layers.encoders.VisionEncoder",
                "MaxText.layers.decoders.Decoder",
                "MaxText.layers.multi_token_prediction.MultiTokenPredictionBlock",
                "MaxText.layers.nnx_wrappers.ToNNX"
            ],
            "parameters": {
                "config": "A Config object containing model hyperparameters like vocab_size, emb_dim, dtype, use_multimodal, mtp_num_layers, etc.",
                "mesh": "The JAX sharding mesh for device partitioning.",
                "quant": "The quantization configuration object.",
                "model_mode": "The operational mode of the model (e.g., 'train', 'prefill', 'autoregressive'), which affects initialization and behavior.",
                "rngs": "An `nnx.Rngs` object for handling random number generation."
            },
            "notes": [
                "This is an NNX-based implementation of the Transformer model.",
                "It uses lazy initialization for the decoder and the optional MTP block, meaning their parameters are created upon the first call with dummy data during `__init__`.",
                "The model's behavior, particularly the shape of dummy tensors for initialization, changes based on the `model_mode`."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the transformer's components: token embedder, vision encoder (optional), decoder, and multi-token prediction block (optional).",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initialize `self.token_embedder`.",
                        "Initialize `self.vision_encoder` if multimodal is enabled.",
                        "Initialize `self.decoder` by wrapping a Linen `Decoder` module.",
                        "Create dummy input tensors based on `model_mode`.",
                        "Call `self.decoder.lazy_init` to build its parameters.",
                        "Initialize and lazy-initialize `self.mtp_block` if MTP is enabled."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Embed",
                        "VisionEncoder",
                        "Decoder",
                        "nnx_wrappers.ToNNX",
                        "MultiTokenPredictionBlock"
                    ],
                    "notes": [
                        "This method sets up all necessary sub-modules for the transformer.",
                        "It uses a lazy initialization pattern, creating parameters for sub-modules by calling them with dummy data."
                    ]
                },
                "no_op": {
                    "purpose": "A no-operation method to allow the model to be used in a lazy context without performing any action.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Returns immediately."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": [
                        "This method is likely a placeholder or utility for compatibility with certain APIs that might require method calls during setup."
                    ]
                },
                "init_cache": {
                    "purpose": "A placeholder method for initializing the Key-Value cache for autoregressive decoding.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Returns True."
                    ],
                    "output": {
                        "shape": "A boolean scalar."
                    },
                    "dependencies": [],
                    "notes": [
                        "The current implementation is a stub and does not actually initialize a cache; it simply returns True."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the transformer model to generate output logits from input tokens.",
                    "input": {
                        "shape": "{'decoder_input_tokens': [batch_size, sequence_length], 'decoder_positions': [batch_size, sequence_length], 'encoder_images': [batch_size, height, width, channels] (optional)}",
                        "dtype": "{'decoder_input_tokens': int32, 'decoder_positions': int32, 'encoder_images': float (e.g., float32 or bfloat16)}"
                    },
                    "processing_steps": [
                        "Optionally, process `encoder_images` with `self.vision_encoder` to get `image_embeddings`.",
                        "Optionally, create a `bidirectional_mask` for multimodal inputs.",
                        "Call `self.decoder` with input tokens, positions, and optional multimodal data to get logits and hidden states.",
                        "If vocabulary tiling is enabled, store the `hidden_state`.",
                        "If MTP is enabled, call `self.mtp_block` with the hidden state and other inputs to compute an auxiliary loss (this does not affect the returned logits).",
                        "Return the final logits."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, vocab_size]"
                    },
                    "dependencies": [
                        "self.vision_encoder",
                        "self.decoder",
                        "self.mtp_block",
                        "multimodal_utils"
                    ],
                    "notes": [
                        "This method orchestrates the main data flow through the model.",
                        "It supports multimodal inputs by conditioning the decoder on image embeddings.",
                        "The optional MTP block is called for its side effect of computing an auxiliary loss during training and does not modify the primary output."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/models.py#ZeroOneTransformer",
        "file_path": "src/MaxText/layers/models.py",
        "code_block": "class ZeroOneTransformer(nn.Module):\n  \"\"\"\n  A wrapper for the base Transformer model designed to implement the Zero-1\n  FSDP optimization.\n\n  The goal of this optimization is to reduce communication overhead. In the standard\n  FSDP implementation, an all-gather operation on the model weights is performed twice\n  for each gradient accumulation microbatch (once for the forward pass, once for the backward pass).\n  This class changes that behavior. When enabled, it performs the all-gather operation\n  only *once* per full gradient accumulation step. It gathers the full weights into\n  memory, runs all the microbatch forward and backward passes, and then releases the\n  full weights. This trades higher peak memory usage for significantly reduced\n  network communication, which can improve training speed if sufficient memory is\n  available.\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n  quant: Quant\n  # Possible model_mode values can be found in MaxText.common_types.\n  # We generally use MaxText.common_types.MODEL_MODE_TRAIN or\n  # MaxText.common_types.MODEL_MODE_PREFILL for initializations here.\n  # TODO: Make model_mode required after confirming no users are affected.\n  model_mode: str = MODEL_MODE_TRAIN  # May be different than the model_mode passed to __call__\n\n  def setup(self):\n    \"\"\"Sets up the underlying Transformer model.\n\n    This method initializes the `self.model` attribute by calling the\n    `transformer_as_linen` factory function.\n    \"\"\"\n    self.model = transformer_as_linen(self.config, self.mesh, self.quant, self.model_mode)\n\n  def __call__(\n      self,\n      decoder_input_tokens: jnp.ndarray,\n      decoder_positions: jnp.ndarray,\n      decoder_segment_ids=None,\n      encoder_images: None | jnp.ndarray = None,\n      enable_dropout=True,\n      previous_chunk=None,\n      true_length: None | int = None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      partition_spec=None,\n      decoder_target_tokens: None | jnp.ndarray = None,\n      decoder_target_mask: None | jnp.ndarray = None,\n      nnx_method: str | None = None,\n  ):\n    \"\"\"Applies the Zero-1 FSDP wrapped Transformer model.\n\n    This method handles the all-gather operation for model weights before\n    applying the underlying Transformer model, and then releases them.\n\n    Args:\n      decoder_input_tokens: Input tokens for the decoder.\n      decoder_positions: Positional encodings for the decoder inputs.\n      decoder_segment_ids: Segment IDs for the decoder inputs (optional).\n      encoder_images: Encoder images for multimodal models (optional).\n      enable_dropout: Whether to enable dropout. Defaults to True.\n      previous_chunk: Previous chunk for incremental decoding (optional).\n      true_length: True length of the prompt before padding (optional).\n      slot: An integer representing the decode batch index selected for this\n        request (optional).\n      page_state: Page state for paged attention (optional).\n      partition_spec: Partition specification for FSDP all-gather.\n      decoder_target_tokens: Target tokens for the decoder (optional, used in\n        MTP).\n      decoder_target_mask: Target mask for the decoder (optional, used in MTP).\n      nnx_method: Method to call on the NNX module (optional).\n\n    Returns:\n      Logits from the Transformer model.\n    \"\"\"\n    if self.is_initializing():\n      return self.model(\n          decoder_input_tokens=decoder_input_tokens,\n          decoder_positions=decoder_positions,\n          decoder_segment_ids=decoder_segment_ids,\n          encoder_images=encoder_images,\n          enable_dropout=enable_dropout,\n          previous_chunk=previous_chunk,\n          true_length=true_length,\n          slot=slot,\n          page_state=page_state,\n      )\n    all_model_weights = all_gather_over_fsdp(\n        self.model.variables, partition_spec, mesh=self.mesh, logical_axis_rules=self.config.logical_axis_rules\n    )\n\n    return self.model.apply(\n        all_model_weights,\n        decoder_input_tokens=decoder_input_tokens,\n        decoder_positions=decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        encoder_images=encoder_images,\n        enable_dropout=enable_dropout,\n        model_mode=self.model_mode,\n        previous_chunk=previous_chunk,\n        true_length=true_length,\n        slot=slot,\n        page_state=page_state,\n        mutable=False,\n        decoder_target_tokens=decoder_target_tokens,\n        decoder_target_mask=decoder_target_mask,\n        nnx_method=nnx_method,\n    )",
        "analysis": {
            "module_type": "zero_one_transformer",
            "purpose": "A wrapper for a base Transformer model that implements the Zero-1 FSDP optimization to reduce communication overhead by performing a single weight all-gather per gradient accumulation step.",
            "input": {
                "shape": "decoder_input_tokens: [batch_size, sequence_length]",
                "dtype": "jnp.ndarray"
            },
            "processing_steps": [
                "Initializes the underlying Transformer model via `transformer_as_linen` in the `setup` method.",
                "During the forward pass (`__call__`), it checks if the model is being initialized. If so, it calls the underlying model directly.",
                "If not initializing, it gathers the full, unsharded model weights from all devices using `all_gather_over_fsdp`.",
                "Applies the underlying model using `self.model.apply` with the gathered weights and the provided inputs to produce logits."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, vocab_size]"
            },
            "dependencies": [
                "flax.linen.Module",
                "transformer_as_linen",
                "all_gather_over_fsdp",
                "Config",
                "Mesh",
                "Quant"
            ],
            "parameters": {
                "config": "The main configuration object specifying model hyperparameters.",
                "mesh": "The JAX sharding mesh for device partitioning.",
                "quant": "The quantization configuration/module to use.",
                "model_mode": "The operational mode for the model (e.g., train, prefill, autoregressive)."
            },
            "notes": [
                "This class trades higher peak memory usage for reduced network communication. It holds the full model weights in memory for the duration of all microbatch forward/backward passes within a single gradient accumulation step.",
                "The core optimization logic is conditional and does not run during model initialization."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the underlying Transformer model using a factory function.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `transformer_as_linen` with the class's config, mesh, quant, and model_mode attributes.",
                        "Assigns the returned model to `self.model`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "transformer_as_linen"
                    ],
                    "notes": [
                        "This method is part of the standard Flax/Linen module lifecycle and is called automatically upon initialization."
                    ]
                },
                "__call__": {
                    "purpose": "Performs a forward pass, applying the Zero-1 FSDP optimization by first gathering all model weights.",
                    "input": {
                        "shape": "decoder_input_tokens: [batch_size, sequence_length]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Checks if the model is being initialized using `self.is_initializing()` and performs a standard forward pass if true.",
                        "Calls `all_gather_over_fsdp` to collect the full model weights from all devices.",
                        "Calls `self.model.apply` with the fully gathered weights and input data to compute the logits."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, vocab_size]"
                    },
                    "dependencies": [
                        "all_gather_over_fsdp"
                    ],
                    "notes": [
                        "This method differentiates between initialization and regular execution to apply the weight gathering logic only when needed.",
                        "It accepts numerous optional arguments for different decoding and training scenarios (e.g., `page_state`, `decoder_target_tokens`)."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#_sort_activations",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def _sort_activations(\n    inputs: jax.Array,\n    sort_indices: jax.Array,\n    use_custom_vjp: bool,\n) -> jax.Array:\n  \"\"\"Sort activations by `sort_indices`.\n\n  If `use_custom_vjp=True`, then we use a custom backward pass that\n  reverses the sort order. Specifically, this unsort operation is simply a sort\n  with `jnp.argsort(sort_indices)` as the sort indices. This is only needed in\n  the case where the compiler generates a less efficient backward pass op.\n\n  Note that `use_custom_vjp=True` assumes that `sort_indices` is a permutation\n  of `jnp.arange(inputs.shape[0])`.\n\n  Args:\n    inputs: `(tokens, ...)`-shaped array of input activations to sort.\n    sort_indices: `(tokens,)`-shaped array containing the sort order.\n    use_custom_vjp: Whether to use the explicit backward pass.\n\n  Returns:\n    `(tokens, ...)`-shaped array of input activations sorted by `sort_indices`.\n  \"\"\"\n  assert inputs.shape[0] == sort_indices.shape[0]\n\n  with jax.named_scope(\"sort_activations\"):\n    if use_custom_vjp:\n      return _sort_activations_custom(inputs, sort_indices)\n    return inputs[sort_indices, ...]",
        "analysis": {
            "functionality": "This function sorts a JAX array `inputs` along its first dimension using the provided `sort_indices`. It can optionally use a custom VJP (Vector-Jacobian Product) for the backward pass to improve performance.",
            "usage": "To use this function, provide an input JAX array `inputs` of shape `(tokens, ...)`, a 1D JAX array `sort_indices` of shape `(tokens,)` specifying the new order, and a boolean `use_custom_vjp`. The function returns a new JAX array of the same shape as `inputs`, sorted according to `sort_indices`."
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#_sort_activations_custom",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def _sort_activations_custom(inputs: jax.Array, sort_indices: jax.Array) -> jax.Array:\n  \"\"\"Sort functions with custom vjp.\"\"\"\n  return inputs[sort_indices, ...]",
        "analysis": {
            "functionality": "Sorts an input tensor along its first axis according to provided indices, utilizing a custom vector-Jacobian product (VJP) for the backward pass in automatic differentiation.",
            "usage": "This function is used to reorder a tensor. Call it with an input tensor `inputs` of shape `[N, ...]` and an integer tensor `sort_indices` of shape `[N,]`. It returns a new tensor of shape `[N, ...]` where the elements along the first axis are permuted according to `sort_indices`. The custom VJP ensures an efficient gradient calculation during backpropagation by performing an inverse sort operation."
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#_sort_activations_custom_fwd",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def _sort_activations_custom_fwd(inputs: jax.Array, sort_indices: jax.Array) -> tuple[jax.Array, jax.Array]:\n  \"\"\"Forward pass of the custom vjp for `_sort_activations()`.\"\"\"\n  return _sort_activations_custom(inputs, sort_indices), sort_indices",
        "analysis": {
            "functionality": "This function serves as the forward pass for a custom JAX Vector-Jacobian Product (VJP). It sorts an input tensor based on provided indices and passes these indices along as residuals for the corresponding backward pass.",
            "usage": "This function is not intended for direct use. It is registered with JAX's custom VJP mechanism via `_sort_activations_custom.defvjp`. It takes an input tensor `inputs` and a `sort_indices` tensor, and returns a tuple containing the sorted `inputs` and the original `sort_indices`."
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#_sort_activations_custom_bwd",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def _sort_activations_custom_bwd(residuals: jax.Array, grads: jax.Array) -> tuple[jax.Array, None]:\n  \"\"\"Backward pass of the custom vjp for `_sort_activations()`.\"\"\"\n  sort_indices = residuals\n  return _sort_activations_custom(grads, jnp.argsort(sort_indices)), None",
        "analysis": {
            "module_type": "custom_vjp_backward_pass",
            "purpose": "Implements the custom backward pass for the `_sort_activations_custom` function, which unsorts the incoming gradients to their original positions.",
            "input": {
                "shape": "A tuple of (residuals: [tokens], grads: [tokens, ...])",
                "dtype": "jax.Array"
            },
            "processing_steps": [
                "Assign the `residuals` (which are the `sort_indices` from the forward pass) to a local variable `sort_indices`.",
                "Compute the 'unsort' indices by applying `jnp.argsort` to `sort_indices`.",
                "Unsort the incoming `grads` array by calling `_sort_activations_custom` with the `grads` and the computed 'unsort' indices.",
                "Return a tuple containing the unsorted gradients and None."
            ],
            "output": {
                "shape": "A tuple of (unsorted_grads: [tokens, ...], None)"
            },
            "dependencies": [
                "_sort_activations_custom",
                "jax.numpy.argsort"
            ],
            "parameters": {},
            "notes": [
                "This function is registered as the backward pass for `_sort_activations_custom` using `defvjp`.",
                "The `residuals` input is the `sort_indices` array saved during the corresponding forward pass.",
                "The gradient with respect to the `sort_indices` input of the original function is considered to be None."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#random_routing",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def random_routing(rng_key, gate_logits, num_experts_per_tok):\n  \"\"\"Performs random routing of tokens to experts.\n\n  Args:\n    rng_key: A JAX PRNGKey for randomness.\n    gate_logits: A JAX array of shape (batch_size, sequence_length, num_experts)\n      representing the logits for each expert.\n    num_experts_per_tok: The number of experts to select for each token.\n\n  Returns:\n    A tuple containing:\n      - top_k_indices: JAX array of shape (batch_size, sequence_length,\n      num_experts_per_tok)\n                       representing the indices of the selected experts for each\n                       token.\n      - top_k_weights: JAX array of shape (batch_size, sequence_length,\n      num_experts_per_tok)\n                       representing the weights for the selected experts.\n  \"\"\"\n  bs, seq_len, num_experts = gate_logits.shape\n  indices = jnp.arange(num_experts).repeat(bs * seq_len)\n  selected_num = bs * seq_len * num_experts_per_tok\n  top_k_indices = jax.random.choice(rng_key, indices, shape=(selected_num,)).reshape(bs, seq_len, num_experts_per_tok)\n  top_k_weights = jnp.take_along_axis(gate_logits, top_k_indices, axis=-1)\n  return top_k_weights, top_k_indices",
        "analysis": {
            "module_type": "random_expert_routing",
            "purpose": "Performs random routing of tokens to experts by randomly selecting a specified number of experts for each token, ignoring the gate logit values for selection.",
            "input": {
                "shape": "[batch_size, sequence_length, num_experts]",
                "dtype": "jax.Array (float)"
            },
            "processing_steps": [
                "Extract shape dimensions (batch_size, sequence_length, num_experts) from the input `gate_logits`.",
                "Create a flat array of all possible expert indices for all tokens.",
                "Use `jax.random.choice` to randomly select `num_experts_per_tok` for each token from the flat array of indices.",
                "Reshape the selected indices to `[batch_size, sequence_length, num_experts_per_tok]` to get `top_k_indices`.",
                "Use `jnp.take_along_axis` to gather the gate logits corresponding to the randomly selected `top_k_indices`, resulting in `top_k_weights`.",
                "Return the gathered weights and indices."
            ],
            "output": {
                "shape": "A tuple of two tensors, both with shape [batch_size, sequence_length, num_experts_per_tok]."
            },
            "dependencies": [
                "jax",
                "jax.numpy"
            ],
            "parameters": {
                "num_experts_per_tok": "The number of experts to randomly select for each token."
            },
            "notes": [
                "This routing method does not use the magnitude of the `gate_logits` to determine which experts to route to; the selection is purely random.",
                "The returned `top_k_weights` are the original logit values for the randomly chosen experts, not normalized probabilities."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#GateLogit",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "class GateLogit(nnx.Module):\n  \"\"\"A layer used to compute gate logits, allowing to return the pre bias values for DeepSeek routing.\"\"\"\n\n  def __init__(\n      self,\n      in_features_shape: Union[Iterable[int], int],\n      out_features_shape: Union[Iterable[int], int],\n      model_name: str,\n      rngs: nnx.Rngs,\n      axis: Union[Iterable[int], int] = -1,\n      weight_dtype: ctypes.DType = jnp.float32,\n      dtype: ctypes.DType = jnp.float32,\n      kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n      kernel_axes: Tuple[Optional[str], ...] = (),\n      use_bias: bool = False,\n      score_func: str = \"\",\n      quant: Optional[quantizations.AqtQuantization] = None,\n      matmul_precision: str = \"default\",\n  ):\n    \"\"\"Initializes the GateLogit module.\n\n    Attributes:\n      in_features_shape: The shape of the input features.\n      out_features_shape: The shape of the output features, typically the number of experts.\n      model_name: The name of the model.\n      rngs: An `nnx.Rngs` object used for initializing parameters.\n      axis: The axis or axes over transformation is applied.\n      weight_dtype: The data type of the kernel weights.\n      dtype: The data type for the computation.\n      kernel_init: The initializer function for the kernel weight matrix.\n      kernel_axes: A tuple of logical axis names for partitioning the kernel.\n      use_bias: Whether to add learnable bias in gate logit scores. When enabled,\n        this bias aids expert load balancing (like in DeepSeek V3), and is not\n        part of the loss calculation.\n      score_func: Scoring function for output normalization before applying bias.\n      quant: The quantization configuration. If None, no quantization is applied.\n      matmul_precision: The precision level for the matrix multiplication.\n    \"\"\"\n    self.in_features_shape = linears.canonicalize_tuple(in_features_shape)\n    self.out_features_shape = linears.canonicalize_tuple(out_features_shape)\n    self.model_name = model_name\n    self.axis = linears.canonicalize_tuple(axis)\n    self.weight_dtype = weight_dtype\n    self.dtype = dtype\n    self.kernel_init = kernel_init\n    self.kernel_axes = kernel_axes\n    self.use_bias = use_bias\n    self.score_func = score_func\n    self.quant = quant\n    self.matmul_precision = matmul_precision\n\n    # Parameter initialization\n    kernel_shape = self.in_features_shape + self.out_features_shape\n    kernel_in_axis = np.arange(len(self.axis))\n    kernel_out_axis = np.arange(len(self.axis), len(self.axis) + len(self.out_features_shape))\n\n    if not quantizations.in_serve_mode(self.quant):\n      self.kernel = nnx.Param(\n          self.kernel_init(\n              rngs.params(),\n              kernel_shape,\n              self.weight_dtype,\n              kernel_in_axis,\n              kernel_out_axis,\n          ),\n          sharding=self.kernel_axes,\n      )\n\n    if self.use_bias:\n      bias_axes = self.kernel_axes[-len(self.out_features_shape) :]\n      bias_shape = kernel_shape[-len(self.out_features_shape) :]\n      self.bias = nnx.Param(\n          default_bias_init(rngs.params(), bias_shape, self.weight_dtype),\n          sharding=bias_axes,\n      )\n    else:\n      self.bias = None\n\n    if quant:\n      dot_general_cls = quant.dot_general_cls(mesh_axes=kernel_axes)\n      dot_general_linen = dot_general_cls()\n      quant_dot_general = nnx_wrappers.ToNNX(dot_general_linen, rngs=rngs)\n      self._quant_dot_general_name = f\"{type(dot_general_linen).__name__}_0\"\n      setattr(self, self._quant_dot_general_name, quant_dot_general)\n      dummy_inputs = jnp.zeros((1, *self.in_features_shape), dtype=self.dtype)\n      self(dummy_inputs, _initializing=True)\n    else:\n      self._quant_dot_general_name = None\n\n  @property\n  def quant_dot_general(self) -> nnx_wrappers.ToNNX | None:\n    if self._quant_dot_general_name is None:\n      return None\n    return getattr(self, self._quant_dot_general_name)\n\n  def __call__(self, inputs: jax.Array, _initializing: bool = False) -> Tuple[jax.Array, Optional[jax.Array]]:\n\n    inputs = jnp.asarray(inputs, self.dtype)\n    norm_axis = linears.normalize_axes(self.axis, inputs.ndim)\n\n    if quantizations.in_serve_mode(self.quant):\n      kernel_shape = self.in_features_shape + self.out_features_shape\n      kernel = jnp.zeros(kernel_shape, dtype=self.dtype)\n    else:\n      kernel = self.kernel[...]\n    kernel = jnp.asarray(kernel, self.dtype)\n\n    contract_ind = tuple(range(0, len(norm_axis)))\n    output = linears._compute_dot_general_nnx(\n        inputs,\n        kernel,\n        norm_axis,\n        contract_ind,\n        self.matmul_precision,\n        self.quant_dot_general,\n        _initializing,\n    )\n    pre_bias_logits = None\n\n    if self.score_func:\n      output = linears._convert_to_activation_function(self.score_func)(output)\n      if self.model_name.startswith(\"deepseek3\"):\n        pre_bias_logits = output\n\n    if self.use_bias:\n      bias = jnp.asarray(self.bias[...], self.dtype)\n      output += bias\n    return output, pre_bias_logits",
        "analysis": {
            "module_type": "gate_logit",
            "purpose": "Computes gate logits for routing tokens to experts in a Mixture-of-Experts (MoE) model, with an option to return pre-bias logits for specific routing strategies like DeepSeek's.",
            "input": {
                "shape": "N/A (class constructor)",
                "dtype": "N/A (class constructor)"
            },
            "processing_steps": [
                "Initializes kernel and optional bias parameters.",
                "Sets up quantization-aware dot-product operation if quantization is configured."
            ],
            "output": {
                "shape": "N/A (class instance)"
            },
            "dependencies": [
                "flax.nnx",
                "jax",
                "jax.numpy",
                "numpy",
                "MaxText.layers.linears",
                "MaxText.layers.quantizations",
                "MaxText.layers.nnx_wrappers",
                "MaxText.layers.initializers"
            ],
            "parameters": {
                "in_features_shape": "The shape of the input features.",
                "out_features_shape": "The shape of the output features, typically the number of experts.",
                "model_name": "The name of the model, used for special routing logic (e.g., 'deepseek3').",
                "use_bias": "Whether to add a learnable bias to the gate logit scores.",
                "score_func": "Scoring function (e.g., 'sigmoid') for output normalization before applying bias.",
                "quant": "The quantization configuration. If None, no quantization is applied."
            },
            "notes": [
                "This is an nnx.Module.",
                "The module has special logic to return pre-bias logits if the model_name starts with 'deepseek3', which is used in its specific routing mechanism."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the GateLogit layer, setting up parameters like kernel and optional bias, and configuring quantization if specified.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Canonicalize input/output feature shapes and axis.",
                        "Store configuration parameters.",
                        "Initialize the kernel parameter `self.kernel` using `kernel_init` if not in quantization serve mode.",
                        "Initialize the bias parameter `self.bias` if `use_bias` is True.",
                        "If quantization is enabled, set up the quantized dot-general operator and perform a dummy forward pass for initialization."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.Param",
                        "nnx.Rngs",
                        "linears.canonicalize_tuple",
                        "initializers.nd_dense_init",
                        "initializers.default_bias_init",
                        "quantizations.AqtQuantization",
                        "nnx_wrappers.ToNNX"
                    ],
                    "notes": [
                        "The kernel parameter is not created if `quantizations.in_serve_mode(self.quant)` is true, as weights are expected to be handled by the quantization mechanism."
                    ]
                },
                "quant_dot_general": {
                    "purpose": "Provides access to the quantized dot-general operator if quantization is configured, otherwise returns None.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if `_quant_dot_general_name` is set.",
                        "If set, return the attribute with that name using `getattr`.",
                        "Otherwise, return None."
                    ],
                    "output": {
                        "shape": "An `nnx_wrappers.ToNNX` instance or `None`."
                    },
                    "dependencies": [
                        "nnx_wrappers.ToNNX"
                    ],
                    "notes": [
                        "This is a read-only property."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass to compute the gate logits.",
                    "input": {
                        "shape": "[..., in_features_shape]",
                        "dtype": "Matches `self.dtype` specified during initialization."
                    },
                    "processing_steps": [
                        "Cast input tensor to the specified `dtype`.",
                        "Retrieve the kernel, either from `self.kernel` or a zero tensor if in quantization serving mode.",
                        "Compute the dot product between inputs and the kernel using `linears._compute_dot_general_nnx`.",
                        "If `score_func` is specified, apply the corresponding activation function.",
                        "If the model is a 'deepseek3' variant, store the result before adding bias as `pre_bias_logits`.",
                        "If `use_bias` is True, add the bias to the output.",
                        "Return the final output logits and the optional `pre_bias_logits`."
                    ],
                    "output": {
                        "shape": "A tuple of `(jax.Array, Optional[jax.Array])`. The first array has shape `[..., out_features_shape]`. The second, if not None, has the same shape."
                    },
                    "dependencies": [
                        "jnp.asarray",
                        "linears.normalize_axes",
                        "quantizations.in_serve_mode",
                        "linears._compute_dot_general_nnx",
                        "linears._convert_to_activation_function"
                    ],
                    "notes": [
                        "The method can return pre-bias logits, which is a specific requirement for models like DeepSeek v3."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#RoutedMoE",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "class RoutedMoE(nnx.Module):\n  \"\"\"Implements a routed MoE block.\"\"\"\n\n  def __init__(\n      self,\n      config: ctypes.Config,\n      num_experts: int,\n      num_experts_per_tok: int,\n      mesh: jax.sharding.Mesh,\n      kernel_init: attentions.NdInitializer,\n      kernel_axes: Tuple[Optional[str], ...],\n      rngs: nnx.Rngs,\n      intermediate_dim: int = 2048,\n      weight_dtype: ctypes.DType = jnp.float32,\n      dtype: ctypes.DType = jnp.float32,\n      quant: Optional[quantizations.AqtQuantization] = None,\n  ):\n    \"\"\"Initializes the RoutedMoE module.\n\n    Attributes:\n      config: The main config setting.\n      num_experts: Number of experts.\n      num_experts_per_tok: Number of experts for each token.\n      mesh: Mesh, device mesh.\n      kernel_init: The initializer function for the kernel weight matrix.\n      kernel_axes: A tuple of logical axis names for partitioning the kernel.\n      rngs: An `nnx.Rngs` object used for initializing parameters.\n      intermediate_dim: Intermediate dimension of MoE.\n      weight_dtype: The data type of the kernel weights.\n      dtype: The data type for the computation.\n      quant: The quantization configuration. If None, no quantization is applied.\n    \"\"\"\n    self.config = config\n    self.num_experts = num_experts\n    self.num_experts_per_tok = num_experts_per_tok\n    self.mesh = mesh\n    self.kernel_init = kernel_init\n    self.kernel_axes = kernel_axes\n    self.intermediate_dim = intermediate_dim\n    self.weight_dtype = weight_dtype\n    self.dtype = dtype\n    self.quant = quant\n    self.rngs = rngs\n\n    if self.config.fsdp_shard_on_exp:\n      # special sharding for dsv3\n      self.wi_kernel_axes = (\"embed_no_exp\", None, \"mlp\")\n      self.wo_kernel_axes = (\"embed_no_exp\", \"mlp\", None)\n    else:\n      self.wi_kernel_axes = (\"exp\", \"embed_no_exp\", \"mlp\")\n      self.wo_kernel_axes = (\"exp\", \"mlp\", \"embed_no_exp\")\n\n    self.gate = GateLogit(\n        in_features_shape=self.config.emb_dim,\n        out_features_shape=self.num_experts,\n        model_name=self.config.model_name,\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        kernel_init=self.kernel_init,\n        kernel_axes=self.kernel_axes,\n        use_bias=self.config.routed_bias,\n        score_func=self.config.routed_score_func,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n\n    # pylint: disable=protected-access\n    self.activation_fn = linears._convert_to_activation_function(self.config.mlp_activations[0])\n\n    kernel_in_axis = np.arange(1)\n    kernel_out_axis = np.arange(1, 2)\n\n    if quantizations.in_serve_mode(self.quant):\n      # During aqt convert state we delete kernel weight from params to save\n      # memory. Instead they are retrieved from the tensors stored in the 'aqt'\n      # collection.\n      self.wi_0 = jnp.zeros((num_experts, self.config.emb_dim, intermediate_dim))\n      self.wi_1 = jnp.zeros((num_experts, self.config.emb_dim, intermediate_dim))\n      self.wo = jnp.zeros((num_experts, intermediate_dim, self.config.emb_dim))\n    else:\n      self.wi_0 = nnx.Param(\n          self.kernel_init(\n              self.rngs.params(),\n              (num_experts, self.config.emb_dim, intermediate_dim),\n              weight_dtype,\n              kernel_in_axis,\n              kernel_out_axis,\n          ),\n          sharding=self.wi_kernel_axes,\n      )\n      self.wi_1 = nnx.Param(\n          self.kernel_init(\n              self.rngs.params(),\n              (num_experts, self.config.emb_dim, intermediate_dim),\n              weight_dtype,\n              kernel_in_axis,\n              kernel_out_axis,\n          ),\n          sharding=self.wi_kernel_axes,\n      )\n      self.wo = nnx.Param(\n          self.kernel_init(\n              self.rngs.params(),\n              (self.num_experts, self.intermediate_dim, self.config.emb_dim),\n              self.weight_dtype,\n              kernel_in_axis,\n              kernel_out_axis,\n          ),\n          sharding=self.wo_kernel_axes,\n      )\n\n    if self.config.mlp_bias:\n      wi_bias_axes = (\"exp\", \"activation_mlp\")\n      wo_bias_axes = (\"exp\", \"activation_embed\")\n      wi_bias_shape = (self.num_experts, self.intermediate_dim)\n      wo_bias_shape = (self.num_experts, self.config.emb_dim)\n      self.wi_0_bias = nnx.Param(\n          default_bias_init(self.rngs.params(), wi_bias_shape, self.weight_dtype),\n          sharding=wi_bias_axes,\n      )\n      self.wi_1_bias = nnx.Param(\n          default_bias_init(self.rngs.params(), wi_bias_shape, self.weight_dtype),\n          sharding=wi_bias_axes,\n      )\n      self.wo_bias = nnx.Param(\n          default_bias_init(self.rngs.params(), wo_bias_shape, self.weight_dtype),\n          sharding=wo_bias_axes,\n      )\n    else:\n      self.wi_0_bias = None\n      self.wi_1_bias = None\n      self.wo_bias = None\n\n  def get_expert_parallelism_size(self):\n    return self.mesh.shape[\"expert\"]\n\n  def get_tensor_parallelism_size(self):\n    return self.mesh.shape[\"tensor\"]\n\n  def get_tensor_transpose_parallelism_size(self):\n    return self.mesh.shape[\"tensor_transpose\"]\n\n  def get_context_autoregressive_parallelism_size(self):\n    return self.mesh.shape[\"context_autoregressive\"]\n\n  def get_topk(self, gate_logits, pre_bias_logits, rngs=None):\n    \"\"\"get topk.\"\"\"\n    # shape of top_k_weights & top_k_indices:\n    # (batch, sequence, num_experts_per_tok).\n    if self.config.use_random_routing:\n      if rngs is None:\n        raise ValueError(\"The random key cannot be None for random routing.\")\n      # Re-use the 'dropout' RNG stream to ensure random routing\n      rng = rngs.dropout()\n      top_k_weights, top_k_indices = random_routing(rng, gate_logits, self.num_experts_per_tok)\n      return top_k_weights, top_k_indices\n\n    if self.config.model_name.startswith(\"deepseek3\"):\n      top_k_weights, top_k_indices = self.deepseek_routing(gate_logits, pre_bias_logits)\n    else:\n      top_k_weights, top_k_indices = jax.lax.top_k(gate_logits, self.num_experts_per_tok)\n\n    if self.config.decoder_block == ctypes.DecoderBlockType.DEEPSEEK:\n      top_k_weights = self.deepseek_scale_weights(top_k_weights)\n    elif self.config.decoder_block != ctypes.DecoderBlockType.LLAMA4:\n      top_k_weights = jax.nn.softmax(top_k_weights.astype(jnp.float32), axis=-1).astype(self.dtype)\n\n    # This is the Qwen3-specific normalization of router weights.\n    if self.config.norm_topk_prob:\n      top_k_weights /= top_k_weights.sum(axis=-1, keepdims=True)\n\n    return top_k_weights, top_k_indices\n\n  def deepseek_scale_weights(self, weights):\n    \"\"\"Scales weights according to DeepSeek's v3 reference implementation.\"\"\"\n    # https://github.com/deepseek-ai/DeepSeek-V3/blob/2f7b80eecebf3d1c84da5a0d465f6639ea175012/inference/model.py#L592-L594.\n    if self.config.routed_score_func == \"sigmoid\":\n      weights /= weights.sum(-1, keepdims=True)\n    weights *= self.config.routed_scaling_factor\n    return weights\n\n  def expert_group_mask(self, gate_logits: jax.Array) -> jax.Array:\n    \"\"\"Returns a mask that selects only the top-k groups of experts.\n\n    Groups of experts are selected based on the sum of the top-2 expert scores\n    for each group.\n\n    Args:\n      gate_logits: Array of shape `(batch, seq, num_experts)`.\n\n    Returns:\n      Array of shape `(batch, seq, num_experts)` that is 1 for experts in the\n      top-k groups and 0 elsewhere.\n    \"\"\"\n    # Find top groups based on each group's top-2 expert scores, where\n    # `scores_grouped.shape =\n    # (batch * seq, n_routing_groups, experts_per_group)`.\n    scores_grouped = jnp.reshape(\n        gate_logits,\n        gate_logits.shape[:-1] + (self.config.n_routing_groups, -1),\n    )\n    top2_in_group_vals, _ = jax.lax.top_k(scores_grouped, k=2)\n    group_scores = jnp.sum(jnp.astype(top2_in_group_vals, jnp.float32), axis=-1)\n    _, group_idx = jax.lax.top_k(group_scores, k=self.config.topk_routing_group)\n\n    # Mask selected groups so that only those experts are considered.\n    group_mask = jax.nn.one_hot(group_idx, num_classes=self.config.n_routing_groups, dtype=jnp.float32)\n    group_mask = jnp.sum(group_mask, axis=-2)\n\n    # Apply masks and get top-k indices.\n    score_mask_expanded = jnp.broadcast_to(\n        group_mask[..., None],\n        group_mask.shape + (self.num_experts // self.config.n_routing_groups,),\n    )\n    return jnp.reshape(\n        score_mask_expanded,\n        score_mask_expanded.shape[:-2] + (self.num_experts,),\n    )\n\n  def deepseek_routing(self, gate_logits: jax.Array, pre_bias_logits: jax.Array) -> tuple[jax.Array, jax.Array]:\n    \"\"\"DeepSeek routing logit.\n\n    If the configuration does not specify routing groups (`n_routing_groups` is\n    -1), we use a standard top-k routing mechanism. Otherwise, we force all\n    selected experts to be from the a subset of the highest rated expert groups.\n\n    The selection process uses post_bias logits, while the return weights use\n    pre_bias logits.\n\n    Args:\n      gate_logits: Array of shape `(batch, seq, num_experts)`.\n      pre_bias_logits: Array of shape `(batch, seq,num_experts)`.\n\n    Returns:\n      - top_k_weights: `(batch, seq, num_experts_per_tok)` array of weight values for\n        each selected expert.\n      - top_k_indices: `(batch, seq, num_experts_per_tok)` array of indices\n        identifying the selected experts for each token.\n    \"\"\"\n    expert_mask = 1 if self.config.n_routing_groups == -1 else self.expert_group_mask(gate_logits)\n    _, top_k_indices = jax.lax.top_k(\n        jnp.where(expert_mask > 0, gate_logits, -jnp.inf),\n        k=self.num_experts_per_tok,\n    )\n    top_k_weights = jnp.take_along_axis(pre_bias_logits, top_k_indices, axis=-1)\n    return top_k_weights, top_k_indices\n\n  def apply_ffn_activation(self, layer_w0, layer_w1):\n    \"\"\"Applies FFN activation function.\"\"\"\n    with jax.named_scope(\"ffn_act\"):\n      if self.config.decoder_block == ctypes.DecoderBlockType.GPT_OSS:\n        layer_w0 = jnp.clip(layer_w0, a_min=None, a_max=self.config.mlp_activations_limit)\n        layer_w1 = jnp.clip(layer_w1, a_min=-self.config.mlp_activations_limit, a_max=self.config.mlp_activations_limit)\n        layer_act = self.activation_fn(layer_w0 * 1.702)\n        glu = jnp.multiply(layer_w0, layer_act)\n        intermediate_layer = jnp.multiply(glu, (layer_w1 + 1))\n      else:\n        layer_act = self.activation_fn(layer_w0)\n        intermediate_layer = jnp.multiply(layer_act, layer_w1)\n      return intermediate_layer.astype(self.dtype)\n\n  def permute(self, inputs, gate_logits, pre_bias_logits, use_custom_sort_vjp=True, rngs=None, roll_to_expert_id=None):\n    \"\"\"Permute tokens to group by expert to fit gmm call.\"\"\"\n    # reshape inputs (batch, sequence, emb) to (batch * sequence, emb)\n    inputs_shape = inputs.shape\n    bsz_times_seq_len = inputs_shape[0] * inputs_shape[1]\n    inputs_2d = jnp.reshape(inputs, (bsz_times_seq_len, inputs_shape[2]))\n    weights, selected_experts = self.get_topk(gate_logits, pre_bias_logits, rngs)\n\n    if self.config.decoder_block == ctypes.DecoderBlockType.LLAMA4:\n      # weights will be of shape (batch_size, seq_len, num_experts_per_tok)\n      router_scores = jax.nn.sigmoid(weights.astype(jnp.float32))  # weights are top_k_weights here\n      # Squeeze router_scores to (batch_size * seq_len, num_experts_per_tok)\n      inputs_2d = inputs_2d * router_scores.reshape(bsz_times_seq_len, -1)\n\n    flatten_selected_experts = jnp.ravel(selected_experts)\n    if roll_to_expert_id is not None:\n      flatten_selected_experts = (flatten_selected_experts - roll_to_expert_id) % self.num_experts\n    sorted_selected_experts = jnp.argsort(flatten_selected_experts)\n    # sort inputs for number of selected experts\n    replicated_inputs_2d = jnp.repeat(inputs_2d, self.num_experts_per_tok, axis=0)\n    sorted_inputs = _sort_activations(replicated_inputs_2d, sorted_selected_experts, use_custom_sort_vjp).astype(\n        self.dtype\n    )\n    group_size = jnp.bincount(flatten_selected_experts, length=self.num_experts)\n    # Return the experts for each sorted input.\n    expert_indices = jnp.arange(self.num_experts)\n    sorted_experts = jnp.repeat(\n        expert_indices,\n        repeats=group_size,\n        total_repeat_length=flatten_selected_experts.shape[0],\n    )\n    return (\n        sorted_inputs,\n        sorted_selected_experts,\n        weights,\n        group_size,\n        sorted_experts,\n    )\n\n  def unpermute(\n      self,\n      intermediate,\n      sorted_selected_experts,\n      weights,\n      batch_size,\n      sequence_length,\n      use_custom_sort_vjp=True,\n  ):\n    \"\"\"Unpermute tokens to original order and combine weights.\"\"\"\n\n    unsort_intermediate = _sort_activations(\n        intermediate,\n        jnp.argsort(sorted_selected_experts),\n        use_custom_sort_vjp,\n    )\n    reshaped_weights = jnp.reshape(weights, (-1, self.num_experts_per_tok))\n    reshaped_intermediate = jnp.reshape(\n        unsort_intermediate,\n        (reshaped_weights.shape[0], self.num_experts_per_tok, -1),\n    )\n    with jax.named_scope(\"weight_sum\"):\n      matmul_precision = jax.lax.Precision(self.config.matmul_precision)\n      if self.config.decoder_block == ctypes.DecoderBlockType.LLAMA4:\n        # For Llama4, combine using weights of 1 for selected experts\n        reshaped_weights = jnp.ones_like(reshaped_weights)\n      output = jnp.einsum(\n          \"BKE,BK -> BE\",\n          reshaped_intermediate.astype(jnp.float32),\n          reshaped_weights.astype(jnp.float32),\n          precision=matmul_precision,\n      )\n    return output.reshape(batch_size, sequence_length, -1).astype(self.dtype)\n\n  @staticmethod\n  def local_permute(\n      inputs,\n      global_group_sizes,\n      local_expert_size,\n      shard_index,\n      is_offset=False,\n      global_sorted_experts=None,\n      use_custom_sort_vjp=True,\n  ):\n    \"\"\"Permutes tokens locally within an expert shard.\n\n    This function prepares the input tokens for processing by the experts\n    located\n    on the current shard. It groups the tokens by their assigned local expert\n    index (0 to local_expert_size - 1).\n\n    Args:\n      inputs: The input data (tokens) assigned to the experts on this shard.\n        Shape `[tokens, emb_dim]`.\n      global_group_sizes: The count of tokens assignments for each global expert\n        across all the batch shards. Shape `[num_batch_shards, num_experts].\n      local_expert_size: The number of experts handled by the current shard.\n      shard_index: The index of the current expert shard (0 to\n        num_expert_parallelism - 1).\n      is_offset: If True, assumes `inputs` are pre-sorted by global expert ID\n        and selects the slice relevant to this shard's assigned experts. If\n        False, assumes that `inputs` corresponding to the shard's experts start\n        from the beginning of the tensor but need to be permuted by expert ID.\n      global_sorted_experts: Global expert IDs for the `inputs` used when\n        `is_offset` is True. Shape `[total_tokens_for_this_shard]`.\n\n    Returns:\n      A tuple containing:\n        sorted_inputs: Input data permuted local expert ID.\n        sorted_indices: Indices used to permute the inputs.\n        local_group_size: Number of tokens assigned to each local expert on this\n          shard.\n        sorted_experts_ids: expert ID corresponding to each token of the permuted\n        inputs.\n    \"\"\"\n\n    # Slice the count of local expert IDs in each batch shard.\n    # all_shard_local_sizes.shape: [expert_shard, local_expert_size]\n    all_shard_local_sizes = jax.lax.dynamic_slice_in_dim(\n        global_group_sizes,\n        shard_index * local_expert_size,\n        local_expert_size,\n        axis=1,\n    )\n    local_sizes = all_shard_local_sizes.reshape(-1)\n\n    # Total count of the local expert IDs is the sum of the counts across all\n    # batch shards, since all batch shards will send their contributions to the\n    # current expert shard.\n    local_group_size = jnp.sum(all_shard_local_sizes, axis=0)\n\n    # In this case, the data that needs to be processed by the local shard\n    # does not start from row 0 but actually starts at\n    # (jnp.concatenate((jnp.array([0]),\n    #  jnp.cumsum(local_group_sizes[:-1]))[shard_id]).\n    # This happens if batches (`inputs`) are replicated across expert shards and\n    # pre-sorted by global Expert ID (via permute()).\n    if is_offset:\n      divided_assignments = jnp.floor_divide(global_sorted_experts, local_expert_size)\n      expert_indices = jnp.where(\n          divided_assignments == shard_index,\n          jnp.mod(global_sorted_experts, local_expert_size),\n          local_expert_size,\n      )\n\n    # In this case the `input` data has been received from the batch shards and\n    # needs to be reorganized in order of local Expert IDs.\n    else:\n      base_indices = jnp.mod(jnp.arange(local_sizes.shape[0]), local_expert_size)\n      expert_indices = jnp.repeat(base_indices, local_sizes, total_repeat_length=inputs.shape[0])\n\n    sorted_indices = jnp.argsort(expert_indices)\n    sorted_inputs = _sort_activations(inputs, sorted_indices, use_custom_sort_vjp)\n    sorted_experts_ids = expert_indices[sorted_indices]\n    return (\n        sorted_inputs,\n        sorted_indices,\n        local_group_size,\n        sorted_experts_ids,\n    )\n\n  @staticmethod\n  def get_all_to_all_params(\n      all_shards_group_sizes,\n      shard_id,\n      num_expert_parallelism,\n      is_batch_sharded=True,\n  ):\n    \"\"\"Generates input offsets, send sizes, output offsets, and receive sizes used for ragged_all_to_all.\"\"\"\n\n    class TransformStrategy(enum.Enum):\n      INPUT_OFFSET = enum.auto()\n      SEND_SIZE = enum.auto()\n      OUTPUT_OFFSET = enum.auto()\n      RECV_SIZE = enum.auto()\n\n    def transform_array(input_array, shard_id, strategy, is_batch_sharded):\n      \"\"\"Transforms the input array based on the specified strategy.\"\"\"\n      # Prepares it for the usage with `ragged_all_to_all` API. The\n      # transformation determines how data is sent and received between shards.\n      if is_batch_sharded:\n        if strategy == TransformStrategy.INPUT_OFFSET:\n          # Index of input array for the send\n          local_array = input_array[shard_id]\n          return jnp.concatenate((jnp.array([0]), jnp.cumsum(local_array)[:-1]))\n        elif strategy == TransformStrategy.SEND_SIZE:\n          # Size of input array for the send\n          return input_array[shard_id]\n        elif strategy == TransformStrategy.OUTPUT_OFFSET:\n          # Received index in the target output\n          zero_row = jnp.zeros((1,) + input_array.shape[1:], dtype=input_array.dtype)\n          array_with_zeros = jnp.concatenate((zero_row, input_array), axis=0)\n          cumulated_array = jnp.cumsum(array_with_zeros, axis=0, dtype=input_array.dtype)\n          return cumulated_array[shard_id]\n        elif strategy == TransformStrategy.RECV_SIZE:\n          # Received size in the target output\n          return input_array[:, shard_id]\n        else:\n          raise ValueError(f\"Unknown transform array strategy: {strategy}\")\n\n      # If the batch is unsharded then we send the same data slice to all other\n      # shards. We also assume each shard will have the local processed inputs\n      # sorted to start from index 0. Finally, len(input_array.shape) == 1 since\n      # there is only one batch shard.\n      else:\n        if strategy == TransformStrategy.INPUT_OFFSET:\n          # The data on each shard always starts at 0.\n          return jnp.zeros(num_expert_parallelism, dtype=input_array.dtype)\n        elif strategy == TransformStrategy.SEND_SIZE:\n          # The send amount is always the amount of data the current expert\n          # shard needs to process.\n          return jnp.repeat(input_array[shard_id], num_expert_parallelism)\n        elif strategy == TransformStrategy.OUTPUT_OFFSET:\n          # The offset in each shard will just be the start of the group which\n          # that shard is responsible for.\n          output_offset = jnp.concatenate((jnp.array([0]), jnp.cumsum(input_array[:-1])))[shard_id]\n          return jnp.repeat(output_offset, num_expert_parallelism)\n        # The amount that each shard receives from all other shards is\n        # equivalent to the group sizes (aka input_array).\n        elif strategy == TransformStrategy.RECV_SIZE:\n          # Received size in the target output\n          return input_array\n        else:\n          raise ValueError(f\"Unknown transform array strategy: {strategy}\")\n\n    input_offsets = transform_array(\n        all_shards_group_sizes,\n        shard_id,\n        TransformStrategy.INPUT_OFFSET,\n        is_batch_sharded,\n    )\n    send_sizes = transform_array(\n        all_shards_group_sizes,\n        shard_id,\n        TransformStrategy.SEND_SIZE,\n        is_batch_sharded,\n    )\n    output_offsets = transform_array(\n        all_shards_group_sizes,\n        shard_id,\n        TransformStrategy.OUTPUT_OFFSET,\n        is_batch_sharded,\n    )\n    recv_sizes = transform_array(\n        all_shards_group_sizes,\n        shard_id,\n        TransformStrategy.RECV_SIZE,\n        is_batch_sharded,\n    )\n    return input_offsets, send_sizes, output_offsets, recv_sizes\n\n  def transform_bias(self, experts_index, *biases):\n    \"\"\"Selects bias values for a variable number of bias tensors based on chosen experts.\"\"\"\n    return tuple(bias[experts_index] for bias in biases)\n\n  def sparse_matmul(\n      self,\n      inputs,\n      gate_logits,\n      pre_bias_logits,\n      w0_kernel,\n      w1_kernel,\n      wo_kernel,\n      w0_bias,\n      w1_bias,\n      wo_bias,\n  ):\n    \"\"\"Perform sparse matrix multiplication of inputs and Experts.\"\"\"\n\n    def gmm(inputs, kernel, group_sizes, expert_assignments):\n      tile_size = (\n          self.config.tile_batch_seq,\n          self.config.tile_activation_dim,\n          self.config.tile_weight_dim,\n      )\n      pad_length = self.config.tile_batch_seq\n      hs_shape = inputs.shape\n      # pad length is the 1st dimension of tiling size in gmm call\n      if inputs.shape[0] != expert_assignments.shape[0]:\n        raise ValueError(\"The number of input tokens must match the number of expert\" \" assignments!\")\n      padding_amount = 0\n      if hs_shape[0] % pad_length:\n        padding_amount = pad_length - hs_shape[0] % pad_length\n        inputs = jax.lax.pad(inputs, jnp.array(0.0, dtype=inputs.dtype), [(0, padding_amount, 0), (0, 0, 0)])\n\n      inputs = inputs.astype(self.dtype)\n      kernel = kernel.astype(self.dtype)\n\n      lhs_quantize_dtype, rhs_quantize_dtype = None, None\n      if self.quant is not None:\n        quant_dg = self.quant.quant_dg\n        lhs_quantize_dtype = quant_dg.fwd.dg_quantizer.lhs.numerics.get_dtype()\n        rhs_quantize_dtype = quant_dg.fwd.dg_quantizer.rhs.numerics.get_dtype()\n      if self.config.use_qwix_quantization:\n        quantization_rule = qpl.get_current_rule(\"dot_general\")\n        if quantization_rule is not None:\n          lhs_quantize_dtype = quantization_rule.act_qtype\n          rhs_quantize_dtype = quantization_rule.weight_qtype\n      m, k, n = inputs.shape[0], inputs.shape[1], kernel.shape[2]\n      tiling = (\n          min(tile_size[0], m),\n          min(tile_size[1], k),\n          min(tile_size[2], n),\n      )\n      if self.config.megablox:\n        output = mblx.gmm(\n            lhs=inputs,\n            rhs=kernel,\n            group_sizes=group_sizes,\n            preferred_element_type=self.dtype,\n            tiling=tiling,\n            lhs_quantize_dtype=lhs_quantize_dtype,\n            rhs_quantize_dtype=rhs_quantize_dtype,\n            use_qwix_quantization=self.config.use_qwix_quantization,\n        )\n      else:\n        rhs_inputs = kernel\n        if isinstance(kernel, aqt.QTensor):\n          if kernel.bias or kernel.sparsity_mask or len(kernel.scale) > 1:\n            raise ValueError(\"Unsupported usecase for ragged_dot with quantized kernel.\")\n          rhs_inputs = kernel.qvalue\n        with set_xla_metadata(ragged_dot_tiling=\",\".join([str(t) for t in tiling])):\n          output = jax.lax.ragged_dot(\n              lhs=inputs,\n              rhs=rhs_inputs,\n              group_sizes=group_sizes,\n              preferred_element_type=self.dtype,\n          )\n        if isinstance(kernel, aqt.QTensor):\n          # Multiply outputs by the kernely scale\n          scales = jnp.take(kernel.scale[0].squeeze(), indices=expert_assignments, axis=0)\n          if padding_amount > 0:\n            scales = jax.lax.pad(\n                scales,\n                jnp.array(0.0, dtype=scales.dtype),\n                [(0, padding_amount, 0), (0, 0, 0)],\n            )\n          output *= scales\n      if padding_amount > 0:\n        output = output[: hs_shape[0]]\n      return output\n\n    # Currently, we support data, tensor, and expert parallelism with Megablox.\n    # We all gather the input activations over tensor parallelism to follow\n    # https://parsa.epfl.ch/course-info/cs723/papers/Megatron.pdf.\n\n    # Check if the batch should be sharded by expert and whether the batch_size\n    # supports this. For example, for interleaved inference, prefill always has\n    # batch_size=1 while decode can have batch_size > 1.\n    try:\n      is_batch_sharded_by_expert = (\n          \"expert\"\n          in tuple(\n              filter(\n                  lambda tup: tup[0] == \"activation_batch\",\n                  self.config.logical_axis_rules,\n              )\n          )[\n              0\n          ][1]\n      )\n    except:  # pylint: disable=bare-except\n      is_batch_sharded_by_expert = False\n    if is_batch_sharded_by_expert and inputs.shape[0] > 1:\n      batch_logical_axis = \"activation_batch\"\n    else:\n      batch_logical_axis = \"activation_batch_no_exp\"\n\n    if self.get_tensor_transpose_parallelism_size() > 1:\n      input_partition_pspec = nn.logical_to_mesh_axes((batch_logical_axis, \"activation_norm_length\", \"activation_embed\"))\n      w0_bias_pspec = nn.logical_to_mesh_axes((\"exp\", None))\n      w1_bias_pspec = nn.logical_to_mesh_axes((\"exp\", None))\n      wo_bias_pspec = nn.logical_to_mesh_axes((\"exp\", \"activation_embed\"))\n    else:\n      input_partition_pspec = nn.logical_to_mesh_axes((batch_logical_axis, \"activation_norm_length\", None))\n      w0_bias_pspec = nn.logical_to_mesh_axes((\"exp\", \"activation_mlp\"))\n      w1_bias_pspec = nn.logical_to_mesh_axes((\"exp\", \"activation_mlp\"))\n      wo_bias_pspec = nn.logical_to_mesh_axes((\"exp\", \"activation_embed\"))\n\n    gate_logits_pspec = nn.logical_to_mesh_axes((batch_logical_axis, \"activation_norm_length\", None))\n    if self.config.model_name.startswith(\"deepseek3\"):\n      pre_bias_logits_pspec = nn.logical_to_mesh_axes((batch_logical_axis, \"activation_norm_length\", None))\n    else:\n      # pre_bias_logits is None for non-DeepSeek v3 models\n      pre_bias_logits_pspec = None\n\n    # w0, w1, wo needs to be un sharded on fsdp / fsdp_transpose axis, so use\n    # mlp_no_fsdp axis\n    if self.config.fsdp_shard_on_exp:\n      # special sharding for dsv3 to remove overhead between gmm/AG\n      w0_pspec = nn.logical_to_mesh_axes((\"embed_tensor_transpose\", None, \"mlp_no_fsdp\"))\n      w1_pspec = nn.logical_to_mesh_axes((\"embed_tensor_transpose\", None, \"mlp_no_fsdp\"))\n      wo_pspec = nn.logical_to_mesh_axes((\"embed_tensor_transpose\", \"mlp_no_fsdp\", None))\n    else:\n      w0_pspec = nn.logical_to_mesh_axes((\"exp\", \"embed_tensor_transpose\", \"mlp_no_fsdp\"))\n      w1_pspec = nn.logical_to_mesh_axes((\"exp\", \"embed_tensor_transpose\", \"mlp_no_fsdp\"))\n      wo_pspec = nn.logical_to_mesh_axes((\"exp\", \"mlp_no_fsdp\", \"embed_tensor_transpose\"))\n    if isinstance(w0_kernel, aqt.QTensor):\n      w0_pspec = aqt.partition_spec(w0_pspec, (1,), w0_kernel.dtype, use_bias=False)\n    if isinstance(w1_kernel, aqt.QTensor):\n      w1_pspec = aqt.partition_spec(w1_pspec, (1,), w1_kernel.dtype, use_bias=False)\n    if isinstance(wo_kernel, aqt.QTensor):\n      wo_pspec = aqt.partition_spec(wo_pspec, (1,), wo_kernel.dtype, use_bias=False)\n\n    @functools.partial(\n        shard_map.shard_map,\n        mesh=self.mesh,\n        in_specs=(\n            input_partition_pspec,\n            gate_logits_pspec,\n            pre_bias_logits_pspec,\n            w0_pspec,\n            w1_pspec,\n            wo_pspec,\n            w0_bias_pspec,\n            w1_bias_pspec,\n            wo_bias_pspec,\n            None,\n        ),\n        out_specs=(nn.logical_to_mesh_axes((batch_logical_axis, \"activation_norm_length\", \"activation_embed\"))),\n        check_rep=False,\n    )\n    def wrapper(x, logits, pre_bias_logits, w0, w1, wo, w0_bias, w1_bias, wo_bias, rngs):\n      batch_size, sequence_length, _ = x.shape\n      expert_axis_name = \"expert\"\n      expert_shard_id = jax.lax.axis_index(expert_axis_name)\n      num_expert_parallelism = self.get_expert_parallelism_size()\n      if self.config.use_ring_of_experts:\n        # The ring-of-experts strategy first duplicates the inputs to all\n        # expert shards, and then routes within each shard.\n\n        # Duplicate inputs to all expert shards.\n        x, logits, pre_bias_logits = tuple(\n            jax.lax.all_gather(z, axis_name=expert_axis_name, tiled=True) for z in (x, logits, pre_bias_logits)\n        )\n\n        # \"Route\" tokens within each shard.\n        num_experts_per_shard = self.config.num_experts // num_expert_parallelism\n        x, sorted_selected_experts, weights, group_sizes, selected_experts = self.permute(\n            x,\n            logits,\n            pre_bias_logits,\n            self.config.use_custom_sort_vjp,\n            roll_to_expert_id=num_experts_per_shard * expert_shard_id,\n        )\n\n        # Filter down to the group sizes that apply to only the experts in the\n        # current shard.\n        group_sizes = group_sizes[:num_experts_per_shard]\n        mask = jnp.arange(x.shape[0]) < jnp.sum(group_sizes)\n        x = jnp.where(mask[:, None], x, 0)\n      else:\n        x, sorted_selected_experts, weights, group_sizes, selected_experts = self.permute(\n            x, logits, pre_bias_logits, self.config.use_custom_sort_vjp, rngs\n        )\n\n        if num_expert_parallelism > 1:\n          batch_axis = \"expert\" if is_batch_sharded_by_expert else \"data\"\n          # get group sizes for all shards\n          local_expert_size = self.config.num_experts // num_expert_parallelism\n          reshaped_group_sizes = jnp.sum(group_sizes.reshape(-1, local_expert_size), axis=1)\n          global_group_sizes = group_sizes\n          if is_batch_sharded_by_expert:\n            all_shards_group_sizes = jax.lax.all_gather(reshaped_group_sizes, axis_name=batch_axis)\n            input_offsets, send_sizes, output_offsets, recv_sizes = RoutedMoE.get_all_to_all_params(\n                all_shards_group_sizes,\n                expert_shard_id,\n                num_expert_parallelism,\n            )\n\n            # TODO(ranran): For better performance, we could update output buffer to a smaller\n            # size to replace self.get_expert_parallelism_size() for efficiency,\n            # Or we could apply capacity_factor for excessive experts.\n            # Note: Reducing buffer increase the risk of token dropping under unbalanced distribution.\n\n            # In the worst case, all of the global input data is assigned to each expert in the current shard.\n            # This would result in num_expert_shards * input_size * experts_per_shard assignments. However, if\n            # experts_per_shard > num_experts_per_tok we cannot assign more than num_experts_per_tok to all of the inputs.\n            max_local_experts_per_tok = min(local_expert_size, self.config.num_experts_per_tok)\n            buffer_size = int(\n                num_expert_parallelism\n                * self.config.per_device_batch_size\n                * self.config.max_target_length\n                * max_local_experts_per_tok\n            )\n            output_shape = jnp.zeros((buffer_size, self.config.emb_dim), dtype=x.dtype)\n\n            x = jax.lax.ragged_all_to_all(\n                x,\n                output_shape,\n                input_offsets,\n                send_sizes,\n                output_offsets,\n                recv_sizes,\n                axis_name=expert_axis_name,\n            )\n            global_group_sizes = jax.lax.all_gather(group_sizes, axis_name=expert_axis_name)\n            x, local_sorted_indices, group_sizes, selected_experts = RoutedMoE.local_permute(\n                x,\n                global_group_sizes,\n                local_expert_size,\n                shard_index=expert_shard_id,\n                use_custom_sort_vjp=self.config.use_custom_sort_vjp,\n            )\n          else:\n            x, local_sorted_indices, group_sizes, selected_experts = RoutedMoE.local_permute(\n                x,\n                global_group_sizes[None, :],\n                local_expert_size,\n                shard_index=expert_shard_id,\n                is_offset=True,\n                global_sorted_experts=selected_experts,\n                use_custom_sort_vjp=self.config.use_custom_sort_vjp,\n            )\n\n      if self.config.mlp_bias:\n        w0_bias, w1_bias, wo_bias = self.transform_bias(selected_experts, w0_bias, w1_bias, wo_bias)\n\n      gmm_fn = functools.partial(\n          gmm,\n          group_sizes=group_sizes,\n          expert_assignments=selected_experts,\n      )\n      layer_w0 = gmm_fn(x, w0)\n      if self.get_tensor_transpose_parallelism_size() > 1:\n        layer_w0 = jax.lax.psum(layer_w0, \"tensor_transpose\")\n      if self.config.mlp_bias:\n        layer_w0 = layer_w0 + w0_bias\n      layer_w0 = adc.checkpoint_name(layer_w0, \"mlpwi_0\")\n\n      layer_w1 = gmm_fn(x, w1)\n      if self.get_tensor_transpose_parallelism_size() > 1:\n        layer_w1 = jax.lax.psum(layer_w1, \"tensor_transpose\")\n      if self.config.mlp_bias:\n        layer_w1 = layer_w1 + w1_bias\n      layer_w1 = adc.checkpoint_name(layer_w1, \"mlpwi_1\")\n      intermediate_layer = self.apply_ffn_activation(layer_w0, layer_w1)\n\n      intermediate_output = gmm_fn(intermediate_layer, wo)\n      if self.get_tensor_parallelism_size() > 1:\n        intermediate_output = jax.lax.psum_scatter(intermediate_output, \"tensor\", scatter_dimension=1, tiled=True)\n      if self.config.mlp_bias:\n        intermediate_output = intermediate_output + wo_bias\n      intermediate_output = adc.checkpoint_name(intermediate_output, \"mlpwo\")\n\n      if self.config.use_ring_of_experts:\n        # Set the outputs of tokens which were not processed to 0.\n        mask = jnp.arange(intermediate_output.shape[0]) < jnp.sum(group_sizes)\n        intermediate_output = jnp.where(mask[:, None], intermediate_output, 0)\n\n        # Unsort and deduplicate the outputs locally.\n        output = self.unpermute(\n            intermediate_output,\n            sorted_selected_experts,\n            weights,\n            batch_size=batch_size,\n            sequence_length=sequence_length,\n            use_custom_sort_vjp=self.config.use_custom_sort_vjp,\n        )\n\n        # Sum up the partial outputs across the expert shards.\n        output = jnp.reshape(output, (-1, sequence_length, self.config.emb_dim))\n        output = jax.lax.psum_scatter(output, expert_axis_name, scatter_dimension=0, tiled=True)\n\n      else:\n        if num_expert_parallelism > 1:\n          original_inputs_first_dim = batch_size * sequence_length * self.config.num_experts_per_tok\n          if sorted_selected_experts.shape[0] != original_inputs_first_dim:\n            raise ValueError(\"original_inputs_first_dim does not match the original tensor\" \" shape!\")\n          output_shape = jnp.zeros(\n              (\n                  original_inputs_first_dim,\n                  self.config.emb_dim // self.get_tensor_parallelism_size(),\n              ),\n              dtype=intermediate_output.dtype,\n          )\n          if is_batch_sharded_by_expert:\n            # locally unpermute back to the original order\n            local_output = _sort_activations(\n                intermediate_output,\n                jnp.argsort(local_sorted_indices),  # pylint: disable=undefined-variable\n                self.config.use_custom_sort_vjp,\n            )\n            input_offsets, send_sizes, output_offsets, recv_sizes = RoutedMoE.get_all_to_all_params(\n                jnp.transpose(all_shards_group_sizes),  # pylint: disable=undefined-variable\n                expert_shard_id,\n                num_expert_parallelism,\n            )\n            intermediate_output = jax.lax.ragged_all_to_all(\n                local_output,\n                output_shape,\n                input_offsets,\n                send_sizes,\n                output_offsets,\n                recv_sizes,\n                axis_name=expert_axis_name,\n            )\n          else:\n            # If bach is replicated across EP shards then each shard should send\n            # 0..local_shard_size data to the other shards and receive the\n            # local_shard data from all of the other shards using\n            # ragged_all_to_all.\n            input_offsets, send_sizes, output_offsets, recv_sizes = RoutedMoE.get_all_to_all_params(\n                reshaped_group_sizes,  # pylint: disable=undefined-variable\n                expert_shard_id,\n                num_expert_parallelism,\n                is_batch_sharded=False,\n            )\n            intermediate_output = jax.lax.ragged_all_to_all(\n                intermediate_output,\n                output_shape,\n                input_offsets,\n                send_sizes,\n                output_offsets,\n                recv_sizes,\n                axis_name=expert_axis_name,\n            )\n\n        output = self.unpermute(\n            intermediate_output,\n            sorted_selected_experts,\n            weights,\n            batch_size=batch_size,\n            sequence_length=sequence_length,\n            use_custom_sort_vjp=self.config.use_custom_sort_vjp,\n        )\n\n      return output, None\n\n    if self.config.moe_fsdp_use_two_stage_all_gather:\n      # Unshard on fsdp axis\n      w0_kernel = nn.with_logical_constraint(w0_kernel, (\"exp\", \"embed_tensor_transpose\", \"mlp\"))\n      w1_kernel = nn.with_logical_constraint(w1_kernel, (\"exp\", \"embed_tensor_transpose\", \"mlp\"))\n\n      # Unshard on fsdp_transpose axis\n      wo_kernel = nn.with_logical_constraint(wo_kernel, (\"exp\", \"mlp\", \"embed_tensor_transpose\"))\n\n      # Make sure XLA does not optimize by combining above All-Gather to unshard\n      # on FSDP axis and the subsequent unshard on fsdp_transpose axis\n      w0_kernel = jax.lax.optimization_barrier(w0_kernel)\n      w1_kernel = jax.lax.optimization_barrier(w1_kernel)\n      wo_kernel = jax.lax.optimization_barrier(wo_kernel)\n\n      # Unshard on both fsdp and fsdp_transpose transpose\n      w0_kernel = nn.with_logical_constraint(w0_kernel, (\"exp\", \"embed_tensor_transpose\", \"mlp_no_fsdp\"))\n      w1_kernel = nn.with_logical_constraint(w1_kernel, (\"exp\", \"embed_tensor_transpose\", \"mlp_no_fsdp\"))\n      wo_kernel = nn.with_logical_constraint(wo_kernel, (\"exp\", \"mlp_no_fsdp\", \"embed_tensor_transpose\"))\n\n    return wrapper(\n        inputs, gate_logits, pre_bias_logits, w0_kernel, w1_kernel, wo_kernel, w0_bias, w1_bias, wo_bias, self.rngs\n    )\n\n  def reshape_and_update_weights(self, weights, indices):\n    \"\"\"reshape and update weights.\"\"\"\n    # input of weights and indices: (batch_size, seq_len, num_experts_per_tok)\n    # output of updated weights: (batch_size, seq_len, num_experts)\n    update_weights = jnp.zeros((weights.shape[0], weights.shape[1], self.num_experts), dtype=self.dtype)\n    index_update = (\n        jnp.arange(weights.shape[0])[:, None, None],\n        jnp.arange(weights.shape[1])[:, None],\n        indices,\n    )\n    update_weights = update_weights.at[index_update].set(weights)\n    return update_weights\n\n  def get_context_partition_and_sub_seq(self, seq_len):\n    cp = self.get_context_autoregressive_parallelism_size()\n    if seq_len % cp != 0:\n      cp = 1\n    sub_seq = seq_len // cp\n    return cp, sub_seq\n\n  def generate_masks_subgroup(self, top_k_indices, softmax_probs):\n    \"\"\"Subgroup mask generation for inference only.\"\"\"\n    # calculate\n    # expert_capacity = (tokens_per_batch / num_experts) * capacity_factor\n    batch_size, seq_len, _ = top_k_indices.shape\n    cp, sub_seq = self.get_context_partition_and_sub_seq(seq_len)\n\n    # Break sequence into subsequences (groups) of tokens, and route only within\n    # each group.\n    top_k_indices = jnp.reshape(top_k_indices, (batch_size, cp, sub_seq, top_k_indices.shape[2]))\n\n    tokens_per_batch = sub_seq * self.num_experts_per_tok\n    # this is to avoid expert_capacity_per_batch = 0\n    expert_capacity_per_batch = int(\n        max(\n            math.ceil(tokens_per_batch / self.num_experts) * self.config.capacity_factor,\n            self.config.capacity_factor,\n        )\n    )\n    max_logging.log(\"Applying potential token dropping with a batch expert_capacity of\" f\" {expert_capacity_per_batch}\")\n\n    # calculate expert mask and drop tokens if needed\n    # shape of output expert mask: (batch, sequence, num_experts_per_tok)\n    #\n    # A small example:\n    # give num_experts=4 & num_experts_per_tok=2, and two tokens are routed to\n    # expert [0, 1] & [1, 3],\n    # then expert_mask becomes\n    # [[[[1, 0, 0, 0],[0, 1, 0, 0]], [[0, 1, 0, 0],[0, 0, 0, 1]]]],\n    # after cumsum, expert_token_count becomes\n    # [[[[1, 0, 0, 0],[1, 1, 0, 0]], [[1, 2, 0, 0],[1, 2, 0, 1]]]],\n    # if we set expert_capacity=1,\n    # trunc_expert_mask becomes\n    # [[[[1, 0, 0, 0],[0, 1, 0, 0]], [[0, 0, 0, 0],[0, 0, 0, 1]]]],\n    # so the 2nd token for expert #1 ([0, 1] & [1, 3]) is dropped, output of\n    # updated_expert_mask is [[[1, 1],[0, 1]]].\n    expert_mask = jax.nn.one_hot(top_k_indices, num_classes=self.num_experts, dtype=jnp.int32)\n    expert_mask_fused = jnp.reshape(\n        expert_mask,\n        (batch_size, cp, sub_seq * self.num_experts_per_tok, self.num_experts),\n    )\n    expert_mask_fused = nn.with_logical_constraint(expert_mask_fused, (\"activation_batch\", None, None, None))\n    expert_token_count_fused = jnp.cumsum(expert_mask_fused, axis=2)\n    expert_token_count = jnp.reshape(\n        expert_token_count_fused,\n        ((batch_size, cp, sub_seq, self.num_experts_per_tok, self.num_experts)),\n    )\n    expert_token_count = nn.with_logical_constraint(\n        expert_token_count,\n        (\"activation_batch\", \"activation_norm_length\", None, None, None),\n    )\n    trunc_expert_mask = expert_mask * jnp.less_equal(expert_token_count, expert_capacity_per_batch)\n    combined_expert_mask = jnp.sum(trunc_expert_mask, axis=3)\n\n    # reshape & update weights\n    softmax_probs = jnp.reshape(\n        softmax_probs,\n        ((batch_size, cp, sub_seq, self.num_experts)),\n    )\n    softmax_probs *= combined_expert_mask\n\n    # calculate token position in expert capacity dimension\n    expert_token_position_fused = expert_mask_fused * expert_token_count_fused\n    expert_token_position = jnp.reshape(\n        expert_token_position_fused,\n        (batch_size, cp, sub_seq, self.num_experts_per_tok, self.num_experts),\n    )\n    combined_expert_token_position = jnp.sum(expert_token_position, axis=3) * combined_expert_mask\n    expert_token_position_in_capacity = jax.nn.one_hot(\n        combined_expert_token_position,\n        num_classes=expert_capacity_per_batch + 1,\n        dtype=jnp.int32,\n    )\n\n    # shape of combine_mask is\n    # (batch_size, seq_len, num_experts, expert_capacity_per_batch + 1),\n    # and cut 0-dimension which is always 0\n    combine_mask = softmax_probs[..., None] * expert_token_position_in_capacity\n    combine_mask = combine_mask[..., 1:]\n    dispatch_mask = combine_mask.astype(bool)\n\n    # ici_context_parallelism\n    dispatch_mask = jnp.reshape(\n        dispatch_mask,\n        (batch_size, cp, sub_seq, self.num_experts, expert_capacity_per_batch),\n    )\n    combine_mask = jnp.reshape(\n        combine_mask,\n        (batch_size, cp, sub_seq, self.num_experts, expert_capacity_per_batch),\n    )\n\n    return dispatch_mask, combine_mask\n\n  def generate_masks(self, top_k_indices, softmax_probs):\n    \"\"\"Generate masks.\"\"\"\n    # calculate\n    # expert_capacity = (tokens_per_batch / num_experts) * capacity_factor\n    batch_size, seq_len, _ = top_k_indices.shape\n\n    tokens_per_batch = seq_len * self.num_experts_per_tok\n    # this is to avoid expert_capacity_per_batch = 0\n    expert_capacity_per_batch = int(\n        max(\n            math.ceil(tokens_per_batch / self.num_experts) * self.config.capacity_factor,\n            self.config.capacity_factor,\n        )\n    )\n    max_logging.log(\"Applying potential token dropping with a batch expert_capacity of\" f\" {expert_capacity_per_batch}\")\n\n    # calculate expert mask and drop tokens if needed\n    # shape of output expert mask: (batch, sequence, num_experts_per_tok)\n    #\n    # A small example:\n    # give num_experts=4 & num_experts_per_tok=2, and two tokens are routed to\n    # expert [0, 1] & [1, 3],\n    # then expert_mask becomes\n    # [[[[1, 0, 0, 0],[0, 1, 0, 0]], [[0, 1, 0, 0],[0, 0, 0, 1]]]],\n    # after cumsum, expert_token_count becomes\n    # [[[[1, 0, 0, 0],[1, 1, 0, 0]], [[1, 2, 0, 0],[1, 2, 0, 1]]]],\n    # if we set expert_capacity=1,\n    # trunc_expert_mask becomes\n    # [[[[1, 0, 0, 0],[0, 1, 0, 0]], [[0, 0, 0, 0],[0, 0, 0, 1]]]],\n    # so the 2nd token for expert #1 ([0, 1] & [1, 3]) is dropped, output of\n    # updated_expert_mask is [[[1, 1],[0, 1]]].\n    expert_mask = jax.nn.one_hot(top_k_indices, num_classes=self.num_experts, dtype=jnp.int32)\n    expert_mask_fused = jnp.reshape(\n        expert_mask,\n        (batch_size, seq_len * self.num_experts_per_tok, self.num_experts),\n    )\n    expert_mask_fused = nn.with_logical_constraint(expert_mask_fused, (\"activation_batch\", None, None))\n    expert_token_count_fused = jnp.cumsum(expert_mask_fused, axis=1)\n    expert_token_count = jnp.reshape(\n        expert_token_count_fused,\n        ((batch_size, seq_len, self.num_experts_per_tok, self.num_experts)),\n    )\n    expert_token_count = nn.with_logical_constraint(\n        expert_token_count,\n        (\"activation_batch\", \"activation_norm_length\", None, None),\n    )\n    trunc_expert_mask = expert_mask * jnp.less_equal(expert_token_count, expert_capacity_per_batch)\n    combined_expert_mask = jnp.sum(trunc_expert_mask, axis=2)\n\n    softmax_probs *= combined_expert_mask\n\n    # calculate token position in expert capacity dimension\n    expert_token_position_fused = expert_mask_fused * expert_token_count_fused\n    expert_token_position = jnp.reshape(\n        expert_token_position_fused,\n        (batch_size, seq_len, self.num_experts_per_tok, self.num_experts),\n    )\n    combined_expert_token_position = jnp.sum(expert_token_position, axis=2) * combined_expert_mask\n    expert_token_position_in_capacity = jax.nn.one_hot(\n        combined_expert_token_position,\n        num_classes=expert_capacity_per_batch + 1,\n        dtype=jnp.int32,\n    )\n\n    # shape of combine_mask is\n    # (batch_size, seq_len, num_experts, expert_capacity_per_batch + 1),\n    # and cut 0-dimension which is always 0\n    combine_mask = softmax_probs[..., None] * expert_token_position_in_capacity\n    combine_mask = combine_mask[..., 1:]\n    dispatch_mask = combine_mask.astype(bool)\n\n    return dispatch_mask, combine_mask\n\n  # See Switch Transformer (https://arxiv.org/abs/2101.03961) for more details.\n  def load_balance_loss(self, top_k_indices, logits) -> jax.Array:\n    \"\"\"Compute the load balance loss.\"\"\"\n    expert_mask = jax.nn.one_hot(top_k_indices, num_classes=self.num_experts, dtype=jnp.int32)\n    summed_expert_mask = jnp.sum(expert_mask, axis=2)\n    # Get fraction of tokens dispatched to each expert\n    density = jnp.mean(summed_expert_mask, axis=1)\n    # get fraction of probability allocated to each expert\n    density_prob = jnp.mean(logits, axis=1)\n    loss = jnp.mean(density * density_prob) * (self.num_experts**2) * self.config.load_balance_loss_weight\n    return loss\n\n  def get_einsum(\n      self,\n      rhs_mesh_axes: Tuple[Optional[str], ...] = (),\n      einsum_name: str | None = None,\n  ):\n    \"\"\"Get the Einstein summation.\"\"\"\n\n    # the check is to prevent aqteinsum as einsum op for dispatch and combine\n    # einsums in ase when capacity_factor > 0\n    # this is necessary to load pre-quantized weights in case of inference\n    if self.config.model_call_mode == \"inference\" and einsum_name in (\n        DISPATCH,\n        COMBINE,\n    ):\n      return jnp.einsum\n\n    if self.quant:\n\n      def aqt_einsum(*args, **kwargs):  # pylint: disable=unused-argument\n        # simply skip kwargs, since aqt einsum doesn't support any kwargs\n        # like precision\n        is_aqt = not isinstance(self.quant, quantizations.Fp8Quantization)\n        kw = {\"mesh_axes\": rhs_mesh_axes} if is_aqt else {\"dtype\": self.dtype}\n        return self.quant.einsum(**kw)(*args)  # pytype: disable=attribute-error\n\n      einsum_op = aqt_einsum\n    else:\n      einsum_op = jnp.einsum\n    return einsum_op\n\n  def maybe_all_gather_kernel_weight_in_expert_parallelism(\n      self, kernel: jax.Array, kernel_axes: Tuple[Optional[str], ...]\n  ):\n    \"\"\"All-gather kernel weight in expert parallelism if needed.\"\"\"\n    if self.get_expert_parallelism_size() > 1:\n      # This will trigger all-gather using weight_dtype\n      # relax it unless really necessary in expert parallelism only\n      # Otherwise compiler will handle communication automatically\n      # esp. with int8 quantization, kernel will be all-gathered in int8 instead\n      # of weight_dtype\n      kernel = nn.with_logical_constraint(kernel, kernel_axes)\n    return kernel\n\n  def dense_matmul(\n      self,\n      inputs,\n      gate_logits,\n      pre_bias_logits,\n      w0_kernel,\n      w1_kernel,\n      wo_kernel,\n      w0_bias,\n      w1_bias,\n      wo_bias,\n  ) -> tuple[jax.Array, Optional[jax.Array]]:\n    \"\"\"Dense matrix multiplication.\"\"\"\n    # gate_logits: batch, length, expert\n    gate_logits = nn.with_logical_constraint(gate_logits, (\"activation_batch\", \"activation_norm_length\", None))\n    if self.config.model_name.startswith(\"deepseek3\"):\n      # pre_bias_logits is None for non-DeepSeek v3 models\n      pre_bias_logits = nn.with_logical_constraint(pre_bias_logits, (\"activation_batch\", \"activation_norm_length\", None))\n    top_k_weights, top_k_indices = self.get_topk(gate_logits, pre_bias_logits, self.rngs)\n    is_llama4_decoder_layer = self.config.decoder_block == ctypes.DecoderBlockType.LLAMA4\n    if is_llama4_decoder_layer:\n      router_scores = jax.nn.sigmoid(top_k_weights.astype(jnp.float32)).astype(self.dtype)\n      inputs = inputs * router_scores\n    else:\n      weights = self.reshape_and_update_weights(top_k_weights, top_k_indices)\n    matmul_precision = jax.lax.Precision(self.config.matmul_precision)\n\n    if self.config.model_call_mode != \"inference\":\n      softmax_probs = jax.nn.softmax(gate_logits.astype(jnp.float32), axis=-1).astype(self.dtype)\n      loss = self.load_balance_loss(top_k_indices, softmax_probs)\n    else:\n      loss = None\n    batch_size = inputs.shape[0]\n    seq_len = inputs.shape[1]\n\n    cp, sub_seq = self.get_context_partition_and_sub_seq(seq_len)\n\n    if self.config.capacity_factor > 0:\n      # token dropping if needed\n      if self.config.model_call_mode != \"inference\":\n        # TODO(b/425930949): remove this pylint by refactoring the logic here.\n        dispatch_mask, combine_mask = self.generate_masks(\n            top_k_indices, weights  # pylint: disable=undefined-variable,possibly-used-before-assignment\n        )\n        mask_axes = (\"activation_batch\", \"activation_norm_length\", None, None)\n        dispatch_axis = (\n            \"activation_exp\",\n            \"activation_batch_no_exp\",\n            None,\n            \"activation_embed\",\n        )\n        mlp_axis = (\n            \"activation_exp\",\n            \"activation_batch_no_exp\",\n            None,\n            \"activation_mlp\",\n        )\n        dispatch_eimsum = \"BSM,BSEC -> EBCM\"\n        mlp_up_einsum = \"EBCM,EMH -> EBCH\"\n        mlp_down_einsum = \"EBCH,EHM -> EBCM\"\n        output_einsum = \"EBCM,BSEC -> BSM\"\n      else:\n        # TODO(b/425930507): Try replacing `softmax_probs` with padded weights\n        # and verify with decode acc tests.\n        softmax_probs = jax.nn.softmax(gate_logits.astype(jnp.float32), axis=-1).astype(self.dtype)\n        dispatch_mask, combine_mask = self.generate_masks_subgroup(top_k_indices, softmax_probs)\n        if self.get_context_autoregressive_parallelism_size() > 0 and cp == 1:\n          mask_axes = (\n              \"activation_norm_length\",\n              \"activation_batch\",\n              None,\n              None,\n              None,\n          )\n          input_axis = (\n              \"activation_norm_length\",\n              \"activation_batch\",\n              None,\n              \"activation_embed\",\n          )\n          dispatch_axis = (\n              \"activation_exp\",\n              \"activation_batch_no_exp\",\n              None,\n              None,\n              \"activation_embed\",\n          )\n          mlp_axis = (\n              \"activation_exp\",\n              \"activation_batch_no_exp\",\n              None,\n              None,\n              \"activation_mlp\",\n          )\n        else:\n          mask_axes = (\n              \"activation_batch\",\n              \"activation_norm_length\",\n              None,\n              None,\n              None,\n          )\n          input_axis = (\n              \"activation_batch\",\n              \"activation_norm_length\",\n              None,\n              \"activation_embed\",\n          )\n          dispatch_axis = (\n              \"activation_exp\",\n              \"activation_batch_no_exp\",\n              None,\n              None,\n              \"activation_embed\",\n          )\n          mlp_axis = (\n              \"activation_exp\",\n              \"activation_batch_no_exp\",\n              None,\n              None,\n              \"activation_mlp\",\n          )\n        dispatch_eimsum = \"BNSM,BNSEC -> EBNCM\"\n        mlp_up_einsum = \"EBNCM,EMH -> EBNCH\"\n        mlp_down_einsum = \"EBNCH,EHM -> EBNCM\"\n        output_einsum = \"EBNCM,BNSEC -> BNSM\"\n\n        inputs = jnp.reshape(inputs, (batch_size, cp, sub_seq, inputs.shape[2]))\n        inputs = nn.with_logical_constraint(inputs, input_axis)\n\n      dispatch_mask = nn.with_logical_constraint(dispatch_mask, mask_axes)\n      combine_mask = nn.with_logical_constraint(combine_mask, mask_axes)\n\n      with jax.named_scope(\"dispatch\"):\n        # only cp during prefill\n        dispatch = self.get_einsum(rhs_mesh_axes=mask_axes, einsum_name=DISPATCH)(\n            dispatch_eimsum, inputs, dispatch_mask, precision=matmul_precision\n        )\n        if cp > 1:\n          dispatch = nn.with_logical_constraint(\n              dispatch,\n              (\n                  None,\n                  \"activation_batch_no_exp\",\n                  \"activation_norm_length\",\n                  None,\n                  \"activation_embed\",\n              ),\n          )\n        dispatch = nn.with_logical_constraint(\n            dispatch,\n            dispatch_axis,\n        )\n      with jax.named_scope(\"wi_0\"):\n        w0_kernel_axes = (\"exp\", None, \"mlp\")\n        w0_kernel = self.maybe_all_gather_kernel_weight_in_expert_parallelism(w0_kernel, w0_kernel_axes)\n        layer_w0 = self.get_einsum(rhs_mesh_axes=w0_kernel_axes)(\n            mlp_up_einsum, dispatch, w0_kernel, precision=matmul_precision\n        )\n        if self.config.mlp_bias:\n          w0_bias = w0_bias[:, None, None, :]\n          layer_w0 = layer_w0 + w0_bias\n\n        if self.config.activations_in_float32:\n          layer_w0 = layer_w0.astype(jnp.float32)\n        layer_w0 = nn.with_logical_constraint(\n            layer_w0,\n            mlp_axis,\n        )\n        layer_w0 = adc.checkpoint_name(layer_w0, \"mlpwi_0\")\n      with jax.named_scope(\"wi_1\"):\n        w1_kernel_axes = (\"exp\", None, \"mlp\")\n        w1_kernel = self.maybe_all_gather_kernel_weight_in_expert_parallelism(w1_kernel, w1_kernel_axes)\n        layer_w1 = self.get_einsum(rhs_mesh_axes=w1_kernel_axes)(\n            mlp_up_einsum, dispatch, w1_kernel, precision=matmul_precision\n        )\n        if self.config.mlp_bias:\n          w1_bias = w1_bias[:, None, None, :]\n          layer_w1 = layer_w1 + w1_bias\n        if self.config.activations_in_float32:\n          layer_w1 = layer_w1.astype(jnp.float32)\n        layer_w1 = nn.with_logical_constraint(\n            layer_w1,\n            mlp_axis,\n        )\n        layer_w1 = adc.checkpoint_name(layer_w1, \"mlpwi_1\")\n      layer_multiply = self.apply_ffn_activation(layer_w0, layer_w1)\n      with jax.named_scope(\"wo\"):\n        wo_kernel_axes = (\"exp\", \"mlp\", None)\n        wo_kernel = self.maybe_all_gather_kernel_weight_in_expert_parallelism(wo_kernel, wo_kernel_axes)\n        intermediate_layer = self.get_einsum(rhs_mesh_axes=wo_kernel_axes)(\n            mlp_down_einsum,\n            layer_multiply,\n            wo_kernel,\n            precision=matmul_precision,\n        )\n        if self.config.mlp_bias:\n          wo_bias = wo_bias[:, None, None, :]\n          intermediate_layer = intermediate_layer + wo_bias\n        if self.config.activations_in_float32:\n          intermediate_layer = intermediate_layer.astype(jnp.float32)\n        if self.config.model_call_mode != \"inference\":\n          intermediate_layer = nn.with_logical_constraint(\n              intermediate_layer,\n              (\n                  \"activation_exp\",\n                  \"activation_batch_no_exp\",\n                  None,\n                  \"activation_embed\",\n              ),\n          )\n        intermediate_layer = adc.checkpoint_name(intermediate_layer, \"mlpwo\")\n      with jax.named_scope(\"combine\"):\n        # Matmul & element wise operation\n        output = self.get_einsum(rhs_mesh_axes=mask_axes, einsum_name=COMBINE)(\n            output_einsum,\n            intermediate_layer,\n            combine_mask,\n            precision=matmul_precision,\n        )\n        if output.ndim == 4:\n          output = jnp.reshape(\n              output,\n              (\n                  output.shape[0],\n                  output.shape[1] * output.shape[2],\n                  output.shape[3],\n              ),\n          )\n      return output, loss\n    else:\n      inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n      with jax.named_scope(\"wi_0\"):\n        layer_w0 = self.get_einsum(rhs_mesh_axes=self.wi_kernel_axes)(\n            \"BSM,EMH -> BSEH\", inputs, w0_kernel, precision=matmul_precision\n        )\n        if self.config.mlp_bias:\n          layer_w0 = layer_w0 + w0_bias[None, None, :, :]\n        if self.config.activations_in_float32:\n          layer_w0 = layer_w0.astype(jnp.float32)\n        layer_w0 = adc.checkpoint_name(layer_w0, \"mlpwi_0\")\n      with jax.named_scope(\"wi_1\"):\n        layer_w1 = self.get_einsum(rhs_mesh_axes=self.wi_kernel_axes)(\n            \"BSM,EMH -> BSEH\", inputs, w1_kernel, precision=matmul_precision\n        )\n        if self.config.mlp_bias:\n          layer_w1 = layer_w1 + w1_bias[None, None, :, :]\n        if self.config.activations_in_float32:\n          layer_w1 = layer_w1.astype(jnp.float32)\n        layer_w1 = adc.checkpoint_name(layer_w1, \"mlpwi_1\")\n      layer_multiply = self.apply_ffn_activation(layer_w0, layer_w1)\n\n      with jax.named_scope(\"wo\"):\n        intermediate_layer = self.get_einsum(rhs_mesh_axes=self.wo_kernel_axes)(\n            \"BSEH,EHM -> BSEM\",\n            layer_multiply,\n            wo_kernel,\n            precision=matmul_precision,\n        )\n        if self.config.mlp_bias:\n          intermediate_layer = intermediate_layer + wo_bias[None, None, :, :]\n        if self.config.activations_in_float32:\n          intermediate_layer = intermediate_layer.astype(jnp.float32)\n        intermediate_layer = adc.checkpoint_name(intermediate_layer, \"mlpwo\")\n      with jax.named_scope(\"w_sum\"):\n        if is_llama4_decoder_layer:\n          weights = self.reshape_and_update_weights(jnp.ones_like(top_k_weights), top_k_indices)\n        # cast to f32 for sum up in einsum op\n        output = jnp.einsum(\n            \"BSEM,BSE -> BSM\",\n            intermediate_layer.astype(jnp.float32),\n            weights.astype(jnp.float32),  # pylint: disable=undefined-variable,possibly-used-before-assignment\n            precision=matmul_precision,\n        ).astype(self.dtype)\n      return output, None\n\n  def retrieve_quantized_weight(\n      self,\n      inputs,\n      gate_logits,\n      pre_bias_logits,\n      w0_kernel,\n      w1_kernel,\n      wo_kernel,\n      w0_bias,\n      w1_bias,\n      wo_bias,\n  ) -> tuple[aqt.QTensor, aqt.QTensor, aqt.QTensor]:\n    \"\"\"Retrieve quantized weights.\"\"\"\n    # This is called only during tracing. This is to invoke creation of\n    # quantized tensor inside AqtEinsum.  After jit, this will become no-op and\n    # will not affect performance.\n    _ = self.dense_matmul(\n        inputs, gate_logits, pre_bias_logits, w0_kernel, w1_kernel, wo_kernel, w0_bias, w1_bias, wo_bias\n    )\n\n    w0_kernel = self.variables[\"aqt\"][\"AqtEinsum_0\"][\"AqtDotGeneral_0\"][\"qrhs\"][\"frozen\"]\n    w1_kernel = self.variables[\"aqt\"][\"AqtEinsum_1\"][\"AqtDotGeneral_0\"][\"qrhs\"][\"frozen\"]\n    wo_kernel = self.variables[\"aqt\"][\"AqtEinsum_2\"][\"AqtDotGeneral_0\"][\"qrhs\"][\"frozen\"]\n\n    w0_kernel = max_utils.unbox_logicallypartioned(w0_kernel)\n    w1_kernel = max_utils.unbox_logicallypartioned(w1_kernel)\n    wo_kernel = max_utils.unbox_logicallypartioned(wo_kernel)\n    return w0_kernel, w1_kernel, wo_kernel\n\n  def __call__(self, inputs: jax.Array) -> tuple[jax.Array, Optional[jax.Array]]:\n    cfg = self.config\n    inputs = inputs.astype(cfg.dtype)\n    gate_logits, pre_bias_logits = self.gate(inputs)\n\n    w0_kernel = jnp.asarray(self.wi_0[...], self.dtype)\n    w1_kernel = jnp.asarray(self.wi_1[...], self.dtype)\n    wo_kernel = jnp.asarray(self.wo[...], self.dtype)\n\n    if cfg.mlp_bias:\n      w0_bias = jnp.asarray(self.wi_0_bias[...], self.dtype)\n      w1_bias = jnp.asarray(self.wi_1_bias[...], self.dtype)\n      wo_bias = jnp.asarray(self.wo_bias[...], self.dtype)\n    else:\n      w0_bias, w1_bias, wo_bias = None, None, None\n\n    if cfg.sparse_matmul:\n      if quantizations.in_serve_mode(self.quant):\n        w0_kernel, w1_kernel, wo_kernel = self.retrieve_quantized_weight(\n            inputs,\n            gate_logits,\n            pre_bias_logits,\n            w0_kernel,\n            w1_kernel,\n            wo_kernel,\n            w0_bias,\n            w1_bias,\n            wo_bias,\n        )\n      return self.sparse_matmul(\n          inputs, gate_logits, pre_bias_logits, w0_kernel, w1_kernel, wo_kernel, w0_bias, w1_bias, wo_bias\n      )\n    else:\n      return self.dense_matmul(\n          inputs, gate_logits, pre_bias_logits, w0_kernel, w1_kernel, wo_kernel, w0_bias, w1_bias, wo_bias\n      )",
        "analysis": {
            "functionality": "Implements a Mixture of Experts (MoE) layer that routes each input token to a subset of available experts. It supports two primary modes of operation: a 'sparse' mode that groups tokens by expert for efficient computation using grouped matrix multiplication, and a 'dense' mode that uses einsum operations. The class handles various parallelism strategies (data, tensor, expert), model-specific routing logic (e.g., for DeepSeek, Llama4), optional quantization, and can compute a load-balancing loss during training.",
            "usage": "Instantiate the class with a configuration object, mesh, and other parameters. Call the instance with an input tensor of shape `[batch_size, sequence_length, emb_dim]`. The module will return a tuple containing the output tensor of the same shape and an optional load-balancing loss scalar. The choice between sparse and dense computation is controlled by the `config.sparse_matmul` flag."
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#RoutedAndSharedMoE",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "class RoutedAndSharedMoE(nnx.Module):\n  \"\"\"Implements a block which combines shared and routed experts.\"\"\"\n\n  def __init__(\n      self,\n      config: ctypes.Config,\n      mesh: jax.sharding.Mesh,\n      kernel_init: NdInitializer,\n      kernel_axes: Tuple[Optional[str], ...],\n      rngs: nnx.Rngs,\n      weight_dtype: ctypes.DType = jnp.float32,\n      dtype: ctypes.DType = jnp.float32,\n      quant: Optional[quantizations.AqtQuantization] = None,\n  ):\n    \"\"\"nitializes the RoutedAndSharedMoE module.\n\n    Attributes:\n      config: The main config setting.\n      mesh: Mesh, device mesh.\n      kernel_init: The initializer function for the kernel weight matrix.\n      kernel_axes: A tuple of logical axis names for partitioning the kernel.\n      rngs: An `nnx.Rngs` object used for initializing parameters.\n      weight_dtype: The data type of the kernel weights.\n      dtype: The data type for the computation.\n      quant: The quantization configuration. If None, no quantization is applied.\n    \"\"\"\n    self.config = config\n    self.mesh = mesh\n    self.kernel_init = kernel_init\n    self.kernel_axes = kernel_axes\n    self.weight_dtype = weight_dtype\n    self.dtype = dtype\n    self.quant = quant\n    self.rngs = rngs\n    # NOTE: the name MoeBlock_0 is to ensure reverse compatibility with\n    # existing checkpoints for routed experts.\n    self.MoeBlock_0 = RoutedMoE(\n        config=self.config,\n        num_experts=self.config.num_experts,\n        num_experts_per_tok=self.config.num_experts_per_tok,\n        mesh=self.mesh,\n        kernel_init=nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        intermediate_dim=self.config.moe_mlp_dim,\n        dtype=self.config.dtype,\n        weight_dtype=self.config.weight_dtype,\n        quant=self.quant,\n        rngs=self.rngs,\n    )\n    self.shared_experts = linears.MlpBlock(\n        in_features=self.config.emb_dim,\n        intermediate_dim=self.config.shared_experts * self.config.moe_mlp_dim,\n        activations=self.config.mlp_activations,\n        intermediate_dropout_rate=self.config.dropout_rate,\n        dtype=self.config.dtype,\n        weight_dtype=self.config.weight_dtype,\n        config=self.config,\n        quant=self.quant,\n        rngs=self.rngs,\n    )\n\n  @property\n  def routed_moe(self):\n    return self.MoeBlock_0\n\n  def __call__(self, inputs: jax.Array) -> jax.Array:\n    routed_experts, _ = self.routed_moe(inputs)\n    shared_experts = self.shared_experts(inputs)\n    return routed_experts + shared_experts",
        "analysis": {
            "module_type": "routed_and_shared_mixture_of_experts",
            "purpose": "Implements a block that combines the outputs of a routed Mixture-of-Experts (MoE) layer and a shared MLP (Multi-Layer Perceptron) block.",
            "input": {
                "shape": "[batch_size, sequence_length, embedding_dim]",
                "dtype": "jnp.float32"
            },
            "processing_steps": [
                "Process the input through the `RoutedMoE` layer.",
                "Process the input through the shared `MlpBlock` layer.",
                "Sum the outputs from the routed and shared layers."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, embedding_dim]"
            },
            "dependencies": [
                "RoutedMoE",
                "linears.MlpBlock",
                "nnx.Module",
                "ctypes.Config",
                "jax.sharding.Mesh",
                "NdInitializer",
                "quantizations.AqtQuantization"
            ],
            "parameters": {
                "config": "The main configuration object containing parameters like `num_experts`, `num_experts_per_tok`, `moe_mlp_dim`, `shared_experts`, `emb_dim`, etc.",
                "mesh": "The JAX device mesh for model parallelism.",
                "quant": "Optional quantization configuration."
            },
            "notes": [
                "The routed MoE module is named `MoeBlock_0` to ensure reverse compatibility with existing checkpoints."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the module by creating a `RoutedMoE` instance and a shared `MlpBlock` instance.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters.",
                        "Instantiate `RoutedMoE` as `self.MoeBlock_0`.",
                        "Instantiate `linears.MlpBlock` as `self.shared_experts`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "RoutedMoE",
                        "linears.MlpBlock",
                        "nd_dense_init"
                    ],
                    "notes": [
                        "The name `MoeBlock_0` is specifically chosen for reverse compatibility with existing checkpoints for routed experts."
                    ]
                },
                "routed_moe": {
                    "purpose": "Provides access to the internal `RoutedMoE` module instance via a property.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Return the `self.MoeBlock_0` attribute."
                    ],
                    "output": {
                        "shape": "An instance of the `RoutedMoE` class."
                    },
                    "dependencies": [
                        "RoutedMoE"
                    ],
                    "notes": [
                        "This is a `@property`, so it is accessed like an attribute."
                    ]
                },
                "__call__": {
                    "purpose": "Processes the input tensor through both the routed and shared expert paths and sums their outputs.",
                    "input": {
                        "shape": "[batch_size, sequence_length, embedding_dim]",
                        "dtype": "jnp.float32"
                    },
                    "processing_steps": [
                        "Call the `routed_moe` module with the input tensor to get the routed expert output.",
                        "Call the `shared_experts` module with the input tensor to get the shared expert output.",
                        "Perform an element-wise addition of the routed and shared expert outputs."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, embedding_dim]"
                    },
                    "dependencies": [],
                    "notes": [
                        "The `routed_moe` call returns a tuple `(output, loss)`, but only the output tensor is used in the final summation."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#get_gate_logit",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def get_gate_logit(\n    inputs_shape: tuple[int, ...],\n    out_features_shape: Union[Iterable[int], int],\n    model_name: str,\n    axis: Union[Iterable[int], int] = -1,\n    weight_dtype: ctypes.DType = jnp.float32,\n    dtype: ctypes.DType = jnp.float32,\n    kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n    kernel_axes: Tuple[Optional[str], ...] = (),\n    use_bias: bool = False,\n    score_func: str = \"\",\n    quant: Optional[quantizations.AqtQuantization] = None,\n    matmul_precision: str = \"default\",\n    name: Optional[str] = None,\n):\n  \"\"\"Creates a GateLogit Linen module.\"\"\"\n\n  axis = linears.canonicalize_tuple(axis)\n  in_features_shape = tuple(inputs_shape[ax] for ax in linears.normalize_axes(axis, len(inputs_shape)))\n\n  module = nnx_wrappers.to_linen(\n      GateLogit,\n      in_features_shape=in_features_shape,\n      out_features_shape=out_features_shape,\n      model_name=model_name,\n      axis=axis,\n      weight_dtype=weight_dtype,\n      dtype=dtype,\n      kernel_init=kernel_init,\n      kernel_axes=kernel_axes,\n      use_bias=use_bias,\n      score_func=score_func,\n      quant=quant,\n      matmul_precision=matmul_precision,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )\n  return module",
        "analysis": {
            "module_type": "gate_logit_factory",
            "purpose": "A factory function that creates a Flax Linen-compatible version of the GateLogit NNX module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Canonicalizes the `axis` parameter to a tuple.",
                "Derives the `in_features_shape` from the full `inputs_shape` and the specified `axis`.",
                "Wraps the `GateLogit` NNX module into a Flax Linen module using `nnx_wrappers.to_linen`, passing along all configuration parameters.",
                "Returns the created Linen module."
            ],
            "output": {
                "shape": "Returns an instance of a Flax Linen Module."
            },
            "dependencies": [
                "GateLogit",
                "nnx_wrappers.to_linen",
                "linears.canonicalize_tuple",
                "linears.normalize_axes",
                "initializers.variable_to_logically_partitioned"
            ],
            "parameters": {
                "inputs_shape": "The shape of the tensor that will be passed to the created module, used to derive the input feature shape.",
                "out_features_shape": "The shape of the output features for the gate, typically the number of experts.",
                "model_name": "The name of the model, used for specific routing logic within the GateLogit module (e.g., 'deepseek3').",
                "axis": "The axis or axes over which the linear transformation is applied.",
                "use_bias": "A boolean indicating whether to add a learnable bias to the gate logit scores.",
                "score_func": "An optional scoring function (e.g., 'sigmoid') to apply to the output before the bias."
            },
            "notes": [
                "This function acts as a builder, abstracting the conversion of an NNX module (`GateLogit`) into a standard Flax Linen module.",
                "The `abstract_init=False` argument passed to `to_linen` indicates that the module is initialized immediately upon creation rather than waiting for the first call."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#get_routed_moe",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def get_routed_moe(\n    config: ctypes.Config,\n    num_experts: int,\n    num_experts_per_tok: int,\n    mesh: jax.sharding.Mesh,\n    kernel_init: NdInitializer,\n    kernel_axes: Tuple[Optional[str], ...],\n    intermediate_dim: int = 2048,\n    weight_dtype: ctypes.DType = jnp.float32,\n    dtype: ctypes.DType = jnp.float32,\n    quant: Optional[quantizations.AqtQuantization] = None,\n    name: Optional[str] = None,\n):\n  \"\"\"Creates a RoutedMoE Linen module.\"\"\"\n\n  module = nnx_wrappers.to_linen(\n      RoutedMoE,\n      config=config,\n      num_experts=num_experts,\n      num_experts_per_tok=num_experts_per_tok,\n      mesh=mesh,\n      kernel_init=kernel_init,\n      kernel_axes=kernel_axes,\n      intermediate_dim=intermediate_dim,\n      weight_dtype=weight_dtype,\n      dtype=dtype,\n      quant=quant,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )\n  return module",
        "analysis": {
            "module_type": "routed_moe_factory",
            "purpose": "A factory function that creates and wraps a `RoutedMoE` NNX module into a Flax Linen module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `RoutedMoE` NNX module into a Flax Linen module.",
                "Passes all configuration parameters to the `RoutedMoE` constructor.",
                "Returns the created Linen module."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance, not a tensor. Shape is N/A."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "RoutedMoE",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "config": "The main configuration object containing model settings like embedding dimension, MLP activations, etc.",
                "num_experts": "The total number of experts in the Mixture-of-Experts layer.",
                "num_experts_per_tok": "The number of experts each token is routed to.",
                "mesh": "The JAX device mesh used for model parallelism and sharding.",
                "intermediate_dim": "The intermediate dimension of the feed-forward network within each expert.",
                "quant": "Optional AQT quantization configuration for the module."
            },
            "notes": [
                "This function acts as a bridge between the NNX-defined `RoutedMoE` module and the Flax Linen API.",
                "It sets `abstract_init=False`, meaning the module is initialized immediately upon creation.",
                "It provides `variable_to_logically_partitioned` as the `metadata_fn` for handling parameter sharding annotations."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#get_routed_and_shared_moe",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def get_routed_and_shared_moe(\n    config: ctypes.Config,\n    mesh: jax.sharding.Mesh,\n    kernel_init: NdInitializer,\n    kernel_axes: Tuple[Optional[str], ...],\n    weight_dtype: ctypes.DType = jnp.float32,\n    dtype: ctypes.DType = jnp.float32,\n    quant: Optional[quantizations.AqtQuantization] = None,\n    name: Optional[str] = None,\n):\n  \"\"\"Creates a RoutedAndSharedMoE Linen module.\"\"\"\n\n  module = nnx_wrappers.to_linen(\n      RoutedAndSharedMoE,\n      config=config,\n      mesh=mesh,\n      kernel_init=kernel_init,\n      kernel_axes=kernel_axes,\n      weight_dtype=weight_dtype,\n      dtype=dtype,\n      quant=quant,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )\n  return module",
        "analysis": {
            "module_type": "routed_and_shared_moe_factory",
            "purpose": "A factory function that wraps the `RoutedAndSharedMoE` NNX module into a Flax Linen module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `RoutedAndSharedMoE` NNX module into a Flax Linen module.",
                "Passes configuration parameters (`config`, `mesh`, `kernel_init`, etc.) to the `RoutedAndSharedMoE` constructor during the wrapping process."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance, not a tensor. Shape is N/A."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "RoutedAndSharedMoE",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "config": "A `ctypes.Config` object containing model and MoE configurations.",
                "mesh": "A `jax.sharding.Mesh` object for distributed computation.",
                "kernel_init": "An `NdInitializer` for initializing kernel weights.",
                "kernel_axes": "A tuple of logical axis names for weight partitioning.",
                "weight_dtype": "The data type for the module's weights.",
                "dtype": "The data type for computations within the module.",
                "quant": "An optional `AqtQuantization` object for quantization."
            },
            "notes": [
                "This function serves as a bridge between the NNX-defined `RoutedAndSharedMoE` module and the Flax Linen API.",
                "The `abstract_init=False` argument suggests that the module's parameters are initialized concretely when the wrapper is created."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/multi_token_prediction.py#roll_and_mask",
        "file_path": "src/MaxText/layers/multi_token_prediction.py",
        "code_block": "def roll_and_mask(x: jnp.ndarray, shift: int = -1) -> jnp.ndarray:\n  \"\"\"\n  Performs a leftward roll on the sequence axis (axis=1) and masks the\n  newly created invalid positions at the end of the sequence.\n  Assumes input `x` has a batch dimension at axis 0 and sequence at axis 1.\n\n  Args:\n    x: The input array of shape [batch, seq_len, ...].\n    shift: The number of positions to shift left.\n\n  Returns:\n    The rolled array of the same shape as x.\n  \"\"\"\n  # If shift is 0, it's a no-op. Return the original array.\n  if shift == 0:\n    return x\n\n  # to set the last `abs(shift)` elements of the sequence to zero.\n  return jnp.roll(x, shift, axis=1).at[:, shift:, ...].set(0)",
        "analysis": {
            "module_type": "sequence_roll_and_mask",
            "purpose": "Performs a leftward circular shift on the sequence axis (axis=1) of a tensor and masks the newly wrapped-around elements at the end by setting them to zero.",
            "input": {
                "shape": "[batch_size, sequence_length, ...]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "If `shift` is 0, return the input tensor `x` without modification.",
                "Perform a circular shift on `x` along the sequence axis (axis=1) by `shift` positions using `jnp.roll`.",
                "Set the last `abs(shift)` elements of the sequence axis to 0 to mask the wrapped-around values."
            ],
            "output": {
                "shape": "Same as input: [batch_size, sequence_length, ...]"
            },
            "dependencies": [
                "jax.numpy.roll"
            ],
            "parameters": {
                "shift": "The number of positions to shift left. A negative value indicates a leftward shift. Defaults to -1."
            },
            "notes": [
                "The function assumes the input tensor `x` has a batch dimension at axis 0 and a sequence dimension at axis 1.",
                "This operation is primarily intended for leftward shifts (negative `shift` value)."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/multi_token_prediction.py#MultiTokenPredictionLayer",
        "file_path": "src/MaxText/layers/multi_token_prediction.py",
        "code_block": "class MultiTokenPredictionLayer(nn.Module):\n  \"\"\"\n  Implements Multi-Token Prediction (MTP) step:\n      1. Normalization of previous hidden state and target token embedding.\n      2. Concatenation and Projection of normalized features.\n      3. Processing through a Transformer Decoder Layer.\n\n      Equation Representation (Conceptual):\n          norm_h = RMSNorm(h_prev)\n          norm_e = RMSNorm(e_target)\n          h_proj = W_p(concat(norm_h, norm_e))\n          h_next = TransformerLayer(h_proj, pos_ids, segment_ids, ...)\n\n      It takes the previous hidden state and target embedding as input and outputs the\n      processed hidden state from its internal transformer block.\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n  layer_number: int\n  transformer_layer_module: Type[DecoderLayer] = DecoderLayer\n\n  @nn.compact\n  def __call__(\n      self,\n      prev_hidden_state: jnp.ndarray,\n      target_token_embedding: jnp.ndarray,\n      position_ids: jnp.ndarray,\n      decoder_segment_ids: None | jnp.ndarray,\n      deterministic: bool,\n      model_mode: str = MODEL_MODE_TRAIN,\n  ) -> jnp.ndarray:\n    \"\"\"\n    Applies the MTP combination, projection, and internal transformer processing.\n\n    Args:\n        prev_hidden_state: Hidden state from the previous step/layer.\n                           Shape: [batch, seq_len, hidden_size]\n        target_token_embedding: Embedding of the target token. In the context of MTP,\n                                this often refers to a token at a position relative\n                                to the current step, where the offset is determined\n                                by the layer number `k` (i.e., token t+k).\n                                Shape: [batch, seq_len, embed_dim]\n        position_ids: Original position IDs for the sequence.\n                      Shape: [batch, seq_len]\n        decoder_segment_ids: Original segment IDs for the sequence (for attention mask).\n                             Shape: [batch, seq_len]\n        deterministic: If true, disable dropout.\n        model_mode: The current operational mode (train, eval, decode).\n\n    Returns:\n        next_hidden_state: The hidden state produced by this MTP step's internal transformer.\n                           Shape: [batch, seq_len, hidden_size]\n    \"\"\"\n    cfg = self.config\n    mesh = self.mesh\n    k = self.layer_number\n\n    # --- 1. Normalize Hidden State and Embedding ---\n    embedding_norm_layer = rms_norm(\n        num_features=target_token_embedding.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=f\"mtp_{k}_embedding_norm\",\n        epsilon=cfg.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n    )\n    embedding_norm = embedding_norm_layer(target_token_embedding)\n\n    hidden_state_norm_layer = rms_norm(\n        num_features=prev_hidden_state.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=f\"mtp_{k}_hidden_state_norm\",\n        epsilon=cfg.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n    )\n\n    hidden_state_norm = hidden_state_norm_layer(prev_hidden_state)\n\n    # --- 2. Concatenate Normalized Representations ---\n    # Shape: [B, S, 2*H]\n    concatenated_features = jnp.concatenate([embedding_norm, hidden_state_norm], axis=-1)\n\n    # --- 3. Project Concatenated Features ---\n    # Projects from 2*H back down to H\n    projection_layer = dense_general(\n        inputs_shape=concatenated_features.shape,\n        out_features_shape=cfg.base_emb_dim,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        use_bias=False,\n        kernel_axes=(\"concat_embed\", \"embed\"),\n        name=f\"mtp_{k}_projection\",\n    )\n    # Shape: [B, S, H]\n    projected_features = projection_layer(concatenated_features)\n\n    # --- 4. Pass through MTP Transformer Block ---\n    output = self.transformer_layer_module(\n        config=cfg, mesh=mesh, model_mode=model_mode, name=f\"mtp_{k}_transformer_layer\"\n    )(\n        inputs=projected_features,\n        decoder_segment_ids=decoder_segment_ids,\n        decoder_positions=position_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n\n    if isinstance(output, tuple):\n      # Handles the scan=True case, where the output is a tuple.\n      next_hidden_state = output[0]\n    else:\n      # Handles the scan=False case, where the output is a single tensor.\n      next_hidden_state = output\n\n    # Shape: [B, S, H]\n    # --- Return Processed Hidden State ---\n    return next_hidden_state",
        "analysis": {
            "module_type": "multi_token_prediction_layer",
            "purpose": "Implements a single step of the Multi-Token Prediction (MTP) mechanism by normalizing, combining, and projecting a previous hidden state with a target token's embedding, then processing the result through a transformer decoder layer.",
            "input": {
                "shape": "prev_hidden_state: [batch, seq_len, hidden_size], target_token_embedding: [batch, seq_len, embed_dim]",
                "dtype": "jnp.ndarray (float)"
            },
            "processing_steps": [
                "Normalize the previous hidden state and the target token embedding using separate RMSNorm layers.",
                "Concatenate the two normalized tensors along the feature dimension.",
                "Apply a dense projection layer to map the concatenated feature dimension back to the model's hidden size.",
                "Process the projected features through an internal transformer decoder layer (`DecoderLayer`).",
                "Return the resulting hidden state from the transformer layer."
            ],
            "output": {
                "shape": "[batch, seq_len, hidden_size]"
            },
            "dependencies": [
                "flax.linen.Module",
                "DecoderLayer",
                "rms_norm",
                "dense_general",
                "jax.numpy.concatenate"
            ],
            "parameters": {
                "config": "Configuration object containing model hyperparameters like dtype, embedding dimensions, and normalization epsilon.",
                "mesh": "JAX sharding mesh for distributed computation.",
                "layer_number": "The sequential index (k) of this MTP layer, used for naming sub-modules.",
                "transformer_layer_module": "The class type for the internal transformer layer, defaulting to `DecoderLayer`."
            },
            "notes": [
                "This layer represents a single 'head' in the MTP chain, responsible for predicting the k-th future token.",
                "The internal transformer layer performs self-attention on the combined representation of the previous state and the target token embedding.",
                "It handles different output formats from the transformer layer (tuple vs. tensor) to support usage with `jax.lax.scan`."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Applies the MTP combination, projection, and internal transformer processing to generate the next hidden state.",
                    "input": {
                        "shape": "prev_hidden_state: [batch, seq_len, hidden_size], target_token_embedding: [batch, seq_len, embed_dim], position_ids: [batch, seq_len], decoder_segment_ids: [batch, seq_len]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Apply RMSNorm to `target_token_embedding`.",
                        "Apply RMSNorm to `prev_hidden_state`.",
                        "Concatenate the normalized hidden state and embedding using `jnp.concatenate`.",
                        "Project the concatenated features back to the hidden dimension using a `dense_general` layer.",
                        "Pass the projected features through the internal `transformer_layer_module`.",
                        "Extract the hidden state from the transformer layer's output, handling both tuple and tensor return types."
                    ],
                    "output": {
                        "shape": "[batch, seq_len, hidden_size]"
                    },
                    "dependencies": [
                        "rms_norm",
                        "dense_general",
                        "DecoderLayer"
                    ],
                    "notes": [
                        "The `target_token_embedding` corresponds to the embedding of a future token (e.g., at position t+k).",
                        "The `deterministic` flag controls whether dropout is enabled in the internal transformer layer."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/multi_token_prediction.py#MultiTokenPredictionBlock",
        "file_path": "src/MaxText/layers/multi_token_prediction.py",
        "code_block": "class MultiTokenPredictionBlock(nn.Module):\n  \"\"\"Orchestrates the MTP process by running a sequence of MTP layers.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  transformer_layer_module: Type[DecoderLayer]\n  decoder: Decoder\n\n  @nn.compact\n  def __call__(\n      self,\n      shared_embedding,\n      main_hidden_state,\n      input_ids,\n      target_ids,\n      target_mask,\n      position_ids,\n      decoder_segment_ids,\n      deterministic,\n  ):\n    cfg = self.config\n    # The initial hidden state for the MTP chain is the raw output from the main model.\n    mtp_hidden_state = main_hidden_state\n\n    # These variables are updated sequentially in each loop iteration,\n    # moving the prediction window one token to the right each time.\n    rolled_input_ids = input_ids\n    rolled_target_ids = target_ids\n    rolled_target_mask = target_mask\n    rolled_position_id = position_ids\n\n    # Range chosen to align with the naming convention of the paper\n    for k in range(1, cfg.mtp_num_layers + 1):\n      # Sequentially roll all tensors to prepare data for predicting the k-th future token.\n      rolled_input_ids = roll_and_mask(rolled_input_ids)\n      rolled_target_ids = roll_and_mask(rolled_target_ids)\n      rolled_target_mask = roll_and_mask(rolled_target_mask)\n      rolled_position_id = roll_and_mask(rolled_position_id)\n\n      # Embed the k-th future input tokens using the shared embedding module\n      target_token_embedding = self.decoder._apply_embedding(\n          shared_embedding, rolled_input_ids, rolled_position_id, deterministic, self.decoder.model_mode\n      )\n\n      # Instantiate and apply the MTP layer for this step\n      mtp_layer = MultiTokenPredictionLayer(\n          config=cfg,\n          mesh=self.mesh,\n          layer_number=k,\n          name=f\"mtp_layer_{k}\",\n          transformer_layer_module=self.transformer_layer_module,\n      )\n\n      next_mtp_hidden_state = mtp_layer(\n          mtp_hidden_state,\n          target_token_embedding,\n          position_ids,\n          decoder_segment_ids,\n          deterministic,\n          self.decoder.model_mode,\n      )\n\n      # Project to logits using the shared embedding transpose\n      mtp_logits = self.decoder._apply_output_head(shared_embedding, next_mtp_hidden_state, deterministic)\n\n      # Calculate cross-entropy loss for this specific layer's prediction\n      mtp_xent, _ = max_utils.cross_entropy_with_logits(\n          mtp_logits, jax.nn.one_hot(rolled_target_ids, cfg.vocab_size), 0.0\n      )\n      mtp_xent_masked = mtp_xent * rolled_target_mask\n\n      # This logic doesn't run during model initialization to avoid unwated population of the mutable collections.\n      if not self.is_initializing():\n        # For evaluation, save the top prediction and a valid token mask.\n        # This is only active for the target layer during an eval run.\n        if cfg.mtp_eval_target_module == k and self.is_mutable_collection(\"mtp_acceptance\"):\n          mtp_top_1_pred = jnp.argmax(mtp_logits, axis=-1)\n          self.sow(\"mtp_acceptance\", \"mtp_preds\", mtp_top_1_pred)\n          self.sow(\"mtp_acceptance\", \"mtp_mask\", rolled_target_mask)\n\n        # For training, save the loss components for this MTP head.\n        # This is only active during a training run.\n        if self.is_mutable_collection(\"mtp_losses\"):\n          self.sow(\"mtp_losses\", \"losses\", jnp.sum(mtp_xent_masked))\n          self.sow(\"mtp_losses\", \"weights\", jnp.sum(rolled_target_mask))\n\n      # The output of this layer is the input for the next, maintaining the causal chain.\n      mtp_hidden_state = next_mtp_hidden_state",
        "analysis": {
            "module_type": "multi_token_prediction_block",
            "purpose": "Orchestrates the Multi-Token Prediction (MTP) process by iteratively running a sequence of MTP layers to predict multiple future tokens.",
            "input": {
                "shape": "N/A (Inputs are passed to the __call__ method).",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes the MTP hidden state with the main model's hidden state.",
                "Iterates from 1 to `config.mtp_num_layers`.",
                "In each iteration, it rolls input tensors to align them for the next token prediction.",
                "Embeds the rolled input tokens.",
                "Passes the current hidden state and new embedding through a `MultiTokenPredictionLayer`.",
                "Calculates logits and cross-entropy loss for the current prediction step.",
                "Saves losses (for training) or predictions (for evaluation) to mutable collections.",
                "Updates the hidden state for the next iteration."
            ],
            "output": {
                "shape": "This module does not return a tensor. It populates mutable collections ('mtp_losses', 'mtp_acceptance') with intermediate results via `self.sow()`."
            },
            "dependencies": [
                "flax.linen.Module",
                "MultiTokenPredictionLayer",
                "Decoder",
                "DecoderLayer",
                "roll_and_mask",
                "max_utils.cross_entropy_with_logits"
            ],
            "parameters": {
                "config.mtp_num_layers": "The number of future tokens to predict, which corresponds to the number of MTP layers to run.",
                "config.mtp_eval_target_module": "Specifies which MTP layer's predictions to save for evaluation.",
                "config.vocab_size": "The size of the vocabulary for one-hot encoding targets."
            },
            "notes": [
                "The module operates as a sequential chain, where the output hidden state of one MTP layer becomes the input for the next.",
                "It uses Flax's `sow` mechanism to store intermediate losses and predictions, which are then collected and processed by downstream functions like `calculate_mtp_loss`."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Executes the multi-token prediction loop, calculating and storing losses and predictions for each future token step.",
                    "input": {
                        "shape": "shared_embedding: [vocab_size, hidden_dim], main_hidden_state: [batch_size, sequence_length, hidden_dim], input_ids/target_ids/target_mask/position_ids/decoder_segment_ids: [batch_size, sequence_length]",
                        "dtype": "float32 for embeddings and hidden states, int32 for IDs and masks."
                    },
                    "processing_steps": [
                        "Initialize `mtp_hidden_state` with `main_hidden_state`.",
                        "Loop `k` from 1 to `cfg.mtp_num_layers`.",
                        "Apply `roll_and_mask` to shift input, target, mask, and position tensors for the k-th prediction.",
                        "Embed the rolled input IDs via `self.decoder._apply_embedding`.",
                        "Instantiate and apply `MultiTokenPredictionLayer` to get `next_mtp_hidden_state`.",
                        "Project `next_mtp_hidden_state` to logits via `self.decoder._apply_output_head`.",
                        "Calculate cross-entropy loss using `max_utils.cross_entropy_with_logits`.",
                        "If not initializing, conditionally `sow` losses into 'mtp_losses' collection during training.",
                        "If not initializing, conditionally `sow` predictions and mask into 'mtp_acceptance' collection during evaluation for the target layer.",
                        "Update `mtp_hidden_state` to `next_mtp_hidden_state` for the next loop iteration."
                    ],
                    "output": {
                        "shape": "N/A. The method does not return a value but has side effects of populating mutable collections."
                    },
                    "dependencies": [
                        "roll_and_mask",
                        "MultiTokenPredictionLayer",
                        "Decoder._apply_embedding",
                        "Decoder._apply_output_head",
                        "max_utils.cross_entropy_with_logits",
                        "jax.nn.one_hot",
                        "jnp.argmax",
                        "self.sow"
                    ],
                    "notes": [
                        "The logic to `sow` variables is conditional on the model's operational state (training vs. evaluation) and is skipped during model initialization."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/multi_token_prediction.py#calculate_mtp_loss",
        "file_path": "src/MaxText/layers/multi_token_prediction.py",
        "code_block": "def calculate_mtp_loss(intermediate_outputs, config):\n  \"\"\"Calculates the Multi Token Prediction loss from intermediate outputs.\"\"\"\n  losses_path = (\"mtp_losses\", \"mtp_block\", \"losses\")\n  weights_path = (\"mtp_losses\", \"mtp_block\", \"weights\")\n\n  mtp_losses = maxtext_utils.get_nested_value(intermediate_outputs, losses_path, default=())\n  mtp_weights = maxtext_utils.get_nested_value(intermediate_outputs, weights_path, default=())\n\n  if not mtp_losses:  # MTP heads did not run\n    return 0.0\n\n  sum_of_all_mtp_losses = jnp.sum(jnp.array(mtp_losses))\n  sum_of_all_mtp_weights = jnp.sum(jnp.array(mtp_weights))\n\n  avg_mtp_loss = sum_of_all_mtp_losses / (sum_of_all_mtp_weights + EPS)\n  scaled_mtp_loss = avg_mtp_loss * config.mtp_loss_scaling_factor\n  return scaled_mtp_loss",
        "analysis": {
            "module_type": "loss_calculation_function",
            "purpose": "Calculates the scaled Multi-Token Prediction (MTP) loss from intermediate model outputs generated during training.",
            "input": {
                "shape": "intermediate_outputs: A nested dictionary containing model outputs. config: A configuration object.",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Extract the list of MTP losses and weights from the `intermediate_outputs` dictionary using predefined paths.",
                "If no MTP losses are found, return 0.0.",
                "Sum all extracted MTP losses and weights.",
                "Calculate the average MTP loss by dividing the total loss by the total weight, adding a small epsilon for numerical stability.",
                "Scale the average MTP loss by the `mtp_loss_scaling_factor` from the config.",
                "Return the final scaled MTP loss."
            ],
            "output": {
                "shape": "A scalar value representing the final loss."
            },
            "dependencies": [
                "maxtext_utils.get_nested_value",
                "jax.numpy.sum",
                "jax.numpy.array",
                "MaxText.globals.EPS"
            ],
            "parameters": {
                "config.mtp_loss_scaling_factor": "A float value used to scale the final calculated average MTP loss."
            },
            "notes": [
                "This function is designed to work with the outputs 'sown' by the `MultiTokenPredictionBlock` during training.",
                "It gracefully handles cases where MTP heads are not active (e.g., during standard training or evaluation without MTP) by returning a loss of 0.0."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/multi_token_prediction.py#calculate_mtp_acceptance_rate",
        "file_path": "src/MaxText/layers/multi_token_prediction.py",
        "code_block": "def calculate_mtp_acceptance_rate(intermediate_outputs, config):\n  \"\"\"Calculates the MTP acceptance rate from intermediate outputs.\"\"\"\n\n  sown_data = maxtext_utils.get_nested_value(intermediate_outputs, (\"mtp_acceptance\", \"mtp_block\"), {})\n  mtp_preds = maxtext_utils.get_nested_value(sown_data, (\"mtp_preds\",), [None])[0]\n  valid_mask = maxtext_utils.get_nested_value(sown_data, (\"mtp_mask\",), [None])[0]\n\n  # These values are only \"sown\" (saved) during an evaluation run and only for the specific\n  # MTP layer specified by `config.mtp_eval_target_module`. This check handles cases\n  # where the required data is absent (e.g., during a training step) and prevents errors.\n  if mtp_preds is None or valid_mask is None:\n    return 0.0\n\n  # Get the main model's greedy predictions from the logits.\n  main_model_preds = jnp.argmax(intermediate_outputs[\"logits\"], axis=-1)\n\n  # Roll the main model's predictions to align them in time with the MTP head's target.\n  rolled_main_preds = main_model_preds\n  for _ in range(config.mtp_eval_target_module):\n    rolled_main_preds = roll_and_mask(rolled_main_preds)\n\n  # Compare the aligned predictions. The `valid_mask` ensures that the comparison\n  # only happens on valid tokens, ignoring the placeholder values introduced at the\n  # end of the sequence by the `roll_and_mask` operation.\n  correct_predictions = jnp.sum((mtp_preds == rolled_main_preds) * valid_mask)\n  total_valid_tokens = jnp.sum(valid_mask)\n\n  # Return acceptance rate as a percentage\n  return (correct_predictions / (total_valid_tokens + EPS)) * 100",
        "analysis": {
            "functionality": "Calculates the Multi-Token Prediction (MTP) acceptance rate, which measures the percentage of agreement between a specific MTP head's prediction and the main model's greedy prediction for the same future token.",
            "usage": "This function is used during evaluation. It takes the intermediate outputs from a model step (a dictionary containing logits and sown MTP data) and a configuration object. It returns a single float value representing the acceptance rate as a percentage. If the required MTP data is not present in the intermediate outputs (e.g., during a training step), it returns 0.0."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#is_vanilla_variable",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def is_vanilla_variable(vs: variablelib.VariableState) -> bool:\n  \"\"\"A variables state is vanilla if its metadata is essentially blank.\n\n  Returns False only if it has non-empty hooks or any non-built-in attribute.\n  \"\"\"\n  for key, value in vs.get_metadata().items():\n    if key.endswith(\"_hooks\"):\n      if value != ():\n        return False\n    else:\n      return False\n  return True",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Checks if a `VariableState` object has default or empty metadata, returning True if it's 'vanilla' and False otherwise.",
            "input": {
                "shape": "N/A",
                "dtype": "flax.nnx.variablelib.VariableState"
            },
            "processing_steps": [
                "Get metadata from the input VariableState object using `vs.get_metadata()`.",
                "Iterate through the metadata items (key-value pairs).",
                "If a key ends with '_hooks', check if its value is an empty tuple. If not, return False.",
                "If a key does not end with '_hooks', return False immediately.",
                "If the loop completes, return True."
            ],
            "output": {
                "shape": "N/A",
                "dtype": "bool"
            },
            "dependencies": [
                "flax.nnx.variablelib.VariableState"
            ],
            "parameters": {},
            "notes": [
                "A 'vanilla' variable state is defined as one whose metadata is either empty or only contains keys ending in '_hooks' with empty tuple values.",
                "The function returns False as soon as it finds any non-vanilla attribute."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#to_linen_var",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def to_linen_var(vs: variablelib.VariableState) -> meta.AxisMetadata:\n  metadata = vs.get_metadata()\n  if \"linen_meta_type\" in metadata:\n    linen_type = metadata[\"linen_meta_type\"]\n    if hasattr(linen_type, \"from_nnx_metadata\"):\n      return linen_type.from_nnx_metadata({\"value\": vs.value, **metadata})\n    return linen_type(vs.value, **metadata)\n  if is_vanilla_variable(vs):\n    return vs.value\n  return nnx.bridge.NNXMeta(vs.type, vs.value, metadata)",
        "analysis": {
            "functionality": "Converts an NNX `VariableState` object into a Linen-compatible variable representation. It handles different cases based on the metadata associated with the variable state.",
            "usage": "Call this function with an `nnx.variablelib.VariableState` instance as input. It returns a representation suitable for use in a Linen module's variable dictionary. The output can be a raw value (e.g., a JAX array), a `flax.core.meta.AxisMetadata` instance, or a special `nnx.bridge.NNXMeta` wrapper object, depending on the input's metadata."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#get_col_name",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def get_col_name(keypath: tp.Sequence[Any]) -> str:\n  \"\"\"Given the keypath of a Flax variable type, return its Linen collection name.\"\"\"\n  # Infer variable type from the leaf's path, which contains its Linen collection name\n  assert isinstance(keypath[0], jax.tree_util.DictKey)\n  return str(keypath[0].key)",
        "analysis": {
            "module_type": "keypath_parser",
            "purpose": "Given the keypath of a Flax variable from a JAX tree traversal, this function extracts and returns its top-level Linen collection name as a string.",
            "input": {
                "shape": "A sequence where the first element is a `jax.tree_util.DictKey`.",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Assert that the first element of the `keypath` is a `jax.tree_util.DictKey`.",
                "Access the `key` attribute from the first element of the `keypath`.",
                "Convert the key to a string and return it."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "jax.tree_util.DictKey"
            ],
            "parameters": {},
            "notes": [
                "The function assumes the keypath originates from a nested dictionary structure where the top-level keys correspond to Linen collection names (e.g., 'params', 'batch_stats').",
                "It is used within `linen_vars_to_nnx_attrs` to determine the variable type from its path in the variable tree."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#to_nnx_var",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def to_nnx_var(col: str, x: meta.AxisMetadata | Any) -> variablelib.Variable:\n  \"\"\"Convert a Linen variable to an NNX variable.\"\"\"\n  vtype = variablelib.variable_type_from_name(col, allow_register=True)\n  if isinstance(x, nnx.bridge.NNXMeta):\n    assert vtype == x.var_type, f\"Type stored in NNXMeta {x.var_type} != type inferred from collection name {vtype}\"\n    return x.to_nnx_variable()\n  if isinstance(x, meta.AxisMetadata):\n    x_metadata = vars(x)\n    if hasattr(x, \"to_nnx_metadata\"):\n      x_metadata = x.to_nnx_metadata()\n    assert hasattr(x, \"value\")\n    return vtype(**x_metadata, linen_meta_type=type(x))\n  return vtype(x)",
        "analysis": {
            "module_type": "linen_to_nnx_variable_converter",
            "purpose": "Converts a single Linen-style variable representation into an NNX Variable object.",
            "input": {
                "shape": "N/A",
                "dtype": "col: str, x: flax.core.meta.AxisMetadata | flax.nnx.bridge.NNXMeta | Any"
            },
            "processing_steps": [
                "Determine the NNX variable type from the input collection name string 'col' using `variablelib.variable_type_from_name`.",
                "If the input 'x' is an `nnx.bridge.NNXMeta` instance, assert type consistency and convert it to an NNX variable using its `to_nnx_variable` method.",
                "If 'x' is a `flax.core.meta.AxisMetadata` instance, extract its value and metadata (potentially calling `to_nnx_metadata` if available) to create a new NNX variable of the determined type.",
                "If 'x' is any other type, treat it as a raw value and wrap it in the determined NNX variable type."
            ],
            "output": {
                "shape": "Returns a flax.nnx.variablelib.Variable object. The shape of the internal value matches the input value's shape."
            },
            "dependencies": [
                "flax.nnx.variablelib",
                "flax.core.meta",
                "flax.nnx.bridge.NNXMeta"
            ],
            "parameters": {
                "col": "A string representing the Linen variable collection name (e.g., 'params', 'batch_stats').",
                "x": "The Linen variable representation, which can be a raw value, an AxisMetadata object, or an NNXMeta object."
            },
            "notes": [
                "The function handles three distinct types of input for 'x', providing a bridge between different ways Linen represents variables.",
                "It includes an assertion to ensure the variable type inferred from the collection name matches the type stored within an NNXMeta object, if provided.",
                "When converting from AxisMetadata, it preserves the metadata by passing it to the NNX Variable constructor and also stores the original Linen metadata type."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#_recursive_merge",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def _recursive_merge(dict1, dict2):\n  \"\"\"Recursively merge two dicts.\"\"\"\n  flat_map = nnx.traversals.flatten_mapping(dict1)\n  flat_map |= nnx.traversals.flatten_mapping(dict2)\n  return nnx.traversals.unflatten_mapping(flat_map)",
        "analysis": {
            "functionality": "Recursively merges two dictionaries, with values from the second dictionary overwriting values from the first in case of conflicting keys.",
            "usage": "Call the function with two dictionaries, `dict1` and `dict2`. It returns a new dictionary representing the merged result. The function handles nested dictionaries by flattening them, merging, and then unflattening the result using `nnx.traversals`."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#linen_vars_to_nnx_attrs",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def linen_vars_to_nnx_attrs(variables: tp.Mapping[str, Any]) -> dict[str, Any]:\n  \"\"\"Convert a dict of Linen-style variables to NNX variables.\"\"\"\n  nnx_vars = jax.tree_util.tree_map_with_path(\n      lambda kp, x: to_nnx_var(get_col_name(kp), x),\n      variables,\n      is_leaf=lambda x: not isinstance(x, dict),\n  )\n\n  flat_paths: dict[tuple, tp.Any] = {}\n\n  for col_name, col_variables in nnx_vars.items():  # pylint: disable=unused-variable\n    for path, variable in nnx.traversals.flatten_mapping(col_variables).items():\n      if path in flat_paths:\n        raise ValueError(\n            f\"Found duplicate variable path {path} with variables \"\n            f\"{flat_paths[path]} and {variable}. \"\n            \"This is not allowed in NNX.\"\n        )\n      flat_paths[path] = variable\n\n  nnx_vars = nnx.traversals.unflatten_mapping(flat_paths)\n  return nnx_vars",
        "analysis": {
            "module_type": "linen_to_nnx_variable_converter",
            "purpose": "Converts a nested dictionary of Flax Linen-style variables into a nested dictionary of Flax NNX attributes containing NNX Variable objects, ensuring no duplicate variable paths exist.",
            "input": {
                "shape": "A nested dictionary representing Linen variables, typically `{'collection_name': {'module_name': {'param_name': value}}}`.",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Map over the input Linen variables dictionary using `jax.tree_util.tree_map_with_path`.",
                "For each leaf, determine its collection name using `get_col_name` and convert it to an NNX `Variable` using `to_nnx_var`.",
                "Flatten the resulting nested dictionary of NNX variables from all collections into a single flat dictionary mapping paths to variables.",
                "Check for duplicate variable paths across different collections, raising a `ValueError` if any are found.",
                "Unflatten the merged flat dictionary back into a nested dictionary structure using `nnx.traversals.unflatten_mapping`.",
                "Return the final nested dictionary of NNX attributes."
            ],
            "output": {
                "shape": "A nested dictionary representing NNX attributes, where leaves are NNX `Variable` objects."
            },
            "dependencies": [
                "jax.tree_util.tree_map_with_path",
                "nnx.traversals.flatten_mapping",
                "nnx.traversals.unflatten_mapping",
                "to_nnx_var",
                "get_col_name"
            ],
            "parameters": {
                "variables": "A mapping (e.g., a dictionary) representing Flax Linen variables, where keys are collection names (like 'params') and values are nested dictionaries of parameters."
            },
            "notes": [
                "This function is a key part of the interoperability bridge between Flax Linen and Flax NNX.",
                "It enforces the NNX constraint that all variable paths within a module must be unique, even if they belong to different Linen collections (e.g., 'params' and 'batch_stats')."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#nnx_attrs_to_linen_vars",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def nnx_attrs_to_linen_vars(nnx_attrs: dict) -> dict:\n  \"\"\"Convert a dict of NNX variables (or variable states) to Linen-style variables.\"\"\"\n  linen_structured = {}\n  for kp, v in nnx.traversals.flatten_mapping(nnx_attrs).items():\n    if isinstance(v, variablelib.Variable):\n      col_name = variablelib.variable_name_from_type(type(v))\n      v = to_linen_var(v.to_state())\n    elif isinstance(v, variablelib.VariableState):\n      col_name = variablelib.variable_name_from_type(v.type)\n      v = to_linen_var(v)\n    else:\n      raise ValueError(f\"Cannot infer collection name from value: {v}\")\n    linen_structured[(col_name, *kp)] = v\n  variables = nnx.traversals.unflatten_mapping(linen_structured)\n  return variables",
        "analysis": {
            "functionality": "Converts a dictionary of Flax NNX variables or variable states into a Flax Linen-style variable dictionary.",
            "usage": "Call this function with a dictionary of NNX attributes (e.g., from `nnx.state(module)`) to get a Linen-compatible variable dictionary. The output dictionary will be structured with top-level keys representing variable collections like 'params'."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#_set_initializing",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def _set_initializing(module: Module, initializing: bool):\n  for _, value in graph.iter_graph(module):\n    if isinstance(value, Pytree):\n      value._object__state._initializing = initializing",
        "analysis": {
            "functionality": "Recursively traverses an NNX Module's graph and sets the `_initializing` flag on all `Pytree` objects found within it.",
            "usage": "Call this function with an `nnx.Module` instance and a boolean value. It modifies the module in-place to either enter or exit an initialization state. It does not return any value."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#lazy_init",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def lazy_init(fn: Module | tp.Callable[..., tp.Any], *args, **kwargs):\n  \"\"\"To run through an arbitrary nnx.Module method and initialize all its needed state.\n\n  Here used to trigger initialization of all `LinenToNNX` module variables.\"\"\"\n  if isinstance(fn, Module):\n    module = fn\n    assert callable(fn)\n  else:\n    if not (hasattr(fn, \"__self__\") and isinstance(fn.__self__, Module)):\n      raise ValueError(f\"{fn = } needs to be a method of an NNX Module.\")\n    module = fn.__self__\n  _set_initializing(module, True)\n  try:\n    _ = fn(*args, **kwargs)\n  finally:\n    _set_initializing(module, False)\n  return fn",
        "analysis": {
            "module_type": "lazy_module_initializer",
            "purpose": "Executes a method of an `nnx.Module` in an 'initializing' mode to trigger the creation and setup of its state, particularly for `LinenToNNX` wrappers.",
            "input": {
                "shape": "The first argument `fn` is a callable (nnx.Module or its method). Subsequent `*args` and `**kwargs` are the inputs to this callable.",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Identify the nnx.Module instance from the input function `fn`.",
                "Set an internal '_initializing' flag to True for the module and all its submodules using `_set_initializing`.",
                "Execute the input function `fn` with the provided `*args` and `**kwargs`.",
                "Reset the '_initializing' flag to False for the module and its submodules in a `finally` block to ensure it runs even on error.",
                "Return the original input function `fn`."
            ],
            "output": {
                "shape": "Returns the original callable `fn` that was passed as input."
            },
            "dependencies": [
                "flax.nnx.module.Module",
                "_set_initializing"
            ],
            "parameters": {},
            "notes": [
                "The return value of the executed function `fn(*args, **kwargs)` is discarded; the function's primary side effect is state initialization.",
                "This is a key utility for the `ToNNX` bridge class to initialize Linen modules that require a sample input.",
                "The use of a `try...finally` block ensures the module's state is correctly reset even if the initialization call fails."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#current_linen_module",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def current_linen_module() -> linen.Module | None:\n  \"\"\"Get the current Linen module from the Linen context.\"\"\"\n  if linen.module._context.module_stack:  # pylint: disable=W0212\n    return linen.module._context.module_stack[-1]  # pylint: disable=W0212\n  return None",
        "analysis": {
            "module_type": "context_utility",
            "purpose": "Retrieves the currently active Flax Linen module from its internal context stack.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check if the internal `linen.module._context.module_stack` is non-empty.",
                "If the stack is non-empty, return the top module (the last element).",
                "If the stack is empty, return None."
            ],
            "output": {
                "shape": "An instance of `linen.Module` or `None`. Shape is not applicable."
            },
            "dependencies": [
                "flax.linen"
            ],
            "parameters": {},
            "notes": [
                "This function accesses a protected member (`_context.module_stack`) of the `flax.linen.module` to get the current module, as indicated by the `pylint: disable=W0212` comments.",
                "It returns `None` if no Linen module is currently active in the context."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#ToNNX",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "class ToNNX(Module):\n  \"\"\"A wrapper to turn any Linen module into an NNX module.\n\n  The result NNX module can be used standalone with all NNX APIs, or as a submodule of\n  another NNX module.\n\n  Since Linen module initialization requires a sample input, you need to call `lazy_init`\n  with an argument to initialize the variables.\n\n  Example::\n\n    >>> from flax import linen as nn, nnx\n    >>> import jax\n    >>> linen_module = nn.Dense(features=64)\n    >>> x = jax.numpy.ones((1, 32))\n    >>> # Like Linen init(), initialize with a sample input\n    >>> model = nnx.bridge.ToNNX(linen_module, rngs=nnx.Rngs(0)).lazy_init(x)\n    >>> # Like Linen apply(), but using NNX's direct call method\n    >>> y = model(x)\n    >>> model.kernel.shape\n    (32, 64)\n\n  Args:\n    module: The Linen Module instance.\n    rngs: The `nnx.Rngs` instance being passed to any NNX module.\n\n  Returns:\n    A stateful NNX module that behaves the same as the wrapped Linen module.\n  \"\"\"\n\n  def __init__(\n      self,\n      module: linen.Module,\n      rngs: Rngs | jax.Array | None = None,\n  ):\n    self.to_nnx__module = module\n\n    self.to_nnx__rngs: Rngs | None\n    if isinstance(rngs, jax.Array):\n      self.to_nnx__rngs = Rngs(params=rngs)\n    elif isinstance(rngs, nnx.Rngs):\n      self.to_nnx__rngs = rngs.fork() if hasattr(type(rngs), \"fork\") else nnx.clone(rngs)  # type: ignore\n    else:\n      self.to_nnx__rngs = rngs\n\n  def lazy_init(self, *args, **kwargs):\n    \"\"\"A shortcut of calling `nnx.bridge.lazy_init()` upon this module.\"\"\"\n    return lazy_init(self, *args, **kwargs)\n\n  def __getattr__(self, name: str):\n    if hasattr(super(), name):\n      return super().__getattribute__(name)\n    maybe_method = getattr(type(self.to_nnx__module), name, None)\n    if callable(maybe_method):\n      method = partial(self.__call__, method=maybe_method)\n      method.__self__ = self\n      return method\n    return super().__getattribute__(name)\n\n  def __call__(\n      self,\n      *args: Any,\n      rngs: Rngs | jax.Array | None = None,\n      method: tp.Callable[..., Any] | str | None = None,\n      mutable: tp.Any = None,\n      **kwargs: Any,\n  ) -> Any:\n    # Shape-based lazy init of the flax variables\n    if rngs is None:\n      rngs = self.to_nnx__rngs\n    if isinstance(rngs, nnx.Rngs):\n      _rngs = {name: stream() for name, stream in rngs.items()}\n    elif isinstance(rngs, jax.Array):\n      _rngs = {\"params\": rngs}\n    else:\n      _rngs = {}\n    # rename default to params\n    if \"params\" not in _rngs and \"default\" in _rngs:\n      _rngs[\"params\"] = _rngs.pop(\"default\")\n    if self._object__state.initializing:\n      out, updates = self.to_nnx__module.init_with_output(_rngs, *args, method=method, **kwargs)\n    else:\n      nnx_attrs = {\n          k: v\n          for k, v in vars(self).items()\n          if not k.startswith(\"to_nnx__\") and not k.startswith(\"_pytree__\") and not k.startswith(\"_object__\")\n      }\n      variables = nnx_attrs_to_linen_vars(nnx_attrs)\n\n      # Get `mutable` from top level bridge.Module context if any\n      if mutable is not None:\n        pass\n      elif (m := bdg_module.current_module()) is not None:\n        assert m.scope is not None\n        mutable = m.scope.mutable\n      elif (m := current_linen_module()) is not None:\n        assert m.scope is not None\n        mutable = m.scope.mutable\n      else:\n        mutable = False\n\n      out = self.to_nnx__module.apply(variables, *args, rngs=_rngs, method=method, mutable=mutable, **kwargs)\n\n      # Split out the updates if `mutable` is passed into the Flax module\n      if mutable is not False:\n        out, updates = out\n      else:\n        updates = None\n\n    # Split out the updates if `mutable` is passed into the Flax module\n    if updates:\n      nnx_attrs = linen_vars_to_nnx_attrs(updates)\n      # nnx.update(self, nnx_attrs)\n      # TODO(cgarciae): ideally we just do an update but currently dictionaries don't allow\n      # insertion of new keys, we need to enable this in NNX to simplify the code below\n      # to the simple nnx.update(self, nnx_attrs) above.\n      for attr_name, value in nnx_attrs.items():\n        if hasattr(self, attr_name) and isinstance(value, dict):\n          original_value = getattr(self, attr_name)\n          new_values = _recursive_merge(original_value, value)\n          setattr(self, attr_name, nnx.data(new_values))\n        else:\n          setattr(self, attr_name, nnx.data(value))\n\n    return out",
        "analysis": {
            "module_type": "linen_to_nnx_wrapper",
            "purpose": "A wrapper to turn any Flax Linen module into a stateful Flax NNX module, making it compatible with the NNX API.",
            "input": {
                "shape": "Depends on the wrapped Linen module.",
                "dtype": "Depends on the wrapped Linen module."
            },
            "processing_steps": [
                "The class is instantiated with a `flax.linen.Module` instance.",
                "The `lazy_init` method is called with sample input data to initialize the Linen module's parameters and populate the NNX wrapper's state.",
                "The instance is called like a function, which triggers the `__call__` method to execute the forward pass of the wrapped Linen module.",
                "Method calls are dynamically forwarded to the underlying Linen module via `__getattr__`."
            ],
            "output": {
                "shape": "Depends on the wrapped Linen module."
            },
            "dependencies": [
                "flax.nnx.Module",
                "flax.linen.Module",
                "flax.nnx.Rngs",
                "lazy_init",
                "nnx_attrs_to_linen_vars",
                "linen_vars_to_nnx_attrs"
            ],
            "parameters": {
                "module": "The `flax.linen.Module` instance to be wrapped.",
                "rngs": "An `nnx.Rngs` instance, a JAX PRNGKey, or None, used for stochastic operations."
            },
            "notes": [
                "This module acts as a bridge, allowing a standard Linen module to be used within an NNX graph.",
                "Initialization is lazy and requires an explicit call to `lazy_init` with sample inputs, similar to how `linen.Module.init` works.",
                "The wrapper dynamically exposes methods from the underlying Linen module."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the wrapper with a Linen module and optional RNGs.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores the provided `linen.Module` instance.",
                        "Processes the `rngs` argument, converting it to an `nnx.Rngs` object if necessary."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "flax.linen.Module",
                        "flax.nnx.Rngs"
                    ],
                    "notes": [
                        "Internal attributes are prefixed with `to_nnx__` to prevent name collisions with the wrapped module's attributes."
                    ]
                },
                "lazy_init": {
                    "purpose": "A shortcut to initialize the module's state using sample inputs.",
                    "input": {
                        "shape": "Sample input tensors with the shape expected by the wrapped Linen module.",
                        "dtype": "Sample input tensors with the dtype expected by the wrapped Linen module."
                    },
                    "processing_steps": [
                        "Calls the external `lazy_init` bridge function on the current module instance (`self`)."
                    ],
                    "output": {
                        "shape": "Returns the initialized module instance itself."
                    },
                    "dependencies": [
                        "lazy_init"
                    ],
                    "notes": [
                        "This method must be called after instantiation and before the module is used for a forward pass."
                    ]
                },
                "__getattr__": {
                    "purpose": "Dynamically forwards method calls to the wrapped Linen module.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks if an attribute exists on the parent `nnx.Module`.",
                        "If not, it looks for a callable attribute (a method) on the wrapped Linen module's type.",
                        "If a method is found, it returns a partial function of `self.__call__` with the `method` argument pre-filled.",
                        "Otherwise, it falls back to the default attribute access."
                    ],
                    "output": {
                        "shape": "A callable method that will execute a specific method on the Linen module."
                    },
                    "dependencies": [
                        "functools.partial"
                    ],
                    "notes": [
                        "This enables calling `model.encode(x)` on the wrapper if the original Linen module has an `encode` method."
                    ]
                },
                "__call__": {
                    "purpose": "Executes either the initialization or the forward pass (`apply`) of the wrapped Linen module.",
                    "input": {
                        "shape": "Input tensors for the wrapped Linen module's method.",
                        "dtype": "Input dtypes for the wrapped Linen module's method."
                    },
                    "processing_steps": [
                        "Prepare RNGs for the Linen call.",
                        "If in initialization mode, call `module.init_with_output`, convert the resulting Linen variables to NNX attributes, and update the wrapper's state.",
                        "If in application mode, convert the wrapper's NNX attributes to Linen variables, call `module.apply`, and get the output.",
                        "If the call is `mutable`, receive updated state from `module.apply`.",
                        "If any state was updated, convert the new Linen variables back to NNX attributes and update the wrapper's state.",
                        "Return the output from the Linen module call."
                    ],
                    "output": {
                        "shape": "The output tensor(s) from the wrapped Linen module's method."
                    },
                    "dependencies": [
                        "linen.Module.init_with_output",
                        "linen.Module.apply",
                        "nnx_attrs_to_linen_vars",
                        "linen_vars_to_nnx_attrs",
                        "_recursive_merge"
                    ],
                    "notes": [
                        "This is the core method that bridges the execution and state management between NNX and Linen.",
                        "It has two distinct code paths depending on whether `self._object__state.initializing` is true."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#linen_rngs_dict",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def linen_rngs_dict(linen_module: linen.Module, add_default: bool = False):\n  \"\"\"Given a module, split out one of its every active RNG key collections.\"\"\"\n  assert linen_module.scope is not None, \"linen_rngs_dict() must be called inside a Linen module.\"\n  rngs: dict[str, tp.Any] = {name: linen_module.make_rng(name) for name in linen_module.scope.rngs.keys()}\n  if add_default and \"default\" not in rngs:\n    rngs[\"default\"] = 0\n  return rngs",
        "analysis": {
            "module_type": "linen_rng_utility",
            "purpose": "Generates a dictionary of JAX PRNGKeys by splitting the active RNG streams from a given Flax Linen module's scope.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Assert that the provided `linen_module` has an active scope.",
                "Create a dictionary by iterating over the RNG stream names in `linen_module.scope.rngs`.",
                "For each stream name, call `linen_module.make_rng(name)` to generate a new JAX PRNGKey.",
                "If `add_default` is True and a 'default' stream does not exist, add a 'default' key with a value of 0 to the dictionary.",
                "Return the dictionary of RNG keys."
            ],
            "output": {
                "shape": "A dictionary mapping RNG stream names (str) to JAX PRNGKeys or an integer (e.g., {'params': KeyArray, 'dropout': KeyArray})."
            },
            "dependencies": [
                "flax.linen.Module"
            ],
            "parameters": {
                "linen_module": "The Flax Linen module instance from which to extract RNGs. Must be called within its scope.",
                "add_default": "A boolean flag that, if True, adds a 'default' key with value 0 to the output dictionary if it's not already present."
            },
            "notes": [
                "This function must be called from within the context of a Linen module (e.g., inside a `setup` or `__call__` method), as it relies on `linen_module.scope` being active.",
                "The purpose is to prepare the `rngs` dictionary required by many Linen methods by splitting the main RNG keys into per-stream keys."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#_get_module_method",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def _get_module_method(module, method: tp.Callable[..., Any] | str | None):\n  \"\"\"Get a callable method from the module, or raise TypeError.\"\"\"\n  if method is None:\n    method = \"__call__\"\n\n  if isinstance(method, str):\n    attribute_name = method\n    method = getattr(type(module), attribute_name)\n    if not callable(method):\n      class_name = type(module).__name__\n      raise TypeError(f\"'{class_name}.{attribute_name}' must be a callable, got {type(method)}.\")\n  if not callable(method):\n    class_name = type(module).__name__\n    raise TypeError(f\"'{method}' must be a callable, got {type(method)}.\")\n\n  return method",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Resolves a method from a module, specified either as a callable, a string name, or None (defaults to '__call__'), and ensures it is callable.",
            "input": {
                "shape": "N/A",
                "dtype": "module: object, method: Callable | str | None"
            },
            "processing_steps": [
                "If the 'method' argument is None, default it to the string '__call__'.",
                "If 'method' is a string, retrieve the attribute with that name from the module's class.",
                "Check if the retrieved attribute is callable. If not, raise a TypeError.",
                "Perform a final check to ensure the resolved method is callable, raising a TypeError if it is not.",
                "Return the resolved callable method."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [],
            "parameters": {},
            "notes": [
                "This function retrieves the method from the module's class (type(module)) rather than the instance itself.",
                "It is designed to provide robust method resolution with clear error handling for non-callable attributes."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#_fix_for_qwix_quantization",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def _fix_for_qwix_quantization(module: Module):\n  \"\"\"Process the nnx module to make it compatible with QWIX quantization.\n\n  Normally Qwix only works with pure Linen modules or pure NNX modules. When\n  NNX modules are called inside Linen modules, Qwix will have issues to\n    * detect the correct module path when a Jax op (e.g. einsum) is called.\n    * detect the input types (whether it's a weight) of the Jax op.\n\n  This function will fix those issues.\n\n  Args:\n    module: The NNX module to be processed.\n  \"\"\"\n\n  # Wrap the __call__ function of the nnx modules to make sure the linen module\n  # path is updated correctly.\n  def wrap(call_fn, name: str):\n    def wrapped(*args, **kwargs):\n      if not linen.module._context.module_stack:  # pylint: disable=W0212\n        return call_fn(*args, **kwargs)\n      nn_module = linen.module._context.module_stack[-1]  # pylint: disable=W0212\n      old_path = nn_module.path\n      # We modify the path of the current nn module in place. This is a little\n      # bit hacky but should be good as a temporary solution.\n      nn_module.scope.path += (name,)\n      try:\n        return call_fn(*args, **kwargs)\n      finally:\n        nn_module.scope.path = old_path\n\n    return wrapped\n\n  for path, node in nnx.iter_graph(module):\n    # Only enable it on non-root nnx modules.\n    if path and isinstance(node, nnx.Module):\n      node.__class__ = type(\n          node.__class__.__name__,\n          (node.__class__,),\n          {\n              \"__call__\": wrap(node.__class__.__call__, str(path[-1])),\n          },\n      )\n\n  # Set the correct weight names. We call QtProvider.process_model_inputs here\n  # to avoid using Qwix internal APIs.\n  qwix.QtProvider.process_model_inputs(None, module, None, None)",
        "analysis": {
            "module_type": "qwix_quantization_fixer",
            "purpose": "Processes an NNX module in-place to make it compatible with QWIX quantization, particularly when the NNX module is called from within a Flax Linen module.",
            "input": {
                "shape": "N/A",
                "dtype": "flax.nnx.Module"
            },
            "processing_steps": [
                "Define a `wrap` function that temporarily appends the NNX submodule's name to the current Linen module's path during its `__call__` execution.",
                "Iterate through the graph of the input `module` using `nnx.iter_graph`.",
                "For each non-root submodule, dynamically replace its class to override its `__call__` method with the `wrap` function.",
                "Call `qwix.QtProvider.process_model_inputs` to correctly set weight names for quantization."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.nnx",
                "flax.linen",
                "qwix.QtProvider"
            ],
            "parameters": {
                "module": "The NNX module to be processed in-place."
            },
            "notes": [
                "This function modifies the input module and its submodules in-place.",
                "It is designed to fix issues where Qwix cannot detect the correct module path or input types (weights) for Jax ops when NNX modules are nested inside Linen modules.",
                "The in-place modification of the Linen module's path is noted in the code as a 'hacky' temporary solution.",
                "The function does not return a value."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#ToLinen",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "class ToLinen(linen.Module):\n  \"\"\"A wrapper to turn any NNX module into a Linen module.\n\n  The result Linen module can be used standalone with all Linen APIs, or as a\n  submodule of\n  another Linen module.\n\n  Since NNX modules are stateful and owns the state, we only create it once\n  during init\n  time, and will track its state and static data as separate variables.\n\n  Example::\n\n    >>> from flax import linen as nn, nnx\n    >>> import jax\n    >>> model = nnx.bridge.ToLinen(nnx.Linear, args=(32, 64))\n    >>> x = jax.numpy.ones((1, 32))\n    >>> y, variables = model.init_with_output(jax.random.key(0), x)\n    >>> y.shape\n    (1, 64)\n    >>> variables['params']['kernel'].shape\n    (32, 64)\n    >>> # The static GraphDef of the underlying NNX module\n    >>> variables.keys()\n    dict_keys(['params'])\n\n  Args:\n    nnx_class: The NNX Module class (not instance!).\n    args: The arguments that normally would be passed in to create the NNX\n      module.\n    kwargs: The keyword arguments that normally would be passed in to create the\n      NNX module.\n    skip_rng: True if this NNX module doesn't need `rngs` arg during\n      initialization (not common).\n\n  Returns:\n    A stateful NNX module that behaves the same as the wrapped Linen module.\n  \"\"\"\n\n  nnx_class: tp.Callable[..., Module]\n  args: tp.Sequence = ()\n  kwargs: tp.Mapping[str, tp.Any] = FrozenDict({})\n  skip_rng: bool = False\n  metadata_fn: tp.Callable[[variablelib.VariableState], tp.Any] | None = to_linen_var\n\n  @linen.compact\n  def __call__(self, *args, nnx_method: tp.Callable[..., Any] | str | None = None, **kwargs):\n    def _module_kwargs():\n      maybe_add_default = not self.is_initializing()\n      module_kwargs = dict(self.kwargs)\n      if not self.skip_rng:\n        module_kwargs[\"rngs\"] = nnx.Rngs(**linen_rngs_dict(self, add_default=maybe_add_default))\n      return module_kwargs\n\n    # init codepath\n    if self.is_initializing():\n      module = self.nnx_class(*self.args, **_module_kwargs())\n      # TODO: add lazy_init here in case there's an `ToNNX` submodule under `module`.\n      # update linen variables before call module to save initial state\n      self._update_variables(module)\n      _fix_for_qwix_quantization(module)\n      method_fn = _get_module_method(module, nnx_method)\n      out = method_fn(module, *args, **kwargs)\n      return out\n\n    # create the nnx module\n    module = self.nnx_class(*self.args, **_module_kwargs())\n\n    # update nnx module from linen variables\n    def maybe_unbox(x):\n      if isinstance(x, meta.AxisMetadata):\n        return x.unbox()\n      return x\n\n    states = jtu.tree_map(\n        maybe_unbox,\n        list(core.unfreeze(self.variables).values()),  # type: ignore[wrong-arg-types]\n        is_leaf=lambda x: isinstance(x, meta.AxisMetadata),\n    )\n    if not states:\n      states = ({},)\n\n    new_state = nnx.merge_state(*states)\n    new_state_flat = nnx.traversals.flatten_mapping(new_state)\n    current_state_flat = nnx.traversals.flatten_mapping(nnx.state(module))\n    unknown_state_flat = {path: v for path, v in new_state_flat.items() if path not in current_state_flat}\n\n    if unknown_state_flat:\n      paths_str = \"\"\n      for path, _ in unknown_state_flat.items():\n        paths_str += f\"\\n  - {'/'.join(map(str, path))}\"\n\n      warnings.warn(f\"Found unknown module paths in incoming state:{paths_str}\")\n\n    nnx.update(module, new_state)\n\n    _fix_for_qwix_quantization(module)\n    method_fn = _get_module_method(module, nnx_method)\n    out = method_fn(module, *args, **kwargs)\n    self._update_variables(module)\n    return out\n\n  def __getattr__(self, name: str):\n    if hasattr(super(), name):\n      return super().__getattribute__(name)\n    if name in self.kwargs:\n      return self.kwargs[name]\n    maybe_method = getattr(self.nnx_class, name, None)\n    if callable(maybe_method):\n      method = partial(self.__call__, nnx_method=maybe_method)\n      method.__self__ = self\n      return method\n    return super().__getattribute__(name)\n\n  def _update_variables(self, module):\n    \"\"\"Store the NNX module's graph def and state inside Linen module variables.\"\"\"\n    state = nnx.state(module, nnx.Not(nnx.RngState))\n\n    collection_flat_state: dict[str, list[tuple[tuple[str, ...], tp.Any]]] = {}\n\n    # group state by collection\n    for path, leaf in nnx.to_flat_state(state):\n      type_ = leaf.type if isinstance(leaf, nnx.VariableState) else type(leaf)\n      collection = variablelib.variable_name_from_type(type_, allow_register=True)\n      if collection not in collection_flat_state:\n        collection_flat_state[collection] = []\n      collection_flat_state[collection].append((path, leaf))\n\n    # update linen variables\n    for collection, flat_state in collection_flat_state.items():\n      if self.is_mutable_collection(collection):\n\n        def _to_linen_var(x):\n          if isinstance(x, nnx.VariableState):\n            if self.metadata_fn is not None:\n              return self.metadata_fn(x)  # pylint: disable=too-many-function-args\n            else:\n              return x.value\n          return x\n\n        collection_state = nnx.traversals.unflatten_mapping(flat_state)\n        collection_state = jax.tree.map(\n            _to_linen_var,\n            collection_state,\n            is_leaf=lambda x: isinstance(x, nnx.VariableState),\n        )\n        for k, v in collection_state.items():\n          self.put_variable(collection, k, v)",
        "analysis": {
            "module_type": "nnx_to_linen_wrapper",
            "purpose": "A wrapper to convert a Flax NNX module class into a Flax Linen module instance, enabling its use within Linen APIs and models.",
            "input": {
                "shape": "N/A (Inputs are passed to the __call__ method).",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes a specified NNX module class with given arguments.",
                "During the first call (initialization), it captures the state of the NNX module and stores it as Linen variables.",
                "On subsequent calls, it reconstructs the NNX module, loads its state from the stored Linen variables, executes a method, and updates the Linen variables with the new state."
            ],
            "output": {
                "shape": "N/A (Outputs are returned by the __call__ method)."
            },
            "dependencies": [
                "flax.linen.Module",
                "flax.nnx.Module",
                "flax.core.FrozenDict",
                "flax.nnx.variablelib",
                "functools.partial",
                "_get_module_method",
                "_fix_for_qwix_quantization",
                "linen_rngs_dict",
                "nnx.merge_state",
                "nnx.update",
                "nnx.state",
                "nnx.traversals"
            ],
            "parameters": {
                "nnx_class": "The NNX Module class to be wrapped.",
                "args": "Positional arguments for instantiating the `nnx_class`.",
                "kwargs": "Keyword arguments for instantiating the `nnx_class`.",
                "skip_rng": "If True, RNGs are not passed to the NNX module during instantiation.",
                "metadata_fn": "A function to convert an NNX VariableState to a Linen variable, defaulting to `to_linen_var`."
            },
            "notes": [
                "This class acts as a bridge, allowing stateful NNX modules to be integrated into the functional paradigm of Linen.",
                "The state of the wrapped NNX module is managed within Linen's variable collections.",
                "It uses `__getattr__` to provide a convenient API for calling methods of the wrapped NNX module directly."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Executes a method of the wrapped NNX module, handling state synchronization between the NNX module and the parent Linen module.",
                    "input": {
                        "shape": "Depends on the `nnx_method` being called (e.g., [batch_size, *features]).",
                        "dtype": "Depends on the `nnx_method` being called."
                    },
                    "processing_steps": [
                        "Determine if the module is being initialized using `self.is_initializing()`.",
                        "If initializing: instantiate the NNX module, store its initial state into Linen variables via `_update_variables`, and execute the requested method.",
                        "If not initializing: instantiate the NNX module, load its state from existing Linen variables, execute the method, and update the Linen variables with the new state via `_update_variables`.",
                        "Applies a compatibility fix for QWIX quantization via `_fix_for_qwix_quantization` in both paths."
                    ],
                    "output": {
                        "shape": "Depends on the `nnx_method` being called."
                    },
                    "dependencies": [
                        "self.is_initializing",
                        "linen_rngs_dict",
                        "self._update_variables",
                        "_fix_for_qwix_quantization",
                        "_get_module_method",
                        "nnx.merge_state",
                        "nnx.update"
                    ],
                    "notes": [
                        "The logic is split into two distinct paths for initialization and subsequent application.",
                        "The `nnx_method` argument specifies which method of the wrapped NNX module to execute, defaulting to `__call__`."
                    ]
                },
                "__getattr__": {
                    "purpose": "Provides access to attributes and methods of the wrapped `nnx_class` as if they were attributes of this `ToLinen` instance.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if the attribute exists on the parent class or in `self.kwargs`.",
                        "If the attribute name corresponds to a callable method on `self.nnx_class`, return a partial function of `self.__call__` with `nnx_method` pre-filled.",
                        "Otherwise, fall back to the default attribute access."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "functools.partial"
                    ],
                    "notes": [
                        "This enables a more ergonomic API, allowing calls like `model.some_method(*args)` instead of `model(*args, nnx_method='some_method')`."
                    ]
                },
                "_update_variables": {
                    "purpose": "Extracts the state from the wrapped NNX module and stores it in the appropriate Linen variable collections.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Get the state of the NNX module using `nnx.state()`, excluding RNG state.",
                        "Flatten the state and group it by variable collection name (e.g., 'params', 'batch_stats').",
                        "For each mutable collection, convert the NNX variable states to Linen-compatible values.",
                        "Store the resulting key-value pairs in the Linen module's variables using `self.put_variable()`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.state",
                        "nnx.to_flat_state",
                        "flax.nnx.variablelib.variable_name_from_type",
                        "nnx.traversals.unflatten_mapping",
                        "self.put_variable",
                        "self.is_mutable_collection"
                    ],
                    "notes": [
                        "This method is the core mechanism for synchronizing the NNX state back into the Linen world."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#_Missing",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "class _Missing:\n  ...",
        "analysis": {
            "module_type": "sentinel_class",
            "purpose": "Defines a unique type to create a singleton sentinel object, used for distinguishing missing arguments from arguments explicitly set to None.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Instantiates an object of the `_Missing` type."
            ],
            "output": {
                "shape": "An instance of the `_Missing` class."
            },
            "dependencies": [],
            "parameters": {},
            "notes": [
                "This class is used to create a singleton instance `_MISSING` which serves as a default value for function arguments.",
                "It allows functions to differentiate between a user not providing an argument and a user explicitly providing `None` as the argument, as seen in the `to_linen_class` function's `__init__` method."
            ],
            "methods": {}
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#to_linen",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def to_linen(\n    nnx_class: tp.Callable[..., Module],\n    *args,\n    metadata_fn: tp.Callable[[variablelib.VariableState], tp.Any] | None = to_linen_var,\n    name: str | None = None,\n    skip_rng: bool = False,\n    abstract_init: bool = True,\n    **kwargs,\n):\n  \"\"\"Shortcut of `nnx.bridge.ToLinen` if user is not changing any of its default fields.\"\"\"\n  return ToLinen(\n      nnx_class,\n      args=args,\n      kwargs=FrozenDict(kwargs),\n      metadata_fn=metadata_fn,\n      skip_rng=skip_rng,\n      name=name,\n  )",
        "analysis": {
            "module_type": "bridge_utility_function",
            "purpose": "A convenience function that simplifies the creation of a `ToLinen` module instance by wrapping an NNX module class and its initialization arguments.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Collects positional arguments (`*args`) into a tuple.",
                "Collects keyword arguments (`**kwargs`) into a `FrozenDict`.",
                "Instantiates the `ToLinen` class with the provided `nnx_class`, collected arguments, and other configuration options (`metadata_fn`, `skip_rng`, `name`)."
            ],
            "output": {
                "shape": "An instance of the `ToLinen` class."
            },
            "dependencies": [
                "ToLinen",
                "flax.core.FrozenDict",
                "to_linen_var"
            ],
            "parameters": {
                "nnx_class": "The NNX Module class to be wrapped into a Linen module.",
                "args": "Positional arguments for the `nnx_class` constructor.",
                "kwargs": "Keyword arguments for the `nnx_class` constructor.",
                "metadata_fn": "An optional function to convert an NNX `VariableState` to a Linen-compatible variable format.",
                "skip_rng": "A boolean indicating whether to skip passing RNGs to the NNX module during initialization.",
                "name": "An optional name for the resulting Linen module."
            },
            "notes": [
                "This function acts as a factory or shortcut for creating `ToLinen` instances with a more direct function-call syntax.",
                "The `abstract_init` parameter is accepted but not used within the function's logic."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#to_linen_class",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def to_linen_class(\n    base_nnx_class: type[M],\n    base_metadata_fn: tp.Callable[[variablelib.VariableState], tp.Any] | None = to_linen_var,\n    base_skip_rng: bool = False,\n    **partial_kwargs: tp.Any,\n) -> type[ToLinen]:\n  \"\"\"Dynamically wraps an NNX module class into a Flax Linen module class.\"\"\"\n\n  class ToLinenPartial(ToLinen):\n    \"\"\"A dynamically created Linen Module that wraps a specific NNX Module.\n\n    This class is not meant to be used directly. Instead, it is created and\n    returned by the `to_linen_class` function. It acts as a \"partially applied\"\n    version of the `ToLinen` wrapper, where the NNX module to be wrapped and\n    its default arguments are pre-configured.\n\n    When you instantiate this class, it behaves like a standard Linen module.\n    The arguments you provide during instantiation can override the defaults\n    that were set when this class was created by `to_linen_class`.\n\n    For example:\n      >>> from flax import linen as nn, nnx\n      >>> from MaxText.layers import linears\n      >>> # Create a specialized Linen wrapper for linears.DenseGeneral\n      >>> LinenDenseGeneral = to_linen_class(linears.DenseGeneral)\n      >>> # Now, LinenDenseGeneral can be used like a regular Linen module\n      >>> class MyModel(nn.Module):\n      ...   def setup(self):\n      ...     # Instantiate the wrapped linears.DenseGeneral with its arguments\n      ...     self.dense = LinenDenseGeneral(\n      ...         in_features_shape=10, out_features_shape=5\n      ...     )\n      ...   def __call__(self, x):\n      ...     return self.dense(x)\n\n    Attributes:\n      (The attributes are dynamically set by the `ToLinen` parent class based\n       on the arguments provided during instantiation.)\n    \"\"\"\n\n    def __init_subclass__(cls, **kwargs):\n      super().__init_subclass__(**kwargs)\n\n      def __init__(\n          self,\n          args=None,\n          kwargs=None,\n          nnx_class=None,\n          skip_rng=None,\n          metadata_fn=None,\n          name=_MISSING,\n          parent=_MISSING,\n          **other_kwargs,\n      ):\n        linen_kwargs = {}\n        if not isinstance(parent, _Missing):\n          linen_kwargs[\"parent\"] = parent\n        if not isinstance(name, _Missing):\n          linen_kwargs[\"name\"] = name\n        ToLinen.__init__(\n            self,\n            nnx_class=nnx_class or base_nnx_class,\n            args=args or (),\n            metadata_fn=metadata_fn or base_metadata_fn,\n            skip_rng=skip_rng or base_skip_rng,\n            kwargs=FrozenDict({**partial_kwargs, **(kwargs or {}), **other_kwargs}),\n            **linen_kwargs,\n        )\n\n      cls.__init__ = __init__\n\n  return ToLinenPartial",
        "analysis": {
            "module_type": "nnx_to_linen_class_factory",
            "purpose": "A higher-order function that dynamically creates a Flax Linen module class that wraps a given NNX module class, with pre-configured default arguments.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Defines a nested class `ToLinenPartial` that inherits from `ToLinen`.",
                "Dynamically creates and assigns a custom `__init__` method to `ToLinenPartial` within its `__init_subclass__`.",
                "The custom `__init__` method merges pre-configured arguments (from `to_linen_class`) with instantiation-time arguments.",
                "The custom `__init__` calls the parent `ToLinen.__init__` with the final combined arguments.",
                "Returns the `ToLinenPartial` class type."
            ],
            "output": {
                "shape": "Returns a `type` which is a subclass of `ToLinen`."
            },
            "dependencies": [
                "ToLinen",
                "flax.core.FrozenDict",
                "flax.nnx.variablelib.VariableState",
                "to_linen_var"
            ],
            "parameters": {
                "base_nnx_class": "The NNX Module class to be wrapped into a Linen module.",
                "base_metadata_fn": "A function to convert NNX VariableState to a Linen-compatible format.",
                "base_skip_rng": "A boolean indicating if the wrapped NNX module's initialization should skip receiving an `rngs` argument.",
                "partial_kwargs": "A dictionary of keyword arguments that are pre-configured as defaults for the wrapped NNX module's constructor."
            },
            "notes": [
                "This function acts as a factory, returning a class rather than an instance.",
                "The returned class is a 'partially applied' version of the `ToLinen` wrapper, allowing for cleaner integration of NNX modules into Linen architectures.",
                "Instantiation arguments for the returned class can override the pre-configured `partial_kwargs`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/normalizations.py#RMSNorm",
        "file_path": "src/MaxText/layers/normalizations.py",
        "code_block": "class RMSNorm(nnx.Module):\n  \"\"\"RMS normalization.\"\"\"\n\n  def __init__(\n      self,\n      num_features: int,\n      epsilon: float = 1e-6,\n      dtype: Any = jnp.float32,\n      weight_dtype: Any = jnp.float32,\n      kernel_axes: tuple[None | str, ...] = (),\n      scale_init: Initializer = nn.initializers.ones,\n      parameter_memory_host_offload: bool = False,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.num_features = num_features\n    self.epsilon = epsilon\n    self.dtype = dtype\n    self.weight_dtype = weight_dtype\n    self.kernel_axes = kernel_axes\n    self.scale_init = scale_init\n    self.parameter_memory_host_offload = parameter_memory_host_offload\n    self.scale = nnx.Param(\n        scale_init(rngs.params(), (num_features,), weight_dtype),\n        sharding=kernel_axes,\n    )\n\n  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Applies layer normalization on the input.\"\"\"\n    x = jnp.asarray(x, jnp.float32)\n    mean2 = jnp.mean(lax.square(x), axis=-1, keepdims=True)\n    y = jnp.asarray(x * lax.rsqrt(mean2 + self.epsilon), self.dtype)\n    scale = self.scale.value\n    # Move scale to device if parameter offloading is enabled\n    if self.parameter_memory_host_offload:\n      max_logging.log(\"normalizations.py: Moving scale parameter to device\")\n      scale = jax.device_put(scale, max_utils.device_space())\n\n    scale = jnp.asarray(scale, self.dtype)\n    return y * scale",
        "analysis": {
            "module_type": "rms_normalization",
            "purpose": "Applies Root Mean Square (RMS) normalization to an input tensor, a technique to stabilize layer activations by re-scaling based on the root mean square of the activations.",
            "input": {
                "shape": "The `__call__` method expects a tensor of shape `[..., num_features]`.",
                "dtype": "jnp.ndarray"
            },
            "processing_steps": [
                "The `__init__` method initializes a learnable `scale` parameter of shape `[num_features]`.",
                "The `__call__` method calculates the mean of the squared input tensor along the last dimension.",
                "It normalizes the input tensor using the reciprocal square root of the calculated mean plus an epsilon.",
                "It scales the normalized tensor by the learnable `scale` parameter."
            ],
            "output": {
                "shape": "The output shape is identical to the input shape, `[..., num_features]`."
            },
            "dependencies": [
                "flax.nnx",
                "jax",
                "jax.numpy",
                "max_utils",
                "max_logging"
            ],
            "parameters": {
                "num_features": "The number of features in the input tensor, corresponding to the size of the last dimension.",
                "epsilon": "A small float added to the variance to avoid division by zero.",
                "dtype": "The data type of the output tensor.",
                "weight_dtype": "The data type for the learnable `scale` parameter.",
                "parameter_memory_host_offload": "If True, the `scale` parameter is stored on the host and moved to the device only during the forward pass."
            },
            "notes": [
                "This is a Flax NNX module.",
                "Normalization is applied over the last dimension of the input tensor.",
                "It contains a single learnable parameter, `scale`."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the RMSNorm layer, setting up its configuration and creating the learnable `scale` parameter.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store initialization parameters (`num_features`, `epsilon`, etc.) as attributes.",
                        "Initialize the `scale` parameter as an `nnx.Param` using the provided `scale_init` function and `rngs`.",
                        "Assign sharding annotations to the `scale` parameter."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "flax.nnx",
                        "flax.linen.initializers"
                    ],
                    "notes": [
                        "Requires an `nnx.Rngs` object for parameter initialization."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the RMS normalization transformation to the input tensor.",
                    "input": {
                        "shape": "[..., num_features]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Cast the input tensor `x` to `jnp.float32`.",
                        "Compute the mean of the square of `x` along the last axis.",
                        "Normalize `x` by multiplying with the reciprocal square root of `(mean_of_squares + epsilon)`.",
                        "Retrieve the `scale` parameter's value.",
                        "If `parameter_memory_host_offload` is enabled, explicitly move the `scale` parameter to the device.",
                        "Cast the `scale` parameter to the module's `dtype`.",
                        "Multiply the normalized tensor by the `scale` parameter."
                    ],
                    "output": {
                        "shape": "Same as input, `[..., num_features]`."
                    },
                    "dependencies": [
                        "jax.numpy",
                        "jax.lax",
                        "max_logging",
                        "max_utils"
                    ],
                    "notes": [
                        "The input is internally cast to `float32` for the normalization calculation, but the output is cast to the `dtype` specified during initialization."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/normalizations.py#rms_norm",
        "file_path": "src/MaxText/layers/normalizations.py",
        "code_block": "def rms_norm(\n    num_features: int,\n    epsilon: float = 1e-6,\n    dtype: Any = jnp.float32,\n    weight_dtype: Any = jnp.float32,\n    kernel_axes: tuple[None | str, ...] = (),\n    scale_init: Initializer = nn.initializers.ones,\n    name: None | str = None,\n    parameter_memory_host_offload: bool = False,\n):\n  \"\"\"Creates a RMSNorm module.\"\"\"\n  module = nnx_wrappers.to_linen(\n      RMSNorm,\n      num_features=num_features,\n      epsilon=epsilon,\n      dtype=dtype,\n      weight_dtype=weight_dtype,\n      kernel_axes=kernel_axes,\n      scale_init=scale_init,\n      parameter_memory_host_offload=parameter_memory_host_offload,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n  )\n  return module",
        "analysis": {
            "module_type": "rms_norm_factory",
            "purpose": "A factory function that creates and returns a Flax Linen-compatible RMSNorm module by wrapping the `RMSNorm` nnx.Module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `RMSNorm` nnx.Module into a Flax Linen module.",
                "Passes configuration parameters such as `num_features`, `epsilon`, `dtype`, etc., to the `RMSNorm` constructor via the wrapper.",
                "Specifies `variable_to_logically_partitioned` as the `metadata_fn` for parameter partitioning.",
                "Returns the created Linen module."
            ],
            "output": {
                "shape": "Returns a Flax Linen module object. When called, this module will return a tensor with the same shape as its input."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "RMSNorm",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "num_features": "The number of features in the input tensor, which determines the size of the learnable scale parameter.",
                "epsilon": "A small float added to the variance to avoid division by zero.",
                "dtype": "The data type for the computation and output.",
                "weight_dtype": "The data type for the learnable scale parameter.",
                "kernel_axes": "A tuple specifying the sharding annotations for the scale parameter.",
                "scale_init": "The initializer function for the learnable scale parameter.",
                "name": "The name of the module.",
                "parameter_memory_host_offload": "A boolean indicating whether to offload the scale parameter to host memory."
            },
            "notes": [
                "This function serves as a wrapper to make the `nnx`-based `RMSNorm` class compatible with the Flax `Linen` API."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/pipeline.py#Pipeline",
        "file_path": "src/MaxText/layers/pipeline.py",
        "code_block": "class Pipeline(nn.Module):\n  \"\"\"Module that implements pipelining across stages.\n\n  This module will loop over microbatches and execute the main body with a vmap for both the inputs and weights.\n  This will produce a pipeline pattern if the stage dimension is sharded.\n\n  Supports circular pipelines, and multiple layers per stage are used when a module that executes multiple layers\n  is passed as the layers input.\n\n  Attributes:\n    config: Importantly contains num_pipeline_microbatches, num_pipeline_repeats.\n    layers: A module instance that each stage can execute. It can either be a single layer such as a\n      LlamaDecoderLayer instance or scanned/looped set of decoder layers to execute multiple layers per stage.\n    mesh:  The device mesh of the system.\n    remat_policy: Remat policy to use for the loop iterations\n  \"\"\"\n\n  config: Config\n  layers: nn.Module  # The name of this property (layers) is reflected in the state pytree and thus also checkpoints.\n  mesh: Mesh\n  remat_policy: Any = None\n\n  def setup(self):\n    self.num_stages = self.config.ici_pipeline_parallelism * self.config.dcn_pipeline_parallelism\n    self.forwarding_delay = 2 if self.config.pipeline_delay_activation_forwarding else 1\n    self.pipeline_microbatch_size = self.config.micro_batch_size_to_train_on // self.config.num_pipeline_microbatches\n    microbatches_per_stage = self.config.num_pipeline_microbatches // self.num_stages\n    self.microbatches_per_stage = microbatches_per_stage\n    self.use_circ_storage = self.need_circ_storage()\n\n  def need_circ_storage(self):\n    return (\n        self.config.num_pipeline_repeats > 1\n        and self.config.num_pipeline_microbatches > self.num_stages * self.forwarding_delay\n    )\n\n  def iterations_to_complete_first_microbatch_one_repeat(self):\n    # Return the number of iterations it takes for microbatch 0 to finish a repeat\n    return self.forwarding_delay * (self.num_stages - 1)\n\n  def iterations_to_complete_first_microbatch(self):\n    # Return the number of iterations it takes for microbatch 0 to finish the last stage of the last repeat\n    return (\n        self.config.num_pipeline_microbatches * (self.config.num_pipeline_repeats - 1)\n        + self.iterations_to_complete_first_microbatch_one_repeat()\n    )\n\n  def init_states(self, inputs):\n    \"\"\"Initialize components of state: state_io, shift, circular_storage and circular_storage_mover\n    Assumes input has already been reshaped into microbatches: [num_micro_batches, micro_batch_size, sequence, embed]\n\n    Returns a dictionary with properties\n      shift: zeros shape [num_stages, micro_size, sequence, embed]\n      prev_outputs: same shape as shift, only used when pipeline_delay_activation_forwarding is set to true, else None\n      state_io: reshaped inputs [num_stages, microbatches/stages, micro_size, sequence, embed]\n      circ_storage: zeros [num_stages, microbatches, micro_size, sequence, embed] when needed, else None\n      circ_storage_mover: zeros[num_stages, micro_size, sequence, embed] when needed, else None\n      loop_iteration: scalar set initially to 0.\n    \"\"\"\n\n    # Shift is used to rotate the output of each pipeline into the input of the next\n    # shift has shape [num_stages, micro_size, sequence, embed]\n    shift = jnp.zeros((self.num_stages,) + inputs.shape[1:], dtype=inputs.dtype)\n    shift = nn.with_logical_constraint(\n        shift,\n        (\"activation_stage\", \"activation_batch\", \"activation_length\", \"activation_embed\"),\n        rules=self.config.logical_axis_rules,\n        mesh=self.mesh,\n    )\n\n    # Prev outputs has the same shape of the output (and shift)\n    if self.config.pipeline_delay_activation_forwarding:\n      prev_outputs = jnp.zeros((self.num_stages,) + inputs.shape[1:], dtype=inputs.dtype)\n      prev_outputs = nn.with_logical_constraint(\n          prev_outputs,\n          (\"activation_stage\", \"activation_batch\", \"activation_length\", \"activation_embed\"),\n          rules=self.config.logical_axis_rules,\n          mesh=self.mesh,\n      )\n    else:\n      prev_outputs = None\n\n    # state_io (state input output) at first holds all of the input batches, but also will hold the outputs\n    #   as the pipeline runs/finishes\n    # state_io has shape [num_stages, microbatches/stages, micro_size, sequence, embed]\n    state_io = jnp.reshape(inputs, (self.num_stages, self.microbatches_per_stage) + inputs.shape[1:])\n    # We shard the pipeline_microbatch_size axis by data/fsdp, not num_microbatches since those are looped over.\n    state_io = nn.with_logical_constraint(\n        state_io,\n        (\"activation_stage\", None, \"activation_batch\", \"activation_length\", \"activation_embed\"),\n        rules=self.config.logical_axis_rules,\n        mesh=self.mesh,\n    )\n\n    # circ_storage is used to hold the final pipeline stage outputs before it is used for the next repeat. It is only\n    # needed when num_microbatches > num_stages, else instead the final stage will immediately pass to the first without\n    # additional storage.\n    # circ_storage has shape [num_stages, microbatches, micro_size, sequence, embed].\n    # Note that this shape is a factor of num_stages larger than necessary - each stage holds the global batch, but only\n    # stage 0 holds the real activations (since it will use them), the rest hold dummy ones. This amount of storage\n    # [global_batch, sequence, embed] is fine as long as there is some amount of additional sharding axes, e.g. FSDP,\n    # TP, DP (e.g. there are many devices that shard stage 0)\n    # We may look into alternatives using less storage if this becomes an issue (ideas in b/347603101).\n    if self.use_circ_storage:\n      circ_storage = jnp.zeros((self.num_stages,) + inputs.shape, dtype=inputs.dtype)\n    else:\n      circ_storage = None\n\n    # circ_storage_mover is used to push the microbatches from the pipeline into circ_storage with one buffer iteration\n    # of delay circ_storage_mover shape is same as shift: [num_stages, micro_size, sequence, embed]\n    if self.use_circ_storage:\n      circ_storage_mover = shift\n    else:\n      circ_storage_mover = None\n\n    init_loop_state = {\n        \"state_io\": state_io,\n        \"shift\": shift,\n        \"circ_storage\": circ_storage,\n        \"circ_storage_mover\": circ_storage_mover,\n        \"loop_iteration\": 0,\n        \"prev_outputs\": prev_outputs,\n    }\n    return init_loop_state\n\n  def get_iteration_inputs(self, loop_iteration, state_io, circ_storage, shift):\n    \"\"\"\n    Construct stages_in: the global array that is operated on for this iteration, shape same as\n    shift=[stages, micro_size, sequence, embed]\n    This is almost a rotated version of the last outputs, except for the first stage which must grab a new batch from\n    state_io or an old one from circ_storage\n    \"\"\"\n\n    # Setup potential input from state_io, which has a rotating microbatch index (size of microbatches_per_stage)\n    state_io_batch_idx = loop_iteration % self.microbatches_per_stage\n    state_io_slice = state_io[:, state_io_batch_idx]\n\n    if self.use_circ_storage:\n      # Setup potential input from circ_storage, which also has a rotating index for microbatch,\n      # size of num_microbatches\n      circ_storage_batch_idx = loop_iteration % self.config.num_pipeline_microbatches\n      circular_stage_in = circ_storage[:, circ_storage_batch_idx]\n    else:\n      # The last stage immediately flows into the first stage, use this rotated shift instead of circular storage\n      circular_stage_in = shift\n\n    # For early loop iterations we grab a new input for stage 0 from the state_io. Once each microbatch has left\n    # state_io we instead grab from the last stage's output (possibly buffered when num_microbatches > num_stages, e.g.\n    # from circ_storage).\n    first_stage_in = jnp.where(loop_iteration < self.config.num_pipeline_microbatches, state_io_slice, circular_stage_in)\n\n    # Note that first_stage_in may correspond to bubble computation during the last few iterations.\n    # However, these bubble computation results remain in the shift buffer (do not make it back to state_io) and are\n    # thus discarded / not returned.\n    # The final returned output is stored in the state_io, which has the appropriate total size of num_microbatches. The\n    # state_io will not contain bubble results at the end of the last iteration.\n\n    def select_state_or_input(first_stage_in, shift):\n      # Selects input for stage 0, shift for other stages\n      return jnp.where(jax.lax.broadcasted_iota(\"int32\", shift.shape, 0) == 0, first_stage_in, shift)\n\n    # Selects input (from stream_io) for stage 0, other stages get from shift (the rotated previous output)\n    stages_in = select_state_or_input(first_stage_in, shift)\n    stages_in = nn.with_logical_constraint(\n        stages_in,\n        (\"activation_stage\", \"activation_batch\", \"activation_length\", \"activation_embed\"),\n        rules=self.config.logical_axis_rules,\n        mesh=self.mesh,\n    )\n    return stages_in\n\n  def shard_dim_by_stages(self, x, dim: int):\n    # Shards a dimension by stages. Currently, the sharding of other dimensions are left up the compiler, alternatively\n    # we may want to copy over the sharding from the other input axes.\n    dims_mapping = [jax.sharding.PartitionSpec.UNCONSTRAINED] * x.ndim\n    dims_mapping[dim] = \"stage\"\n    dims_mapping = tuple(dims_mapping)\n    sharding = jax.sharding.NamedSharding(self.mesh, jax.sharding.PartitionSpec(*dims_mapping))\n    return jax.lax.with_sharding_constraint(x, sharding)\n\n  def get_microbatch_and_repeat_ids(self, loop_iteration):\n    \"\"\"Gets the microbatch_ids and repeat_ids for all stages on this loop_iteration. Works for both circular and\n    non-circular\"\"\"\n    # Stage 0 has processed one microbatch every loop_iter, but Stage 1 is 1 behind due to bubble, etc for other stages\n    microbatches_processed = jnp.maximum(loop_iteration - self.forwarding_delay * jnp.arange(self.num_stages), 0)\n    microbatch_ids = microbatches_processed % self.config.num_pipeline_microbatches\n    repeat_ids = microbatches_processed // self.config.num_pipeline_microbatches\n    return microbatch_ids, repeat_ids\n\n  def vmap_parallel_gather(self, weights, repeat_ids, repeat_dim_in_weights, stages_dim_in_weights):\n    \"\"\"Use vmap to implement a sharded parallel gather.\n    Parallel gather means each stage has its own weights, and gets one slice from it.\n    Args:\n      weights: Per-stage data to be gathered from.\n      repeat_ids: Integer tensor of shape [num_stages], the repeats of the stages.\n      repeat_dim_in_weights: The dimension in weights where repeat_ids are applied. The output will not\n        have this dimension.\n      stages_dim_in_weights: The dimension in weights that represents parallel stages.\n    Returns:\n      The per-stage gathered values. The shape is weights.shape but with repeat_dim_in_weights\n        removed.\n    \"\"\"\n\n    def _gather_one(x, repeat_id):\n      return jnp.squeeze(jax.lax.dynamic_slice_in_dim(x, repeat_id, 1, repeat_dim_in_weights), repeat_dim_in_weights)\n\n    gathered_weights_stage_dim = 0\n    repeat_ids = self.shard_dim_by_stages(repeat_ids, 0)\n    weights = self.shard_dim_by_stages(weights, stages_dim_in_weights)\n    stage_weights = jax.vmap(_gather_one, in_axes=(stages_dim_in_weights, 0), out_axes=gathered_weights_stage_dim)(\n        weights, repeat_ids\n    )\n    stage_weights = self.shard_dim_by_stages(stage_weights, gathered_weights_stage_dim)\n    return stage_weights\n\n  def vmap_gather(self, xs, ids, ids_dim):\n    \"\"\"Use vmap to implement a stage-wise sharded gather.\n\n    The stages share the same input, but they have different offsets.\n\n    Args:\n      xs: Data shared by all stages, to be gathered from.\n      ids: Integer tensor of shape [num_stages], the offsets of the stages.\n      ids_dim: The dimension in xs where ids are applied. In the output, this\n        dimension will be [num_stages], since each stage gets one slice.\n\n    Returns:\n      The per-stage gathered values. The shape is xs.shape but with ids_dim size\n        replaced with [num_stages].\n    \"\"\"\n\n    def _gather_one(x, i):\n      return jnp.squeeze(jax.lax.dynamic_slice_in_dim(x, i, 1, ids_dim), ids_dim)\n\n    ids = self.shard_dim_by_stages(ids, 0)\n    outs = jax.vmap(_gather_one, in_axes=(None, 0), out_axes=ids_dim)(xs, ids)\n    return self.shard_dim_by_stages(outs, 0)\n\n  def get_new_loop_state(self, output, loop_state):\n    \"\"\"\n    Update the various buffers given the output of the most recent iteration\n    * state_io: rotates left/up by 1 (the whole created in the last slot is filled with the most recent pipeline output)\n       * Pushing inputs up from top of state_io into first stage of shift\n       * Pulling outputs up from last stage of shift into bottom of state_io\n    * shift: rotate output (or prev_outputs if using delay) right/down by 1 - we imagine the pipeline moves to\n               right/down\n    * circ_storage: pushes circ_storage_mover (the output of the previous iteration) into rotating index of circ_storage\n    * circ_storage_mover: assigned to rotated output and pushed into circ_storage on the next iteration\n    * prev_outputs: is set to the current output\n    \"\"\"\n\n    old_state_io = loop_state[\"state_io\"]\n    old_circ_storage = loop_state[\"circ_storage\"]\n    old_circ_storage_mover = loop_state[\"circ_storage_mover\"]\n    loop_iteration = loop_state[\"loop_iteration\"]\n    old_prev_outputs = loop_state[\"prev_outputs\"]\n\n    def _rotate_right(arr):\n      # Use lax.slice to avoid generating a gather.\n      last = jax.lax.slice_in_dim(arr, self.num_stages - 1, self.num_stages, axis=0)\n      except_last = jax.lax.slice_in_dim(arr, 0, self.num_stages - 1, axis=0)\n      return jnp.concatenate([last, except_last], axis=0)\n\n    def _shift_right(arr):\n      padding = [[1, 0]] + [[0, 0]] * (arr.ndim - 1)\n      # Use lax.slice to guarantee the gradient is a pad.\n      return jax.lax.slice(jnp.pad(arr, padding), [0] * arr.ndim, arr.shape)\n\n    # Shift either rotates or shifts depending on if the last stage immediately must send to first or not\n    # For non-circular pipelines, the last stage does not need to send to first\n    # For circular pipelines with #micro = #stages, last stage immediately sends to first\n    # For circular pipelines with #micro > stages (circ_storage), last stage sends to circ storage\n    def _update_shift(output_in):\n      if self.config.num_pipeline_repeats == 1 or self.use_circ_storage:\n        return _shift_right(output_in)  # last stage does not have to send to first immediately\n      else:\n        return _rotate_right(output_in)  # last stage must immediately send to first\n\n    if self.config.pipeline_delay_activation_forwarding:\n      new_shift = _update_shift(old_prev_outputs)\n      new_prev_outputs = output\n    else:\n      new_shift = _update_shift(output)\n      new_prev_outputs = None\n\n    if self.use_circ_storage:\n      # Insert the circ_storage_mover into new_circ_storage at a microbatch-rotating index.\n      # circ_storage_mover still points to the output of PREVIOUS iteration, which should aid in allowing overlapped\n      # compute/async transfers\n      def _rotate_right_and_update(circ_storage_mover_in, circ_storage_in):\n        rotated = _rotate_right(circ_storage_mover_in)\n        rotated = jnp.expand_dims(rotated, 1)\n        # We rotate the pushing index into circ storage, and ensure that microbatch 0 lands in index 0\n        offset = (\n            loop_iteration - self.iterations_to_complete_first_microbatch_one_repeat() - 1\n        ) % self.config.num_pipeline_microbatches  # Note extra -1 b/c grabbing from the\n        # previous output - using circ_storage_mover before it is updated\n        return jax.lax.dynamic_update_slice_in_dim(circ_storage_in, rotated, offset, axis=1)\n\n      new_circ_storage = _rotate_right_and_update(old_circ_storage_mover, old_circ_storage)\n      new_circ_storage_mover = output\n    else:\n      new_circ_storage = None\n      new_circ_storage_mover = None\n\n    # Rotate stream_io left/up by 1 on rotating micro/stage index (stream_buf_idx), replacing the last/bottom with the\n    # last stage output\n    stream_buf_idx = loop_iteration % self.microbatches_per_stage\n    stream_slice = old_state_io[:, stream_buf_idx]\n\n    def _update_state_io(state_in, stream_slice, output):\n      # Shift the current slice to the left, then fill the last stage with the final output.\n      padding = [[0, 1]] + [[0, 0]] * (stream_slice.ndim - 1)\n      stream_slice = jax.lax.slice_in_dim(jnp.pad(stream_slice, padding), 1, stream_slice.shape[0] + 1, axis=0)\n      stream_slice = jnp.where(\n          jax.lax.broadcasted_iota(\"int32\", stream_slice.shape, 0) == self.num_stages - 1, output, stream_slice\n      )\n      stream_slice = jnp.expand_dims(stream_slice, 1)\n      return jax.lax.dynamic_update_slice_in_dim(state_in, stream_slice, stream_buf_idx, axis=1)\n\n    new_state = _update_state_io(old_state_io, stream_slice, output)\n\n    new_loop_state = {\n        \"state_io\": new_state,\n        \"shift\": new_shift,\n        \"circ_storage\": new_circ_storage,\n        \"circ_storage_mover\": new_circ_storage_mover,\n        \"loop_iteration\": loop_iteration + 1,\n        \"prev_outputs\": new_prev_outputs,\n    }\n    return new_loop_state\n\n  def permute_output_micro_per_stage_dim(self, output):\n    # The first real output (microbatch 0) takes a certain amount of loop iterations to finish and be pushed to\n    # state_io - it will land on a different index of state_io depending on the number of iterations.\n    microbatch_0_idx = self.iterations_to_complete_first_microbatch() % self.microbatches_per_stage\n    permutation = (\n        np.arange(self.microbatches_per_stage) + microbatch_0_idx\n    ) % self.microbatches_per_stage  # permute so the value in land_idx is moved into idx 0, and (land_idx + 1) appear\n    # in idx 1, etc\n    output = output[:, permutation]\n    return output\n\n  def get_current_stage_weights(self, pipeline_weights, loop_iteration):\n    \"\"\"\n    Gets the current weights used for one iteration. Outputs a pytree whose arrays have leading dimension of stages, e.g.\n    {'mlp': 'wo': [stages, mlp, embed]}. Stage 0 will use the 0th index of this pytree, Stage 1 the 1st index, etc.\n    For non-circular pipelines, this simply returns all weights - every weight is used in every iteraiton. However\n    for circular pipelines each stage grabs only the weights corresponding to the current repeat.\n    \"\"\"\n    if self.config.num_pipeline_repeats > 1:\n      return self.get_current_repeat_from_stages(pipeline_weights, loop_iteration)\n    else:\n      return pipeline_weights\n\n  def get_current_repeat_from_stages(self, weights, loop_iteration):\n    \"\"\"get current repeat from stages\"\"\"\n    _, repeat_ids = self.get_microbatch_and_repeat_ids(loop_iteration)\n\n    def gather_weights_for_stages_in(weights):\n      return jax.tree.map(\n          functools.partial(\n              self.vmap_parallel_gather, repeat_ids=repeat_ids, repeat_dim_in_weights=0, stages_dim_in_weights=1\n          ),\n          weights,\n      )\n\n    circular_metadata_params = {\n        nn.PARTITION_NAME: \"circular_repeats\",\n        \"sub_weight_split_dims_mapping\": (None,),\n        \"is_initializing\": self.is_initializing(),\n        \"x_times\": self.config.num_pipeline_repeats,\n        \"optimizer_dims_mapping\": None,\n    }\n    weights = meta.remove_axis(\n        weights, 0, circular_metadata_params\n    )  # Remove the circular metadata axis, this axis will be removed when passed to the main vmap, only one circular\n    # entry per stage.\n    weights = gather_weights_for_stages_in(weights)\n    return weights\n\n  def get_vmap_func_for_init(self):\n    \"\"\"This vmap func is used to initialize the weights only on init.\"\"\"\n\n    def func_to_vmap(body_instance, stages_inputs, stages_segment_ids, stages_positions, deterministic, model_mode):\n      \"\"\"nn.vmap requires either a nn.module class or a function whose first argument is a nn.module instance.\"\"\"\n      return body_instance(stages_inputs, stages_segment_ids, stages_positions, deterministic, model_mode)\n\n    vmap_func = nn.vmap(\n        func_to_vmap,\n        in_axes=(0, 0, 0, None, None),\n        spmd_axis_name=\"stage\",\n        variable_axes={\"params\": 0, \"_overwrite_with_gradient\": 0},\n        split_rngs={\"params\": self.is_initializing(), \"dropout\": self.config.enable_dropout},\n        metadata_params={\n            nn.PARTITION_NAME: \"layers\",\n            \"sub_weight_split_dims_mapping\": (None),\n            \"is_initializing\": self.is_initializing(),\n            \"x_times\": self.num_stages,\n        },\n    )\n    return vmap_func\n\n  def get_main_vmap_func_for_iterations(self):\n    \"\"\"\n    Returns main stage function vmapped by number of stages.\n    This becomes a vmap over a single layer instance if body_instance is a single layer,\n    else a set of layers if body_instance is a set of layers.\n    \"\"\"\n\n    def func_to_vmap(\n        body_instance, weights, stages_inputs, stages_segment_ids, stages_positions, deterministic, model_mode\n    ):\n      \"\"\"nn.vmap requires either a nn.module class or a function whose first argument is a nn.module instance.\"\"\"\n      weights = meta.remove_axis(\n          weights,\n          0,\n          {\n              nn.PARTITION_NAME: \"layers\",\n              \"sub_weight_split_dims_mapping\": (None,),\n              \"is_initializing\": self.is_initializing(),\n              \"x_times\": self.num_stages,\n          },\n      )\n      return body_instance.apply(weights, stages_inputs, stages_segment_ids, stages_positions, deterministic, model_mode)\n\n    vmap_func = nn.vmap(\n        func_to_vmap,\n        in_axes=(0, 0, 0, 0, None, None),\n        spmd_axis_name=\"stage\",\n        variable_axes={\"params\": 0},\n        split_rngs={\"params\": self.is_initializing(), \"dropout\": self.config.enable_dropout},\n        metadata_params={\n            nn.PARTITION_NAME: \"layers\",\n            \"sub_weight_split_dims_mapping\": (None),\n            \"is_initializing\": self.is_initializing(),\n            \"x_times\": self.num_stages,\n        },\n    )\n    return vmap_func\n\n  def run_one_iteration(\n      self, loop_state, pipeline_weights, positions, segment_ids, deterministic, model_mode, decoder_layer_instance\n  ):\n    \"\"\"Run one loop iteration - gets weights and inputs for each stage, run the stages in parallel,\n    and update the loop state.\"\"\"\n    state_io = loop_state[\"state_io\"]\n    shift = loop_state[\"shift\"]\n    circ_storage = loop_state[\"circ_storage\"]\n    loop_iteration = loop_state[\"loop_iteration\"]\n\n    microbatch_ids, _ = self.get_microbatch_and_repeat_ids(loop_iteration)\n\n    stages_inputs = self.get_iteration_inputs(loop_iteration, state_io, circ_storage, shift)\n    # We checkpoint stages_inputs since we are grabbing only one slice of the state_io, don't need to save the entire\n    # buffer.\n    stages_inputs = jax.ad_checkpoint.checkpoint_name(stages_inputs, \"iteration_input\")\n    stages_positions = self.vmap_gather(positions, microbatch_ids, 0) if positions is not None else None\n    stages_segment_ids = self.vmap_gather(segment_ids, microbatch_ids, 0) if segment_ids is not None else None\n\n    vmap_func = self.get_main_vmap_func_for_iterations()\n\n    if self.config.num_pipeline_repeats > 1:\n      _, repeat_ids = self.get_microbatch_and_repeat_ids(loop_iteration)\n\n      def prepare_vars_for_main_vmap(weights):\n        def gather_weights_for_stages_in(weights):\n          return jax.tree.map(\n              functools.partial(\n                  self.vmap_parallel_gather, repeat_ids=repeat_ids, repeat_dim_in_weights=0, stages_dim_in_weights=1\n              ),\n              weights,\n          )\n\n        circular_metadata_params = {\n            nn.PARTITION_NAME: \"circular_repeats\",\n            \"sub_weight_split_dims_mapping\": (None,),\n            \"is_initializing\": self.is_initializing(),\n            \"x_times\": self.config.num_pipeline_repeats,\n            \"optimizer_dims_mapping\": None,\n        }\n        weights = meta.remove_axis(\n            weights, 0, circular_metadata_params\n        )  # Remove the circular metadata axis, this axis will be removed when passed to the main vmap, only one\n        # circular entry per stage.\n        weights = gather_weights_for_stages_in(weights)\n        return weights\n\n      vmap_func = nn.map_variables(\n          vmap_func,\n          mapped_collections=[\"params\", \"_overwrite_with_gradient\", \"non_trainable\", \"summaries\", \"intermediates\"],\n          mutable=True,\n          trans_in_fn=prepare_vars_for_main_vmap,\n      )\n\n    stage_weights = self.get_current_stage_weights(pipeline_weights, loop_iteration)\n    stages_output = vmap_func(\n        decoder_layer_instance,\n        stage_weights,\n        stages_inputs,\n        stages_segment_ids,\n        stages_positions,\n        deterministic,\n        model_mode,\n    )\n    if self.config.scan_layers:\n      stages_output = stages_output[0]\n\n    new_state = self.get_new_loop_state(stages_output, loop_state)\n    return new_state\n\n  def get_pipeline_remat_policy(self):\n    \"\"\"Returns the pipeline remat policy for this pipeline.\"\"\"\n    # We ensure that the decoder layer inputs are saved, although we leave it to a custom\n    # policy if they should be saved to device or offloaded.\n    if self.config.remat_policy == \"custom\":\n      return self.remat_policy\n\n    save_input_policy = jax.checkpoint_policies.save_only_these_names(\"iteration_input\", \"decoder_layer_input\")\n    if self.remat_policy is not None:\n      remat_policy = jax.checkpoint_policies.save_from_both_policies(self.remat_policy, save_input_policy)\n    else:\n      remat_policy = save_input_policy\n    return remat_policy\n\n  def get_weight_sharding(self, *init_args):\n    \"\"\"get weight sharding function for this pipeline.\"\"\"\n    # Returns a partition spec of all weights. Requires passing in arguments to init.\n    key = jax.random.PRNGKey(0)\n    keys = {\"params\": key, \"dropout\": key, \"aqt\": key}\n    weights = self.init(keys, *init_args)\n\n    def get_partition_spec(pytree):\n      def _is_leaf(x):\n        return isinstance(x, nn.spmd.LogicallyPartitioned)\n\n      def get_partition_spec_leaf(leaf):\n        return leaf.get_partition_spec()\n\n      partition_spec_tree = jax.tree.map(get_partition_spec_leaf, pytree, is_leaf=_is_leaf)\n      return partition_spec_tree\n\n    partition_spec_with_extra_layer = get_partition_spec(weights)\n    partition_spec = {\"params\": partition_spec_with_extra_layer[\"params\"][\"layers\"]}\n    return partition_spec\n\n  def get_physical_spec_no_fsdp(self, full_logical):\n    \"\"\"\n    Get physical spec without fsdp.\n\n    TODO: Remove the expert sharding on attention weights as well, since those act like fsdp.\n\n    Args:\n      full_logical: original logical partition specs of all weights\n\n    Returns:\n      Modified physical spec with \"fsdp\" and \"fsdp_transpose\" removed\n    \"\"\"\n\n    def remove_fsdp_sharding(sharding_tree):\n      def _remove_fsdp_from_partition_spec(named_sharding):\n        if isinstance(named_sharding, jax.sharding.NamedSharding):\n          new_spec = []\n          for axis in named_sharding.spec:\n            if axis is None:\n              new_spec.append(None)\n            elif isinstance(axis, str):\n              if axis not in (\"fsdp\", \"fsdp_transpose\"):\n                new_spec.append(axis)\n              else:\n                new_spec.append(None)\n            elif isinstance(axis, (list, tuple)):\n              new_axis = [a for a in axis if a not in (\"fsdp\", \"fsdp_transpose\")]\n              new_spec.append(tuple(new_axis))\n            else:\n              raise ValueError(f\"Unsupported axis type: {type(axis)}\")\n          return jax.sharding.NamedSharding(named_sharding.mesh, jax.sharding.PartitionSpec(*new_spec))\n        return named_sharding\n\n      return jax.tree.map(_remove_fsdp_from_partition_spec, sharding_tree)\n\n    physical = nn.logical_to_mesh_sharding(full_logical, mesh=self.mesh, rules=self.config.logical_axis_rules)\n    physical_no_fsdp = remove_fsdp_sharding(physical)\n    return physical_no_fsdp\n\n  def all_gather_over_fsdp(self, sharding_info):\n    physical_constraint_no_fsdp = self.get_physical_spec_no_fsdp(sharding_info)\n    return jax.lax.with_sharding_constraint(self.layers.variables, physical_constraint_no_fsdp)\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      segment_ids: jnp.ndarray,\n      positions: jnp.ndarray,\n      deterministic: bool,\n      model_mode=MODEL_MODE_TRAIN,\n      partition_spec=None,  # Pytree of sharding specifications of the weights (aka self.layers.variables)\n  ) -> jnp.ndarray:\n    \"\"\"The main method that maps the series of decoder layer inputs to final layer outputs.\n    Has the same signature of a single decoder layer, and expects the same shapes, e.g. the inputs should have shape\n    [global_batch], and internally this will be reshapped into microbatches.\n    \"\"\"\n    # Reshape inputs of [global_batch, ...] to [microbatches, pipeline_microbatch_sizes, ...]\n    inputs = inputs.reshape(\n        (\n            self.config.num_pipeline_microbatches,\n            self.pipeline_microbatch_size,\n            self.config.max_target_length,\n            self.config.emb_dim,\n        )\n    )\n    example_inputs = jax.lax.broadcast(inputs[0], [self.num_stages])  # dummy inputs fed to initialize the module\n    # weights.\n    ag_sharding = jax.sharding.NamedSharding(self.mesh, jax.sharding.PartitionSpec(None, None))\n    if positions is not None:\n      # AG positions\n      positions = jax.lax.with_sharding_constraint(positions, ag_sharding)\n\n      positions = positions.reshape(\n          (self.config.num_pipeline_microbatches, self.pipeline_microbatch_size, self.config.max_target_length)\n      )\n      example_position = jax.lax.broadcast(positions[0], [self.num_stages])\n      position_idx = 0\n    else:\n      example_position = None\n      position_idx = None\n    if segment_ids is not None:\n      segment_ids = jax.lax.with_sharding_constraint(segment_ids, ag_sharding)\n      segment_ids = segment_ids.reshape(\n          (self.config.num_pipeline_microbatches, self.pipeline_microbatch_size, self.config.max_target_length)\n      )\n      example_segmentation = jax.lax.broadcast(segment_ids[0], [self.num_stages])\n      segment_idx = 0\n    else:\n      example_segmentation = None\n      segment_idx = None\n\n    loop_state = self.init_states(inputs)\n\n    # Each microbatch should go through each stage (with repeats) - so there is num_micro * (num_stages * repeats)\n    # compute to perform\n    # Each iteration is vmapped by num_stages, so the number of iterations should be\n    # num_micro * num_stages * repeats / num_stages = num_micro * repeats\n    # However due to the pipeline bubble some iterations process less than num_stages microbatches. It takes\n    # num_micro * repeat iterations for the last microbatch to start the final repeat, then an additional\n    # num_stages - 1 to finish the final repeat.\n    # Thus the total iterations is num_micro * repeat + num_stages - 1, & we may consider the num_stages - 1 as bubble.\n    # The bubble doubles when we use forwarding delay.\n    bubble_iterations = self.forwarding_delay * (self.num_stages - 1)\n    real_iterations = self.config.num_pipeline_microbatches * self.config.num_pipeline_repeats\n    total_iterations = real_iterations + bubble_iterations\n\n    if self.is_initializing():\n      vmap_func = self.get_vmap_func_for_init()\n\n      if self.config.num_pipeline_repeats > 1:\n        # To shard the weights on initialization for the circular pipeline we create weights of\n        # shape [num_repeat, num_stages, ...] (e.g. [num_repeat, num_stages, embed, mlp]) and shard the num_stages axis.\n        # We wrap the main stage vmap with a num_repeat vmap to generate this axis only for parameter initialization.\n        vmap_func = nn.vmap(\n            vmap_func,\n            in_axes=(0, segment_idx, position_idx, None, None),\n            variable_axes={\n                \"params\": 0,\n                \"_overwrite_with_gradient\": 0,\n                \"non_trainable\": 0,\n                \"hyper_params\": 0,\n            },\n            split_rngs={\"params\": True, \"dropout\": self.config.enable_dropout},\n            metadata_params={\n                nn.PARTITION_NAME: \"circular_repeats\",\n                \"sub_weight_split_dims_mapping\": (None,),\n                \"is_initializing\": True,\n                \"x_times\": self.config.num_pipeline_repeats,\n                \"optimizer_dims_mapping\": None,\n            },\n        )\n\n        example_inputs = jax.lax.broadcast(example_inputs, [self.config.num_pipeline_repeats])\n        example_segmentation = (\n            jax.lax.broadcast(example_segmentation, [self.config.num_pipeline_repeats])\n            if example_segmentation is not None\n            else None\n        )\n        example_position = (\n            jax.lax.broadcast(example_position, [self.config.num_pipeline_repeats])\n            if example_position is not None\n            else None\n        )\n      # We only need to run one set of stages to initialize the variables, instead of looping over all microbatches for\n      # the full total_iterations.\n      stage_outputs = vmap_func(\n          self.layers, example_inputs, example_segmentation, example_position, deterministic, model_mode\n      )\n      if self.config.scan_layers:\n        stage_outputs = stage_outputs[0]\n\n      # We return something of the correct shape (global_batch, sequence, embed) by reshaping a single stages output\n      # which has shape [pipeline_microbatch_size, sequence, embed]\n      if self.config.num_pipeline_repeats > 1:\n        stage_outputs = stage_outputs[0]  # Remove extra dimension created for the circular vmap\n      broadcasted_stage_outpus = jax.lax.broadcast(\n          stage_outputs[0], [self.config.micro_batch_size_to_train_on // self.pipeline_microbatch_size]\n      )\n      return jnp.reshape(\n          broadcasted_stage_outpus,\n          [self.config.micro_batch_size_to_train_on, self.config.max_target_length, self.config.emb_dim],\n      )\n\n    if self.config.pipeline_fsdp_ag_once:\n      all_pipeline_weights = all_gather_over_fsdp(\n          self.layers.variables, partition_spec, mesh=self.mesh, logical_axis_rules=self.config.logical_axis_rules\n      )\n    else:\n      all_pipeline_weights = self.layers.variables\n\n    def run_iteration_scannable(model, loop_state, xs):\n      # flax transforms like nn.scan and nn.remat can only be applied to nn.module classes or nn.module instances, so we\n      # explicitly wrap the run_one_iteration in this method - the 1st argument model (`self`) is a nn.module instance.\n      return (\n          model.run_one_iteration(\n              loop_state, all_pipeline_weights, positions, segment_ids, deterministic, model_mode, model.layers\n          ),\n          None,\n      )\n\n    if self.config.set_remat_policy_on_pipeline_iterations:\n      run_iteration_scannable = nn.remat(\n          run_iteration_scannable,\n          prevent_cse=not self.config.scan_pipeline_iterations,  # prevent_cse not used with scan\n          policy=self.get_pipeline_remat_policy(),\n      )\n\n    # The scan cannot be used on init since it broadcasts the weights, which aren't yet initialized.\n    if self.config.scan_pipeline_iterations:\n      variable_carry = []\n      variable_broadcast = [\n          \"params\",\n          \"_overwrite_with_gradient\",\n      ]  # All loop iterations need the weights for the full pipeline.\n      if self.is_mutable_collection(\"non_trainable\"):\n        variable_carry.append(\"non_trainable\")\n      else:\n        variable_broadcast.append(\"non_trainable\")\n      run_all_iterations_scanned = nn.scan(\n          run_iteration_scannable,\n          variable_axes={\n              \"summaries\": 0,\n              \"aux_loss\": 0,\n              \"intermediates\": 0,\n              \"hyper_params\": 0,\n          },\n          variable_broadcast=variable_broadcast,\n          variable_carry=variable_carry,\n          # Dropout/aqt keys will be split for each iteration.\n          split_rngs={\"random\": True},\n          length=total_iterations,\n      )\n      loop_state, _ = run_all_iterations_scanned(self, loop_state, None)\n    else:\n      for _ in range(total_iterations):\n        loop_state, _ = run_iteration_scannable(self, loop_state, None)\n\n    # The final output is located in the input/output array, however the output microbatches may be permuted relative to\n    # the input\n    final_output = self.permute_output_micro_per_stage_dim(loop_state[\"state_io\"])\n\n    # reshape outputs to match input shape of total batch instead of microbatches [batch, sequence, embed]\n    final_output = jnp.reshape(\n        final_output, (self.config.micro_batch_size_to_train_on, self.config.max_target_length, self.config.emb_dim)\n    )\n\n    return final_output",
        "analysis": {
            "module_type": "pipeline_parallelism_wrapper",
            "purpose": "Implements pipeline parallelism by wrapping a layer module, managing microbatching, and executing stages in parallel across devices. It supports both standard and circular pipelining.",
            "input": {
                "shape": "[micro_batch_size_to_train_on, max_target_length, emb_dim]",
                "dtype": "jnp.ndarray"
            },
            "processing_steps": [
                "The __call__ method orchestrates the entire pipeline execution.",
                "Reshape inputs into microbatches.",
                "Initialize pipeline state buffers (state_io, shift, etc.) using init_states.",
                "Determine the total number of loop iterations required, including pipeline bubbles.",
                "If initializing, run a vmapped version of the layer once to initialize parameters and return.",
                "If executing, loop for the total number of iterations.",
                "In each iteration, call run_one_iteration which prepares inputs, selects weights, executes the vmapped layer across stages, and updates the state.",
                "After the loop, permute the output microbatches to their correct order.",
                "Reshape the final output to match the original global batch shape."
            ],
            "output": {
                "shape": "[micro_batch_size_to_train_on, max_target_length, emb_dim]"
            },
            "dependencies": [
                "flax.linen.Module",
                "jax.sharding.Mesh",
                "Config",
                "maxtext_utils.all_gather_over_fsdp"
            ],
            "parameters": {
                "config": "A configuration object containing parameters like num_pipeline_microbatches, num_pipeline_repeats, ici_pipeline_parallelism, dcn_pipeline_parallelism, and remat policies.",
                "layers": "The nn.Module instance (e.g., a single or scanned set of decoder layers) to be executed by each pipeline stage.",
                "mesh": "The JAX device mesh used for sharding.",
                "remat_policy": "The rematerialization policy to apply to the pipeline loop iterations."
            },
            "notes": [
                "This module uses `nn.vmap` to execute the wrapped `layers` module in parallel across different pipeline stages.",
                "It manages complex state buffers (`state_io`, `shift`, `circ_storage`) to handle the flow of activations between stages and across loop iterations.",
                "The execution loop can be implemented with either a Python `for` loop or a more performant `nn.scan`.",
                "It has distinct logic paths for parameter initialization versus actual model execution (training/inference)."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes pipeline configuration attributes based on the main config object.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculate `num_stages` from parallelism degrees.",
                        "Set `forwarding_delay` based on config.",
                        "Calculate `pipeline_microbatch_size` and `microbatches_per_stage`.",
                        "Determine if circular storage is needed by calling `need_circ_storage`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "self.config",
                        "self.need_circ_storage"
                    ],
                    "notes": [
                        "This method is called automatically by Flax during module initialization."
                    ]
                },
                "init_states": {
                    "purpose": "Initializes the state buffers (e.g., for inputs/outputs, inter-stage communication, and circular storage) required for the pipeline execution loop.",
                    "input": {
                        "shape": "[num_micro_batches, micro_batch_size, sequence, embed]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Initialize `shift` buffer with zeros for passing activations between stages.",
                        "Initialize `prev_outputs` buffer if delayed activation forwarding is enabled.",
                        "Reshape the input tensor into the `state_io` buffer, which holds both inputs and final outputs.",
                        "Initialize `circ_storage` and `circ_storage_mover` buffers if circular pipelining is active.",
                        "Initialize `loop_iteration` counter to 0.",
                        "Return all buffers and the counter in a dictionary."
                    ],
                    "output": {
                        "shape": "A dictionary of state tensors."
                    },
                    "dependencies": [
                        "jax.numpy.zeros",
                        "jax.numpy.reshape",
                        "flax.linen.with_logical_constraint"
                    ],
                    "notes": [
                        "The shapes and sharding of these buffers are critical for the pipeline's correctness and performance."
                    ]
                },
                "run_one_iteration": {
                    "purpose": "Executes a single step of the pipeline loop, processing one microbatch per active stage in parallel.",
                    "input": {
                        "shape": "Takes a `loop_state` dictionary, `pipeline_weights` pytree, and other model inputs like `positions` and `segment_ids`.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determine which microbatch and repeat each stage is currently processing.",
                        "Prepare the inputs for each stage using `get_iteration_inputs`.",
                        "Gather the correct `positions` and `segment_ids` for each stage's input.",
                        "Select the appropriate weights for each stage for the current iteration using `get_current_stage_weights`.",
                        "Execute the wrapped layer on all stages in parallel using `nn.vmap`.",
                        "Update the pipeline state buffers with the new outputs using `get_new_loop_state`."
                    ],
                    "output": {
                        "shape": "An updated `loop_state` dictionary."
                    },
                    "dependencies": [
                        "self.get_iteration_inputs",
                        "self.get_current_stage_weights",
                        "self.get_new_loop_state",
                        "flax.linen.vmap"
                    ],
                    "notes": [
                        "This function encapsulates the core computation and data movement for one step of the pipeline."
                    ]
                },
                "__call__": {
                    "purpose": "The main entry point that orchestrates the entire pipeline execution, from input preparation to the final output.",
                    "input": {
                        "shape": "inputs: [global_batch_size, sequence_length, hidden_dim]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Reshape inputs from a global batch into microbatches.",
                        "Initialize the pipeline state using `init_states`.",
                        "Calculate the total number of iterations for the main loop.",
                        "Handle the special case for weight initialization.",
                        "Execute the main loop, calling a (potentially rematerialized) version of `run_one_iteration` at each step.",
                        "Permute the final outputs in the `state_io` buffer to match the original microbatch order.",
                        "Reshape the output from microbatches back to a single global batch tensor."
                    ],
                    "output": {
                        "shape": "[global_batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "self.init_states",
                        "self.run_one_iteration",
                        "self.permute_output_micro_per_stage_dim",
                        "flax.linen.scan",
                        "flax.linen.remat"
                    ],
                    "notes": [
                        "This method contains the high-level control flow for the pipeline, including the main execution loop which can be a Python `for` loop or `nn.scan`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#Quantization",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class Quantization:\n  \"\"\"Base class for quantization configurations\"\"\"\n\n  def dot_general_cls(self, mesh_axes: Tuple[str, ...] = ()):\n    \"\"\"Placeholder for dot_general implementation in subclasses.\"\"\"\n\n  def einsum(self, dtype: DType = jnp.float32):\n    \"\"\"Placeholder for einsum implementation in subclasses.\"\"\"",
        "analysis": {
            "module_type": "quantization_base_class",
            "purpose": "Serves as an abstract base class for defining different quantization configurations and their corresponding operations.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "dataclasses.dataclass",
                "jax.numpy as jnp",
                "MaxText.common_types.DType"
            ],
            "parameters": {},
            "notes": [
                "This class is intended to be subclassed to provide concrete implementations for different quantization strategies (e.g., AQT, FP8).",
                "It defines a common interface for obtaining quantized linear algebra operations."
            ],
            "methods": {
                "dot_general_cls": {
                    "purpose": "Acts as a placeholder method intended to be overridden by subclasses to return a specific quantized `dot_general` class or function.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "This method is a placeholder and performs no operations."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": [
                        "The `mesh_axes` parameter is expected to be used by subclasses to configure tensor sharding for the operation.",
                        "The return value should be a callable (like a Flax Module class) that implements a quantized dot-general operation."
                    ]
                },
                "einsum": {
                    "purpose": "Acts as a placeholder method intended to be overridden by subclasses to return a specific quantized `einsum` function or module.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "This method is a placeholder and performs no operations."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "DType",
                        "jnp"
                    ],
                    "notes": [
                        "The `dtype` parameter is expected to be used by subclasses to set the computation data type for the einsum operation.",
                        "The return value should be a callable that implements a quantized einsum operation."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_tiling_fn",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _tiling_fn(lhs, rhs, dimension_numbers, tile_size):\n  \"\"\"apply tiling function\"\"\"\n  del lhs, rhs\n\n  (lhs_ca, rhs_ca), _ = dimension_numbers\n  ret = tiled_dot_general.Cfg(\n      lhs=tiled_dot_general.TensorTiling(contraction_axes=[], remaining_axes=[]),\n      rhs=tiled_dot_general.TensorTiling(contraction_axes=[], remaining_axes=[]),\n  )\n\n  for lhs_idx, rhs_idx in zip(lhs_ca, rhs_ca):\n    ret.lhs.contraction_axes.append(tiled_dot_general.AxisTiling(axis=lhs_idx, tile_size=tile_size, tile_count=None))\n    ret.rhs.contraction_axes.append(tiled_dot_general.AxisTiling(axis=rhs_idx, tile_size=tile_size, tile_count=None))\n\n  return ret",
        "analysis": {
            "module_type": "tiling_configuration_function",
            "purpose": "Creates a configuration object for tiled dot-general operations by specifying how contracting axes should be tiled.",
            "input": {
                "shape": "lhs: any, rhs: any, dimension_numbers: tuple, tile_size: int",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Delete the `lhs` and `rhs` input arguments as they are not used.",
                "Unpack the contracting axes for the left-hand side (lhs_ca) and right-hand side (rhs_ca) from `dimension_numbers`.",
                "Initialize a `tiled_dot_general.Cfg` object with empty tiling configurations.",
                "Iterate through the corresponding contracting axes from `lhs_ca` and `rhs_ca`.",
                "For each pair of axes, create a `tiled_dot_general.AxisTiling` object specifying the axis index and the `tile_size`.",
                "Append the created `AxisTiling` object to the `contraction_axes` list for the respective lhs and rhs configurations within the main Cfg object.",
                "Return the configured `tiled_dot_general.Cfg` object."
            ],
            "output": {
                "shape": "An instance of `tiled_dot_general.Cfg`."
            },
            "dependencies": [
                "aqt.jax.v2.tiled_dot_general.Cfg",
                "aqt.jax.v2.tiled_dot_general.TensorTiling",
                "aqt.jax.v2.tiled_dot_general.AxisTiling"
            ],
            "parameters": {
                "dimension_numbers": "A tuple specifying the contracting axes for the dot product, e.g., `((lhs_contracting_axes, rhs_contracting_axes), batch_axes)`.",
                "tile_size": "The size of the tiles to be applied along the contracting dimensions."
            },
            "notes": [
                "This function is designed to be used as a callback (`tiling_fn`) for AQT's dot-general operations.",
                "The input tensors `lhs` and `rhs` are ignored, as the function only needs dimension metadata to create the configuration.",
                "It only configures tiling for contraction axes; remaining axes are not configured for tiling."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_rhs_axis_metadata_wrapper",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _rhs_axis_metadata_wrapper(\n    x: jnp.ndarray,\n    tile_map,\n    no_sharding_axis: Sequence[int],\n    mesh_axes: Tuple[str, ...],\n    is_tiled: bool,\n    replicate_scale: bool = False,\n):\n  \"\"\"right-hand-side axis metadata wrapper\"\"\"\n  if replicate_scale:\n    # Temporarily using the shape to identify the scale.\n    # TODO: remove the replication once the 2d sharding quantization\n    # works as expected.\n    if len(x.shape) == 1:\n      return nn.with_logical_partitioning((lambda: x), tuple(None for _ in mesh_axes))()\n\n  mesh_axes = list(mesh_axes)\n  if is_tiled:\n    # tile_map is a mapping between original rank and a list of new, tiled rank.\n    if len(mesh_axes) < len(tile_map):\n      mesh_axes = [None] * (len(tile_map) - len(mesh_axes)) + mesh_axes\n    new_mesh_axes = [None] * len(x.shape)\n    for orig_rank, new_rank in tile_map.items():\n      assert new_rank\n      assert len(new_rank) <= 2\n      new_mesh_axes[new_rank[-1]] = mesh_axes[orig_rank]\n    mesh_axes = new_mesh_axes\n\n  if mesh_axes is not None and len(mesh_axes) > 0:\n    for no_shard_idx in no_sharding_axis:\n      if no_shard_idx < len(mesh_axes):\n        mesh_axes[no_shard_idx] = None\n\n  return nn.with_logical_partitioning((lambda: x), mesh_axes)()",
        "analysis": {
            "functionality": "A wrapper function that applies logical partitioning (sharding) annotations to a JAX tensor. It handles complex sharding scenarios, including remapping sharding axes for tiled tensors and replicating 1D scale tensors across a device mesh.",
            "usage": "This function is used internally to configure the sharding for the right-hand-side (RHS) tensor in quantized matrix multiplications. Call it with a tensor `x` and sharding configuration parameters like `mesh_axes`, `is_tiled`, and `tile_map`. It returns the tensor `x` with the appropriate `with_logical_partitioning` annotations applied, without changing its shape or data."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#AqtQuantization",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class AqtQuantization:\n  \"\"\"Configures AQT quantization github.com/google/aqt.\"\"\"\n\n  quant_dg: aqt_config.DotGeneral\n  quant_mode: aqt_flax.QuantMode = aqt_flax.QuantMode.TRAIN\n  replicate_scale: bool = False\n\n  def _get_mixed_precision_cfg(self):\n    \"\"\"get configuration for mixed precision\"\"\"\n    quant_dg = None\n    is_tiled = False\n    tiling_fn = None\n    # pylint: disable=protected-access\n    module_path = \"/\".join(nn.module._context.module_stack[-1].path)\n    tile_size = -1\n    for layer_name_re, layer_quant_dg in self.quant_dg.items():\n      if re.fullmatch(layer_name_re, module_path):\n        quant_dg, tile_size = layer_quant_dg\n    if quant_dg is None:\n      quant_dg, tile_size = self.quant_dg[DEFAULT]\n    if tile_size != -1:\n      is_tiled = True\n      tiling_fn = functools.partial(_tiling_fn, tile_size=tile_size)\n    return quant_dg, is_tiled, tiling_fn\n\n  def _get_rhs_axis_metadata_wrapper(\n      self, mesh_axes: Tuple[str, ...] = (), is_tiled: bool = False, replicate_scale: bool = False\n  ):\n    if self.quant_mode == aqt_flax.QuantMode.CONVERT:\n      return None\n    return functools.partial(\n        _rhs_axis_metadata_wrapper, mesh_axes=mesh_axes, is_tiled=is_tiled, replicate_scale=replicate_scale\n    )\n\n  def dot_general_cls(self, mesh_axes: Tuple[str, ...] = ()):\n    \"\"\"Returns dot_general configured with aqt params.\"\"\"\n    if isinstance(self.quant_dg, dict):\n      quant_dg, is_tiled, tiling_fn = self._get_mixed_precision_cfg()\n    else:\n      quant_dg, is_tiled, tiling_fn = self.quant_dg, False, None\n    rhs_axis_metadata_wrapper = self._get_rhs_axis_metadata_wrapper(\n        mesh_axes, is_tiled, replicate_scale=self.replicate_scale\n    )\n    # module_path = \"/\".join(nn.module._context.module_stack[-1].path)\n    # print(f\"quant_dg: {quant_dg}, is_tiled: {is_tiled}, module_path: {module_path}\")\n    aqt_dg_cls = functools.partial(\n        aqt_flax.AqtDotGeneral,\n        quant_dg,\n        rhs_quant_mode=self.quant_mode,\n        lhs_freeze_mode=aqt_flax.FreezerMode.NONE,\n        rhs_freeze_mode=aqt_flax.FreezerMode.CALIBRATION_AND_VALUE,\n        rhs_axis_metadata_wrapper=rhs_axis_metadata_wrapper,\n        use_legacy_freezer=False,\n        tiling_fn=tiling_fn,\n    )\n    return aqt_dg_cls\n\n  def einsum(self, mesh_axes: Tuple[str, ...] = ()):\n    \"\"\"Returns einsum configured with aqt params.\"\"\"\n    if isinstance(self.quant_dg, dict):\n      quant_dg, is_tiled, tiling_fn = self._get_mixed_precision_cfg()\n    else:\n      quant_dg, is_tiled, tiling_fn = self.quant_dg, False, None\n\n    rhs_axis_metadata_wrapper = self._get_rhs_axis_metadata_wrapper(\n        mesh_axes, is_tiled, replicate_scale=self.replicate_scale\n    )\n    aqt_einsum = functools.partial(\n        aqt_flax.AqtEinsum(\n            cfg=quant_dg,\n            rhs_quant_mode=self.quant_mode,\n            lhs_freeze_mode=aqt_flax.FreezerMode.NONE,\n            rhs_freeze_mode=aqt_flax.FreezerMode.CALIBRATION_AND_VALUE,\n            rhs_axis_metadata_wrapper=rhs_axis_metadata_wrapper,\n            use_legacy_freezer=False,\n            tiling_fn=tiling_fn,\n        )\n    )\n    return aqt_einsum",
        "analysis": {
            "module_type": "aqt_quantization_config",
            "purpose": "A configuration class that provides factory methods for creating AQT (Algorithm Quantization Toolkit) quantized operations like dot_general and einsum.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "aqt.jax.v2.config.DotGeneral",
                "aqt.jax.v2.flax.aqt_flax.QuantMode",
                "aqt.jax.v2.flax.aqt_flax.AqtDotGeneral",
                "aqt.jax.v2.flax.aqt_flax.AqtEinsum",
                "functools.partial",
                "flax.linen.module"
            ],
            "parameters": {
                "quant_dg": "The core AQT configuration for dot-general operations. Can be a single `aqt_config.DotGeneral` object or a dictionary mapping regex layer names to specific configurations for mixed-precision quantization.",
                "quant_mode": "The quantization mode (e.g., TRAIN, SERVE, CONVERT) from `aqt_flax.QuantMode`.",
                "replicate_scale": "A boolean flag to control whether quantization scales are replicated across mesh axes, which can be useful for 2D sharding."
            },
            "notes": [
                "This class acts as a factory for quantized operations.",
                "It supports mixed-precision quantization by allowing different `quant_dg` configurations for different layers, identified by matching the layer's module path against regular expressions."
            ],
            "methods": {
                "_get_mixed_precision_cfg": {
                    "purpose": "Retrieves the appropriate AQT configuration (`quant_dg`), tiling status, and tiling function for the current Flax module based on its path.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Get the current module's path from the Flax module context stack.",
                        "Iterate through the `self.quant_dg` dictionary, which maps regex patterns to quantization configurations.",
                        "Use `re.fullmatch` to find a configuration matching the module path.",
                        "If no specific match is found, fall back to the default configuration.",
                        "If a `tile_size` is specified, enable tiling and create a partial `tiling_fn`.",
                        "Return the selected configuration, tiling status, and tiling function."
                    ],
                    "output": {
                        "shape": "Returns a tuple of (aqt_config.DotGeneral, bool, callable)."
                    },
                    "dependencies": [
                        "re",
                        "functools.partial",
                        "flax.linen.module",
                        "_tiling_fn"
                    ],
                    "notes": [
                        "This method is key to enabling layer-specific quantization settings."
                    ]
                },
                "_get_rhs_axis_metadata_wrapper": {
                    "purpose": "Creates a wrapper function for applying sharding metadata to the right-hand-side (RHS) tensor of a dot product, especially for tiled quantization.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if the `quant_mode` is `CONVERT`, in which case no metadata is needed and `None` is returned.",
                        "Return a `functools.partial` of the `_rhs_axis_metadata_wrapper` helper function, pre-configured with the provided mesh axes, tiling status, and scale replication flag."
                    ],
                    "output": {
                        "shape": "Returns a callable function or None."
                    },
                    "dependencies": [
                        "functools.partial",
                        "aqt_flax.QuantMode",
                        "_rhs_axis_metadata_wrapper"
                    ],
                    "notes": [
                        "The returned function is intended to be passed to the AQT dot_general implementation to handle tensor sharding annotations."
                    ]
                },
                "dot_general_cls": {
                    "purpose": "Returns a partially configured `aqt_flax.AqtDotGeneral` class, ready to be used as a quantized replacement for `jax.lax.dot_general`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Get the appropriate quantization configuration, potentially calling `_get_mixed_precision_cfg`.",
                        "Get the RHS axis metadata wrapper by calling `_get_rhs_axis_metadata_wrapper`.",
                        "Create a `functools.partial` of `aqt_flax.AqtDotGeneral` with all the necessary quantization parameters pre-filled.",
                        "Return the partially configured class."
                    ],
                    "output": {
                        "shape": "Returns a callable (partially configured class)."
                    },
                    "dependencies": [
                        "self._get_mixed_precision_cfg",
                        "self._get_rhs_axis_metadata_wrapper",
                        "functools.partial",
                        "aqt_flax.AqtDotGeneral"
                    ],
                    "notes": [
                        "This is a primary factory method for obtaining a quantized dot_general operation."
                    ]
                },
                "einsum": {
                    "purpose": "Returns a partially configured `aqt_flax.AqtEinsum` instance, ready to be used as a quantized replacement for `jnp.einsum`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Get the appropriate quantization configuration, potentially calling `_get_mixed_precision_cfg`.",
                        "Get the RHS axis metadata wrapper by calling `_get_rhs_axis_metadata_wrapper`.",
                        "Create a `functools.partial` of an `aqt_flax.AqtEinsum` instance with all necessary quantization parameters pre-filled.",
                        "Return the partially configured einsum function."
                    ],
                    "output": {
                        "shape": "Returns a callable function."
                    },
                    "dependencies": [
                        "self._get_mixed_precision_cfg",
                        "self._get_rhs_axis_metadata_wrapper",
                        "functools.partial",
                        "aqt_flax.AqtEinsum"
                    ],
                    "notes": [
                        "This is a primary factory method for obtaining a quantized einsum operation."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#Fp8Quantization",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class Fp8Quantization(Quantization):\n  \"\"\"Configures Fp8 quantization for NVIDIA GPUs\"\"\"\n\n  quant_mode = \"train\"\n\n  def dot_general_cls(self, mesh_axes: Tuple[str, ...] = ()):\n    \"\"\"Returns dot_general configured with aqt params.\"\"\"\n    return nn.Fp8DirectDotGeneralOp\n\n  def einsum(self, dtype: DType = jnp.float32):\n    return _Fp8EinsumWrapper(dtype=dtype)",
        "analysis": {
            "module_type": "fp8_quantization_config",
            "purpose": "A dataclass that configures and provides operators for Fp8 quantization, specifically for NVIDIA GPUs.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "This class does not process tensors directly but provides methods that return quantization-aware operators."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "Quantization",
                "flax.linen.Fp8DirectDotGeneralOp",
                "_Fp8EinsumWrapper"
            ],
            "parameters": {},
            "notes": [
                "This class inherits from the base `Quantization` class.",
                "The `quant_mode` is a class attribute hardcoded to 'train'."
            ],
            "methods": {
                "dot_general_cls": {
                    "purpose": "Returns the Fp8 dot-general operator class suitable for NVIDIA GPUs.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Returns the `nn.Fp8DirectDotGeneralOp` class."
                    ],
                    "output": {
                        "shape": "Returns a class, not a tensor. Shape is N/A."
                    },
                    "dependencies": [
                        "flax.linen.Fp8DirectDotGeneralOp"
                    ],
                    "notes": [
                        "The `mesh_axes` argument is accepted for API compatibility but is not used in this implementation."
                    ]
                },
                "einsum": {
                    "purpose": "Returns a module that wraps the Fp8 einsum operation to handle a specific computation data type.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "Takes a DType like jnp.float32 as an argument."
                    },
                    "processing_steps": [
                        "Instantiates and returns an `_Fp8EinsumWrapper` with the provided `dtype`."
                    ],
                    "output": {
                        "shape": "Returns a Flax Module instance, not a tensor. Shape is N/A."
                    },
                    "dependencies": [
                        "_Fp8EinsumWrapper"
                    ],
                    "notes": []
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_Fp8EinsumWrapper",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class _Fp8EinsumWrapper(nn.Module):\n  \"\"\"Wrapper for nn.Fp8Einsum to handle computation dtype.\"\"\"\n\n  dtype: DType\n\n  @nn.compact\n  def __call__(self, eqn, lhs, rhs, **kwargs):\n    # nn.Fp8Einsum determines compute dtype from rhs.\n    # We cast rhs to the desired computation dtype.\n    # nn.Fp8Einsum will then cast lhs to the same dtype.\n    rhs = rhs.astype(self.dtype)\n    return nn.Fp8Einsum(name=\"fp8_einsum\")(eqn, lhs, rhs, **kwargs)",
        "analysis": {
            "module_type": "fp8_einsum_wrapper",
            "purpose": "A Flax module that wraps nn.Fp8Einsum to explicitly control the computation data type by casting the right-hand side tensor.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.linen.Module",
                "flax.linen.Fp8Einsum"
            ],
            "parameters": {
                "dtype": "The desired computation data type for the einsum operation."
            },
            "notes": [
                "This wrapper exists because nn.Fp8Einsum infers the computation dtype from its right-hand side (rhs) argument. By casting rhs, this module forces a specific computation dtype."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Performs an FP8 einsum operation with a specific, user-defined computation data type.",
                    "input": {
                        "shape": "eqn: string, lhs: [shape_A], rhs: [shape_B], where shapes are compatible with the 'eqn' string.",
                        "dtype": "float"
                    },
                    "processing_steps": [
                        "Cast the `rhs` tensor to the `self.dtype` specified during module initialization.",
                        "Call `nn.Fp8Einsum` with the equation string, `lhs`, the casted `rhs`, and any additional keyword arguments."
                    ],
                    "output": {
                        "shape": "The output shape is determined by the einsum equation string 'eqn'."
                    },
                    "dependencies": [
                        "flax.linen.Fp8Einsum"
                    ],
                    "notes": [
                        "The key operation is casting `rhs` before the `nn.Fp8Einsum` call to control the computation dtype, as `nn.Fp8Einsum` uses the dtype of `rhs` to determine it."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#Fp8Einsum",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class Fp8Einsum(nn.Module):\n  \"\"\"An fp8 einsum op.\n\n  Attributes:\n    amax_history_length: size of the amax history.\n    e4m3_dtype: e4m3 variants, e.g., e4m3fn, e4m3fnuz.\n    e5m2_dtype: e5m2 variants, e.g., e5m2, e5m2fnuz.\n    dtype: computation dtype.\n  \"\"\"\n\n  amax_history_length: int = 1024\n  e4m3_dtype: DType = jnp.float8_e4m3fn\n  e5m2_dtype: DType = jnp.float8_e5m2\n  dtype: DType = jnp.float32\n\n  def setup(self) -> None:\n    \"\"\"init with input_amax_history, kernel_amax_history, output_grad_amax_history,\n    input_scale, kernel_scale, output_grad_scale\"\"\"\n    scale_args = (\n        flax_initializers.ones_init(),\n        jax.random.PRNGKey(0),\n        (1,),\n        jnp.float32,\n    )\n    amax_history_args = (\n        flax_initializers.zeros_init(),\n        jax.random.PRNGKey(0),\n        (self.amax_history_length,),\n        jnp.float32,\n    )\n\n    OVERWRITE_WITH_GRADIENT = \"_overwrite_with_gradient\"\n    self.input_amax_history = self.variable(OVERWRITE_WITH_GRADIENT, \"input_amax_history\", *amax_history_args)\n    self.kernel_amax_history = self.variable(OVERWRITE_WITH_GRADIENT, \"kernel_amax_history\", *amax_history_args)\n    self.output_grad_amax_history = self.variable(OVERWRITE_WITH_GRADIENT, \"output_grad_amax_history\", *amax_history_args)\n\n    self.input_scale = self.variable(OVERWRITE_WITH_GRADIENT, \"input_scale\", *scale_args)\n    self.kernel_scale = self.variable(OVERWRITE_WITH_GRADIENT, \"kernel_scale\", *scale_args)\n    self.output_grad_scale = self.variable(OVERWRITE_WITH_GRADIENT, \"output_grad_scale\", *scale_args)\n\n  def __call__(self, eqn, *args, **kwargs):\n    assert len(args) == 2\n    x = args[0]\n    k = args[1]\n\n    comp_dtype = self.dtype\n    k = jnp.asarray(k, comp_dtype)\n    x = jnp.asarray(x, comp_dtype)\n\n    x_qdq = fp8_ops.in_qdq(comp_dtype, self.e4m3_dtype, x, self.input_scale.value, self.input_amax_history.value)\n    k_qdq = fp8_ops.in_qdq(comp_dtype, self.e4m3_dtype, k, self.kernel_scale.value, self.kernel_amax_history.value)\n\n    y_qdq = jnp.einsum(eqn, x_qdq, k_qdq, _dot_general=fp8_ops.dot_general_with_precision)\n\n    y = fp8_ops.out_qdq(\n        comp_dtype,\n        self.e5m2_dtype,\n        y_qdq,\n        self.output_grad_scale.value,\n        self.output_grad_amax_history.value,\n    )\n    return y",
        "analysis": {
            "module_type": "fp8_einsum",
            "purpose": "Performs an Einstein summation (einsum) operation on two input tensors using 8-bit floating-point (FP8) precision for the intermediate computation, handling the necessary quantization and dequantization steps.",
            "input": {
                "shape": "An einsum equation string, followed by two tensors whose shapes must be compatible with the equation.",
                "dtype": "Any JAX numeric type, will be cast to the module's `dtype` parameter (e.g., float32)."
            },
            "processing_steps": [
                "Casts the two input tensors to the specified computation `dtype`.",
                "Applies input quantization and de-quantization (`in_qdq`) to both tensors using `e4m3_dtype` and their respective scales and amax histories.",
                "Performs the `jnp.einsum` operation using a custom `dot_general` implementation suitable for FP8.",
                "Applies output quantization and de-quantization (`out_qdq`) to the result, which is used for scaling the gradient in the backward pass.",
                "Returns the final computed tensor."
            ],
            "output": {
                "shape": "The output shape is determined by the provided einsum equation string `eqn`."
            },
            "dependencies": [
                "flax.linen.nn.Module",
                "flax.linen.fp8_ops",
                "jax.numpy",
                "flax.linen.initializers"
            ],
            "parameters": {
                "amax_history_length": "The size of the amax history buffer used for dynamic scaling.",
                "e4m3_dtype": "The FP8 data type (e.g., jnp.float8_e4m3fn) used for quantizing the input tensors.",
                "e5m2_dtype": "The FP8 data type (e.g., jnp.float8_e5m2) used for handling the output gradient scaling.",
                "dtype": "The data type for the intermediate computation (e.g., jnp.float32)."
            },
            "notes": [
                "This module manages its own state for FP8 scaling factors and amax history, which are updated during execution.",
                "The state variables are stored in the `_overwrite_with_gradient` collection, indicating they are intended to be updated during training."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes state variables for amax history and scaling factors for inputs, kernel, and output gradients.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Define arguments for initializing scale variables to ones.",
                        "Define arguments for initializing amax history buffers to zeros.",
                        "Create six variables in the '_overwrite_with_gradient' collection: `input_amax_history`, `kernel_amax_history`, `output_grad_amax_history`, `input_scale`, `kernel_scale`, and `output_grad_scale`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "flax.linen.initializers"
                    ],
                    "notes": [
                        "This method is called automatically by Flax during module initialization."
                    ]
                },
                "__call__": {
                    "purpose": "Executes the FP8-quantized einsum operation on two input tensors.",
                    "input": {
                        "shape": "An einsum equation string `eqn`, followed by two JAX arrays `x` and `k`.",
                        "dtype": "Any JAX numeric type."
                    },
                    "processing_steps": [
                        "Assert that exactly two tensor arguments are provided.",
                        "Cast input tensors `x` and `k` to the computation dtype.",
                        "Apply `fp8_ops.in_qdq` to `x` using `input_scale` and `input_amax_history`.",
                        "Apply `fp8_ops.in_qdq` to `k` using `kernel_scale` and `kernel_amax_history`.",
                        "Perform `jnp.einsum` with a custom `dot_general` for FP8.",
                        "Apply `fp8_ops.out_qdq` to the result for gradient scaling.",
                        "Return the final tensor."
                    ],
                    "output": {
                        "shape": "Determined by the einsum equation `eqn`."
                    },
                    "dependencies": [
                        "jax.numpy",
                        "flax.linen.fp8_ops"
                    ],
                    "notes": [
                        "The inputs are quantized to `e4m3_dtype`, while the output dequantization uses `e5m2_dtype` for the backward pass."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#NANOOFp8Quantization",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class NANOOFp8Quantization(Quantization):\n  \"\"\"Configures NANOO Fp8 quantization for AMD MI300/MI325 GPUs\"\"\"\n\n  quant_mode = \"train\"\n\n  def dot_general_cls(self, mesh_axes: Tuple[str, ...] = ()):\n    \"\"\"Returns dot_general configured with aqt params.\"\"\"\n    return nn.NANOOFp8DotGeneralOp",
        "analysis": {
            "module_type": "nanoo_fp8_quantization",
            "purpose": "Configures NANOO Fp8 quantization for AMD MI300/MI325 GPUs by providing the appropriate dot_general operation class.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "Quantization",
                "flax.linen.NANOOFp8DotGeneralOp"
            ],
            "parameters": {
                "quant_mode": "A class attribute indicating the quantization mode, hardcoded to 'train'."
            },
            "notes": [
                "This class is specifically designed for AMD MI300/MI325 GPUs."
            ],
            "methods": {
                "dot_general_cls": {
                    "purpose": "Returns the Flax Linen class `NANOOFp8DotGeneralOp` to be used for dot_general operations.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Return the `nn.NANOOFp8DotGeneralOp` class."
                    ],
                    "output": {
                        "shape": "Returns a class type, not a tensor. Shape is N/A."
                    },
                    "dependencies": [
                        "flax.linen.NANOOFp8DotGeneralOp"
                    ],
                    "notes": [
                        "The `mesh_axes` parameter is accepted for interface compatibility but is not used in this implementation."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_int8_quant_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_int8_quant_config(config):\n  drhs_bits = None\n  drhs_accumulator_dtype = None\n  drhs_local_aqt = None\n  if config.quantization_local_shard_count != 0:\n    drhs_bits = 8\n    drhs_accumulator_dtype = jnp.int32\n    drhs_local_aqt = aqt_config.LocalAqt(contraction_axis_shard_count=config.quantization_local_shard_count)\n  return aqt_config.config_v3(\n      fwd_bits=8,\n      dlhs_bits=8,\n      drhs_bits=drhs_bits,\n      rng_type=\"jax.uniform\",\n      dlhs_local_aqt=None,\n      drhs_local_aqt=drhs_local_aqt,\n      fwd_accumulator_dtype=jnp.int32,\n      dlhs_accumulator_dtype=jnp.int32,\n      drhs_accumulator_dtype=drhs_accumulator_dtype,\n  )",
        "analysis": {
            "module_type": "quantization_config_generator",
            "purpose": "Creates an AQT (Algorithm for Quantization-aware Training) v3 configuration for 8-bit integer quantization, conditionally enabling local AQT for the backward pass based on sharding settings.",
            "input": {
                "shape": "N/A",
                "dtype": "MaxText.common_types.Config"
            },
            "processing_steps": [
                "Initialize backward-pass right-hand-side (drhs) parameters to None.",
                "Check if `config.quantization_local_shard_count` is non-zero.",
                "If true, configure 8-bit quantization and local AQT for the drhs pass using the shard count.",
                "Call `aqt_config.config_v3` to create the final configuration object with fixed 8-bit forward and backward-pass left-hand-side settings, and the conditionally set drhs settings.",
                "Return the generated AQT configuration object."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "aqt.jax.v2.config.config_v3",
                "aqt.jax.v2.config.LocalAqt",
                "jax.numpy"
            ],
            "parameters": {
                "config.quantization_local_shard_count": "The number of local shards for quantization. If non-zero, it enables 8-bit quantization and local AQT for the right-hand side of the backward pass (drhs)."
            },
            "notes": [
                "This function specifically sets up a symmetric 8-bit quantization configuration.",
                "The forward pass (`fwd`) and the left-hand side of the backward pass (`dlhs`) are always configured for 8-bit quantization with a `jnp.int32` accumulator.",
                "The right-hand side of the backward pass (`drhs`) is only quantized if `config.quantization_local_shard_count` is greater than zero."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#ConstantBoundConfig",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class ConstantBoundConfig:\n  fwd_lhs_bound: float | None = None\n  fwd_rhs_bound: float | None = None\n  dlhs_lhs_bound: float | None = None\n  dlhs_rhs_bound: float | None = None\n  drhs_lhs_bound: float | None = None\n  drhs_rhs_bound: float | None = None",
        "analysis": {
            "module_type": "configuration_dataclass",
            "purpose": "A frozen dataclass to store constant bounds for static quantization calibration in AQT (Accurate Quantized Training).",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes attributes with provided floating-point values or defaults to None."
            ],
            "output": {
                "shape": "An immutable instance of ConstantBoundConfig."
            },
            "dependencies": [
                "dataclasses.dataclass"
            ],
            "parameters": {
                "fwd_lhs_bound": "The constant calibration bound for the left-hand side tensor in the forward pass.",
                "fwd_rhs_bound": "The constant calibration bound for the right-hand side tensor in the forward pass.",
                "dlhs_lhs_bound": "The constant calibration bound for the left-hand side tensor in the backward pass gradient calculation with respect to the left-hand side input (dlhs).",
                "dlhs_rhs_bound": "The constant calibration bound for the right-hand side tensor in the backward pass gradient calculation with respect to the left-hand side input (dlhs).",
                "drhs_lhs_bound": "The constant calibration bound for the left-hand side tensor in the backward pass gradient calculation with respect to the right-hand side input (drhs).",
                "drhs_rhs_bound": "The constant calibration bound for the right-hand side tensor in the backward pass gradient calculation with respect to the right-hand side input (drhs)."
            },
            "notes": [
                "The class is decorated with `@dataclass(frozen=True)`, making its instances immutable after creation.",
                "This configuration is used by the `_build_const_scale_config` function to set up `calibration.ConstantCalibration` for an AQT dot-general operation.",
                "Each attribute corresponds to a specific tensor in the forward or backward pass of a matrix multiplication."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_build_const_scale_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _build_const_scale_config(\n    aqt_dg: aqt_config.DotGeneral,\n    cst_bound_config: ConstantBoundConfig,\n) -> aqt_config.DotGeneral:\n  \"\"\"Build a constant scale config for AQT dot general.\n\n  Args:\n    aqt_dg: The AQT dot general config.\n    cst_bound_config: The constant bound config.\n\n  Returns:\n    The AQT dot general config with constant scale config.\n  \"\"\"\n  if cst_bound_config.fwd_lhs_bound is not None:\n    aqt_dg.fwd.dg_quantizer.lhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.fwd_lhs_bound\n    )\n  if cst_bound_config.fwd_rhs_bound is not None:\n    aqt_dg.fwd.dg_quantizer.rhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.fwd_rhs_bound\n    )\n  if cst_bound_config.dlhs_lhs_bound:\n    aqt_dg.dlhs.dg_quantizer.lhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.dlhs_lhs_bound\n    )\n\n  if cst_bound_config.dlhs_rhs_bound is not None:\n    aqt_dg.dlhs.dg_quantizer.rhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.dlhs_rhs_bound\n    )\n\n  if cst_bound_config.drhs_lhs_bound is not None:\n    aqt_dg.drhs.dg_quantizer.lhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.drhs_lhs_bound\n    )\n\n  if cst_bound_config.drhs_rhs_bound is not None:\n    aqt_dg.drhs.dg_quantizer.rhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.drhs_rhs_bound\n    )\n\n  return aqt_dg",
        "analysis": {
            "module_type": "aqt_config_modifier",
            "purpose": "Modifies an AQT DotGeneral configuration to use constant calibration bounds for quantization scaling.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Conditionally check for a forward pass left-hand-side bound in `cst_bound_config` and update the `aqt_dg` calibration if present.",
                "Conditionally check for a forward pass right-hand-side bound in `cst_bound_config` and update the `aqt_dg` calibration if present.",
                "Conditionally check for a backward pass (dlhs) left-hand-side bound in `cst_bound_config` and update the `aqt_dg` calibration if present.",
                "Conditionally check for a backward pass (dlhs) right-hand-side bound in `cst_bound_config` and update the `aqt_dg` calibration if present.",
                "Conditionally check for a backward pass (drhs) left-hand-side bound in `cst_bound_config` and update the `aqt_dg` calibration if present.",
                "Conditionally check for a backward pass (drhs) right-hand-side bound in `cst_bound_config` and update the `aqt_dg` calibration if present.",
                "Return the modified `aqt_dg` object."
            ],
            "output": {
                "shape": "Returns a modified aqt_config.DotGeneral object."
            },
            "dependencies": [
                "aqt.jax.v2.config.DotGeneral",
                "ConstantBoundConfig",
                "aqt.jax.v2.calibration.ConstantCalibration",
                "functools.partial"
            ],
            "parameters": {
                "aqt_dg": "The AQT dot general configuration object to be modified.",
                "cst_bound_config": "A configuration object containing optional constant bounds for different stages (fwd, dlhs, drhs) and sides (lhs, rhs) of the dot product."
            },
            "notes": [
                "This function implements static quantization scaling by setting fixed calibration bounds.",
                "It modifies the `calibration` attribute of the `aqt_dg` object for each specified bound, using a partial function of `calibration.ConstantCalibration`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#PerTensorScales",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class PerTensorScales:\n  fwd_lhs: bool = False\n  fwd_rhs: bool = False\n  dlhs_lhs: bool = False\n  dlhs_rhs: bool = False\n  drhs_lhs: bool = False\n  drhs_rhs: bool = False",
        "analysis": {
            "functionality": "This code block defines a dataclass named `PerTensorScales` that serves as a configuration object. It holds boolean flags to specify whether per-tensor scaling should be applied to the left-hand-side (lhs) and right-hand-side (rhs) tensors during the forward pass (`fwd`), and the two corresponding backward passes (`dlhs` and `drhs`) of a quantized matrix multiplication.",
            "usage": "Instantiate this class to create a configuration object for AQT quantization. Each attribute (`fwd_lhs`, `fwd_rhs`, etc.) can be set to `True` to enable per-tensor scaling for that specific part of the computation, or `False` (the default) to use other scaling methods like per-channel. This object is then passed to helper functions like `_build_per_tensor_config` to modify the quantization behavior."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_build_per_tensor_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _build_per_tensor_config(\n    aqt_dg: aqt_config.DotGeneral,\n    per_tensor_scales: PerTensorScales,\n) -> aqt_config.DotGeneral:\n  \"\"\"Build a per tensor config for AQT dot general.\n\n  Args:\n    aqt_dg: The AQT dot general config.\n    per_tensor_scales: The per tensor scales config.\n\n  Returns:\n    The AQT dot general config with per tensor config.\n  \"\"\"\n  if per_tensor_scales.fwd_lhs:\n    aqt_dg.fwd.dg_quantizer.lhs.calib_shared_axes = \"per_tensor\"\n  if per_tensor_scales.fwd_rhs:\n    aqt_dg.fwd.dg_quantizer.rhs.calib_shared_axes = \"per_tensor\"\n  if per_tensor_scales.dlhs_lhs:\n    aqt_dg.dlhs.dg_quantizer.lhs.calib_shared_axes = \"per_tensor\"\n  if per_tensor_scales.dlhs_rhs:\n    aqt_dg.dlhs.dg_quantizer.rhs.calib_shared_axes = \"per_tensor\"\n  if per_tensor_scales.drhs_lhs:\n    aqt_dg.drhs.dg_quantizer.lhs.calib_shared_axes = \"per_tensor\"\n  if per_tensor_scales.drhs_rhs:\n    aqt_dg.drhs.dg_quantizer.rhs.calib_shared_axes = \"per_tensor\"\n  return aqt_dg",
        "analysis": {
            "module_type": "aqt_per_tensor_config_builder",
            "purpose": "Modifies an AQT DotGeneral configuration object to enable per-tensor quantization scaling for specified stages of a dot-product operation.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Conditionally set `calib_shared_axes` to 'per_tensor' for the forward pass left-hand side (fwd.lhs) based on `per_tensor_scales.fwd_lhs`.",
                "Conditionally set `calib_shared_axes` to 'per_tensor' for the forward pass right-hand side (fwd.rhs) based on `per_tensor_scales.fwd_rhs`.",
                "Conditionally set `calib_shared_axes` to 'per_tensor' for the backward pass dlhs left-hand side (dlhs.lhs) based on `per_tensor_scales.dlhs_lhs`.",
                "Conditionally set `calib_shared_axes` to 'per_tensor' for the backward pass dlhs right-hand side (dlhs.rhs) based on `per_tensor_scales.dlhs_rhs`.",
                "Conditionally set `calib_shared_axes` to 'per_tensor' for the backward pass drhs left-hand side (drhs.lhs) based on `per_tensor_scales.drhs_lhs`.",
                "Conditionally set `calib_shared_axes` to 'per_tensor' for the backward pass drhs right-hand side (drhs.rhs) based on `per_tensor_scales.drhs_rhs`.",
                "Return the modified aqt_dg configuration object."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "aqt.jax.v2.config.DotGeneral",
                "PerTensorScales"
            ],
            "parameters": {
                "aqt_dg": "The AQT dot general configuration object to be modified.",
                "per_tensor_scales": "A dataclass object containing boolean flags indicating which tensors (lhs/rhs) in which passes (fwd/dlhs/drhs) should use per-tensor scaling."
            },
            "notes": [
                "This is a helper function for constructing AQT quantization configurations.",
                "The function modifies the input `aqt_dg` object in-place and also returns it.",
                "Enabling per-tensor scaling is achieved by setting the `calib_shared_axes` attribute to the specific string 'per_tensor'."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_aqt_fp8_default_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_aqt_fp8_default_config(config):\n  \"\"\"Get aqt for 8-bit floating point quantization configuration.\"\"\"\n  aqt_dg = aqt_config.config_v4(\n      fwd_bits=\"e4m3\",\n      dlhs_bits=\"e5m2\",\n      drhs_bits=\"e5m2\",\n      use_dummy_static_bound=False,\n      fwd_accumulator_dtype=jnp.bfloat16,\n      dlhs_accumulator_dtype=jnp.bfloat16,\n      drhs_accumulator_dtype=jnp.bfloat16,\n      dlhs_use_fwd_quant=False,\n      drhs_use_fwd_quant=False,\n  )\n  constant_bound_config = None\n\n  if len(config.constant_bound_config) == 6:\n    fwd_lhs_bound, fwd_rhs_bound, dlhs_lhs_bound, dlhs_rhs_bound, drhs_lhs_bound, drhs_rhs_bound = (\n        config.constant_bound_config\n    )\n    constant_bound_config = ConstantBoundConfig(\n        fwd_lhs_bound=fwd_lhs_bound,\n        fwd_rhs_bound=fwd_rhs_bound,\n        dlhs_lhs_bound=dlhs_lhs_bound,\n        dlhs_rhs_bound=dlhs_rhs_bound,\n        drhs_lhs_bound=drhs_lhs_bound,\n        drhs_rhs_bound=drhs_rhs_bound,\n    )\n    aqt_dg = _build_const_scale_config(aqt_dg, constant_bound_config)\n\n  aqt_config.set_stochastic_rounding(\n      aqt_dg,\n      vjp_lhs_stochastic_rounding=False,\n      vjp_rhs_stochastic_rounding=False,\n      implementation=\"jax.uniform\",\n  )\n\n  per_tensor_scales = PerTensorScales(\n      fwd_lhs=True,\n      fwd_rhs=True,\n      dlhs_lhs=True,\n      dlhs_rhs=True,\n      drhs_lhs=True,\n      drhs_rhs=True,\n  )\n  return _build_per_tensor_config(aqt_dg, per_tensor_scales)",
        "analysis": {
            "module_type": "aqt_fp8_config_generator",
            "purpose": "Generates a default AQT DotGeneral configuration for 8-bit floating-point (FP8) quantization, including settings for forward and backward passes.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Create a base FP8 AQT configuration using `aqt_config.config_v4` with 'e4m3' for forward and 'e5m2' for backward passes.",
                "Check if `config.constant_bound_config` has 6 elements to determine if static scaling should be used.",
                "If static scaling is enabled, create a `ConstantBoundConfig` and apply it to the AQT configuration using `_build_const_scale_config`.",
                "Configure stochastic rounding settings using `aqt_config.set_stochastic_rounding`, disabling it for the vector-Jacobian product (VJP) steps.",
                "Create a `PerTensorScales` configuration to enable per-tensor scaling for all forward and backward operations.",
                "Apply the per-tensor scaling configuration using `_build_per_tensor_config`.",
                "Return the final AQT DotGeneral configuration object."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "aqt.jax.v2.config",
                "jax.numpy",
                "ConstantBoundConfig",
                "_build_const_scale_config",
                "PerTensorScales",
                "_build_per_tensor_config"
            ],
            "parameters": {
                "config.constant_bound_config": "A sequence of 6 float values for static quantization bounds. If provided, it overrides the default dynamic scaling behavior."
            },
            "notes": [
                "This function configures a full training recipe, including the forward pass (fwd), gradient with respect to activations (dlhs), and gradient with respect to weights (drhs).",
                "The default behavior is dynamic scaling, but static scaling can be enabled by providing `constant_bound_config`.",
                "The configuration uses bfloat16 for accumulator data types."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_aqt_fp8_quant_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_aqt_fp8_quant_config(config):\n  \"\"\"get aqt for 8-bit floating point quantization configuration\"\"\"\n  cfg = aqt_config.config_v4(fwd_bits=\"e4m3\", dlhs_bits=None, drhs_bits=None, fwd_accumulator_dtype=jnp.bfloat16)\n  return cfg",
        "analysis": {
            "module_type": "aqt_fp8_inference_config_generator",
            "purpose": "Creates and returns an AQT configuration for 8-bit floating-point (e4m3) forward-pass-only quantization.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `aqt_config.config_v4` to create a quantization configuration.",
                "Specifies 'e4m3' for forward pass bits (`fwd_bits`).",
                "Sets backward pass bits (`dlhs_bits`, `drhs_bits`) to None, effectively disabling backward pass quantization.",
                "Sets the forward pass accumulator data type to `jnp.bfloat16`.",
                "Returns the generated configuration object."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "aqt.jax.v2.config.config_v4",
                "jax.numpy.bfloat16"
            ],
            "parameters": {
                "config": "A configuration object, which is accepted as an argument but not used within the function."
            },
            "notes": [
                "This configuration is suitable for inference or forward-pass-only scenarios as backward pass quantization is disabled.",
                "The input `config` parameter is unused in the function body.",
                "The quantization format and accumulator dtype are hardcoded."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_dot_general_make",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _dot_general_make(quant_cfg):\n  \"\"\"Create quantization configs for input matrices to a matmul\"\"\"\n  lhs_bits = quant_cfg[_A_BITS]\n  lhs_scale = quant_cfg[_A_SCALE]\n  rhs_bits = quant_cfg[_W_BITS]\n  rhs_scale = quant_cfg[_W_SCALE]\n  aqt_dg = aqt_config.dot_general_make(lhs_bits=lhs_bits, rhs_bits=rhs_bits)\n  if lhs_scale < 1.0:\n    aqt_dg.fwd.dg_quantizer.lhs.calibration = functools.partial(calibration.AbsMaxCalibration, scale=lhs_scale)\n  if rhs_scale < 1.0:\n    aqt_dg.fwd.dg_quantizer.rhs.calibration = functools.partial(calibration.AbsMaxCalibration, scale=rhs_scale)\n  return aqt_dg",
        "analysis": {
            "functionality": "This function creates and configures an AQT (AQuantized Training) `DotGeneral` configuration object for a matrix multiplication. It sets the number of bits for both the left-hand side (activations) and right-hand side (weights) tensors and conditionally applies a fixed-scale calibration method based on the provided scaling factors.",
            "usage": "Call this function with a dictionary `quant_cfg` containing keys for activation bits ('a_bits'), activation scale ('a_scale'), weight bits ('w_bits'), and weight scale ('w_scale'). It returns a configured `aqt_config.DotGeneral` object that can be used to quantize a matrix multiplication operation."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_default_mp_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_default_mp_config(default=None):\n  default_config = {_W_BITS: None, _A_BITS: None, _W_SCALE: 1.0, _A_SCALE: 1.0, _TILE_SIZE: -1}\n  if default:\n    default_config.update(default)\n  return default_config",
        "analysis": {
            "module_type": "configuration_helper",
            "purpose": "Creates a default configuration dictionary for mixed-precision quantization, with an option to override values.",
            "input": {
                "shape": "N/A",
                "dtype": "dict or None"
            },
            "processing_steps": [
                "Initialize a dictionary `default_config` with predefined keys (_W_BITS, _A_BITS, _W_SCALE, _A_SCALE, _TILE_SIZE) and default values.",
                "If the optional `default` dictionary is provided, update `default_config` with its key-value pairs.",
                "Return the `default_config` dictionary."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "_W_BITS",
                "_A_BITS",
                "_W_SCALE",
                "_A_SCALE",
                "_TILE_SIZE"
            ],
            "parameters": {
                "default": "An optional dictionary to override the default configuration values."
            },
            "notes": [
                "This is a helper function used within `_get_mixed_precision_quant_config` to establish a base configuration for mixed-precision quantization layers."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_mixed_precision_quant_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_mixed_precision_quant_config(mixed_precision_config):\n  \"\"\"Set quantization params based on user configuration.\"\"\"\n  ret_config = {}\n  default_mp_config = _get_default_mp_config(default=mixed_precision_config.get(DEFAULT, None))\n  for layer_name_re, layer_quantization_config in mixed_precision_config.items():\n    # Make a copy of default_mp_config to avoid updating original dict\n    quant_config = default_mp_config.copy()\n    # print(f\"Mixed precision config: processing\n    # {layer_name_re} - {layer_quantization_config}, default config - {quant_config}\")\n    if layer_name_re != DEFAULT:\n      for k in quant_config:\n        quant_config[k] = layer_quantization_config.get(k, default_mp_config[k])\n    ret_config[layer_name_re] = [_dot_general_make(quant_config), quant_config[\"tile_size\"]]\n  return ret_config",
        "analysis": {
            "functionality": "This function processes a user-defined mixed-precision configuration dictionary to create a comprehensive AQT (Automatic Quantization Toolkit) quantization configuration for different model layers. It establishes a default configuration and then overrides it with layer-specific settings.",
            "usage": "Call this function with a dictionary `mixed_precision_config`. The dictionary keys are regular expressions matching layer names, and the values are dictionaries with quantization parameters (e.g., 'w_bits', 'a_bits', 'tile_size'). A special key '__default__' can be used to set base parameters for all layers. The function returns a dictionary mapping each layer regex to a list containing an AQT `DotGeneral` configuration object and an integer tile size."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_quant_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_quant_config(config):\n  \"\"\"Set quantization params based on user configuration.\"\"\"\n  if not config.quantization or config.quantization == \"\":\n    return None\n  if config.quantization == \"int8\":\n    return _get_int8_quant_config(config)\n  if config.quantization == \"intmp\":\n    assert config.quant_cfg_path, \"Must specify quant_cfg for mixed precision quantization\"\n    with open(config.quant_cfg_path, \"rt\", encoding=\"utf8\") as config_file:\n      mixed_precision_config = json.load(config_file)\n    return _get_mixed_precision_quant_config(mixed_precision_config)\n  if config.quantization == \"fp8\":\n    return \"fp8\"\n  if config.quantization == \"nanoo_fp8\":\n    return \"nanoo_fp8\"\n  if config.quantization == \"aqt_fp8\":\n    return _get_aqt_fp8_quant_config(config)\n  if config.quantization == \"aqt_fp8_full\":\n    return _get_aqt_fp8_default_config(config)\n\n  raise ValueError(f\"Invalid value configured for quantization {config.quantization}.\")",
        "analysis": {
            "functionality": "This function acts as a factory to generate a quantization configuration based on a string identifier provided in a configuration object. It supports various quantization types like 'int8', 'intmp' (integer mixed precision), 'fp8', 'nanoo_fp8', and several AQT-based fp8 variants.",
            "usage": "Call this function with a configuration object (`config`) that has a `quantization` attribute. The function will return the appropriate quantization settings or `None` if quantization is disabled. For 'intmp' quantization, the `config` object must also have a `quant_cfg_path` attribute pointing to a JSON file."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#in_convert_mode",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def in_convert_mode(quant):\n  return quant and (quant.quant_mode == aqt_flax.QuantMode.CONVERT)",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Checks if the provided quantization configuration object is in 'CONVERT' mode.",
            "input": {
                "shape": "N/A",
                "dtype": "A quantization configuration object (e.g., AqtQuantization) or None."
            },
            "processing_steps": [
                "Check if the input `quant` object is truthy (not None).",
                "If `quant` is truthy, check if its `quant_mode` attribute is equal to `aqt_flax.QuantMode.CONVERT`.",
                "Return the boolean result of the logical AND operation."
            ],
            "output": {
                "shape": "A boolean scalar."
            },
            "dependencies": [
                "aqt_flax.QuantMode"
            ],
            "parameters": {
                "quant": "The quantization configuration object to check."
            },
            "notes": [
                "The function uses short-circuit evaluation. If `quant` is None or evaluates to False, it immediately returns False without accessing the `quant_mode` attribute."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#in_serve_mode",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def in_serve_mode(quant):\n  return quant and (quant.quant_mode == aqt_flax.QuantMode.SERVE)",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Checks if a given quantization configuration object is active and set to SERVE mode.",
            "input": {
                "shape": "N/A",
                "dtype": "AqtQuantization object or None"
            },
            "processing_steps": [
                "Check if the input 'quant' object is truthy.",
                "If 'quant' is truthy, check if its 'quant_mode' attribute is equal to aqt_flax.QuantMode.SERVE.",
                "Return the boolean result of the logical AND operation."
            ],
            "output": {
                "shape": "Scalar boolean"
            },
            "dependencies": [
                "aqt_flax.QuantMode"
            ],
            "parameters": {
                "quant": "The quantization configuration object. Can be None."
            },
            "notes": [
                "This function utilizes short-circuit evaluation. If 'quant' is None or otherwise Falsy, the function will return False without checking the 'quant_mode' attribute."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#get_quant_mode",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def get_quant_mode(quant_mode_str: str = \"train\"):\n  \"\"\"Set quant mode.\"\"\"\n  if quant_mode_str == \"train\":\n    return aqt_flax.QuantMode.TRAIN\n  elif quant_mode_str == \"serve\":\n    return aqt_flax.QuantMode.SERVE\n  elif quant_mode_str == \"convert\":\n    return aqt_flax.QuantMode.CONVERT\n  else:\n    raise ValueError(f\"Invalid quantization mode {quant_mode_str}.\")\n  return None",
        "analysis": {
            "module_type": "quantization_utility_function",
            "purpose": "Converts a string representation of a quantization mode ('train', 'serve', 'convert') into the corresponding `aqt_flax.QuantMode` enum value.",
            "input": {
                "shape": "N/A",
                "dtype": "string"
            },
            "processing_steps": [
                "Check if `quant_mode_str` is 'train', 'serve', or 'convert'.",
                "Return the corresponding `aqt_flax.QuantMode` enum value.",
                "Raise a ValueError if the string is not one of the valid options."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "aqt_flax.QuantMode"
            ],
            "parameters": {
                "quant_mode_str": "A string representing the desired quantization mode. Can be 'train', 'serve', or 'convert'. Defaults to 'train'."
            },
            "notes": [
                "This function is a helper to configure the quantization behavior for different stages like training, inference serving, or model conversion."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#configure_quantization",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def configure_quantization(config: Config, quant_mode_str: str = \"train\"):\n  \"\"\"Configure quantization based on user config and quant mode.\"\"\"\n  if config.use_qwix_quantization:\n    return None\n  quant_cfg = _get_quant_config(config)\n  if quant_cfg:\n    if quant_cfg == \"fp8\":\n      return Fp8Quantization()\n    elif quant_cfg == \"nanoo_fp8\":\n      return NANOOFp8Quantization()\n    quant_mode = get_quant_mode(quant_mode_str)\n    replicate_scale = config.replicate_quant_scale if config.replicate_quant_scale else False\n    return AqtQuantization(quant_dg=quant_cfg, quant_mode=quant_mode, replicate_scale=replicate_scale)\n  return None",
        "analysis": {
            "functionality": "This function acts as a factory to create and configure a quantization object based on a provided configuration and a quantization mode string. It supports AQT, FP8, and NANOO FP8 quantization types.",
            "usage": "Call this function with a `Config` object and an optional `quant_mode_str` ('train', 'serve', 'convert'). It returns an appropriate quantization configuration object (e.g., `AqtQuantization`, `Fp8Quantization`) or `None` if quantization is not specified or a different system like `qwix` is used."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#match_aqt_and_unquantized_param",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def match_aqt_and_unquantized_param(aqt_params, params):\n  \"\"\"match aqt and unquantized params\"\"\"\n  aqt_param_flat, aqt_tree_def = jax.tree_util.tree_flatten_with_path(\n      aqt_params, is_leaf=lambda x: isinstance(x, aqt_tensor.QTensor)\n  )\n  param_tree_flat, _ = jax.tree_util.tree_flatten_with_path(params)\n  aqt_paths = []\n  # Original path of quantized AQT param path.\n  param_paths = []\n\n  for aqt_k, _ in aqt_param_flat:\n    index = None\n    for index, (k, _) in enumerate(param_tree_flat):\n      path_depth = len(k)\n      # every quantized parameter has AQT.. as the leaf node\n      # AqtDotGeneral and AqtEinsum replace leaf node.\n      # Therefore, leaf node should be ignored for path matching\n      # Note: Aqt only operates on kernels so don't pop bias parameters.\n      # Ref: https://github.com/AI-Hypercomputer/maxtext/compare/main...quantize_r1\n      if k[: path_depth - 1] == aqt_k[: path_depth - 1] and k[-1].key != \"bias\":\n        aqt_paths.append(aqt_k)\n        param_paths.append(k)\n        break\n    assert index is not None\n    # since the parameter is already added, we can delete it.\n    param_tree_flat.pop(index)\n  return jax.tree_util.tree_unflatten(aqt_tree_def, param_paths)",
        "analysis": {
            "module_type": "parameter_matching_utility",
            "purpose": "Matches paths of quantized AQT parameters to their corresponding paths in an unquantized parameter tree.",
            "input": {
                "shape": "Two PyTrees: `aqt_params` (containing aqt_tensor.QTensor leaves) and `params` (the original unquantized parameters).",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Flatten the `aqt_params` PyTree with paths, treating `aqt_tensor.QTensor` instances as leaves.",
                "Flatten the `params` PyTree with paths.",
                "Iterate through each path in the flattened `aqt_params`.",
                "For each AQT path, find a corresponding path in the flattened `params` by comparing path prefixes, ignoring the final path element and any 'bias' parameters.",
                "Store the matched unquantized parameter paths.",
                "Remove matched paths from the unquantized parameter list to prevent re-matching.",
                "Unflatten the list of matched unquantized paths into a new PyTree with the same structure as the original `aqt_params`."
            ],
            "output": {
                "shape": "A PyTree with the same structure as `aqt_params`, where each leaf is a tuple representing the full path to the corresponding parameter in the original `params` tree."
            },
            "dependencies": [
                "jax.tree_util.tree_flatten_with_path",
                "jax.tree_util.tree_unflatten",
                "aqt.jax.v2.aqt_tensor.QTensor"
            ],
            "parameters": {
                "aqt_params": "A PyTree containing quantized parameters, where leaves of interest are `aqt_tensor.QTensor` objects.",
                "params": "A PyTree containing the corresponding original, unquantized model parameters."
            },
            "notes": [
                "The matching logic assumes that AQT replaces the leaf node of a parameter (e.g., 'kernel'), so it matches paths up to the second-to-last element.",
                "The function explicitly avoids matching parameters whose final path key is 'bias', as the comments indicate AQT only operates on kernels."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_aqt_key_paths",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_aqt_key_paths(aqt_vars, params):\n  \"\"\"Generate a list of paths which have aqt state\"\"\"\n  aqt_to_unquantized_key_path = match_aqt_and_unquantized_param(aqt_vars, params)\n  aqt_key_paths, _ = jax.tree_util.tree_flatten(aqt_to_unquantized_key_path, is_leaf=lambda x: isinstance(x, tuple))\n  return list(aqt_key_paths)",
        "analysis": {
            "module_type": "quantization_utility_function",
            "purpose": "Generates a list of key paths for parameters that have AQT (quantization) state by matching them with their unquantized counterparts.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Call `match_aqt_and_unquantized_param` to create a PyTree mapping AQT variables to the key paths of their corresponding unquantized parameters.",
                "Use `jax.tree_util.tree_flatten` to extract all the key path tuples from the generated PyTree.",
                "Convert the flattened list of key paths into a standard Python list and return it."
            ],
            "output": {
                "shape": "A list of key path tuples, where each tuple represents the path to a quantized parameter."
            },
            "dependencies": [
                "match_aqt_and_unquantized_param",
                "jax.tree_util.tree_flatten"
            ],
            "parameters": {
                "aqt_vars": "A PyTree containing the AQT quantization state variables.",
                "params": "A PyTree containing the original, unquantized model parameters."
            },
            "notes": [
                "This is a helper function used to identify which parameters in a model have been quantized.",
                "The `is_leaf` argument in `tree_flatten` is set to identify tuples, which represent the key paths, as the leaves of the intermediate PyTree."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#remove_quantized_params",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def remove_quantized_params(params, aqt_vars):\n  \"\"\"Remove param values with aqt tensors to Null to optimize memory.\"\"\"\n  quantized_param_paths = _get_aqt_key_paths(aqt_vars, params)\n  tree_flat, tree_struct = tree_flatten_with_path(params)\n  for i, (k, v) in enumerate(tree_flat):\n    if k in quantized_param_paths:\n      v = {}\n    tree_flat[i] = v\n  return tree_unflatten(tree_struct, tree_flat)",
        "analysis": {
            "functionality": "This function identifies parameters that have been quantized using AQT and replaces their corresponding values in the original parameter tree with empty dictionaries. This is done to optimize memory by removing redundant unquantized parameter values.",
            "usage": "To use this function, provide a PyTree of model parameters (`params`) and a PyTree of AQT state variables (`aqt_vars`). The function returns a new parameter PyTree where the values of quantized parameters have been replaced by empty dictionaries `{}`. For example: `pruned_params = remove_quantized_params(original_params, aqt_state_variables)`."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#configure_kv_quant",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def configure_kv_quant(config):\n  return None if not config.quantize_kvcache else KVQuant(config)",
        "analysis": {
            "functionality": "This function conditionally creates an object for key-value cache quantization. It checks a configuration flag and, if enabled, instantiates and returns a `KVQuant` object; otherwise, it returns `None`.",
            "usage": "Call this function with a configuration object. It will return either a `KVQuant` instance if `config.quantize_kvcache` is true, or `None` if it is false. The returned object is then used to manage KV cache quantization during inference."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#NvidaFp8Provider",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class NvidaFp8Provider(qwix.QtProvider):\n  \"\"\"Wraps nn.Fp8DirectDotGeneralOp with Qwix's provider interface.\"\"\"\n\n  def dot_general(self, *args, **kwargs):\n    # Here we only check if the rule is None or not.\n    rule, op_id = self._get_current_rule_and_op_id(\"dot_general\")\n    if rule is None:\n      return jax.lax.dot_general(*args, **kwargs)\n    return nn.Fp8DirectDotGeneralOp(name=op_id)(*args, **kwargs)\n\n  def einsum(self, *args, **kwargs):\n    rule, op_id = self._get_current_rule_and_op_id(\"einsum\")\n    if rule is None:\n      return jnp.einsum(*args, **kwargs)\n    return nn.Fp8Einsum(name=op_id)(*args, **kwargs)",
        "analysis": {
            "module_type": "fp8_quantization_provider",
            "purpose": "Provides a conditional implementation for dot_general and einsum operations, using specialized NVIDIA FP8 versions if a quantization rule is active, and falling back to standard JAX implementations otherwise.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "N/A"
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "qwix.QtProvider",
                "jax.lax",
                "jax.numpy",
                "flax.linen.Fp8DirectDotGeneralOp",
                "flax.linen.Fp8Einsum"
            ],
            "parameters": {
                "rules": "A list of quantization rules, passed during initialization of the base qwix.QtProvider class."
            },
            "notes": [
                "This class acts as an interface within the Qwix quantization framework.",
                "It intercepts calls to dot_general and einsum to potentially replace them with hardware-accelerated FP8 versions based on configured rules."
            ],
            "methods": {
                "dot_general": {
                    "purpose": "Performs a general dot product, conditionally using an FP8 implementation (nn.Fp8DirectDotGeneralOp) if a quantization rule is active, otherwise defaulting to jax.lax.dot_general.",
                    "input": {
                        "shape": "Variable, accepts arguments for jax.lax.dot_general (e.g., lhs, rhs, dimension_numbers).",
                        "dtype": "Any JAX numeric dtype."
                    },
                    "processing_steps": [
                        "Call self._get_current_rule_and_op_id(\"dot_general\") to check for an active quantization rule.",
                        "If no rule is found, call jax.lax.dot_general with the provided arguments.",
                        "If a rule is found, instantiate and call nn.Fp8DirectDotGeneralOp with the provided arguments."
                    ],
                    "output": {
                        "shape": "The shape resulting from the dot product operation, dependent on input shapes and dimension_numbers."
                    },
                    "dependencies": [
                        "jax.lax.dot_general",
                        "nn.Fp8DirectDotGeneralOp"
                    ],
                    "notes": [
                        "The choice of implementation is determined by the presence of a quantization rule managed by the parent Qwix framework."
                    ]
                },
                "einsum": {
                    "purpose": "Performs an Einstein summation, conditionally using an FP8 implementation (nn.Fp8Einsum) if a quantization rule is active, otherwise defaulting to jnp.einsum.",
                    "input": {
                        "shape": "Variable, accepts arguments for jnp.einsum (e.g., subscripts, *operands).",
                        "dtype": "Any JAX numeric dtype."
                    },
                    "processing_steps": [
                        "Call self._get_current_rule_and_op_id(\"einsum\") to check for an active quantization rule.",
                        "If no rule is found, call jnp.einsum with the provided arguments.",
                        "If a rule is found, instantiate and call nn.Fp8Einsum with the provided arguments."
                    ],
                    "output": {
                        "shape": "The shape resulting from the einsum operation, dependent on input shapes and the subscript string."
                    },
                    "dependencies": [
                        "jnp.einsum",
                        "nn.Fp8Einsum"
                    ],
                    "notes": [
                        "The choice of implementation is determined by the presence of a quantization rule managed by the parent Qwix framework."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#NANOOFp8Provider",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class NANOOFp8Provider(qwix.QtProvider):\n\n  def dot_general(self, *args, **kwargs):\n    # Here we only check if the rule is None or not.\n    rule, op_id = self._get_current_rule_and_op_id(\"dot_general\")\n    if rule is None:\n      return jax.lax.dot_general(*args, **kwargs)\n    return nn.NANOOFp8DotGeneralOp(name=op_id)(*args, **kwargs)",
        "analysis": {
            "module_type": "fp8_quantization_provider",
            "purpose": "Provides a mechanism to conditionally replace standard dot_general operations with a specialized NANOO FP8 implementation for AMD GPUs, based on quantization rules from the Qwix framework.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "N/A"
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "qwix.QtProvider",
                "jax.lax",
                "flax.linen.nn.NANOOFp8DotGeneralOp"
            ],
            "parameters": {},
            "notes": [
                "This class inherits from `qwix.QtProvider` and is designed to be used within the Qwix quantization framework.",
                "It enables selective quantization of matrix multiplication operations for NANOO FP8, which is optimized for AMD GPUs."
            ],
            "methods": {
                "dot_general": {
                    "purpose": "Intercepts `dot_general` calls and dispatches to either the standard JAX implementation or a specialized NANOO FP8 version based on active quantization rules.",
                    "input": {
                        "shape": "Accepts standard arguments for `jax.lax.dot_general` (e.g., lhs, rhs, dimension_numbers, etc.).",
                        "dtype": "The data type of the input tensors."
                    },
                    "processing_steps": [
                        "Check for an active quantization rule for the 'dot_general' operation using `_get_current_rule_and_op_id`.",
                        "If no rule is found, call the standard `jax.lax.dot_general`.",
                        "If a rule is found, instantiate and call the specialized `nn.NANOOFp8DotGeneralOp`."
                    ],
                    "output": {
                        "shape": "The shape of the tensor resulting from the dot-product operation."
                    },
                    "dependencies": [
                        "jax.lax.dot_general",
                        "nn.NANOOFp8DotGeneralOp"
                    ],
                    "notes": [
                        "The decision to quantize is delegated to the underlying Qwix provider's rule-matching mechanism."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#get_quantization_rule",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def get_quantization_rule(config: Config):\n  match config.quantization:\n    case \"int8\":\n      return qwix.QtRule(\n          module_path=\"decoder/.*layers.*\",\n          weight_qtype=jnp.int8,\n          act_qtype=jnp.int8,\n          bwd_qtype=jnp.int8,\n          bwd_weight_grad_tile_size=1 / config.quantization_local_shard_count,\n          op_names=(\"dot_general\",),\n      )\n    case \"fp8\":\n      return qwix.QtRule(\n          module_path=\"decoder/.*layers.*\",\n          weight_qtype=jnp.float8_e4m3fn,\n          act_qtype=jnp.float8_e4m3fn,\n          bwd_qtype=jnp.float8_e4m3fn,\n          bwd_weight_grad_tile_size=1 / config.quantization_local_shard_count,\n          op_names=(\"dot_general\",),\n      )\n    case \"fp8_full\":\n      return qwix.QtRule(\n          module_path=\"decoder/.*layers.*\",\n          weight_qtype=jnp.float8_e4m3fn,\n          act_qtype=jnp.float8_e4m3fn,\n          bwd_qtype=jnp.float8_e5m2,\n          bwd_use_original_residuals=True,\n          disable_channelwise_axes=True,  # per_tensor calibration\n          weight_calibration_method=config.quantization_calibration_method,\n          act_calibration_method=config.quantization_calibration_method,\n          bwd_calibration_method=config.quantization_calibration_method,\n          op_names=(\"dot_general\",),\n          additional_qt_config={\n              \"dlhs_lhs_qtype\": jnp.float8_e5m2,\n              \"dlhs_rhs_qtype\": jnp.float8_e4m3fn,\n              \"drhs_lhs_qtype\": jnp.float8_e4m3fn,\n              \"drhs_rhs_qtype\": jnp.float8_e5m2,\n          },\n      )\n    case \"fp8_gpu\":\n      return qwix.QtRule(\n          module_path=\"decoder/.*layers.*\",\n          weight_qtype=jnp.float8_e4m3fn,\n          act_qtype=jnp.float8_e4m3fn,\n          bwd_qtype=jnp.float8_e4m3fn,\n          bwd_weight_grad_tile_size=1 / config.quantization_local_shard_count,\n          op_names=(\"dot_general\",),\n      )\n    case \"fp8_nanoo\":\n      return qwix.QtRule(\n          module_path=\"decoder/.*layers.*\",\n          weight_qtype=jnp.float8_e4m3fn,\n          act_qtype=jnp.float8_e4m3fn,\n          bwd_qtype=jnp.float8_e4m3fn,\n          bwd_weight_grad_tile_size=1 / config.quantization_local_shard_count,\n          op_names=(\"dot_general\",),\n      )\n    case \"\":\n      return None",
        "analysis": {
            "module_type": "quantization_rule_factory",
            "purpose": "Creates and returns a `qwix.QtRule` object based on the quantization strategy specified in the configuration.",
            "input": {
                "shape": "N/A",
                "dtype": "MaxText.common_types.Config"
            },
            "processing_steps": [
                "Read the `quantization` string from the input `config` object.",
                "Use a match-case statement to select the appropriate quantization configuration.",
                "Instantiate a `qwix.QtRule` object with parameters specific to the selected quantization type (e.g., 'int8', 'fp8', 'fp8_full').",
                "Return the created `qwix.QtRule` object, or `None` if quantization is disabled."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "qwix.QtRule",
                "jax.numpy",
                "MaxText.common_types.Config"
            ],
            "parameters": {
                "config.quantization": "The string that determines which quantization rule to create (e.g., 'int8', 'fp8', 'fp8_full').",
                "config.quantization_local_shard_count": "Used to calculate `bwd_weight_grad_tile_size` for sharded gradient computation.",
                "config.quantization_calibration_method": "Specifies the calibration method for weights, activations, and backward pass in the 'fp8_full' case."
            },
            "notes": [
                "This function is part of the Qwix quantization framework integration.",
                "The function returns `None` if `config.quantization` is an empty string, effectively disabling quantization rule generation.",
                "The `module_path` is hardcoded to 'decoder/.*layers.*', meaning these rules are intended to apply specifically to the decoder layers of the model.",
                "The 'fp8_full' case has a more complex configuration, including per-tensor calibration and specific dtypes for backward pass gradients."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#get_qt_provider",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def get_qt_provider(config):\n  \"\"\"Get quantization rules based on the config.\"\"\"\n  match config.quantization:\n    case \"int8\":\n      return qwix.QtProvider([get_quantization_rule(config)])\n    case \"fp8\":\n      return qwix.QtProvider([get_quantization_rule(config)])\n    case \"fp8_full\":\n      return qwix.QtProvider([get_quantization_rule(config)])\n    case \"fp8_gpu\":\n      return NvidaFp8Provider([get_quantization_rule(config)])\n    case \"fp8_nanoo\":\n      return NANOOFp8Provider([get_quantization_rule(config)])\n  return None",
        "analysis": {
            "module_type": "quantization_provider_factory",
            "purpose": "Creates and returns a quantization provider instance based on the quantization type specified in the configuration.",
            "input": {
                "shape": "N/A",
                "dtype": "Config object"
            },
            "processing_steps": [
                "Read the `quantization` attribute from the input `config` object.",
                "Use a `match` statement to select a provider class based on the `config.quantization` string.",
                "Call `get_quantization_rule(config)` to generate the quantization rule(s).",
                "Instantiate the selected provider class (`qwix.QtProvider`, `NvidaFp8Provider`, or `NANOOFp8Provider`) with the generated rule.",
                "Return the provider instance or `None` if no case matches."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "qwix.QtProvider",
                "NvidaFp8Provider",
                "NANOOFp8Provider",
                "get_quantization_rule"
            ],
            "parameters": {
                "config.quantization": "A string that specifies the type of quantization to use (e.g., 'int8', 'fp8', 'fp8_gpu'). This determines which provider is returned."
            },
            "notes": [
                "The function acts as a factory for different quantization providers.",
                "If the `config.quantization` string does not match any of the predefined cases, the function returns `None`.",
                "For 'fp8_gpu' and 'fp8_nanoo', it uses specialized provider classes (`NvidaFp8Provider`, `NANOOFp8Provider`), while other cases use the base `qwix.QtProvider`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#maybe_quantize_model",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def maybe_quantize_model(model, config):\n  \"\"\"Quantize the model if quantization is enabled.\"\"\"\n  if config.use_qwix_quantization:\n    quantization_provider = get_qt_provider(config)\n    if quantization_provider:\n      model = qwix.quantize_model(model, quantization_provider)\n  return model",
        "analysis": {
            "module_type": "conditional_model_quantization",
            "purpose": "Conditionally applies quantization to a model using the qwix library based on the provided configuration.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check if `config.use_qwix_quantization` is True.",
                "If true, call `get_qt_provider` with the configuration to obtain a quantization provider.",
                "If a quantization provider is successfully obtained, call `qwix.quantize_model` to quantize the input model.",
                "Return the model, which will be the quantized version if the conditions were met, otherwise the original model."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "get_qt_provider",
                "qwix.quantize_model"
            ],
            "parameters": {
                "config.use_qwix_quantization": "A boolean flag that determines whether to attempt model quantization."
            },
            "notes": [
                "The function takes a model (e.g., a Flax nn.Module) and a configuration object as input.",
                "The model is only modified if `config.use_qwix_quantization` is true and `get_qt_provider` returns a valid provider for the specified quantization type in the config."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#self_attention_with_norm",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "def self_attention_with_norm(\n    inputs: jnp.ndarray,\n    cfg: Config,\n    mesh: Mesh,\n    quant: None | Quant,\n    decoder_segment_ids: None | jnp.ndarray,\n    decoder_positions: None | jnp.ndarray,\n    deterministic: bool,\n    model_mode: str,\n):\n  \"\"\"A helper function for self-attention block with normalization.\"\"\"\n\n  inputs_checkpoint = checkpoint_name(inputs, \"decoder_layer_input\")\n\n  # Corresponds to Qwen3's `input_layernorm`\n  lnx = rms_norm(\n      num_features=inputs.shape[-1],\n      dtype=cfg.dtype,\n      weight_dtype=cfg.weight_dtype,\n      name=\"pre_self_attention_layer_norm\",\n      epsilon=cfg.normalization_layer_epsilon,\n      kernel_axes=(\"norm\",),\n  )(inputs_checkpoint)\n  lnx = nn.with_logical_constraint(lnx, (\"activation_batch\", \"activation_length\", \"activation_embed\"))\n\n  # Self-attention block\n  attention_layer = attentions.attention_as_linen(\n      config=cfg,\n      num_query_heads=cfg.num_query_heads,\n      num_kv_heads=cfg.num_kv_heads,\n      head_dim=cfg.head_dim,\n      max_target_length=cfg.max_target_length,\n      max_prefill_predict_length=cfg.max_prefill_predict_length,\n      attention_kernel=cfg.attention,\n      inputs_q_shape=lnx.shape,\n      inputs_kv_shape=lnx.shape,\n      mesh=mesh,\n      dtype=cfg.dtype,\n      weight_dtype=cfg.weight_dtype,\n      dropout_rate=cfg.dropout_rate,\n      name=\"self_attention\",\n      quant=quant,\n      kv_quant=quantizations.configure_kv_quant(cfg),\n      use_qk_norm=cfg.use_qk_norm,\n      query_pre_attn_scalar=(cfg.head_dim**-0.5),  # Qwen3 specific scaling\n      model_mode=model_mode,\n  )\n\n  attention_output = attention_layer(\n      lnx,  # inputs_q\n      lnx,  # inputs_kv\n      decoder_positions,\n      decoder_segment_ids=decoder_segment_ids,\n      deterministic=deterministic,\n      model_mode=model_mode,\n  )\n  attention_output = nn.with_logical_constraint(\n      attention_output, (\"activation_batch\", \"activation_length\", \"activation_embed\")\n  )\n\n  # Residual connection after attention\n  residual_after_attention = inputs_checkpoint + attention_output\n\n  # Post Attention LayerNorm (corresponds to Qwen3's `post_attention_layernorm`)\n  hidden_states = rms_norm(\n      num_features=residual_after_attention.shape[-1],\n      dtype=cfg.dtype,\n      weight_dtype=cfg.weight_dtype,\n      name=\"post_self_attention_layer_norm\",\n      epsilon=cfg.normalization_layer_epsilon,\n      kernel_axes=(\"norm\",),\n  )(residual_after_attention)\n  hidden_states = nn.with_logical_constraint(hidden_states, (\"activation_batch\", \"activation_length\", \"activation_embed\"))\n\n  return hidden_states, residual_after_attention",
        "analysis": {
            "functionality": "This function implements a self-attention block with pre- and post-layer RMS normalization and a residual connection, characteristic of a transformer decoder layer. It applies RMSNorm to the input, performs self-attention, adds the result back to the original input (residual connection), and then applies a final RMSNorm.",
            "usage": "This is a helper function used within a larger transformer decoder layer. To use it, provide an input tensor of shape `[batch_size, sequence_length, hidden_dim]`, along with model configuration, sharding mesh, and other operational parameters. The function returns a tuple containing two tensors: the final normalized hidden states and the intermediate tensor after the first residual connection, both with the same shape as the input."
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#Qwen3DecoderLayer",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "class Qwen3DecoderLayer(nn.Module):\n  \"\"\"Qwen3 Transformer decoder layer (dense).\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      decoder_segment_ids: None | jnp.ndarray,\n      decoder_positions: None | jnp.ndarray,\n      deterministic: bool,\n      model_mode: str,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    cfg = self.config\n\n    hidden_states, residual_after_attention = self_attention_with_norm(\n        inputs,\n        cfg,\n        self.mesh,\n        self.quant,\n        decoder_segment_ids,\n        decoder_positions,\n        deterministic,\n        model_mode,\n    )\n\n    # Dense MLP block\n    mlp_output = linears.mlp_block(\n        in_features=hidden_states.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp\",\n        config=cfg,\n        quant=self.quant,\n    )(hidden_states, deterministic=deterministic)\n\n    # Final residual connection\n    layer_output = residual_after_attention + mlp_output\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        (\"activation_batch\", \"activation_length\", \"activation_embed\"),\n    )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "functionality": "The `Qwen3DecoderLayer` class implements a single, dense decoder layer for a Qwen3-style Transformer model. It processes an input tensor through a self-attention block followed by a dense MLP block, incorporating specific pre/post layer normalizations and residual connections characteristic of the Qwen3 architecture.",
            "usage": "Instantiate the class with a configuration object (`Config`), a JAX sharding mesh (`Mesh`), a model mode string, and an optional quantization configuration. Call the instance with an input tensor of shape `[batch_size, sequence_length, hidden_dim]` and other necessary arguments like `deterministic` mode. The layer returns an output tensor of the same shape, representing the processed hidden states."
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#Qwen3MoeDecoderLayer",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "class Qwen3MoeDecoderLayer(nn.Module):\n  \"\"\"Qwen3 Transformer decoder layer (MoE).\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      decoder_segment_ids: None | jnp.ndarray,\n      decoder_positions: None | jnp.ndarray,\n      deterministic: bool,\n      model_mode: str,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    cfg = self.config\n\n    hidden_states, residual_after_attention = self_attention_with_norm(\n        inputs,\n        cfg,\n        self.mesh,\n        self.quant,\n        decoder_segment_ids,\n        decoder_positions,\n        deterministic,\n        model_mode,\n    )\n\n    # Mixture of Experts block\n    mlp_output, load_balance_loss = moe.get_routed_moe(\n        config=cfg,\n        num_experts=cfg.num_experts,\n        num_experts_per_tok=cfg.num_experts_per_tok,\n        mesh=self.mesh,\n        kernel_init=initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        intermediate_dim=cfg.moe_mlp_dim,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"moe_block\",\n        quant=self.quant,\n    )(hidden_states)\n\n    if load_balance_loss is not None:\n      self.sow(\"intermediates\", \"moe_lb_loss\", load_balance_loss)\n\n    mlp_output = nn.with_logical_constraint(mlp_output, (\"activation_batch\", \"activation_length\", \"activation_embed\"))\n\n    # Final residual connection\n    layer_output = residual_after_attention + mlp_output\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        (\"activation_batch\", \"activation_length\", \"activation_embed\"),\n    )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "qwen3_moe_decoder_layer",
            "purpose": "Implements a single transformer decoder layer for a Qwen3 model, utilizing a Mixture of Experts (MoE) block for the feed-forward network.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "Corresponds to config.dtype (e.g., float32)"
            },
            "processing_steps": [
                "Performs self-attention with pre and post layer normalization using the `self_attention_with_norm` helper function.",
                "Processes the normalized hidden states through a Mixture of Experts (MoE) block via `moe.get_routed_moe`.",
                "Calculates and records a `load_balance_loss` from the MoE routing if it is returned.",
                "Applies a final residual connection by adding the MoE block's output to the residual from the attention block.",
                "Returns the final layer output."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]"
            },
            "dependencies": [
                "self_attention_with_norm",
                "moe.get_routed_moe",
                "flax.linen.Module",
                "initializers.nd_dense_init"
            ],
            "parameters": {
                "config": "The main configuration object containing model hyperparameters.",
                "num_experts": "The total number of experts in the MoE layer.",
                "num_experts_per_tok": "The number of experts to route each token to.",
                "moe_mlp_dim": "The intermediate dimension of the MLP within each expert.",
                "scan_layers": "Boolean flag to control the output format for use with `flax.linen.scan`."
            },
            "notes": [
                "This layer is a variant of a standard transformer decoder, replacing the dense MLP with a sparse MoE block.",
                "The `load_balance_loss` is an auxiliary loss used during training to encourage balanced routing of tokens to experts, and it is recorded using `self.sow`.",
                "The return signature changes from `layer_output` to `(layer_output, None)` when `config.scan_layers` is True."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Executes the forward pass of the Qwen3 MoE decoder layer.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim] for the `inputs` tensor.",
                        "dtype": "Corresponds to config.dtype (e.g., float32)."
                    },
                    "processing_steps": [
                        "Calls `self_attention_with_norm` on the input tensor to get normalized hidden states and a residual.",
                        "Passes the result through the `moe.get_routed_moe` block, obtaining the MLP output and a load balancing loss.",
                        "Sows the `load_balance_loss` to be collected later if it is not None.",
                        "Applies a final residual connection by adding the MoE output to the attention block's residual.",
                        "Returns the layer's output, potentially tupled with `None` if `config.scan_layers` is enabled."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "self_attention_with_norm",
                        "moe.get_routed_moe"
                    ],
                    "notes": [
                        "Accepts additional arguments like `decoder_positions` and `decoder_segment_ids` for attention masking and positional information.",
                        "The `deterministic` flag controls dropout behavior.",
                        "Arguments `previous_chunk`, `page_state`, and `slot` are present for inference compatibility but are not directly used in this layer's logic."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/simple_layer.py#SimpleDecoderLayer",
        "file_path": "src/MaxText/layers/simple_layer.py",
        "code_block": "class SimpleDecoderLayer(nn.Module):\n  \"\"\"Decoder layer consisting of a single [embed, embed] weight matrix.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | quantizations.AqtQuantization = None\n\n  def setup(self):\n    self.weight_mat = self.param(\n        \"weights\",\n        nn.with_logical_partitioning(nn.initializers.lecun_normal(), (\"embed\", \"mlp\")),\n        (self.config.emb_dim, self.config.emb_dim),\n    )\n\n  def __call__(\n      self, inputs: jnp.ndarray, positions, segmentation, deterministic, model_mode, previous_chunk=None, page_state=None\n  ):\n    if self.config.scan_layers:\n      return inputs @ self.weight_mat.astype(inputs.dtype), None\n    else:\n      return inputs @ self.weight_mat.astype(inputs.dtype)",
        "analysis": {
            "functionality": "A simple Flax neural network module that applies a single linear transformation to its input via a learnable weight matrix of shape [embedding_dimension, embedding_dimension].",
            "usage": "Instantiate the class with a configuration object, a JAX mesh, and a model mode. Call the instance with an input tensor of shape `[..., embedding_dimension]`. The module returns a tensor of the same shape after matrix multiplication with its internal weight matrix. The return format (a single tensor or a tuple) is conditional on the `config.scan_layers` flag."
        }
    },
    {
        "block_name": "src/MaxText/layers/simple_layer.py#SimpleMlpDecoderLayer",
        "file_path": "src/MaxText/layers/simple_layer.py",
        "code_block": "class SimpleMlpDecoderLayer(nn.Module):\n  \"\"\"Decoder layer consisting of [embed,mlp] followed by an [mlp,embed] matmul.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | quantizations.AqtQuantization = None\n\n  def setup(self):\n    self.ff_1 = self.param(\n        \"ff_1\",\n        nn.with_logical_partitioning(nn.initializers.lecun_normal(), (\"embed\", \"mlp\")),\n        (self.config.emb_dim, self.config.mlp_dim),\n    )\n    self.ff_2 = self.param(\n        \"ff_2\",\n        nn.with_logical_partitioning(nn.initializers.lecun_normal(), (\"mlp\", \"embed\")),\n        (self.config.mlp_dim, self.config.emb_dim),\n    )\n\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      positions,\n      segmentation,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=0,\n  ):\n    intermediate = inputs @ self.ff_1.astype(inputs.dtype)\n    output = intermediate @ self.ff_2.astype(inputs.dtype)\n    if self.config.scan_layers:\n      return output, None\n    else:\n      return output",
        "analysis": {
            "module_type": "simple_mlp_decoder_layer",
            "purpose": "A simple two-layer MLP (feed-forward network) without activation functions, used as a decoder layer, primarily for testing and debugging.",
            "input": {
                "shape": "[batch_size, sequence_length, emb_dim]",
                "dtype": "jnp.ndarray"
            },
            "processing_steps": [
                "Initializes two weight matrices, `ff_1` ([emb_dim, mlp_dim]) and `ff_2` ([mlp_dim, emb_dim]), in the `setup` method.",
                "In the forward pass, performs a matrix multiplication of the input with `ff_1`.",
                "Performs a second matrix multiplication of the intermediate result with `ff_2`."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, emb_dim]"
            },
            "dependencies": [
                "flax.linen.nn.Module",
                "MaxText.common_types.Config",
                "jax.sharding.Mesh"
            ],
            "parameters": {
                "config.emb_dim": "The embedding dimension of the input and output tensors.",
                "config.mlp_dim": "The intermediate dimension of the MLP.",
                "config.scan_layers": "A boolean flag that alters the return signature for compatibility with scanned layers."
            },
            "notes": [
                "This layer implements a linear transformation as it lacks a non-linear activation function between the two matrix multiplications.",
                "The `quant` attribute is present but not used, meaning no quantization is applied.",
                "The `__call__` method accepts several arguments (`positions`, `segmentation`, `deterministic`, etc.) that are unused, making it a simple drop-in replacement for more complex layers."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the two weight matrices for the MLP.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Defines a parameter `ff_1` of shape `(emb_dim, mlp_dim)` with Lecun normal initialization and logical partitioning.",
                        "Defines a parameter `ff_2` of shape `(mlp_dim, emb_dim)` with Lecun normal initialization and logical partitioning."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nn.with_logical_partitioning",
                        "nn.initializers.lecun_normal"
                    ],
                    "notes": [
                        "This method sets up the trainable parameters of the module before the first forward pass."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the two-layer linear transformation.",
                    "input": {
                        "shape": "[batch_size, sequence_length, emb_dim]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Calculate `intermediate = inputs @ self.ff_1`.",
                        "Calculate `output = intermediate @ self.ff_2`.",
                        "Conditionally return `(output, None)` or just `output` based on `self.config.scan_layers`."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, emb_dim]"
                    },
                    "dependencies": [],
                    "notes": [
                        "The return value is a tuple `(output, None)` if `config.scan_layers` is True, otherwise it is just the `output` tensor.",
                        "Many input arguments are ignored, indicating this is a simplified layer designed for a larger, more complex framework."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/decode_multi.py#_validate_config",
        "file_path": "src/MaxText/inference/decode_multi.py",
        "code_block": "def _validate_config(config):\n  \"\"\"Validate configuration settings.\"\"\"\n  assert config.load_full_state_path == \"\", (\n      \"Decode doesn't operate on full states! Convert to parameter checkpoint first.\"\n      \"Using generate_param_only_checkpoint.\"\n  )\n  assert (\n      0 < _INITIAL_PREFILL_STREAMS <= _NUM_STREAMS\n  ), f\"_INITIAL_PREFILL_STREAMS ({_INITIAL_PREFILL_STREAMS}) must be > 0 and <= _NUM_STREAMS ({_NUM_STREAMS})\"",
        "analysis": {
            "module_type": "validation_function",
            "purpose": "Validates specific configuration settings for the interleaved inference script, ensuring compatibility and logical consistency.",
            "input": {
                "shape": "N/A",
                "dtype": "pyconfig.Config object"
            },
            "processing_steps": [
                "Assert that `config.load_full_state_path` is an empty string, ensuring a parameter-only checkpoint is used.",
                "Assert that the number of initial prefill streams is greater than 0 and less than or equal to the total number of streams."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "_INITIAL_PREFILL_STREAMS",
                "_NUM_STREAMS"
            ],
            "parameters": {
                "config.load_full_state_path": "The path to a full training state, which must be empty for this script.",
                "_INITIAL_PREFILL_STREAMS": "A global constant defining how many streams to prefill before starting generation.",
                "_NUM_STREAMS": "A global constant defining the total number of concurrent streams."
            },
            "notes": [
                "This function does not return a value. It raises an AssertionError if validation fails, halting the program.",
                "It is a helper function called at the start of the main execution flow."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/decode_multi.py#main",
        "file_path": "src/MaxText/inference/decode_multi.py",
        "code_block": "def main(argv: Sequence[str]) -> None:\n  \"\"\"Main function to run interleaved inference.\"\"\"\n  jax.config.update(\"jax_default_prng_impl\", \"unsafe_rbg\")\n  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"0\"\n\n  config = pyconfig.initialize(argv)\n  _validate_config(config)\n  max_utils.print_system_information()\n\n  engine = maxengine.MaxEngine(config)\n  rng = jax.random.PRNGKey(1234)\n  rng, rng_load_params = jax.random.split(rng)\n  params = engine.load_params(rng=rng_load_params)\n\n  text = config.prompt\n  metadata = engine.get_tokenizer()\n  tokenizer_model = engine.build_tokenizer(metadata)\n  tokens, true_length = tokenizer_model.encode(text, is_bos=True, prefill_lengths=[config.max_prefill_predict_length])\n  assert true_length <= config.max_prefill_predict_length, \"Prompt too long for prefill length\"\n\n  batch_size = int(config.per_device_batch_size * jax.device_count())\n  assert (\n      0 < _NUM_STREAMS <= batch_size\n  ), f\"The number of streams {_NUM_STREAMS} must be > 0 and <= batch size {batch_size}\"\n\n  # Initialize decode state\n  rng, rng_init_decode = jax.random.split(rng)\n  decode_state = engine.init_decode_state(rng=rng_init_decode)\n  print(\"Initial decode state initialized.\")\n\n  # Keep track of results per stream (slot)\n  streams_results: dict[int, list[int]] = {i: [] for i in range(_NUM_STREAMS)}\n  streams_active: list[bool] = [False] * _NUM_STREAMS  # Track which slots are active\n  streams_finished: list[bool] = [False] * _NUM_STREAMS  # Track finished streams\n  streams_prefilled_count = 0\n  streams_inserted_count = 0\n\n  # --- Initial Prefill Phase ---\n  print(f\"Starting initial prefill for {_INITIAL_PREFILL_STREAMS} streams...\")\n  prefill_results_to_insert = {}  # Store prefill results before inserting\n  for i in range(_INITIAL_PREFILL_STREAMS):\n    slot_idx = i\n    print(f\"  Prefilling stream for slot {slot_idx}...\")\n    rng, rng_prefill = jax.random.split(rng)\n    request_id = uuid.uuid4()\n    prefill_result, first_token = engine.prefill(\n        params=params,\n        padded_tokens=tokens,\n        true_length=true_length,\n        rng=rng_prefill,\n        slot=slot_idx,\n        request_id=request_id,\n    )\n    prefill_results_to_insert[slot_idx] = prefill_result\n    streams_results[slot_idx].append(first_token.get_result_at_slot(0).tokens.item())\n    streams_prefilled_count += 1\n    print(f\"After prefill stream {slot_idx}\")\n\n  # --- Insert Initial Prefills ---\n  print(\"Inserting initial prefill results...\")\n  for slot_idx, prefill_result in prefill_results_to_insert.items():\n    request_id = uuid.uuid4()\n    decode_state = engine.insert(\n        prefix=prefill_result,\n        decode_state=decode_state,\n        slot=slot_idx,\n        request_id=request_id,  # Pass request_id\n    )\n    streams_active[slot_idx] = True  # Mark stream as active\n    streams_inserted_count += 1\n    print(f\"  Inserted prefill for slot {slot_idx}\")\n\n  print(\"Starting interleaved generation loop...\")\n  total_steps = config.max_target_length - config.max_prefill_predict_length\n  for step in range(total_steps):\n    print(f\"\\n--- Step {step + 1} / {total_steps} ---\")\n\n    # Generate step for all active streams\n    active_stream_indices = [i for i, active in enumerate(streams_active) if active and not streams_finished[i]]\n    if active_stream_indices:\n      print(f\"  Generating for active slots: {active_stream_indices}\")\n      rng, rng_generate = jax.random.split(rng)\n      decode_state, sampled_tokens = engine.generate(params, decode_state, rng=rng_generate)\n\n      # Store the generated token and check for finished streams\n      for slot_idx in active_stream_indices:\n        # Check if the stream finished this step\n        current_len = config.max_prefill_predict_length + step + 1  # Includes prefill + current step\n        finished_this_step = False\n        if current_len >= config.max_target_length:\n          print(f\"    Stream in slot {slot_idx} reached max target length.\")\n          streams_finished[slot_idx] = True\n          streams_active[slot_idx] = False\n          finished_this_step = True\n\n        # Store token if it wasn't already finished before this step or if it finished on this step\n        if not streams_finished[slot_idx] or finished_this_step:\n          # Ensure we don't try to access results for a slot that might not exist\n          if slot_idx < sampled_tokens.data.shape[0]:\n            token_for_slot = sampled_tokens.get_result_at_slot(slot_idx).tokens.item()\n            streams_results[slot_idx].append(token_for_slot)\n          else:\n            print(f\"Warning: Tried to get token for slot {slot_idx}, but batch size seems smaller.\")\n\n        # Call release_pages if finished this step\n        if finished_this_step:\n          print(f\"    Calling engine to release pages for finished slot {slot_idx}...\")\n          engine.release_pages(slot=slot_idx)\n\n    else:\n      print(\"  No active streams to generate for.\")\n\n    # 2. Check if all streams are finished (can exit loop early)\n    if all(streams_finished):\n      print(\"\\nAll streams finished generation.\")\n      break\n\n    # 3. Prefill and Insert new streams if capacity allows\n    num_active_not_finished = sum(1 for i in range(_NUM_STREAMS) if streams_active[i] and not streams_finished[i])\n    available_slots = batch_size - num_active_not_finished\n    can_prefill_more = streams_prefilled_count < _NUM_STREAMS\n\n    if can_prefill_more and available_slots > 0:\n      try:\n        next_available_slot = streams_active.index(False)\n        print(f\"  Prefilling new stream for slot {next_available_slot}...\")\n        rng, rng_prefill = jax.random.split(rng)\n        request_id = uuid.uuid4()\n        prefill_result, first_token = engine.prefill(\n            params=params,\n            padded_tokens=tokens,\n            true_length=true_length,\n            rng=rng_prefill,\n            slot=next_available_slot,\n            request_id=request_id,\n        )\n        streams_prefilled_count += 1\n\n        # Insert the new prefill\n        print(f\"  Inserting new stream into slot {next_available_slot}...\")\n        request_id_insert = uuid.uuid4()\n        decode_state = engine.insert(\n            prefix=prefill_result,\n            decode_state=decode_state,\n            slot=next_available_slot,\n            request_id=request_id_insert,\n        )\n        streams_active[next_available_slot] = True\n        streams_inserted_count += 1\n        streams_results[next_available_slot].append(first_token.get_result_at_slot(0).tokens.item())\n\n      except ValueError:\n        print(\"  Warning: Available slots detected but couldn't find an inactive one.\")\n    elif can_prefill_more:\n      print(\"  Generate step finished, but no available slots to prefill new stream.\")\n    else:\n      print(\"  Generate step finished, all streams already prefilled.\")\n\n  print(\"\\n--- Final Results ---\")\n  for i in range(_NUM_STREAMS):\n    if streams_results[i]:\n      output = tokenizer_model.decode(streams_results[i])\n      print(f\"Stream {i}: Input=`{text}` -> Output=`{output}`\")\n\n      if i == 0:  # Check first stream as an example\n        assert output.startswith(\n            config.autoregressive_decode_assert\n        ), f\"Stream {i} generated text mismatch: `{output}` vs expected start `{config.autoregressive_decode_assert}`\"\n    else:\n      print(f\"Stream {i}: Was not activated.\")",
        "analysis": {
            "module_type": "interleaved_inference_runner",
            "purpose": "Initializes a MaxEngine, loads a model, and runs a multi-stream inference loop that interleaves prefilling new prompts and generating tokens for active prompts.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initialize JAX, environment, and configuration from command-line arguments (`argv`).",
                "Instantiate the `maxengine.MaxEngine`.",
                "Load model parameters using `engine.load_params`.",
                "Tokenize the input prompt specified in the configuration.",
                "Initialize the decoding state using `engine.init_decode_state`.",
                "Perform an initial prefill phase for a subset of streams (`_INITIAL_PREFILL_STREAMS`) by calling `engine.prefill`.",
                "Insert the results of the initial prefill into the main decode state using `engine.insert`.",
                "Enter a generation loop that runs for a specified number of steps.",
                "Inside the loop, generate one token for all active streams simultaneously using `engine.generate`.",
                "Store the generated tokens and check if any streams have reached their target length.",
                "For finished streams, release their resources by calling `engine.release_pages`.",
                "If there is available capacity and more streams to process, prefill a new stream and insert it into an empty slot.",
                "After the loop terminates, decode the complete token sequences for each stream.",
                "Print the final generated text for each stream and perform an assertion check on the output of the first stream."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "jax",
                "maxengine.MaxEngine",
                "pyconfig",
                "max_utils",
                "_validate_config",
                "uuid"
            ],
            "parameters": {
                "config.prompt": "The input text prompt for the language model.",
                "config.max_prefill_predict_length": "The maximum length of the input prompt to be processed in the prefill step.",
                "config.max_target_length": "The total desired length of the output sequence, including the prompt and generated tokens.",
                "config.per_device_batch_size": "The batch size per device, used to calculate the total available slots for concurrent streams.",
                "config.autoregressive_decode_assert": "A string used to validate the beginning of the generated text for the first stream."
            },
            "notes": [
                "This function simulates a continuous batching or interleaved inference scenario, common in LLM serving.",
                "It manages a fixed number of concurrent streams (`_NUM_STREAMS`) within the total available batch size.",
                "The process starts by prefilling an initial number of streams (`_INITIAL_PREFILL_STREAMS`) before beginning the main generation loop.",
                "As streams complete generation, their resources are freed, and new streams can be prefilled and inserted into the available slots."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#reverse_transpose",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "def reverse_transpose(transposed_array, transpose_axis_order):\n  return jax.numpy.moveaxis(transposed_array, (0, 1, 2, 3), transpose_axis_order)",
        "analysis": {
            "module_type": "array_utility",
            "purpose": "Reverses a transpose operation on a 4D array by moving its axes back to a specified original order.",
            "input": {
                "shape": "[dim0, dim1, dim2, dim3]",
                "dtype": "Any JAX numeric dtype."
            },
            "processing_steps": [
                "Calls `jax.numpy.moveaxis` to move the axes of `transposed_array` from their current positions (0, 1, 2, 3) to the destination positions specified by `transpose_axis_order`."
            ],
            "output": {
                "shape": "The input array with its axes permuted. The new shape is a permutation of the input shape according to `transpose_axis_order`."
            },
            "dependencies": [
                "jax.numpy.moveaxis"
            ],
            "parameters": {
                "transposed_array": "The input 4D JAX array that has been transposed.",
                "transpose_axis_order": "A tuple of 4 integers representing the original axis order to restore."
            },
            "notes": [
                "This function is specifically designed for 4-dimensional arrays, as the source axes are hardcoded to (0, 1, 2, 3)."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#transpose_tuple",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "def transpose_tuple(items: tuple[Any, ...], axis_order: AxisIdxes) -> tuple[Any, ...]:\n  return tuple((items[i] for i in axis_order))",
        "analysis": {
            "functionality": "Reorders the elements of an input tuple according to a specified tuple of indices.",
            "usage": "Call the function with an input tuple `items` and a tuple of indices `axis_order`. It returns a new tuple where the element at each position is determined by the index from `axis_order` at that same position."
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#KVQuant",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "class KVQuant:\n  \"\"\"Class to configure quantization for KV cache.\"\"\"\n\n  axis_cfg = \"\"\n  dtype = None\n\n  def __init__(self, config: Config):\n    assert config.quantize_kvcache\n    self.axis_cfg = config.kv_quant_axis\n    self.dtype = self._get_dtype(config.kv_quant_dtype)\n\n  def _get_dtype(self, dtype_cfg: str):\n    if dtype_cfg == \"int4\":\n      return jnp.int4\n    if dtype_cfg == \"int8\":\n      return jnp.int8\n    if dtype_cfg == \"fp8\":\n      return jnp.float8_e4m3fn\n    raise ValueError(f\"Invalid kv_quant_dtype: {dtype_cfg}\")\n\n  def _get_max_axis(self, axis_names: AxisNames):\n    if self.axis_cfg == \"dkv\":\n      return axis_names.index(CACHE_KV)\n    if self.axis_cfg == \"heads_and_dkv\":\n      return (axis_names.index(CACHE_HEADS), axis_names.index(CACHE_KV))\n    raise ValueError(f\"Invalid KV quant axis cfg: {self.axis_cfg}\")\n\n  def quantize(self, kv: Array, axis_names: AxisNames):\n    \"\"\"Quantize key/values stored in kvcache.\"\"\"\n    assert self.axis_cfg, \"KV quant axis cannot be None\"\n    max_axis = self._get_max_axis(axis_names)\n    scale = jnp.max(jnp.abs(kv), axis=max_axis, keepdims=True)\n    if self.dtype == jnp.int8:\n      value = jnp.int8(jnp.rint(kv * (MAX_INT8 / scale)))\n      return value, scale\n    if self.dtype == jnp.int4:\n      value = jnp.int4(jnp.rint(kv * (MAX_INT4 / scale)))\n      return value, scale\n    if self.dtype == jnp.float8_e4m3fn:\n      value = jnp.float8_e4m3fn(kv * (E4M3_MAX / scale))\n      return value, scale\n    raise ValueError(f\"Invalid KV quant dtype:{self.dtype}.\")\n\n  def einsum_fn_with_rhs_qtensor(\n      self,\n      rhs_dequant_mode=None,\n      rhs_calibration_mode=None,\n      lhs_dequant_mode=None,\n      lhs_calibration_mode=None,\n  ):\n    \"\"\"einsum function where QTensor is the right-hand-side\"\"\"\n    # Assumes kv is already quantized.\n    einsum = jnp.einsum\n    if self.dtype != jnp.float8_e4m3fn:\n      num_bits = 4 if self.dtype == jnp.int4 else 8\n      kv_cfg = aqt_config.dot_general_make(\n          lhs_bits=None,\n          rhs_bits=num_bits,\n          bwd_bits=None,\n          use_fwd_quant=False,\n      )\n    else:\n      kv_cfg = aqt_config.config_fwd_fp8()\n\n    if rhs_dequant_mode:\n      aqt_config.set_fwd_dequant_mode(kv_cfg, rhs_dequant_mode=rhs_dequant_mode)\n    if rhs_calibration_mode:\n      aqt_config.set_fwd_calibration_mode(\n          kv_cfg,\n          rhs_calibration_mode=rhs_calibration_mode,\n      )\n    if lhs_dequant_mode:\n      aqt_config.set_fwd_dequant_mode(kv_cfg, lhs_dequant_mode=lhs_dequant_mode)\n    if lhs_calibration_mode:\n      aqt_config.set_fwd_calibration_mode(\n          kv_cfg,\n          lhs_calibration_mode=lhs_calibration_mode,\n      )\n    einsum = aqt_flax.AqtEinsum(\n        rhs_quant_mode=aqt_flax.QuantMode.TRAIN,\n        lhs_freeze_mode=aqt_flax.FreezerMode.NONE,\n        rhs_freeze_mode=aqt_flax.FreezerMode.NONE,\n        cfg=kv_cfg,\n    )\n    return einsum\n\n  def einsum_fn_with_rhs_qtensor_and_dequant(self):\n    \"\"\"Get einstein summation for different dequant modes.\"\"\"\n    if self.dtype == jnp.float8_e4m3fn:\n      return self.einsum_fn_with_rhs_qtensor(\n          lhs_dequant_mode=aqt_config.DequantMode.THIS_INPUT,\n          lhs_calibration_mode=aqt_config.CalibrationMode.REMAINING_AXIS,\n          rhs_dequant_mode=aqt_config.DequantMode.OTHER_INPUT,\n          rhs_calibration_mode=aqt_config.CalibrationMode.REMAINING_AXIS,\n      )\n    else:\n      return self.einsum_fn_with_rhs_qtensor(\n          rhs_dequant_mode=aqt_config.DequantMode.OTHER_INPUT,\n          rhs_calibration_mode=aqt_config.CalibrationMode.REMAINING_AXIS,\n      )",
        "analysis": {
            "module_type": "kv_cache_quantization_config",
            "purpose": "A class to configure and manage the quantization of the Key-Value (KV) cache in a transformer model.",
            "input": {
                "shape": "N/A",
                "dtype": "Config object"
            },
            "processing_steps": [
                "Initializes quantization axis configuration (`axis_cfg`) from `config.kv_quant_axis`.",
                "Initializes quantization data type (`dtype`) from `config.kv_quant_dtype` by calling `_get_dtype`."
            ],
            "output": {
                "shape": "An instance of the KVQuant class."
            },
            "dependencies": [
                "jax.numpy",
                "aqt.jax.v2.config",
                "aqt.jax.v2.flax",
                "MaxText.common_types.Config"
            ],
            "parameters": {
                "config.quantize_kvcache": "A boolean flag that must be true to enable KV cache quantization and use this class.",
                "config.kv_quant_axis": "A string specifying the axis/axes for quantization, e.g., 'dkv' or 'heads_and_dkv'.",
                "config.kv_quant_dtype": "A string specifying the target data type for quantization, e.g., 'int4', 'int8', or 'fp8'."
            },
            "notes": [
                "This class encapsulates the logic for both quantizing KV cache tensors and creating specialized `einsum` functions from the AQT library to handle computations with these quantized tensors."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the KVQuant configuration from a global `Config` object.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "Config"
                    },
                    "processing_steps": [
                        "Assert that `config.quantize_kvcache` is True.",
                        "Set `self.axis_cfg` from `config.kv_quant_axis`.",
                        "Call `self._get_dtype` with `config.kv_quant_dtype` to set `self.dtype`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "self._get_dtype"
                    ],
                    "notes": [
                        "This method sets up the quantization parameters that will be used by other methods in the class."
                    ]
                },
                "_get_dtype": {
                    "purpose": "Converts a string configuration for a data type into a `jnp` dtype object.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "string"
                    },
                    "processing_steps": [
                        "Check if the input string is 'int4', 'int8', or 'fp8'.",
                        "Return the corresponding `jnp` dtype.",
                        "Raise a ValueError if the string is not recognized."
                    ],
                    "output": {
                        "shape": "A jnp dtype object (e.g., jnp.int4)."
                    },
                    "dependencies": [
                        "jax.numpy"
                    ],
                    "notes": [
                        "This is a private helper method for initialization."
                    ]
                },
                "_get_max_axis": {
                    "purpose": "Determines the axis or axes over which to compute the maximum absolute value for quantization scaling, based on the `axis_cfg`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "AxisNames (tuple of strings)"
                    },
                    "processing_steps": [
                        "Check `self.axis_cfg`.",
                        "If 'dkv', find the index of `CACHE_KV` in `axis_names`.",
                        "If 'heads_and_dkv', find the indices of `CACHE_HEADS` and `CACHE_KV`.",
                        "Raise a ValueError for an invalid `axis_cfg`."
                    ],
                    "output": {
                        "shape": "An integer or a tuple of integers representing the axis/axes."
                    },
                    "dependencies": [
                        "MaxText.common_types.AxisNames",
                        "MaxText.common_types.CACHE_KV",
                        "MaxText.common_types.CACHE_HEADS"
                    ],
                    "notes": [
                        "This helper method translates the string configuration into concrete axis indices for `jnp.max`."
                    ]
                },
                "quantize": {
                    "purpose": "Quantizes a key/value tensor to a lower precision integer or float format.",
                    "input": {
                        "shape": "[batch, sequence, heads, hidden_dim]",
                        "dtype": "float"
                    },
                    "processing_steps": [
                        "Call `_get_max_axis` to determine the reduction axis.",
                        "Calculate the `scale` by finding the maximum absolute value of the input tensor along the determined axis.",
                        "Scale the input tensor by the inverse of the `scale`, round it, and cast it to the target dtype (`self.dtype`).",
                        "Return the quantized tensor and the scale."
                    ],
                    "output": {
                        "shape": "A tuple of two tensors: (quantized_value, scale). `quantized_value` has the same shape as the input. `scale` has the same shape but with dimension 1 for the reduction axes."
                    },
                    "dependencies": [
                        "jax.numpy",
                        "self._get_max_axis"
                    ],
                    "notes": [
                        "This method implements symmetric quantization."
                    ]
                },
                "einsum_fn_with_rhs_qtensor": {
                    "purpose": "Creates and configures an `AqtEinsum` function for performing dot-product operations where the right-hand-side (RHS) operand is a quantized tensor.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "Optional string arguments for dequantization and calibration modes."
                    },
                    "processing_steps": [
                        "Create an AQT configuration (`kv_cfg`) based on `self.dtype`.",
                        "Optionally set dequantization and calibration modes on the configuration object.",
                        "Instantiate and return `aqt_flax.AqtEinsum` with the configured `kv_cfg`."
                    ],
                    "output": {
                        "shape": "A callable `aqt_flax.AqtEinsum` object."
                    },
                    "dependencies": [
                        "aqt.jax.v2.config",
                        "aqt.jax.v2.flax.AqtEinsum"
                    ],
                    "notes": [
                        "This method prepares a specialized `einsum` function that can efficiently handle operations with the quantized KV cache tensors."
                    ]
                },
                "einsum_fn_with_rhs_qtensor_and_dequant": {
                    "purpose": "A convenience wrapper that provides specific dequantization configurations for the `AqtEinsum` function based on the quantization dtype.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check `self.dtype`.",
                        "Call `self.einsum_fn_with_rhs_qtensor` with preset dequantization and calibration modes suitable for either fp8 or integer quantization."
                    ],
                    "output": {
                        "shape": "A callable `aqt_flax.AqtEinsum` object."
                    },
                    "dependencies": [
                        "self.einsum_fn_with_rhs_qtensor",
                        "aqt.jax.v2.config"
                    ],
                    "notes": [
                        "This simplifies getting a correctly configured `einsum` function for common attention use cases."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#kv_cache_as_linen",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "def kv_cache_as_linen(\n    *,\n    max_prefill_length: int,\n    max_target_length: int,\n    batch: int,\n    key_seq_len: int,\n    value_seq_len: int,\n    key_heads: int,\n    value_heads: int,\n    key_head_size: int,\n    value_head_size: int,\n    dtype: DType,\n    kv_quant: None | KVQuant = None,\n    prefill_cache_logical_axis_names: AxisNames = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n    cache_logical_axis_names: AxisNames = (CACHE_BATCH, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n    cache_scale_logical_axis_names: AxisNames = (\n        CACHE_SCALE_BATCH,\n        CACHE_SCALE_SEQUENCE,\n        CACHE_SCALE_HEADS,\n        CACHE_SCALE_KV,\n    ),\n    prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    key_axis_order: AxisIdxes = (2, 0, 1, 3),\n    use_chunked_prefill: bool = False,\n    model_mode: str = MODEL_MODE_PREFILL,\n    name: str | None = None,\n):\n  \"\"\"Initializes the KVCache module and returns it as a Linen module.\n\n  Args:\n    max_prefill_length: The maximum prefill length.\n    max_target_length: The maximum target length.\n    batch: The batch size.\n    key_seq_len: The key sequence length.\n    value_seq_len: The value sequence length.\n    key_heads: The number of key heads.\n    value_heads: The number of value heads.\n    key_head_size: The key head size.\n    value_head_size: The value head size.\n    dtype: The data type.\n    kv_quant: The KVQuant configuration.\n    prefill_cache_logical_axis_names: The logical axis names for the prefill cache.\n    cache_logical_axis_names: The logical axis names for the cache.\n    cache_scale_logical_axis_names: The logical axis names for the cache scale.\n    prefill_cache_axis_order: The axis order for the prefill cache.\n    ar_cache_axis_order: The axis order for the autoregressive cache.\n    key_axis_order: The axis order for the key.\n    use_chunked_prefill: Whether to use chunked prefill.\n    model_mode: The model mode.\n    name: The name of the Linen module.\n\n  Returns:\n    A Linen module that wraps the NNX `KVCache` module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      KVCache,\n      max_prefill_length=max_prefill_length,\n      max_target_length=max_target_length,\n      batch=batch,\n      key_seq_len=key_seq_len,\n      value_seq_len=value_seq_len,\n      key_heads=key_heads,\n      value_heads=value_heads,\n      key_head_size=key_head_size,\n      value_head_size=value_head_size,\n      dtype=dtype,\n      kv_quant=kv_quant,\n      prefill_cache_logical_axis_names=prefill_cache_logical_axis_names,\n      cache_logical_axis_names=cache_logical_axis_names,\n      cache_scale_logical_axis_names=cache_scale_logical_axis_names,\n      prefill_cache_axis_order=prefill_cache_axis_order,\n      ar_cache_axis_order=ar_cache_axis_order,\n      key_axis_order=key_axis_order,\n      use_chunked_prefill=use_chunked_prefill,\n      model_mode=model_mode,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n      abstract_init=False,\n  )",
        "analysis": {
            "module_type": "kv_cache_factory",
            "purpose": "A factory function that initializes a `KVCache` NNX module and wraps it as a Flax Linen module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to wrap the `KVCache` NNX module, forwarding all configuration parameters."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance, not a tensor."
            },
            "dependencies": [
                "KVCache",
                "nnx_wrappers.to_linen",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "max_prefill_length": "The maximum sequence length for the prefill phase.",
                "max_target_length": "The maximum total sequence length for autoregressive decoding.",
                "batch": "The batch size for the cache.",
                "key_heads": "The number of key heads.",
                "value_heads": "The number of value heads.",
                "key_head_size": "The dimension of each key head.",
                "value_head_size": "The dimension of each value head.",
                "dtype": "The data type for the cache tensors.",
                "kv_quant": "Optional configuration object for KV cache quantization.",
                "model_mode": "The operational mode of the model, e.g., 'prefill' or 'autoregressive'."
            },
            "notes": [
                "This function acts as a bridge, allowing an NNX-based `KVCache` module to be used within a Flax Linen model.",
                "It passes a `metadata_fn` (`variable_to_logically_partitioned`) to handle tensor sharding annotations during the wrapping process."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#KVCache",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "class KVCache(nnx.Module):\n  \"\"\"Implementation of the KVCache.\"\"\"\n\n  def __init__(\n      self,\n      max_prefill_length: int,\n      max_target_length: int,\n      # TODO(bvandermoon): Can we get batch, key_seq_len, value_seq_len, key_heads,\n      # and value_heads from key/value after migrating Attention to NNX?\n      batch: int,\n      key_seq_len: int,\n      value_seq_len: int,\n      key_heads: int,\n      value_heads: int,\n      key_head_size: int,\n      value_head_size: int,\n      dtype: DType,\n      kv_quant: None | KVQuant = None,\n      prefill_cache_logical_axis_names: AxisNames = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n      cache_logical_axis_names: AxisNames = (CACHE_BATCH, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n      cache_scale_logical_axis_names: AxisNames = (\n          CACHE_SCALE_BATCH,\n          CACHE_SCALE_SEQUENCE,\n          CACHE_SCALE_HEADS,\n          CACHE_SCALE_KV,\n      ),\n      prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      key_axis_order: AxisIdxes = (2, 0, 1, 3),\n      use_chunked_prefill: bool = False,\n      model_mode: str = MODEL_MODE_PREFILL,\n      *,\n      # Not used in KVCache but passed in by nnx_wrappers.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the KVCache module.\n\n    Args:\n      max_prefill_length: The maximum prefill length.\n      max_target_length: The maximum target length.\n      batch: The batch size.\n      key_seq_len: The key sequence length.\n      value_seq_len: The value sequence length.\n      key_heads: The number of key heads.\n      value_heads: The number of value heads.\n      key_head_size: The key head size.\n      value_head_size: The value head size.\n      dtype: The data type.\n      kv_quant: The KVQuant configuration.\n      prefill_cache_logical_axis_names: The logical axis names for the prefill cache.\n      cache_logical_axis_names: The logical axis names for the cache.\n      cache_scale_logical_axis_names: The logical axis names for the cache scale.\n      prefill_cache_axis_order: The axis order for the prefill cache.\n      ar_cache_axis_order: The axis order for the autoregressive cache.\n      key_axis_order: The axis order for the key.\n      model_mode: The model mode.\n      use_chunked_prefill: Whether to use chunked prefill.\n      rngs: The random number generators for initialization.\n    \"\"\"\n    self.max_prefill_length = max_prefill_length\n    self.max_target_length = max_target_length\n    self.batch = batch\n    self.key_seq_len = key_seq_len\n    self.value_seq_len = value_seq_len\n    self.key_heads = key_heads\n    self.value_heads = value_heads\n    self.key_head_size = key_head_size\n    self.value_head_size = value_head_size\n    self.dtype = dtype\n    self.kv_quant = kv_quant\n    self.prefill_cache_logical_axis_names = prefill_cache_logical_axis_names\n    self.cache_logical_axis_names = cache_logical_axis_names\n    self.cache_scale_logical_axis_names = cache_scale_logical_axis_names\n    self.prefill_cache_axis_order = prefill_cache_axis_order\n    self.ar_cache_axis_order = ar_cache_axis_order\n    self.key_axis_order = key_axis_order\n    self.model_mode = model_mode\n    self.use_chunked_prefill = use_chunked_prefill\n\n    self._initialize_prefill_caches(model_mode)\n    self._initialize_ar_cache_vars(model_mode)\n\n  @property\n  def prefill_key_vars(self):\n    return (self.cached_prefill_key, self.cached_prefill_key_scale)\n\n  @property\n  def prefill_value_vars(self):\n    return (self.cached_prefill_value, self.cached_prefill_value_scale)\n\n  @property\n  def ar_key_vars(self):\n    return (self.cached_ar_key, self.cached_ar_key_scale)\n\n  @property\n  def ar_value_vars(self):\n    return (self.cached_ar_value, self.cached_ar_value_scale)\n\n  def _get_cached_kv_dtype(self):\n    return self.kv_quant.dtype if self.kv_quant else self.dtype\n\n  def _get_cache_scale_logical_shape(self, heads, cache_length):\n    assert self.kv_quant\n    if self.kv_quant.axis_cfg == \"dkv\":\n      return (self.batch, cache_length, heads, 1)\n    if self.kv_quant.axis_cfg == \"heads_and_dkv\":\n      return (self.batch, cache_length, 1, 1)\n    raise ValueError(f\"Invalid config for kv_quant_axis:{self.kv_quant.axis_cfg}\")\n\n  def _initialize_prefill_caches(self, model_mode):\n    \"\"\"Get a shaped abstraction of the state\"\"\"\n\n    cache_length = self.max_prefill_length\n    dtype = self._get_cached_kv_dtype()\n\n    if model_mode == MODEL_MODE_PREFILL:\n      cache_logical_axis_names = self.prefill_cache_logical_axis_names\n    else:\n      cache_logical_axis_names = self.cache_logical_axis_names\n    cache_axis_names = transpose_tuple(cache_logical_axis_names, self.prefill_cache_axis_order)\n\n    cache_logical_shape = (self.batch, cache_length, self.key_heads, self.key_head_size)\n    cache_shape_key = transpose_tuple(cache_logical_shape, self.prefill_cache_axis_order)\n\n    cache_logical_shape = (self.batch, cache_length, self.value_heads, self.value_head_size)\n    cache_shape_value = transpose_tuple(cache_logical_shape, self.prefill_cache_axis_order)\n\n    self.cached_prefill_key = nnx.Cache(\n        jnp.zeros(cache_shape_key, dtype=dtype),\n        sharding=cache_axis_names,\n    )\n    self.cached_prefill_value = nnx.Cache(\n        jnp.zeros(cache_shape_value, dtype=dtype),\n        sharding=cache_axis_names,\n    )\n\n    if model_mode == MODEL_MODE_PREFILL:\n      segment_id_axis_names = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE)\n    else:\n      segment_id_axis_names = (CACHE_BATCH, CACHE_SEQUENCE)\n\n    self.cache_prefill_segment_id = nnx.Cache(\n        jnp.zeros((cache_logical_shape[0], cache_length), dtype=jnp.int32),\n        sharding=segment_id_axis_names,\n    )\n\n    if self.kv_quant:\n      cache_scale_axis_names = transpose_tuple(self.cache_scale_logical_axis_names, self.prefill_cache_axis_order)\n\n      cache_scale_logical_shape = self._get_cache_scale_logical_shape(self.key_heads, cache_length)\n      cache_key_scale_shape = transpose_tuple(cache_scale_logical_shape, self.prefill_cache_axis_order)\n\n      cache_scale_logical_shape = self._get_cache_scale_logical_shape(self.value_heads, cache_length)\n      cache_value_scale_shape = transpose_tuple(cache_scale_logical_shape, self.prefill_cache_axis_order)\n\n      self.cached_prefill_key_scale = nnx.Cache(\n          jnp.zeros(cache_key_scale_shape, dtype=jnp.bfloat16),\n          sharding=cache_scale_axis_names,\n      )\n      self.cached_prefill_value_scale = nnx.Cache(\n          jnp.zeros(cache_value_scale_shape, dtype=jnp.bfloat16),\n          sharding=cache_scale_axis_names,\n      )\n    else:\n      self.cached_prefill_key_scale = None\n      self.cached_prefill_value_scale = None\n\n  def _get_prefill_cache_vars(self):\n    return self.prefill_key_vars, self.prefill_value_vars, self.cache_prefill_segment_id\n\n  def _initialize_ar_cache_vars(self, model_mode):\n    \"\"\"get ar cache vars\"\"\"\n\n    dtype = self._get_cached_kv_dtype()\n    if self.max_target_length <= self.max_prefill_length:\n      raise ValueError(\n          f\"max_target_length: {self.max_target_length} should be greater than max_prefill_length:\"\n          f\" {self.max_prefill_length}!\"\n      )\n    cache_length = self.max_target_length - self.max_prefill_length\n\n    if model_mode == MODEL_MODE_PREFILL:\n      cache_logical_axis_names = self.prefill_cache_logical_axis_names\n    else:\n      cache_logical_axis_names = self.cache_logical_axis_names\n    cache_axis_names = transpose_tuple(cache_logical_axis_names, self.ar_cache_axis_order)\n\n    cache_logical_shape = (self.batch, cache_length, self.key_heads, self.key_head_size)\n    cache_shape_key = transpose_tuple(cache_logical_shape, self.ar_cache_axis_order)\n\n    cache_logical_shape = (self.batch, cache_length, self.value_heads, self.value_head_size)\n    cache_shape_value = transpose_tuple(cache_logical_shape, self.ar_cache_axis_order)\n\n    # TODO(b/339703100): investigate the issue why with_logical_partitioning doesn't enforce sharding\n    self.cached_ar_key = nnx.Cache(\n        jnp.zeros(cache_shape_key, dtype=dtype),\n        sharding=cache_axis_names,\n    )\n    self.cached_ar_key.value = nn.with_logical_constraint(\n        self.cached_ar_key.value,\n        cache_axis_names,\n    )\n\n    self.cached_ar_value = nnx.Cache(\n        jnp.zeros(cache_shape_value, dtype=dtype),\n        sharding=cache_axis_names,\n    )\n    self.cached_ar_value.value = nn.with_logical_constraint(\n        self.cached_ar_value.value,\n        cache_axis_names,\n    )\n\n    if model_mode == MODEL_MODE_PREFILL:\n      segment_id_axis_names = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE)\n    else:\n      segment_id_axis_names = (CACHE_BATCH, CACHE_SEQUENCE)\n    self.cache_ar_segment_id = nnx.Cache(\n        jnp.zeros((cache_logical_shape[0], cache_length), dtype=jnp.int32),\n        sharding=segment_id_axis_names,\n    )\n\n    self.cached_ar_lengths = nnx.Cache(\n        jnp.zeros((cache_logical_shape[0],), dtype=jnp.int32),\n        sharding=(CACHE_BATCH,),\n    )\n\n    if self.kv_quant:\n      cache_scale_axis_names = transpose_tuple(self.cache_scale_logical_axis_names, self.ar_cache_axis_order)\n\n      cache_scale_logical_shape = self._get_cache_scale_logical_shape(self.key_heads, cache_length)\n      cache_key_scale_shape = transpose_tuple(cache_scale_logical_shape, self.ar_cache_axis_order)\n\n      cache_scale_logical_shape = self._get_cache_scale_logical_shape(self.value_heads, cache_length)\n      cache_value_scale_shape = transpose_tuple(cache_scale_logical_shape, self.ar_cache_axis_order)\n\n      self.cached_ar_key_scale = nnx.Cache(\n          jnp.zeros(cache_key_scale_shape, dtype=jnp.bfloat16),\n          sharding=cache_scale_axis_names,\n      )\n      self.cached_ar_value_scale = nnx.Cache(\n          jnp.zeros(cache_value_scale_shape, dtype=jnp.bfloat16),\n          sharding=cache_scale_axis_names,\n      )\n    else:\n      self.cached_ar_key_scale = None\n      self.cached_ar_value_scale = None\n\n    self.cache_ar_index = nnx.Cache(\n        jnp.zeros((1,), dtype=jnp.int32),\n        sharding=(),\n    )\n\n  def _get_ar_cache_vars(self):\n    return self.ar_key_vars, self.ar_value_vars, self.cache_ar_segment_id, self.cache_ar_index, self.cached_ar_lengths\n\n  def kv_cache_chunked_prefill(\n      self, key: Array, value: Array, decoder_segment_ids: Array, previous_chunk: None | Array = None\n  ):\n    \"\"\"Update the current kv cache into previous chunk and return needed length.\n\n    The previous chunk kv cache should be in the model's param.\n\n    Prefill cache need to be max prefill length to prevent different shape of kv cache.\n    Different shape of kv cache in previous chunk could produce different compiled graph.\n\n    Args:\n      key: in shape [b, s, n, d].\n      value: in shape [b, s, n, d].\n      decoder_segment_ids: [b, s] -- marking segment ids for tokens\n      previous_chunk:\n        In shape [b, s]. The tokens without padding in previous chunk.\n        Use to preserve the previous kv cache.\n\n    Returns:\n      key, value, decoder_segment_id.\n    \"\"\"\n\n    assert not self.kv_quant, \"Not support kv_quant now.\"\n    if decoder_segment_ids is not None:\n      self.batch, segment_id_seq_len = decoder_segment_ids.shape\n      assert self.key_seq_len == segment_id_seq_len, f\"{self.key_seq_len=}, {segment_id_seq_len=} should match.\"\n\n    assert key.dtype == value.dtype, \"Key and Value Dtypes should match.\"\n    assert self.key_seq_len == self.value_seq_len, f\"{self.key_seq_len=}, {self.value_seq_len=} should match.\"\n\n    next_pos = 0\n    if previous_chunk is not None:\n      # We only have 1 prompt in prefill mode.\n      next_pos = previous_chunk.shape[1]\n\n    cached_prefill_key_vars, cached_prefill_value_vars, cached_prefill_segment_id_var = self._get_prefill_cache_vars()\n    # TODO: Find a way to not enable the ar cache for prefill mode.\n\n    key_shaped_for_cache = jnp.transpose(key, self.prefill_cache_axis_order)\n    value_shaped_for_cache = jnp.transpose(value, self.prefill_cache_axis_order)\n\n    # For quantized kv cached. Could be get without transpose twice.\n    cached_key = self.get_cached_values(cached_prefill_key_vars, key.dtype, self.prefill_cache_axis_order)\n    cached_value = self.get_cached_values(cached_prefill_value_vars, value.dtype, self.prefill_cache_axis_order)\n    cached_key_value = jnp.transpose(cached_key, self.prefill_cache_axis_order)\n    cached_value_value = jnp.transpose(cached_value, self.prefill_cache_axis_order)\n\n    seq_axis = self.prefill_cache_logical_axis_names.index(CACHE_SEQUENCE)\n    cache_seq_axis = self.prefill_cache_axis_order.index(seq_axis)\n\n    assert next_pos + key_shaped_for_cache.shape[cache_seq_axis] <= self.max_prefill_length, (\n        f\"Previous kv cache[{next_pos}] + \"\n        f\"current kv cache[{key_shaped_for_cache.shape[cache_seq_axis]}] \"\n        f\"> max length[{self.max_prefill_length}]\"\n    )\n\n    # We don't zero out remain values. Use segment id to mask out.\n    cached_prefill_key_vars[0].value = jax.lax.dynamic_update_slice_in_dim(\n        cached_key_value, key_shaped_for_cache, next_pos, cache_seq_axis\n    )\n    cached_prefill_value_vars[0].value = jax.lax.dynamic_update_slice_in_dim(\n        cached_value_value, value_shaped_for_cache, next_pos, cache_seq_axis\n    )\n\n    if decoder_segment_ids is not None:\n      # Need zero out the remain values to prevent wrong mask in autoregressive.\n      previous_segment_id = cached_prefill_segment_id_var.value[:, :next_pos]\n      cached_prefill_segment_id_var.value = jnp.zeros_like(cached_prefill_segment_id_var.value, dtype=jnp.int32)\n      cached_prefill_segment_id_var.value = jax.lax.dynamic_update_slice_in_dim(\n          cached_prefill_segment_id_var.value, previous_segment_id, start_index=0, axis=1\n      )\n      cached_prefill_segment_id_var.value = jax.lax.dynamic_update_slice_in_dim(\n          cached_prefill_segment_id_var.value, decoder_segment_ids, next_pos, axis=1\n      )\n\n    # Return needed kv cache to reduce computation of attention.\n    needed_prefill_key_value = jax.lax.dynamic_slice_in_dim(\n        cached_prefill_key_vars[0].value, start_index=0, slice_size=(next_pos + self.key_seq_len), axis=cache_seq_axis\n    )\n    needed_prefill_value_value = jax.lax.dynamic_slice_in_dim(\n        cached_prefill_value_vars[0].value, start_index=0, slice_size=(next_pos + self.value_seq_len), axis=cache_seq_axis\n    )\n    needed_segment_id = None\n    if decoder_segment_ids is not None:\n      needed_segment_id = jax.lax.dynamic_slice_in_dim(\n          cached_prefill_segment_id_var.value, start_index=0, slice_size=(next_pos + segment_id_seq_len), axis=1\n      )\n\n    return (\n        jnp.transpose(needed_prefill_key_value, self.key_axis_order),\n        jnp.transpose(needed_prefill_value_value, self.key_axis_order),\n        needed_segment_id,\n    )\n\n  def kv_cache_prefill(\n      self,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array,\n  ):\n    \"\"\"In prefill mode, we zero out the existing cache, run the computation and\n    prepare the cache as necessary.\n\n    Args:\n      key: in shape [b, s, n, d].\n      value: in shape [b, s, n, d].\n      decoder_segment_ids: [b, s] -- marking segment ids for tokens\n\n    Returns:\n      key, value, decoder_segment_id.\n\n    \"\"\"\n\n    assert key.dtype == value.dtype, \"Key and Value Dtypes should match.\"\n\n    cached_prefill_key_vars, cached_prefill_value_vars, cached_prefill_segment_id_var = self._get_prefill_cache_vars()\n\n    key_shaped_for_cache = jnp.transpose(key, self.prefill_cache_axis_order)\n    value_shaped_for_cache = jnp.transpose(value, self.prefill_cache_axis_order)\n\n    if self.kv_quant:\n      prefill_key_axis_names = transpose_tuple(self.cache_logical_axis_names, self.prefill_cache_axis_order)\n      key_shaped_for_cache, key_scale_shaped_for_cache = self.kv_quant.quantize(\n          key_shaped_for_cache, prefill_key_axis_names\n      )\n      value_shaped_for_cache, value_scale_shaped_for_cache = self.kv_quant.quantize(\n          value_shaped_for_cache, prefill_key_axis_names\n      )\n      cached_prefill_key_vars[1].value = key_scale_shaped_for_cache\n      cached_prefill_value_vars[1].value = value_scale_shaped_for_cache\n\n    cached_prefill_key_vars[0].value = key_shaped_for_cache\n    cached_prefill_value_vars[0].value = value_shaped_for_cache\n\n    if decoder_segment_ids is not None:\n      cached_prefill_segment_id_var.value = decoder_segment_ids\n    return key, value, decoder_segment_ids\n\n  def update_ar_key_value(\n      self,\n      one_token_key: Array,\n      one_token_value: Array,\n      key_caches: tuple[nnx.Cache, nnx.Cache | None],\n      value_caches: tuple[nnx.Cache, nnx.Cache | None],\n      one_hot_indices: Array,\n      lengths: Array,\n      use_ragged_attention: bool,\n  ) -> None:\n    \"\"\"Adds a single token's results to the ar kv cache\n\n    Args:\n        one_token_key (Array): Key of one token to add to the cache\n        one_token_value (Array): Value of one token to add to the cache\n        cached_ar_key (tuple[nnx.Cache, nnx.Cache|None],): Cached keys to add new token key to, possibly with scale\n        cached_ar_value (tuple[nnx.Cache, nnx.Cache|None],: Cached values to add new token value to, possible with scale\n        one_hot_indices (Array): Location of the new token within the cache\n\n    Returns:\n        tuple[Array, Array]: Updated caches for key and value with new token info added\n    \"\"\"\n\n    cached_key, cached_key_scale = key_caches\n    cached_value, cached_value_scale = value_caches\n\n    # In order to update the key, value caches with the current key and\n    # value, we reshape the one_token_key and one_token_value\n    one_token_key_shaped_for_cache = jnp.transpose(one_token_key, self.ar_cache_axis_order)\n    one_token_value_shaped_for_cache = jnp.transpose(one_token_value, self.ar_cache_axis_order)\n\n    ar_cache_axis_names = transpose_tuple(self.cache_logical_axis_names, self.ar_cache_axis_order)\n    if self.kv_quant:\n      one_token_key_shaped_for_cache, one_token_key_scale_shaped_for_cache = self.kv_quant.quantize(\n          one_token_key_shaped_for_cache, ar_cache_axis_names\n      )\n      one_token_value_shaped_for_cache, one_token_value_scale_shaped_for_cache = self.kv_quant.quantize(\n          one_token_value_shaped_for_cache, ar_cache_axis_names\n      )\n\n    ar_cache_update_idx = jnp.squeeze(one_hot_indices)\n    ar_cache_sequence_axis = ar_cache_update_axis = ar_cache_axis_names.index(CACHE_SEQUENCE)\n    ar_cache_batch_axis = ar_cache_axis_names.index(CACHE_BATCH)\n\n    if use_ragged_attention:\n      cache_locations = [slice(None)] * 4\n      new_token_locations = [slice(None)] * 4\n      new_token_locations[ar_cache_sequence_axis] = 0\n\n      def key_body(i, val):\n        cache_locations[ar_cache_batch_axis] = i\n        cache_locations[ar_cache_sequence_axis] = lengths[i]\n        new_token_locations[ar_cache_batch_axis] = i\n        return val.at[tuple(cache_locations)].set(one_token_key_shaped_for_cache[tuple(new_token_locations)])\n\n      def value_body(i, val):\n        cache_locations[ar_cache_batch_axis] = i\n        cache_locations[ar_cache_sequence_axis] = lengths[i]\n        new_token_locations[ar_cache_batch_axis] = i\n        return val.at[tuple(cache_locations)].set(one_token_value_shaped_for_cache[tuple(new_token_locations)])\n\n      cached_key.value = jax.lax.fori_loop(\n          0, one_token_key_shaped_for_cache.shape[0], key_body, cached_key.value, unroll=8\n      )\n      cached_value.value = jax.lax.fori_loop(\n          0, one_token_value_shaped_for_cache.shape[0], value_body, cached_value.value, unroll=8\n      )\n\n    else:\n      one_hot_indices = one_hot_indices.astype(int)\n      cached_key.value = jax.lax.dynamic_update_index_in_dim(\n          cached_key.value, one_token_key_shaped_for_cache, ar_cache_update_idx, ar_cache_update_axis\n      )\n      cached_value.value = jax.lax.dynamic_update_index_in_dim(\n          cached_value.value, one_token_value_shaped_for_cache, ar_cache_update_idx, ar_cache_update_axis\n      )\n    cached_key.value = nn.with_logical_constraint(cached_key.value, ar_cache_axis_names)\n    cached_value.value = nn.with_logical_constraint(cached_value.value, ar_cache_axis_names)\n\n    if self.kv_quant:\n      ar_cache_scale_axis_names = transpose_tuple(self.cache_scale_logical_axis_names, self.ar_cache_axis_order)\n      ar_cache_scale_update_axis = ar_cache_scale_axis_names.index(CACHE_SCALE_SEQUENCE)\n      assert cached_key_scale is not None, \"cached_key_scale_var cannot be None\"\n      assert cached_value_scale is not None, \"cached_value_scale_var cannot be None\"\n      cached_key_scale.value = jax.lax.dynamic_update_index_in_dim(\n          cached_key_scale.value, one_token_key_scale_shaped_for_cache, ar_cache_update_idx, ar_cache_scale_update_axis\n      )\n      cached_value_scale.value = jax.lax.dynamic_update_index_in_dim(\n          cached_value_scale.value,\n          one_token_value_scale_shaped_for_cache,\n          ar_cache_update_idx,\n          ar_cache_scale_update_axis,\n      )\n\n  def get_cached_values(self, cache_vars, target_dtype, cache_axis_order) -> jax.Array | KVTensor:\n    \"\"\"get cached values\"\"\"\n    cache_var, cache_scale_var = cache_vars\n    cache_value = cache_var.value\n    if cache_scale_var is not None:\n      scale_value = cache_scale_var.value\n      dtype = cache_value.dtype\n      if dtype == jnp.int8:\n        scale_value /= MAX_INT8\n      elif dtype == jnp.int4:\n        scale_value /= MAX_INT4\n      elif dtype == jnp.float8_e4m3fn:\n        scale_value /= E4M3_MAX\n\n      cache_value = KVTensor(qvalue=cache_value, scale=[scale_value], scale_t=None, dequant_dtype=target_dtype, bias=[])\n    cache_value_in_logical_shape = jax.tree.map(lambda x: reverse_transpose(x, cache_axis_order), cache_value)\n    return cache_value_in_logical_shape\n\n  def kv_cache_autoregressive(\n      self,\n      key: Array,\n      value: Array,\n      use_ragged_attention: bool = False,\n  ):\n    \"\"\"In autoregressive mode, we update the cache for this entry and\n       then return the full cache.\n\n    Args:\n      key: in shape [b, 1, n, d].\n      value: in shape [b, 1, n, d].\n      decoder_segment_ids: [b, 1] -- marking segment ids for tokens\n\n    Returns:\n      tuple of (key, value, segment_id) for both prefill and ar cache,\n    Raises:\n      ValueError: when key/value shape is not [batch, 1, num_heads, heads_dim].\n    \"\"\"\n    _, sequence, _, _ = value.shape\n    if sequence != 1:\n      raise ValueError(f\"Sequence length should be 1 during autoregression, got {sequence=}\")\n\n    cached_ar_key_vars, cached_ar_value_vars, cached_ar_segment_id_var, cache_ar_index_var, cache_ar_lengths_var = (\n        self._get_ar_cache_vars()\n    )\n\n    self.update_ar_key_value(\n        key,\n        value,\n        cached_ar_key_vars,\n        cached_ar_value_vars,\n        cache_ar_index_var.value,\n        cache_ar_lengths_var.value,\n        use_ragged_attention,\n    )\n    active_indicator = jnp.zeros((self.batch, 1), dtype=jnp.int32) + DECODING_ACTIVE_SEQUENCE_INDICATOR\n    cached_ar_segment_id_var.value = jax.lax.dynamic_update_index_in_dim(\n        cached_ar_segment_id_var.value, active_indicator, jnp.squeeze(cache_ar_index_var.value), 1\n    )\n    cache_ar_index_var.value = jnp.mod(cache_ar_index_var.value + 1, self.max_target_length - self.max_prefill_length)\n    cache_ar_lengths_var.value = cache_ar_lengths_var.value.at[:].add(1)\n\n    cached_prefill_key_vars, cached_prefill_value_vars, cached_prefill_segment_id_var = self._get_prefill_cache_vars()\n\n    cached_prefill = (\n        self.get_cached_values(cached_prefill_key_vars, key.dtype, self.prefill_cache_axis_order),\n        self.get_cached_values(cached_prefill_value_vars, value.dtype, self.prefill_cache_axis_order),\n        cached_prefill_segment_id_var.value,\n    )\n\n    cached_ar = (\n        self.get_cached_values(cached_ar_key_vars, key.dtype, self.ar_cache_axis_order),\n        self.get_cached_values(cached_ar_value_vars, value.dtype, self.ar_cache_axis_order),\n        cached_ar_segment_id_var.value,\n        cache_ar_lengths_var.value,\n    )\n    return cached_prefill, cached_ar\n\n  def __call__(\n      self,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array,\n      model_mode: str,\n      use_ragged_attention: bool = False,\n      previous_chunk: Any = None,\n  ) -> tuple:\n    \"\"\"KV cache takes the current state and updates the state accordingly.\n\n    The key and value have dimension [b, s, n_kv, d],\n    but we cache them with a reshape as defined in *_axis_order config as a TPU\n    fusion optimization. This also enables the \"scatter via one-hot\n    broadcast\" trick, which means we do a one-hot broadcast instead of a\n    scatter/gather operations, resulting in a 3-4x speedup in practice.\n\n    Args:\n      key: in shape [b, s, n_kv, d].\n      value: in shape [b, s, n_kv, d].\n      model_mode: model mode controlling model.\n\n    Returns:\n      two tuples of (k, v, decoder_segments) -- either can be Nones\n\n    \"\"\"\n    if model_mode == MODEL_MODE_PREFILL:\n      if self.use_chunked_prefill:\n        return self.kv_cache_chunked_prefill(key, value, decoder_segment_ids, previous_chunk), None\n      else:\n        return self.kv_cache_prefill(key, value, decoder_segment_ids), None\n    elif model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      return self.kv_cache_autoregressive(key, value, use_ragged_attention)\n    else:\n      raise ValueError(f\"Model Mode isn't supported! {model_mode=}\")",
        "analysis": {
            "module_type": "kv_cache",
            "purpose": "Manages the key-value cache for transformer models during inference, handling both prefill and autoregressive decoding steps with optional quantization.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes prefill and autoregressive cache tensors in the constructor.",
                "The `__call__` method dispatches to the appropriate caching logic based on `model_mode`."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.nnx.Module",
                "flax.nnx.Cache",
                "jax",
                "KVQuant"
            ],
            "parameters": {
                "max_prefill_length": "The maximum sequence length for the prefill phase.",
                "max_target_length": "The maximum total sequence length for generation.",
                "batch": "The batch size.",
                "key_heads": "The number of key heads.",
                "value_heads": "The number of value heads.",
                "key_head_size": "The dimension of each key head.",
                "value_head_size": "The dimension of each value head.",
                "dtype": "The data type for the cache tensors.",
                "kv_quant": "An optional configuration object for quantizing the key-value cache.",
                "use_chunked_prefill": "A boolean indicating whether to use chunked prefill for long sequences.",
                "model_mode": "The operating mode, typically 'prefill' or 'autoregressive'."
            },
            "notes": [
                "The class maintains separate caches for the prefill phase and the autoregressive generation phase.",
                "It uses axis order configurations (`prefill_cache_axis_order`, `ar_cache_axis_order`) to transpose tensors for performance optimization on TPUs.",
                "The `rngs` argument in `__init__` is a placeholder for compatibility with `nnx_wrappers.to_linen` and is not used within the class."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the KVCache module, setting up dimensions and initializing prefill and autoregressive cache variables.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters like max lengths, batch size, and head dimensions.",
                        "Call `_initialize_prefill_caches` to set up storage for the prefill phase.",
                        "Call `_initialize_ar_cache_vars` to set up storage for the autoregressive phase."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "_initialize_prefill_caches",
                        "_initialize_ar_cache_vars"
                    ],
                    "notes": [
                        "The `rngs` argument is a placeholder for compatibility with `nnx_wrappers.to_linen` and is not used."
                    ]
                },
                "prefill_key_vars": {
                    "purpose": "A property that returns the prefill key cache and its optional scale.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Return a tuple of `self.cached_prefill_key` and `self.cached_prefill_key_scale`."
                    ],
                    "output": {
                        "shape": "A tuple containing two `nnx.Cache` objects, the second of which can be None."
                    },
                    "dependencies": [],
                    "notes": []
                },
                "prefill_value_vars": {
                    "purpose": "A property that returns the prefill value cache and its optional scale.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Return a tuple of `self.cached_prefill_value` and `self.cached_prefill_value_scale`."
                    ],
                    "output": {
                        "shape": "A tuple containing two `nnx.Cache` objects, the second of which can be None."
                    },
                    "dependencies": [],
                    "notes": []
                },
                "ar_key_vars": {
                    "purpose": "A property that returns the autoregressive key cache and its optional scale.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Return a tuple of `self.cached_ar_key` and `self.cached_ar_key_scale`."
                    ],
                    "output": {
                        "shape": "A tuple containing two `nnx.Cache` objects, the second of which can be None."
                    },
                    "dependencies": [],
                    "notes": []
                },
                "ar_value_vars": {
                    "purpose": "A property that returns the autoregressive value cache and its optional scale.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Return a tuple of `self.cached_ar_value` and `self.cached_ar_value_scale`."
                    ],
                    "output": {
                        "shape": "A tuple containing two `nnx.Cache` objects, the second of which can be None."
                    },
                    "dependencies": [],
                    "notes": []
                },
                "_initialize_prefill_caches": {
                    "purpose": "Initializes the cache tensors (`key`, `value`, `segment_id`, and optional scales) for the prefill phase.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determine cache length, data type, and logical axis names based on `model_mode`.",
                        "Calculate the physical shape of the cache tensors by transposing the logical shape.",
                        "Create `nnx.Cache` objects for key, value, and segment IDs, initialized with zeros.",
                        "If quantization is enabled, create additional `nnx.Cache` objects for key and value scales."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.Cache",
                        "jnp.zeros",
                        "transpose_tuple"
                    ],
                    "notes": [
                        "This method modifies the instance state by creating attributes like `self.cached_prefill_key`."
                    ]
                },
                "_initialize_ar_cache_vars": {
                    "purpose": "Initializes the cache tensors (`key`, `value`, `segment_id`, `lengths`, `index`, and optional scales) for the autoregressive phase.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determine cache length (`max_target_length` - `max_prefill_length`).",
                        "Calculate the physical shape of the cache tensors by transposing the logical shape.",
                        "Create `nnx.Cache` objects for key, value, segment IDs, lengths, and the current index, initialized with zeros.",
                        "Apply logical constraints to the key and value caches.",
                        "If quantization is enabled, create additional `nnx.Cache` objects for key and value scales."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.Cache",
                        "jnp.zeros",
                        "nn.with_logical_constraint",
                        "transpose_tuple"
                    ],
                    "notes": [
                        "This method modifies the instance state by creating attributes like `self.cached_ar_key`."
                    ]
                },
                "kv_cache_chunked_prefill": {
                    "purpose": "Updates the prefill cache for a chunk of a long sequence, appending to previous chunks.",
                    "input": {
                        "shape": "key/value: [batch, seq_len, num_heads, head_dim], decoder_segment_ids: [batch, seq_len], previous_chunk: [batch, prev_seq_len]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determine the starting position based on the `previous_chunk` length.",
                        "Transpose input key and value to match the cache layout.",
                        "Update the prefill cache by slicing in the new chunk at the determined start position.",
                        "Update the segment ID cache similarly.",
                        "Slice the updated cache to return only the portion needed for the current attention computation.",
                        "Transpose the needed key and value back to the logical layout."
                    ],
                    "output": {
                        "shape": "A tuple of (key, value, segment_id) with sequence length equal to `prev_seq_len + seq_len`."
                    },
                    "dependencies": [
                        "jnp.transpose",
                        "jax.lax.dynamic_update_slice_in_dim",
                        "jax.lax.dynamic_slice_in_dim",
                        "get_cached_values"
                    ],
                    "notes": [
                        "This method is used when `use_chunked_prefill` is True and asserts that KV quantization is not currently supported."
                    ]
                },
                "kv_cache_prefill": {
                    "purpose": "Populates the prefill cache with the initial key-value states.",
                    "input": {
                        "shape": "key/value: [batch, seq_len, num_heads, head_dim], decoder_segment_ids: [batch, seq_len]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Transpose input key and value to match the cache layout.",
                        "If `kv_quant` is enabled, quantize the key and value and store the results and scales in the cache.",
                        "Otherwise, directly store the transposed key and value in the cache.",
                        "Store `decoder_segment_ids` in the segment ID cache.",
                        "Return the original, unmodified inputs."
                    ],
                    "output": {
                        "shape": "A tuple containing the original (key, value, decoder_segment_ids) inputs."
                    },
                    "dependencies": [
                        "jnp.transpose",
                        "KVQuant.quantize"
                    ],
                    "notes": [
                        "This method overwrites the entire prefill cache."
                    ]
                },
                "update_ar_key_value": {
                    "purpose": "Updates the autoregressive cache with a single token's key and value.",
                    "input": {
                        "shape": "one_token_key/value: [batch, 1, num_heads, head_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Transpose the single-token key and value to match the cache layout.",
                        "If `kv_quant` is enabled, quantize the key and value.",
                        "If `use_ragged_attention`, use a `fori_loop` to update the cache at the correct index for each sequence.",
                        "Otherwise, use `jax.lax.dynamic_update_index_in_dim` to update the cache.",
                        "Apply logical constraints to the updated cache tensors.",
                        "If `kv_quant` is enabled, update the scale caches."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "jnp.transpose",
                        "jax.lax.dynamic_update_index_in_dim",
                        "jax.lax.fori_loop",
                        "nn.with_logical_constraint",
                        "KVQuant.quantize"
                    ],
                    "notes": [
                        "This method modifies the cache variables passed to it in place."
                    ]
                },
                "get_cached_values": {
                    "purpose": "Retrieves values from a cache, dequantizing them if necessary, and transposing them back to the logical shape.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Retrieve the value from the cache variable.",
                        "If a scale variable exists, create a `KVTensor` object to represent the quantized value and its scale.",
                        "Reverse the transpose operation to restore the logical tensor shape."
                    ],
                    "output": {
                        "shape": "Tensor in logical shape [batch, sequence_length, num_heads, head_dim]."
                    },
                    "dependencies": [
                        "KVTensor",
                        "reverse_transpose"
                    ],
                    "notes": [
                        "This method abstracts away the dequantization and layout transformation."
                    ]
                },
                "kv_cache_autoregressive": {
                    "purpose": "Updates the autoregressive cache with a new token and returns the full combined (prefill + autoregressive) cache.",
                    "input": {
                        "shape": "key/value: [batch, 1, num_heads, head_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `update_ar_key_value` to insert the new key/value into the autoregressive cache.",
                        "Update the `cache_ar_segment_id` to mark the new token as active.",
                        "Increment the circular `cache_ar_index` and the sequence `cache_ar_lengths`.",
                        "Call `get_cached_values` for both the prefill and autoregressive caches to retrieve them in their logical shape.",
                        "Return two tuples: one for the full prefill cache and one for the full autoregressive cache."
                    ],
                    "output": {
                        "shape": "A tuple of two tuples: `(prefill_cache, ar_cache)`. `prefill_cache` is `(key, value, segment_id)`. `ar_cache` is `(key, value, segment_id, lengths)`."
                    },
                    "dependencies": [
                        "update_ar_key_value",
                        "get_cached_values"
                    ],
                    "notes": [
                        "This is the core logic for a single decoding step."
                    ]
                },
                "__call__": {
                    "purpose": "Main entry point that routes to the correct caching logic based on `model_mode`.",
                    "input": {
                        "shape": "key/value: [batch, seq_len, num_heads, head_dim], decoder_segment_ids: [batch, seq_len]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check `model_mode`.",
                        "If 'prefill', check `use_chunked_prefill` and call either `kv_cache_chunked_prefill` or `kv_cache_prefill`.",
                        "If 'autoregressive', call `kv_cache_autoregressive`.",
                        "Return the results from the called method."
                    ],
                    "output": {
                        "shape": "A tuple of two tuples: `(prefill_cache, ar_cache)`. One of them will be `None` depending on the mode."
                    },
                    "dependencies": [
                        "kv_cache_chunked_prefill",
                        "kv_cache_prefill",
                        "kv_cache_autoregressive"
                    ],
                    "notes": [
                        "Acts as a dispatcher for the different caching strategies."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#mla_kv_cache_as_linen",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "def mla_kv_cache_as_linen(\n    *,\n    max_prefill_length: int,\n    max_target_length: int,\n    batch: int,\n    key_seq_len: int,\n    value_seq_len: int,\n    key_head_size: int,\n    value_head_size: int,\n    dtype: DType,\n    key_heads: int = 1,\n    value_heads: int = 1,\n    kv_quant: None | KVQuant = None,\n    prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    use_chunked_prefill: bool = False,\n    model_mode: str = MODEL_MODE_PREFILL,\n    name: str | None = None,\n):\n  \"\"\"Initializes the MlaKVCache module and returns it as a Linen module.\n\n  Args:\n    max_prefill_length: The maximum prefill length.\n    max_target_length: The maximum target length.\n    batch: The batch size.\n    key_seq_len: The key sequence length.\n    value_seq_len: The value sequence length.\n    key_head_size: The key head size.\n    value_head_size: The value head size.\n    dtype: The data type.\n    key_heads: The number of key heads.\n    value_heads: The number of value heads.\n    kv_quant: The KVQuant configuration.\n    prefill_cache_axis_order: The axis order for the prefill cache.\n    ar_cache_axis_order: The axis order for the autoregressive cache.\n    use_chunked_prefill: Whether to use chunked prefill.\n    model_mode: The model mode.\n    name: The name of the Linen module.\n\n  Returns:\n    A Linen module that wraps the NNX `MlaKVCache` module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      MlaKVCache,\n      max_prefill_length=max_prefill_length,\n      max_target_length=max_target_length,\n      batch=batch,\n      key_seq_len=key_seq_len,\n      value_seq_len=value_seq_len,\n      key_head_size=key_head_size,\n      value_head_size=value_head_size,\n      dtype=dtype,\n      key_heads=key_heads,\n      value_heads=value_heads,\n      kv_quant=kv_quant,\n      prefill_cache_axis_order=prefill_cache_axis_order,\n      ar_cache_axis_order=ar_cache_axis_order,\n      use_chunked_prefill=use_chunked_prefill,\n      model_mode=model_mode,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n      abstract_init=False,\n  )",
        "analysis": {
            "module_type": "mla_kv_cache_factory",
            "purpose": "A factory function that initializes an NNX MlaKVCache module and wraps it to be compatible with the Flax Linen framework.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `MlaKVCache` NNX module into a Linen-compatible module, passing along all configuration parameters."
            ],
            "output": {
                "shape": "Returns a Flax Linen module object, not a tensor."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "MlaKVCache",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "max_prefill_length": "The maximum number of tokens in the prefill phase.",
                "max_target_length": "The maximum total sequence length (prefill + generated tokens).",
                "batch": "The batch size for the cache.",
                "key_seq_len": "The sequence length of the key tensor.",
                "value_seq_len": "The sequence length of the value tensor.",
                "key_head_size": "The feature dimension of each key head.",
                "value_head_size": "The feature dimension of each value head.",
                "dtype": "The data type for the cache tensors (e.g., jnp.bfloat16).",
                "model_mode": "Specifies the operational mode, such as 'prefill' or 'autoregressive'.",
                "kv_quant": "Optional configuration object for quantizing the key-value cache."
            },
            "notes": [
                "This function serves as a compatibility bridge to use an NNX-defined module (`MlaKVCache`) within a Flax Linen model architecture.",
                "It passes `variable_to_logically_partitioned` as a `metadata_fn` to handle sharding annotations for distributed execution.",
                "The `abstract_init` parameter is set to `False`, ensuring the module is fully initialized upon creation."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#MlaKVCache",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "class MlaKVCache(KVCache):\n  \"\"\"Implementation of the KVCache for MLA.\"\"\"\n\n  def __init__(\n      self,\n      max_prefill_length: int,\n      max_target_length: int,\n      # TODO(bvandermoon): Can we get batch, key_seq_len, value_seq_len,\n      # key_head_size, value_head_size, key_heads, and value_heads from\n      # key/value after migrating Attention to NNX?\n      batch: int,\n      key_seq_len: int,\n      value_seq_len: int,\n      key_head_size: int,\n      value_head_size: int,\n      dtype: DType,\n      key_heads: int = 1,\n      value_heads: int = 1,\n      kv_quant: None | KVQuant = None,\n      prefill_cache_logical_axis_names: AxisNames = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE, CACHE_HEADS_NONE, CACHE_KV),\n      cache_logical_axis_names: AxisNames = (CACHE_BATCH, CACHE_SEQUENCE, CACHE_HEADS_NONE, CACHE_KV),\n      cache_scale_logical_axis_names: AxisNames = (\n          CACHE_SCALE_BATCH,\n          CACHE_SCALE_SEQUENCE,\n          CACHE_SCALE_HEADS,\n          CACHE_SCALE_KV,\n      ),\n      prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      key_axis_order: AxisIdxes = (2, 0, 1, 3),\n      use_chunked_prefill: bool = False,\n      model_mode: str = MODEL_MODE_PREFILL,\n      *,\n      # Not used in MlaKVCache but passed in by nnx_wrappers.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the MlaKVCache module.\n\n    Args:\n      max_prefill_length: The maximum prefill length.\n      max_target_length: The maximum target length.\n      batch: The batch size.\n      key_seq_len: The key sequence length.\n      value_seq_len: The value sequence length.\n      key_head_size: The key head size.\n      value_head_size: The value head size.\n      dtype: The data type.\n      key_heads: The number of key heads.\n      value_heads: The number of value heads.\n      kv_quant: The KVQuant configuration.\n      prefill_cache_logical_axis_names: The logical axis names for the prefill\n        cache.\n      cache_logical_axis_names: The logical axis names for the cache.\n      cache_scale_logical_axis_names: The logical axis names for the cache\n        scale.\n      prefill_cache_axis_order: The axis order for the prefill cache.\n      ar_cache_axis_order: The axis order for the autoregressive cache.\n      key_axis_order: The axis order for the key.\n      use_chunked_prefill: Whether to use chunked prefill.\n      model_mode: The model mode.\n      rngs: The random number generators for initialization.\n    \"\"\"\n    super().__init__(\n        max_prefill_length=max_prefill_length,\n        max_target_length=max_target_length,\n        batch=batch,\n        key_seq_len=key_seq_len,\n        value_seq_len=value_seq_len,\n        key_heads=key_heads,\n        value_heads=value_heads,\n        key_head_size=key_head_size,\n        value_head_size=value_head_size,\n        dtype=dtype,\n        kv_quant=kv_quant,\n        prefill_cache_logical_axis_names=prefill_cache_logical_axis_names,\n        cache_logical_axis_names=cache_logical_axis_names,\n        cache_scale_logical_axis_names=cache_scale_logical_axis_names,\n        prefill_cache_axis_order=prefill_cache_axis_order,\n        ar_cache_axis_order=ar_cache_axis_order,\n        key_axis_order=key_axis_order,\n        use_chunked_prefill=use_chunked_prefill,\n        model_mode=model_mode,\n        rngs=rngs,\n    )\n\n  def key_latent_add_head_dim(self, key_latent: Array):\n    b, l, hz = key_latent.shape\n    return key_latent.reshape(b, l, 1, hz)\n\n  def key_latent_remove_head_dim(self, key_latent: Array):\n    b, l, _, hz = key_latent.shape\n    return key_latent.reshape(b, l, hz)\n\n  def __call__(\n      self,\n      key_latent: Array,\n      key_rope: Array,\n      decoder_segment_ids: Array,\n      model_mode: str,\n      use_ragged_attention: bool = False,\n      previous_chunk: Any = None,\n  ) -> tuple[\n      None | tuple[Array, Array, Array],\n      None | tuple[Array, Array, Array, Array],\n  ]:\n    assert model_mode != MODEL_MODE_TRAIN, \"incorrectly updating kvcache in train mode.\"\n    assert self.kv_quant is None, \"kvcache quantization not supported with mla.\"\n    key_latent = self.key_latent_add_head_dim(key_latent)\n    prefill_cache, ar_cache = super().__call__(key_latent, key_rope, decoder_segment_ids, model_mode)\n    if prefill_cache:\n      key_latent, key_rope, decoder_segments_ids = prefill_cache\n      prefill_cache = (\n          self.key_latent_remove_head_dim(key_latent),\n          key_rope,\n          decoder_segments_ids,\n      )\n    if ar_cache:\n      key_latent, key_rope, decoder_segments_ids, lengths = ar_cache\n      ar_cache = (\n          self.key_latent_remove_head_dim(key_latent),\n          key_rope,\n          decoder_segments_ids,\n          lengths,\n      )\n    return prefill_cache, ar_cache",
        "analysis": {
            "module_type": "mla_kv_cache",
            "purpose": "A specialized Key-Value cache for Multi-Layer Attention (MLA) models, which adapts the input and output shapes of the base KVCache by managing a singleton head dimension.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes the parent KVCache with MLA-specific defaults, such as setting the number of key/value heads to 1 and using logical axis names that do not include a head dimension."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "KVCache"
            ],
            "parameters": {
                "max_prefill_length": "The maximum sequence length for the prefill phase.",
                "max_target_length": "The maximum total sequence length for generation.",
                "batch": "The batch size.",
                "key_heads": "The number of key heads, which defaults to 1 for MLA.",
                "prefill_cache_logical_axis_names": "Logical axis names for the prefill cache, defaulting to use `CACHE_HEADS_NONE`.",
                "cache_logical_axis_names": "Logical axis names for the autoregressive cache, defaulting to use `CACHE_HEADS_NONE`."
            },
            "notes": [
                "This class inherits from `KVCache` and is designed to work with attention mechanisms that do not have a separate head dimension for the key latent.",
                "It handles the addition and removal of a dummy head dimension to maintain compatibility with the base `KVCache` implementation."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the MlaKVCache module by calling the parent `KVCache` constructor with specific defaults suitable for MLA.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `super().__init__` with the provided arguments and MLA-specific defaults (e.g., `key_heads=1`, `value_heads=1`, and axis names with `CACHE_HEADS_NONE`)."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "KVCache.__init__"
                    ],
                    "notes": [
                        "The constructor sets default values for `key_heads`, `value_heads`, `prefill_cache_logical_axis_names`, and `cache_logical_axis_names`."
                    ]
                },
                "key_latent_add_head_dim": {
                    "purpose": "Reshapes the input `key_latent` tensor to add a singleton head dimension.",
                    "input": {
                        "shape": "[batch, sequence_length, hidden_dim]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Reshapes the input tensor from `(b, l, hz)` to `(b, l, 1, hz)`."
                    ],
                    "output": {
                        "shape": "[batch, sequence_length, 1, hidden_dim]"
                    },
                    "dependencies": [],
                    "notes": [
                        "This method prepares the `key_latent` to be compatible with the base `KVCache` which expects a head dimension."
                    ]
                },
                "key_latent_remove_head_dim": {
                    "purpose": "Reshapes the input `key_latent` tensor to remove the singleton head dimension.",
                    "input": {
                        "shape": "[batch, sequence_length, 1, hidden_dim]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Reshapes the input tensor from `(b, l, 1, hz)` to `(b, l, hz)`."
                    ],
                    "output": {
                        "shape": "[batch, sequence_length, hidden_dim]"
                    },
                    "dependencies": [],
                    "notes": [
                        "This method reverts the `key_latent` from the base `KVCache` output to the shape expected by MLA layers."
                    ]
                },
                "__call__": {
                    "purpose": "Updates the cache by adding a head dimension to the input `key_latent`, calling the parent cache implementation, and then removing the head dimension from the results.",
                    "input": {
                        "shape": "key_latent: [batch, seq_len, hidden_dim], key_rope: [batch, seq_len, hidden_dim], decoder_segment_ids: [batch, seq_len]",
                        "dtype": "DType"
                    },
                    "processing_steps": [
                        "Assert that the model is not in training mode.",
                        "Assert that KV cache quantization is not enabled.",
                        "Call `self.key_latent_add_head_dim` on the input `key_latent`.",
                        "Call the parent `KVCache`'s `__call__` method with the reshaped `key_latent`.",
                        "If a prefill cache is returned, call `self.key_latent_remove_head_dim` on its `key_latent` component.",
                        "If an autoregressive cache is returned, call `self.key_latent_remove_head_dim` on its `key_latent` component.",
                        "Return the potentially modified prefill and autoregressive caches."
                    ],
                    "output": {
                        "shape": "A tuple of (prefill_cache, ar_cache), where the `key_latent` in each cache has shape `[batch, seq_len, hidden_dim]`."
                    },
                    "dependencies": [
                        "KVCache.__call__",
                        "self.key_latent_add_head_dim",
                        "self.key_latent_remove_head_dim"
                    ],
                    "notes": [
                        "This method acts as an adapter between MLA layers (which do not have a head dimension on `key_latent`) and the general-purpose `KVCache`.",
                        "The `key_rope` argument is passed through to the parent class without modification."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#InputData",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class InputData:\n  \"\"\"Container for input data and metadata.\n\n  Attributes:\n      id: Unique identifier for this input\n      tokens: JAX array containing the tokenized input\n      true_length: Actual length of the input before padding\n  \"\"\"\n\n  id: str\n  tokens: jax.Array | np.ndarray\n  true_length: int",
        "analysis": {
            "module_type": "input_data_container",
            "purpose": "A dataclass to store input data and its associated metadata for inference.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "An instance of the InputData class."
            },
            "dependencies": [
                "dataclasses",
                "jax",
                "numpy"
            ],
            "parameters": {
                "id": "A unique string identifier for the input.",
                "tokens": "A JAX or NumPy array containing the tokenized input sequence, which may be padded.",
                "true_length": "An integer representing the actual length of the input sequence before any padding was applied."
            },
            "notes": [
                "This is a dataclass, which automatically generates methods like __init__ and __repr__.",
                "It is used to encapsulate a single inference request, separating the token data from metadata like its original length and ID."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#CompletionOutput",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class CompletionOutput:\n  \"\"\"Container for model generation output.\n\n  Attributes:\n      index: The index of the output in the request.\n      token_ids: The token IDs of the prompt and generated output text.\n      logprobs: The log probabilities of the prompt and generated output tokens.\n      prompt_length: The number of prompt tokens.\n  \"\"\"\n\n  index: str\n  token_ids: np.ndarray\n  logprobs: np.ndarray\n  prompt_length: int",
        "analysis": {
            "module_type": "data_container",
            "purpose": "A data container for storing the final output of a model generation task, including token IDs and log probabilities.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "numpy",
                "dataclasses"
            ],
            "parameters": {
                "index": "The string index of the output corresponding to the input request.",
                "token_ids": "A numpy array of integers representing the token IDs of the combined prompt and generated text.",
                "logprobs": "A numpy array of floats representing the log probabilities for each token in `token_ids`.",
                "prompt_length": "An integer indicating the number of tokens in the original prompt."
            },
            "notes": [
                "This class is a dataclass, which automatically generates methods like `__init__`.",
                "It is used to structure and return the final results from the `InferenceWorker` and `OfflineEngine`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#TokenOutput",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class TokenOutput:\n  \"\"\"Container for individual token generation result.\"\"\"\n\n  token: np.ndarray\n  log_prob: np.ndarray",
        "analysis": {
            "module_type": "data_container",
            "purpose": "A data container for the result of a single token generation step, holding the token ID and its log probability.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes with a token and its corresponding log probability."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "numpy"
            ],
            "parameters": {
                "token": "The generated token ID, stored as a numpy array.",
                "log_prob": "The log probability of the generated token, stored as a numpy array."
            },
            "notes": [
                "This class is implicitly a dataclass, used to structure the output of each generation step.",
                "Instances of this class are collected in a list within the `InferenceWorker` to build the complete generated sequence for a prompt."
            ],
            "methods": {}
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#SafeThread",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class SafeThread(threading.Thread):\n  \"\"\"Thread class with exception handling to prevent silent failures.\"\"\"\n\n  def run(self):\n    try:\n      super().run()\n    except Exception as _:  # pylint: disable=broad-exception-caught\n      traceback.print_exc()\n      # Kill the process if a thread encounters an error\n      os.kill(os.getpid(), signal.SIGKILL)",
        "analysis": {
            "module_type": "exception_handling_thread",
            "purpose": "A custom thread class that catches any unhandled exceptions within its `run` method, prints a traceback, and then terminates the entire process to prevent silent failures.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Inherits from `threading.Thread`.",
                "Overrides the `run` method to wrap the parent's `run` call in a try-except block."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "threading.Thread",
                "traceback",
                "os",
                "signal"
            ],
            "parameters": {},
            "notes": [
                "This class is designed as a safety mechanism to ensure that if a background thread encounters a critical error, the entire application stops immediately rather than continuing in a potentially inconsistent state.",
                "It uses `os.kill` with `signal.SIGKILL` for termination, which is a forceful stop and does not allow for cleanup operations."
            ],
            "methods": {
                "run": {
                    "purpose": "Executes the thread's main logic and handles any exceptions by printing a traceback and killing the parent process.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls the `run` method of the parent `threading.Thread` class within a `try` block.",
                        "If any `Exception` is caught, it prints the full exception traceback.",
                        "Kills the current process with a `SIGKILL` signal."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "super().run",
                        "traceback.print_exc",
                        "os.kill",
                        "signal.SIGKILL"
                    ],
                    "notes": [
                        "This method overrides the default behavior of `threading.Thread.run`.",
                        "The exception handling is broad (`except Exception`), intended to catch any potential failure in the thread's execution."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#PrefillType",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class PrefillType(Enum):\n  \"\"\"Enumeration of supported prefill processing methods.\"\"\"\n\n  DEFAULT = \"default\"\n  BATCH = \"batch\"",
        "analysis": {
            "module_type": "enumeration",
            "purpose": "Defines an enumeration for the supported prefill processing methods.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "Enum"
            ],
            "parameters": {},
            "notes": [
                "This class provides two named constants for prefill strategies: 'DEFAULT' and 'BATCH'."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#PrefillResult",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class PrefillResult:\n  \"\"\"Result from prefill processing operation.\"\"\"\n\n  result_tokens: engine_api.ResultTokens\n  slot: int\n  prompt_logp: None | jax.Array",
        "analysis": {
            "functionality": "A dataclass that encapsulates the results of a prefill processing operation.",
            "usage": "This class is used to store and pass around the output of a prefill step. It is instantiated with the generated tokens, the KV cache slot index, and the log probabilities of the prompt. An instance of this class is created by a `PrefillHelper` and consumed by the `InferenceWorker`'s `prefill_done` callback."
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#PrefillHelper",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class PrefillHelper:\n  \"\"\"Abstraction layer for different prefill processing strategies.\n\n  Provides a unified interface for both default (single-sequencse) and batch\n  (packed multi-sequence) prefill processing methods.\n  \"\"\"\n\n  def __init__(\n      self,\n      prefill_type: PrefillType,\n      engine: MaxEngine,\n      prefill_lengths: list[int],\n      batch_prefill_max_batch_size: int = 16,\n      rng=None,\n  ):\n    \"\"\"Initialize the PrefillHelper.\n\n    Args:\n        type: The type of prefill processor to use (\"default\" or \"batch\")\n        engine: The MaxEngine instance to use for prefill operations\n        prefill_lengths: list of prompt lengths to support\n        batch_prefill_max_batch_size: Maximum number of prompts in one packed\n            sequence for batch prefill\n    \"\"\"\n    self._type = prefill_type\n    self.engine = engine\n    self.prefill_lengths = sorted(prefill_lengths)\n    self.max_prefill_length = self.prefill_lengths[-1]\n    self.batch_prefill_max_batch_size = batch_prefill_max_batch_size\n    self.rng = jax.random.PRNGKey(0) if rng is None else rng\n    if prefill_type == PrefillType.DEFAULT:\n      self._processor = PrefillProcessor(engine)\n    elif prefill_type == PrefillType.BATCH:\n      self._batch_processor = BatchedPrefillProcessor(\n          engine=engine,\n          max_batch_size=batch_prefill_max_batch_size,\n          auto_layout_supported=False,\n      )\n      # Keep fallback processor for edge cases\n      self._processor = PrefillProcessor(engine)\n    else:\n      raise ValueError(f\"Invalid prefill type: {prefill_type}\")\n\n  @functools.partial(jax.jit, static_argnums=(0), donate_argnames=(\"decode_state\",))\n  def _jitted_single_prefill(\n      self, params, tokens, slot, true_length, decode_state, rng\n  ) -> tuple[jax.Array, jax.Array, DecodeState, jax.Array] | tuple[jax.Array, jax.Array, DecodeState]:\n    \"\"\"Prefill a single input.\"\"\"\n    # pylint: disable=protected-access\n    first_token, decode_state = self._processor._process(\n        params,\n        tokens,\n        slot,\n        true_length,\n        decode_state,\n        rng,\n        return_prompt_logp=True,\n    )\n    # ResultTokens, decode_state, prompt_logp\n    return (\n        first_token,\n        decode_state,\n        decode_state[\"prompt_logp\"],\n    )\n\n  def process(\n      self,\n      model_params: Params,\n      decode_state: DecodeState,\n      decode_slot: int,\n      input_id: int,\n      input_tokens_padded: jax.Array,\n      input_true_length: int,\n      prefill_done: Callable,\n  ) -> None:\n    \"\"\"Process an input through the appropriate prefill processor.\n\n    Args:\n        model_params: Model parameters for inference\n        decode_state: Current decode state\n        decode_slot: The decode slot index to use for this input\n        input_id: Unique identifier for this input\n        input_tokens_padded: Padded token array for the input\n        input_true_length: Actual length of the input before padding\n        prefill_done: Callback function called when prefill completes\n    \"\"\"\n    padded_length = len(input_tokens_padded)\n    # Use default processor if configured or if input is already at max length\n    if self._type == PrefillType.DEFAULT or padded_length == self.max_prefill_length:\n      first_token, decode_state, prompt_logp = self._jitted_single_prefill(\n          model_params,\n          input_tokens_padded,\n          decode_slot,\n          input_true_length,\n          decode_state,\n          self.rng,\n      )\n      prefill_done(\n          [PrefillResult(first_token, decode_slot, prompt_logp)],\n          [input_id],\n          decode_state,\n      )\n    # Use batch processor for inputs that can benefit from prefill packing\n    elif self._type == PrefillType.BATCH:\n      self._batch_processor.process(\n          model_params,\n          decode_state,\n          decode_slot,\n          input_id,\n          input_tokens_padded[:input_true_length],\n          padded_length,\n          self.max_prefill_length,\n          prefill_done,\n          return_prompt_logp=True,\n      )\n\n  def finalize(\n      self,\n      model_params: Params,\n      decode_state: DecodeState,\n      prefill_done: Callable,\n  ) -> None:\n    \"\"\"Finalize prefill operations, flushing any pending inputs.\n\n    Args:\n        model_params: Model parameters for inference\n        decode_state: Current decoder state\n        prefill_done: Callback function called when prefill completes\n    \"\"\"\n    if self._type == PrefillType.DEFAULT:\n      # No finalization needed for default processor\n      pass\n    elif self._type == PrefillType.BATCH:\n      # Flush any remaining inputs in the batch processor\n      self._batch_processor.flush(model_params, decode_state, prefill_done, return_prompt_logp=True)",
        "analysis": {
            "module_type": "prefill_helper",
            "purpose": "Provides a unified interface to abstract away different prefill processing strategies, specifically handling both default (single-sequence) and batch (packed multi-sequence) methods.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes either a `PrefillProcessor` for single sequences or a `BatchedPrefillProcessor` for packed sequences based on the `prefill_type`.",
                "The `process` method dispatches an input sequence to the appropriate processor.",
                "The `finalize` method flushes any pending inputs from the batched processor."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "MaxEngine",
                "PrefillProcessor",
                "BatchedPrefillProcessor",
                "PrefillType",
                "jax"
            ],
            "parameters": {
                "prefill_type": "The type of prefill processor to use, either 'default' or 'batch'.",
                "engine": "The MaxEngine instance used for prefill operations.",
                "prefill_lengths": "A list of supported prompt lengths for padding and bucketing.",
                "batch_prefill_max_batch_size": "The maximum number of prompts to pack into a single sequence for batch prefill."
            },
            "notes": [
                "This class acts as a dispatcher or strategy pattern implementation for prefill operations.",
                "In 'batch' mode, it maintains a fallback `PrefillProcessor` for edge cases, such as when an input sequence is already at the maximum prefill length."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the PrefillHelper by selecting and creating the appropriate prefill processor based on the `prefill_type`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters like `prefill_type`, `engine`, and `prefill_lengths`.",
                        "Determine the maximum prefill length from the provided list.",
                        "Initialize a JAX random number generator key.",
                        "If `prefill_type` is `DEFAULT`, instantiate `PrefillProcessor`.",
                        "If `prefill_type` is `BATCH`, instantiate `BatchedPrefillProcessor` and a fallback `PrefillProcessor`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "PrefillType",
                        "MaxEngine",
                        "PrefillProcessor",
                        "BatchedPrefillProcessor",
                        "jax.random.PRNGKey"
                    ],
                    "notes": [
                        "Raises a ValueError if an invalid `prefill_type` is provided."
                    ]
                },
                "_jitted_single_prefill": {
                    "purpose": "A JIT-compiled internal method to perform a prefill operation for a single input sequence.",
                    "input": {
                        "shape": "params: PyTree, tokens: [padded_length], slot: scalar, true_length: scalar, decode_state: PyTree, rng: PRNGKey",
                        "dtype": "tokens: int32"
                    },
                    "processing_steps": [
                        "Calls the `_process` method of the internal `PrefillProcessor` instance.",
                        "Extracts the first generated token, the updated decode state, and the prompt log probabilities.",
                        "Returns the extracted values as a tuple."
                    ],
                    "output": {
                        "shape": "A tuple of (first_token, updated_decode_state, prompt_logp)."
                    },
                    "dependencies": [
                        "jax.jit",
                        "PrefillProcessor._process"
                    ],
                    "notes": [
                        "This method is decorated with `jax.jit` for performance.",
                        "The `decode_state` is donated, meaning it can be modified in-place for efficiency."
                    ]
                },
                "process": {
                    "purpose": "Processes a single input by dispatching it to the appropriate prefill processor (single or batched).",
                    "input": {
                        "shape": "model_params: PyTree, decode_state: PyTree, decode_slot: scalar, input_id: scalar, input_tokens_padded: [padded_length], input_true_length: scalar, prefill_done: Callable",
                        "dtype": "input_tokens_padded: int32"
                    },
                    "processing_steps": [
                        "Check if the prefill type is `DEFAULT` or if the input length equals the maximum prefill length.",
                        "If true, call `_jitted_single_prefill` to process the input immediately and invoke the `prefill_done` callback with the result.",
                        "If false (and type is `BATCH`), call the `_batch_processor.process` method to add the input to a batch for later processing."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "_jitted_single_prefill",
                        "BatchedPrefillProcessor.process",
                        "PrefillResult"
                    ],
                    "notes": [
                        "This method returns `None`. The results of the prefill operation are communicated asynchronously via the `prefill_done` callback."
                    ]
                },
                "finalize": {
                    "purpose": "Finalizes prefill operations by flushing any pending inputs from the batched processor.",
                    "input": {
                        "shape": "model_params: PyTree, decode_state: PyTree, prefill_done: Callable",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if the prefill type is `BATCH`.",
                        "If so, call the `flush` method on the `_batch_processor` to process any remaining sequences in its internal buffers."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "BatchedPrefillProcessor.flush"
                    ],
                    "notes": [
                        "This method is a no-op if the prefill type is `DEFAULT`.",
                        "It is crucial to call this at the end of an inference run to ensure all submitted inputs are processed."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#InferenceWorker",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class InferenceWorker:\n  \"\"\"\n  InferenceWorker runs continuous batching over\n       a queue of inputs.\n\n      Continuous batching workflow:\n    1. Process inputs one at a time from queue\n    2. Prefill input and insert into KV cache\n    3. Continue prefilling until enough samples for batch decode\n    4. Decode until at least one sequence completes\n    5. Refill newly available decode slots with prefill\n    6. Repeat until all sequences complete\n\n    Prefill Packing:\n        When enable_batch_prefill is True, the prefill processor\n        will pack multiple inputs into a single sequence before\n        doing the prefill.\n\n        There are multiple buckets for packed sequences, where each bucket\n        contains inputs with the same padded length. Only inputs with the same\n        padded length can be packed together.\n\n        It is important to sort the inputs by padded length so that the\n        buckets fill up quickly.\n\n        When a decode slot frees up, the prefill processor will add the\n        sequence to a bucket. If the bucket becomes full, the packed sequence\n        will be prefilled.\n\n        E.g.\n        Bucket for length 64: [...seq1, ...seq2, ...seq3, ...seq4]\n        Bucket for length 128: [...seq1, ...seq2]\n        Bucket for length 256: [...seq1]\n\n  \"\"\"\n\n  def __init__(\n      self,\n      config: MaxTextConfig,\n      params: Params | None,\n      min_decode_steps: int,\n      enable_batch_prefill: bool,\n      devices: list[Any],\n      tokenizer: Any,\n      eos_ids: list[int],\n      prefill_lengths: list[int],\n      max_decode_length: int,\n      batch_prefill_max_batch_size: int,\n      is_pw_reshard: bool = True,\n      rng: jax.random.PRNGKey = None,\n      mesh: Mesh = None,\n      debug: bool = False,\n  ):\n    \"\"\"\n    Args:\n        config: MaxText configuration\n        params: Model parameters, if None, the params will be loaded from the config\n        min_decode_steps: Minimum number of decode steps to run at once\n        enable_batch_prefill: Whether to enable batch prefill\n        devices: JAX devices to use for this worker\n        tokenizer: Tokenizer to use\n        eos_ids: End-of-sequence token IDs\n        prefill_lengths: list of supported prefill lengths\n        max_decode_length: Maximum tokens to generate per sequence\n        batch_prefill_max_batch_size: Maximum batch size for batch prefill\n        run_as_a_thread: Whether to run in a separate thread\n        rng: Random number generator key\n        mesh: JAX mesh for distributed computation\n        is_pw_reshard: Whether to use Pathways for resharding\n    \"\"\"\n    # Configurations\n    self.config = config\n    self.params = params\n    self.devices = devices\n    self.is_pw_reshard = is_pw_reshard\n    self.enable_batch_prefill = enable_batch_prefill\n    self.prefill_type = PrefillType.BATCH if enable_batch_prefill else PrefillType.DEFAULT\n    self.prefill_lengths = prefill_lengths\n    self.max_prefill_length = self.prefill_lengths[-1]\n    self.max_decode_length = max_decode_length\n    self.eos_ids = eos_ids\n    self.tokenizer = tokenizer\n    self.batch_prefill_max_batch_size = batch_prefill_max_batch_size\n    self.min_decode_steps = min_decode_steps\n    self.mesh = mesh\n    self.rng = jax.random.PRNGKey(0) if rng is None else rng\n    self.debug = debug\n\n    # Inference state (initialized later)\n    self.running = False\n    self.generated_token_backlog = queue.Queue()\n    self.empty_decode_slots: list[int] = []\n    self.slot_to_id: dict[int, None | int] = {}\n    self.decode_state: DecodeState = None\n    self.completion_tokens_by_id: dict[Hashable, list[TokenOutput]] = {}\n    self.prompt_logprobs_by_id: dict[Hashable, list[np.ndarray]] = {}\n    self.true_lengths: dict[Hashable, int] = {}\n    # Model components (initialized later)\n    self.engine = None\n    self.decode_batch_size = None\n    self.prefill_helper = None\n    self.generate_fn = None\n\n    start_time = time.time()\n    # Initialize MaxEngine(s)\n    self.params, self.engine = self._init_engine(self.params)\n    self.tokenizer = self._init_tokenizer()\n    self.decode_batch_size = self.engine.max_concurrent_decodes\n\n    # Initialize prefill helper\n    self.prefill_helper = PrefillHelper(\n        self.prefill_type,\n        self.engine,\n        self.prefill_lengths,\n        self.batch_prefill_max_batch_size,\n        rng=self.rng,\n    )\n\n    # Initialize decode state\n    start_time_decode_state = time.time()\n    self.generate_fn = self.engine.generate\n    self.decode_state = self.engine.init_decode_state(self.rng)\n\n    if self.debug:\n      max_logging.log(f\"time taken to initialize decode_state: {time.time() - start_time_decode_state} seconds\")\n    max_logging.log(f\"Initialized Inference worker in {time.time() - start_time} seconds\")\n\n  def _init_engine(self, params):\n    \"\"\"Initialize the MaxEngine.\n\n    Args:\n        params: Model parameters\n\n    Returns:\n        tuple of (params, engine)\n    \"\"\"\n    start_time = time.time()\n    engine = MaxEngine(self.config, self.devices)\n    params = engine.load_params(params=params, rng=self.rng)\n    max_logging.log(f\"Time taken to initialize engine: {time.time() - start_time} seconds\")\n    return params, engine\n\n  def _init_tokenizer(self):\n    \"\"\"Initialize the tokenizer.\n\n    Returns:\n        Initialized tokenizer\n    \"\"\"\n    if self.eos_ids is None and self.tokenizer is None:\n      tokenizer_params = self.engine.get_tokenizer()\n      self.tokenizer = self.engine.build_tokenizer(tokenizer_params)\n    if self.eos_ids is None:\n      self.eos_ids = [self.tokenizer.eos_id]\n    return self.tokenizer\n\n  def update_params(\n      self,\n      params: Params,\n      destination_sharding: jax.sharding.NamedSharding,\n      is_pw_reshard: bool,\n  ):\n    \"\"\"Update the model parameters\"\"\"\n    if is_pw_reshard:\n      with (\n          jax.transfer_guard_device_to_host(\"disallow_explicit\"),\n          jax.transfer_guard_host_to_device(\"disallow_explicit\"),\n      ):\n        self.params = pathways_reshard.reshard(params, destination_sharding, cache_resharding_plans=True)\n    else:\n      self.params = jax.device_put(params, destination_sharding)\n\n  def run_inference(self, data: list[InputData], rng=None):\n    \"\"\"Start the inference process.\n\n    Args:\n        data: list of InputData objects containing input sequences\n        rng: Random number generator key. If None, the previous key will be used.\n    \"\"\"\n\n    # Reset rng\n    if rng is not None:\n      self.rng = rng\n\n    # Reset state for new inference run\n    self.completion_tokens_by_id = defaultdict(list)\n    self.prompt_logprobs_by_id = defaultdict(list)\n    self.empty_decode_slots = list(range(self.decode_batch_size))\n    self.slot_to_id = {}\n    self.running = True\n    self.true_lengths = {input.id: input.true_length for input in data}\n\n    max_logging.log(\"Continuous batching started\")\n\n    self._run_continous_batching(data)\n\n    return self._build_final_outputs(data)\n\n  def _run_continous_batching(\n      self,\n      data: list[InputData],\n  ):\n    \"\"\"Run inference on a batch of inputs.\n\n    Args:\n        data: list of InputData objects containing input sequences\n    \"\"\"\n\n    # Start token emission thread\n    token_emission_thread = SafeThread(\n        target=functools.partial(\n            self.background_token_emission,\n        ),\n        name=\"token_emission\",\n    )\n    token_emission_thread.start()\n\n    # Process each input\n    for row in data:\n      # 1. Wait for an empty slot\n      while not self.empty_decode_slots:\n        self.decode()\n\n      # 2. Get an available slot\n      slot = self.empty_decode_slots.pop()\n      # 3. Prefill and insert kv cache\n      self.prefill_helper.process(\n          model_params=self.params,\n          decode_state=self.decode_state,\n          decode_slot=slot,\n          input_id=int(row.id),\n          input_tokens_padded=row.tokens,\n          input_true_length=row.true_length,\n          prefill_done=self.prefill_done,\n      )\n\n    # 4. Flush any pending inputs in batch prefill mode\n    self.prefill_helper.finalize(self.params, self.decode_state, self.prefill_done)\n\n    # 5. Continue decoding until all sequences are complete\n    while not all(value is None for value in self.slot_to_id.values()):\n      self.decode()\n\n    # Wait for detokenization to complete\n    self.running = False\n    max_logging.log(\n        f\"Inference worker: joining token emission thread. \"\n        f\"There are {self.generated_token_backlog.qsize()} elements in the backlog\"\n    )\n    start_time = time.time()\n    with jax.profiler.TraceAnnotation(\"Flushing token emission thread\"):\n      token_emission_thread.join()\n    max_logging.log(f\"Inference worker: token emission thread joined in {time.time() - start_time} seconds\")\n\n  def _build_final_outputs(self, input_data: list[InputData]) -> list[CompletionOutput]:\n    \"\"\"Build the final list of CompletionOutput.\"\"\"\n\n    with jax.profiler.TraceAnnotation(\"offline_engine.batch_inference.return_final_output\"):\n      completion_outputs = []\n      for row in input_data:\n        input_id = row.id\n        prompt_length = row.true_length\n        prompt_tokens = row.tokens[: row.true_length].squeeze()\n        completion_tokens = np.array(\n            [token_output.token for token_output in self.completion_tokens_by_id[input_id]]\n        ).flatten()\n        logprobs = np.array(\n            [token_output.log_prob.flatten() for token_output in self.completion_tokens_by_id[input_id]]\n        ).flatten()\n        if isinstance(self.prompt_logprobs_by_id[input_id], np.ndarray):\n          prompt_logprobs = cast(np.ndarray, self.prompt_logprobs_by_id[input_id]).flatten()\n        else:\n          prompt_logprobs = self.prompt_logprobs_by_id[input_id]\n        completion_outputs.append(\n            CompletionOutput(\n                index=str(input_id),\n                prompt_length=prompt_length,\n                token_ids=np.concatenate(\n                    (\n                        prompt_tokens,\n                        completion_tokens,\n                    )\n                ),\n                logprobs=np.concatenate(\n                    (\n                        prompt_logprobs,\n                        logprobs,\n                    )\n                ),\n            )\n        )\n    return completion_outputs\n\n  def prefill_done(self, prefill_result: list[PrefillResult], prompt_ids: list[int], decode_state: DecodeState):\n    \"\"\"Callback function called when prefill completes.\n    This function adds the prefill tokens to the detokenization queue,\n    which manages the token emission and decode slot evictions.\n\n    Args:\n        prefill_result: list of (token, slot) tuples\n        prompt_ids: list of prompt IDs\n        decode_state: Updated decode state\n    \"\"\"\n    # Update decode state\n    self.decode_state = decode_state\n    # Process each prefill result\n    for i, result in enumerate(prefill_result):\n      input_id = prompt_ids[i]\n      result_tokens = result.result_tokens\n      slot = result.slot\n      prompt_logp = result.prompt_logp\n      true_length = self.true_lengths[input_id]\n\n      self.slot_to_id[slot] = input_id\n\n      # Add token to detokenization queue\n      start_time = time.time()\n\n      with jax.profiler.TraceAnnotation(\"convert_to_numpy\"):\n        first_token = np.array(result_tokens.data[:, 0])\n        log_prob = np.array(result_tokens.log_prob)\n        prompt_logp = np.array(prompt_logp)[:, :true_length]\n\n      if self.debug:\n        max_logging.log(f\"Inference worker: convert to numpy in Prefill in {time.time() - start_time} seconds\")\n      self.generated_token_backlog.put_nowait((first_token, log_prob, True, prompt_ids[i], slot, prompt_logp))\n\n  def decode(self):\n    \"\"\"Run decode steps on current decoder state.\n\n    Performs `self.min_decode_steps` decode operations\n    and puts results in the detokenization queue.\n    \"\"\"\n\n    buffer = []\n    for _ in range(self.min_decode_steps):\n      # Generate next tokens\n      self.decode_state, result_tokens, log_prob = self._jitted_generate_fn(self.params, self.decode_state, self.rng)\n      # Add token to detokenization queue\n      start_time = time.time()\n      with jax.profiler.TraceAnnotation(\"convert_to_numpy\"):\n        result_tokens = np.array(result_tokens)\n        log_prob = np.array(log_prob)\n\n      if self.debug:\n        max_logging.log(f\"Inference worker: convert to numpy \" f\"in Decode in {time.time() - start_time} seconds\")\n\n      buffer.append((result_tokens, log_prob))\n\n    # Add results to detokenization queue\n    self.generated_token_backlog.put_nowait(\n        (\n            [result_token for result_token, _ in buffer],\n            [log_prob for _, log_prob in buffer],\n            False,\n            0,\n            0,\n            None,\n        )\n    )\n\n  @functools.partial(jax.jit, static_argnums=(0,), donate_argnums=(2,))\n  def _jitted_generate_fn(self, params, decode_state, rng):\n    decode_state, result_tokens = self.engine.generate(params, decode_state, rng=rng)\n    return decode_state, result_tokens.data[:, 0], result_tokens.log_prob\n\n  def background_token_emission(self):\n    \"\"\"Emit tokens and manage decode slots.\n\n    Runs in a background thread to process tokens from\n    the backlog, emit tokens, and free up\n    decode slots when sequences complete.\n    \"\"\"\n    max_logging.log(\"Inference worker: starting background token emission thread\")\n    while self.running or not self.generated_token_backlog.empty():\n      newly_empty = []\n\n      # Get next item from queue with timeout\n      try:\n        result_tokens, log_prob, is_first_token, row_id, slot, prompt_logp = self.generated_token_backlog.get(\n            timeout=0.01\n        )\n      except queue.Empty:\n        if not self.running:\n          break\n        continue\n\n      # Process generated tokens\n      start_time = time.time()\n      if is_first_token:\n        should_terminate = self.emit_token(row_id, int(result_tokens), log_prob, prompt_logp=prompt_logp)\n        if should_terminate:\n          newly_empty.append(slot)\n      else:\n        for decode_step in range(self.min_decode_steps):\n          for slot, id_ in self.slot_to_id.items():\n            if id_ is None:\n              continue\n            log_prob_at_slot = log_prob[decode_step][slot]\n            result_tokens_at_slot = result_tokens[decode_step][slot]\n            should_terminate = self.emit_token(id_, int(result_tokens_at_slot), log_prob_at_slot)\n            if should_terminate:\n              newly_empty.append(slot)\n\n      # Update decode slots\n      for slot in newly_empty:\n        self.slot_to_id[slot] = None\n        if slot not in self.empty_decode_slots:\n          self.empty_decode_slots.append(slot)\n      end_time = time.time()\n      if self.debug:\n        max_logging.log(f\"Inference worker: token emission in {end_time - start_time} seconds\")\n\n  def emit_token(\n      self,\n      prompt_id,\n      result_token: int,\n      log_prob: float,\n      prompt_logp: None | np.ndarray = None,\n  ):\n    \"\"\"Adds the token to the results for the specified prompt ID and\n    determines if generation should terminate.\n\n    Args:\n        prompt_id: ID of the prompt\n        token: Token to emit\n\n    Returns:\n        True if this token signals the end of generation, False otherwise\n    \"\"\"\n    # Return if already reached max decode length\n    if len(self.completion_tokens_by_id[prompt_id]) == self.max_decode_length:\n      return True\n\n    # Return if already reached eos\n    if (\n        len(self.completion_tokens_by_id[prompt_id]) > 0\n        and self.completion_tokens_by_id[prompt_id][-1].token in self.eos_ids\n    ):\n      return True\n\n    index = len(self.completion_tokens_by_id[prompt_id])\n    if prompt_logp is not None:\n      self.prompt_logprobs_by_id[prompt_id] = [cast(np.ndarray, prompt_logp)]\n    self.completion_tokens_by_id[prompt_id].append(TokenOutput(np.array(result_token), np.array(log_prob)))\n    return (result_token in self.eos_ids) or (index + 1 == self.max_decode_length)",
        "analysis": {
            "module_type": "inference_worker",
            "purpose": "Manages and executes a continuous batching inference workflow for language models, handling prefill, decoding, and token emission.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes a MaxEngine, tokenizer, and prefill helper.",
                "Receives a list of `InputData` objects via the `run_inference` method.",
                "Runs a continuous batching loop (`_run_continous_batching`) that manages prefill and decode stages.",
                "Uses a background thread (`background_token_emission`) to process generated tokens and manage decode slots.",
                "Builds and returns a list of `CompletionOutput` objects."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "MaxEngine",
                "PrefillHelper",
                "SafeThread",
                "InputData",
                "CompletionOutput",
                "PrefillResult",
                "TokenOutput",
                "jax",
                "numpy",
                "queue"
            ],
            "parameters": {
                "config": "MaxText configuration object.",
                "min_decode_steps": "Minimum number of decode steps to run at once before checking for completion.",
                "enable_batch_prefill": "A boolean flag to enable or disable packing multiple inputs into a single prefill sequence.",
                "max_decode_length": "Maximum number of tokens to generate per sequence.",
                "prefill_lengths": "A list of supported padded lengths for input sequences."
            },
            "notes": [
                "The core logic follows a continuous batching workflow: prefill new inputs into available KV cache slots, run batched decoding steps, and refill slots as sequences complete.",
                "When `enable_batch_prefill` is true, it uses a prefill packing strategy to group inputs of similar lengths into buckets to improve efficiency.",
                "A background thread is used for token emission to decouple it from the JAX-based generation loop, allowing for concurrent processing."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the InferenceWorker, setting up the configuration, model engine, tokenizer, prefill helper, and initial decode state.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters from arguments.",
                        "Call `_init_engine` to initialize the MaxEngine and load parameters.",
                        "Call `_init_tokenizer` to set up the tokenizer and EOS IDs.",
                        "Initialize the `PrefillHelper` for managing prefill operations.",
                        "Initialize the `decode_state` using `engine.init_decode_state`."
                    ],
                    "output": {
                        "shape": "An initialized InferenceWorker instance."
                    },
                    "dependencies": [
                        "MaxEngine",
                        "PrefillHelper",
                        "PrefillType",
                        "jax.random.PRNGKey"
                    ],
                    "notes": [
                        "This method sets up all necessary components for running the inference loop."
                    ]
                },
                "_init_engine": {
                    "purpose": "Initializes the MaxEngine and loads model parameters.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Instantiate `MaxEngine` with the provided config and devices.",
                        "Call `engine.load_params` to load the model weights."
                    ],
                    "output": {
                        "shape": "A tuple containing the loaded parameters and the MaxEngine instance."
                    },
                    "dependencies": [
                        "MaxEngine"
                    ],
                    "notes": [
                        "This is a private helper method called during initialization."
                    ]
                },
                "_init_tokenizer": {
                    "purpose": "Initializes the tokenizer if one is not provided during construction.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "If tokenizer is None, get tokenizer parameters from the engine and build it.",
                        "If `eos_ids` are None, get the EOS ID from the tokenizer."
                    ],
                    "output": {
                        "shape": "The initialized tokenizer instance."
                    },
                    "dependencies": [
                        "MaxEngine"
                    ],
                    "notes": [
                        "Ensures that a valid tokenizer and EOS IDs are available for the worker."
                    ]
                },
                "update_params": {
                    "purpose": "Updates the model parameters, resharding them across devices if necessary.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "If `is_pw_reshard` is true, use `pathways_reshard.reshard`.",
                        "Otherwise, use `jax.device_put`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "pathways_reshard",
                        "jax.device_put",
                        "jax.sharding.NamedSharding"
                    ],
                    "notes": [
                        "This method allows for hot-swapping model weights during runtime."
                    ]
                },
                "run_inference": {
                    "purpose": "Starts and manages the entire continuous batching inference process for a list of inputs.",
                    "input": {
                        "shape": "A list of `InputData` objects.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Reset internal state for the new inference run (e.g., completion tokens, empty slots).",
                        "Call `_run_continous_batching` to execute the main inference loop.",
                        "Call `_build_final_outputs` to format and return the results."
                    ],
                    "output": {
                        "shape": "A list of `CompletionOutput` objects."
                    },
                    "dependencies": [
                        "_run_continous_batching",
                        "_build_final_outputs",
                        "InputData",
                        "CompletionOutput"
                    ],
                    "notes": [
                        "This is the main public entry point for running inference with the worker."
                    ]
                },
                "_run_continous_batching": {
                    "purpose": "Implements the core continuous batching loop, managing prefill and decode stages.",
                    "input": {
                        "shape": "A list of `InputData` objects.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Start the `background_token_emission` thread.",
                        "Iterate through each input, waiting for an empty decode slot.",
                        "Call `prefill_helper.process` for each input to fill the KV cache.",
                        "Call `prefill_helper.finalize` to flush any pending packed prefills.",
                        "Continuously call `decode` until all sequences are complete.",
                        "Stop and join the background thread."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "SafeThread",
                        "PrefillHelper",
                        "decode",
                        "prefill_done"
                    ],
                    "notes": [
                        "This private method contains the primary logic for orchestrating the inference process."
                    ]
                },
                "_build_final_outputs": {
                    "purpose": "Constructs the final list of `CompletionOutput` objects from the internal state after inference is complete.",
                    "input": {
                        "shape": "A list of `InputData` objects.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Iterate through the original input data.",
                        "Retrieve the generated tokens and log probabilities for each input ID.",
                        "Concatenate prompt and completion tokens/logprobs.",
                        "Create and append a `CompletionOutput` object for each input."
                    ],
                    "output": {
                        "shape": "A list of `CompletionOutput` objects."
                    },
                    "dependencies": [
                        "CompletionOutput",
                        "InputData",
                        "numpy"
                    ],
                    "notes": [
                        "This is a private helper for formatting the final results."
                    ]
                },
                "prefill_done": {
                    "purpose": "A callback function that processes the results of a completed prefill operation and queues the first generated token.",
                    "input": {
                        "shape": "list[PrefillResult], list[int], DecodeState",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Update the worker's `decode_state`.",
                        "For each prefill result, associate the decode slot with the prompt ID.",
                        "Convert JAX array results to NumPy arrays.",
                        "Put the first token, log probability, and metadata into the `generated_token_backlog` queue."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "PrefillResult",
                        "numpy",
                        "queue.Queue"
                    ],
                    "notes": [
                        "This function acts as a bridge between the JAX-based prefill computation and the Python-based token emission thread."
                    ]
                },
                "decode": {
                    "purpose": "Runs a fixed number of autoregressive decoding steps for all active sequences in the batch.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Loop `self.min_decode_steps` times.",
                        "Call the JIT-compiled `_jitted_generate_fn` in each iteration.",
                        "Buffer the generated tokens and log probabilities.",
                        "Put the entire buffer of results for all steps into the `generated_token_backlog` queue."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "_jitted_generate_fn",
                        "numpy",
                        "queue.Queue"
                    ],
                    "notes": [
                        "Performs batched decoding for a chunk of steps to improve efficiency."
                    ]
                },
                "_jitted_generate_fn": {
                    "purpose": "A JIT-compiled function to perform a single token generation step for the entire batch.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `self.engine.generate` with the current parameters and decode state."
                    ],
                    "output": {
                        "shape": "A tuple of (updated_decode_state, result_tokens, log_probs)."
                    },
                    "dependencies": [
                        "MaxEngine.generate",
                        "jax.jit"
                    ],
                    "notes": [
                        "Decorated with `jax.jit` for performance. `donate_argnums` is used to allow JAX to modify the decode state in-place, saving memory."
                    ]
                },
                "background_token_emission": {
                    "purpose": "Runs in a background thread to process generated tokens from a queue, manage sequence completion, and free up decode slots.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Loop while the worker is running or the backlog queue is not empty.",
                        "Get a result from the `generated_token_backlog` queue.",
                        "Call `emit_token` for each generated token.",
                        "If `emit_token` indicates a sequence has finished, mark its decode slot as empty.",
                        "Update the list of available `empty_decode_slots`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "queue.Queue",
                        "emit_token"
                    ],
                    "notes": [
                        "This decouples the Python-based state management (like checking for EOS) from the high-performance JAX decoding loop."
                    ]
                },
                "emit_token": {
                    "purpose": "Appends a generated token to the results for a specific prompt and checks for termination conditions.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "int, float, numpy.ndarray"
                    },
                    "processing_steps": [
                        "Check if max decode length has been reached or if an EOS token was previously generated.",
                        "Append the new token and its log probability to the results for the given `prompt_id`.",
                        "Check if the new token is an EOS token or if the max decode length is now met."
                    ],
                    "output": {
                        "shape": "A boolean indicating if the sequence should terminate."
                    },
                    "dependencies": [
                        "TokenOutput",
                        "numpy"
                    ],
                    "notes": [
                        "This method manages the output for a single sequence and determines when it is complete."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#OfflineEngine",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class OfflineEngine:\n  \"\"\"Class for handling offline inference on batches of inputs.\"\"\"\n\n  def __init__(\n      self,\n      config: Any,\n      params: None | Params = None,\n      enable_batch_prefill: bool = False,\n      min_decode_steps: int = 10,\n      tokenizer: Any = None,\n      eos_ids: list[int] | None = None,\n      prefill_lengths: list[int] | str = \"auto\",\n      batch_prefill_max_batch_size: int = 16,\n      mesh: Mesh = None,\n      rng: jax.random.PRNGKey = None,\n      debug: bool = False,\n  ):\n    \"\"\"Initialize the OfflineEngine.\n\n    Args:\n        config: The MaxText config object which will be used to\n          create MaxEngine instance(s).\n        params: Model parameters (loaded from engine if None)\n        enable_batch_prefill: Whether to use prefill packing.\n            config.scan_layers must be False if this is True\n        min_decode_steps: Number of decode steps to perform at a time,\n            before checking for completion.\n        eos_ids: list of EOS token IDs for checking sequence completion.\n          If None, the tokenizer's EOS token will be used.\n        tokenizer: Tokenizer instance for encoding/decoding text. If None,\n          will be created using the config if eos_ids is not provided.\n        prefill_lengths: list of expected prefill lengths, or \"auto\" to\n            automatically determine appropriate lengths from the engine\n            config. Input sequences will be padded to the nearest length\n            in this list.\n        batch_prefill_max_batch_size: Maximum number of inputs to pack\n          into a single prefill. This is only used when enable_batch_prefill\n          is True.\n        mesh: JAX Mesh object. Use this\n          argument if you want to use only some of the devices for OfflineEngine and\n          reserve the rest for other tasks. If None, OfflineEngine will create the mesh\n          automatically.\n        rng: Random number generator key. If None, a new key will be created.\n    \"\"\"\n    max_logging.log(\"Initializing OfflineEngine\")\n    # Configurations\n    self.config = config\n    self.params = params\n    self.min_decode_steps = min_decode_steps\n    self.enable_batch_prefill = enable_batch_prefill\n    self.mesh = mesh\n    self.tokenizer = tokenizer\n    self.eos_ids = eos_ids\n    self.prefill_lengths = prefill_lengths\n    self.batch_prefill_max_batch_size = batch_prefill_max_batch_size\n    self.max_prefill_length = self.config.max_prefill_predict_length\n    self.max_decode_length = self.config.max_target_length - self.max_prefill_length\n    self.rng = jax.random.PRNGKey(0) if rng is None else rng\n    self.debug = debug\n    self._validate_config()\n\n    # Create prefill buckets: [0, 64], (64, 128], (128, 256], ..., [max_length//2, max_length]\n    if prefill_lengths == \"auto\":\n      self.prefill_lengths = [2**i for i in range(6, max(6, (self.max_prefill_length - 1).bit_length()) + 1)]\n    else:\n      self.prefill_lengths = sorted(prefill_lengths)\n\n    # Create meshes\n    if not self.mesh:\n      self.mesh = OfflineEngine.create_mesh(jax.devices(), self.config)\n\n    self.worker = InferenceWorker(\n        config=self.config,\n        params=self.params,\n        min_decode_steps=self.min_decode_steps,\n        enable_batch_prefill=self.enable_batch_prefill,\n        mesh=self.mesh,\n        devices=self.mesh.devices.flatten(),\n        tokenizer=self.tokenizer,\n        eos_ids=self.eos_ids,\n        prefill_lengths=self.prefill_lengths,\n        max_decode_length=self.max_decode_length,\n        batch_prefill_max_batch_size=self.batch_prefill_max_batch_size,\n        rng=self.rng,\n        debug=self.debug,\n    )\n\n    self.tokenizer = self.worker.tokenizer\n\n  def update_params(\n      self,\n      params: Params,\n      parition_spec: PartitionSpec,\n      is_pw_reshard: bool,\n  ):\n    \"\"\"Update model weights.\"\"\"\n    self.worker.update_params(\n        params,\n        jax.tree_util.tree_map(\n            lambda ps: jax.sharding.NamedSharding(self.mesh, ps),\n            parition_spec,\n        ),\n        is_pw_reshard,\n    )\n\n  def batch_inference(\n      self,\n      data: list[InputData] | list[jax.Array] | list[np.ndarray],\n      desc: str = \"\",\n      rng=None,\n  ) -> list[CompletionOutput]:\n    \"\"\"Run inference on a batch of inputs.\n\n    Args:\n        data: list of InputData objects, or JAX or numpy arrays.\n            If input is JAX or numpy array, it must not contain padding tokens.\n        desc: Description string for logging\n        rng: Random number generator key. If None, the previous key will be used.\n\n    Returns:\n        list of CompletionOutput objects, one for each input in data\n    \"\"\"\n    data = self.prepare_data(data)\n\n    return self.worker.run_inference(data, rng)\n\n  def prepare_data(self, data: list[InputData | jax.Array | np.ndarray]) -> list[InputData]:\n    \"\"\"Pad and if batch prefill is enabled, sort data by length.\n\n    Args:\n        data: list of InputData objects, or JAX or numpy arrays\n\n    Returns:\n        list of prepared InputData objects\n    \"\"\"\n    # Convert JAX arrays to numpy arrays\n    if isinstance(data[0], jax.Array):\n      data = [np.array(array) for array in data]\n\n    # Convert numpy arrays to InputData objects\n    if isinstance(data[0], np.ndarray):\n      max_logging.log(\n          \"When you provide JAX/numpy arrays to Offline Engine, \"\n          \"make sure that the arrays are not padded with padding tokens.\"\n      )\n      data = [InputData(id=str(i), tokens=array, true_length=len(array)) for i, array in enumerate(data)]\n\n    # Make sure all data id is unique\n    if len(data) != len({item.id for item in data}):\n      raise ValueError(\"All data ids must be unique\")\n\n    data = self.pad_data(data)\n\n    if self.enable_batch_prefill:\n      return sorted(data, key=lambda x: x.tokens.shape[0])\n\n    return data\n\n  def pad_data(self, data: list[InputData]) -> list[InputData]:\n    \"\"\"For each input, pad it to the next length in self.prefill_lengths\n    that is greater than or equal to its true length.\n\n    Args:\n        data: list of InputData objects\n\n    Returns:\n        list of padded InputData objects\n    \"\"\"\n    padded_data = []\n\n    for item in data:\n      # Find the smallest prefill length that can accommodate this input\n      target_length = None\n      for length in self.prefill_lengths:\n        if length >= item.true_length:\n          target_length = length\n          break\n\n      # If no suitable length found, use the maximum prefill length\n      if target_length is None:\n        target_length = self.max_prefill_length\n\n      # Pad or truncate as needed\n      if len(item.tokens) < target_length:\n        # Pad with zeros\n        padded_tokens = np.zeros(target_length, dtype=item.tokens.dtype)\n        padded_tokens[: item.true_length] = item.tokens[: item.true_length]\n      else:\n        # Input is too long, truncate to max_prefill_length\n        padded_tokens = item.tokens[:target_length]\n\n      # Create new InputData with padded tokens\n      padded_data.append(InputData(id=item.id, tokens=padded_tokens, true_length=item.true_length))\n\n    return padded_data\n\n  @staticmethod\n  def create_mesh(devices, config):\n    \"\"\"Create data parallelism meshes for each Inference worker.\"\"\"\n    ici_parallelism = max_utils.fill_unspecified_mesh_axes(config.ici_parallelism.copy(), len(devices), \"ICI\")\n    devices_array = mesh_utils.create_device_mesh(\n        ici_parallelism,\n        devices,\n        contiguous_submeshes=False,\n        allow_split_physical_axes=config.allow_split_physical_axes or False,\n    )\n    mesh = Mesh(devices_array.reshape(ici_parallelism), config.mesh_axes)\n    return mesh\n\n  def _validate_config(self):\n    \"\"\"Validate configuration parameters and check for incompatible settings.\"\"\"\n    if not self.config.return_log_prob:\n      raise ValueError(\"return_log_prob must be True when using OfflineEngine\")\n    if self.enable_batch_prefill and self.config.scan_layers:\n      raise ValueError(\"scan_layers must be False if enable_batch_prefill is True\")\n\n    if self.max_decode_length <= 0:\n      raise ValueError(\"Make sure max_target_length - max_prefill_predict_length is greater than 0\")\n    if self.config.scan_layers:\n      max_logging.log(\n          \"WARNING: scan_layers=True will result in slow step time. \" \"It is recommended for debugging purposes only.\"\n      )",
        "analysis": {
            "module_type": "offline_inference_engine",
            "purpose": "A high-level class for handling offline inference on batches of inputs, managing resources and delegating the core logic to an InferenceWorker.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes configuration parameters such as `min_decode_steps`, `enable_batch_prefill`, etc.",
                "Validates the configuration for incompatible settings via `_validate_config`.",
                "Determines the prefill length buckets, either from the provided list or automatically.",
                "Creates a JAX device mesh using `create_mesh` if one is not provided.",
                "Instantiates an `InferenceWorker` which handles the detailed logic of continuous batching, prefilling, and decoding.",
                "Sets the engine's tokenizer to the one initialized by the worker."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "InferenceWorker",
                "jax",
                "numpy",
                "jax.sharding.Mesh",
                "InputData",
                "CompletionOutput"
            ],
            "parameters": {
                "config": "The MaxText config object used to create MaxEngine instances.",
                "params": "Model parameters. If None, they are loaded from the engine.",
                "enable_batch_prefill": "A boolean to enable or disable prefill packing for efficiency.",
                "min_decode_steps": "The number of decode steps to perform at a time before checking for sequence completion.",
                "prefill_lengths": "A list of integers representing prefill bucket sizes, or 'auto' to determine them automatically.",
                "mesh": "An optional pre-configured JAX Mesh object."
            },
            "notes": [
                "This class serves as a user-friendly facade for the more complex `InferenceWorker`.",
                "It handles data preparation, including padding and sorting, before passing it to the worker."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the OfflineEngine, setting up configuration, mesh, and the core InferenceWorker.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters (`config`, `params`, `min_decode_steps`, etc.).",
                        "Call `_validate_config` to check for incompatible settings.",
                        "Automatically determine prefill bucket lengths if `prefill_lengths` is 'auto'.",
                        "Call `create_mesh` to set up the JAX device mesh if not provided.",
                        "Instantiate `InferenceWorker` with the prepared configuration.",
                        "Set the engine's tokenizer to the one initialized by the worker."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "InferenceWorker",
                        "jax.random.PRNGKey",
                        "OfflineEngine.create_mesh",
                        "OfflineEngine._validate_config"
                    ],
                    "notes": [
                        "This method orchestrates the setup of all necessary components for running inference."
                    ]
                },
                "update_params": {
                    "purpose": "Updates the model weights in the underlying InferenceWorker.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Create a `jax.sharding.NamedSharding` object from the mesh and partition spec.",
                        "Call the `update_params` method of the `InferenceWorker` instance."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "InferenceWorker.update_params",
                        "jax.tree_util.tree_map",
                        "jax.sharding.NamedSharding"
                    ],
                    "notes": [
                        "This allows for hot-swapping model parameters without re-initializing the entire engine."
                    ]
                },
                "batch_inference": {
                    "purpose": "Runs inference on a batch of inputs and returns the generated completions.",
                    "input": {
                        "shape": "A list of 1D tensors, where each tensor has shape `[sequence_length]`.",
                        "dtype": "int (token IDs)"
                    },
                    "processing_steps": [
                        "Call `prepare_data` to convert all inputs to `InputData` objects, pad them, and sort if necessary.",
                        "Call the `run_inference` method of the `InferenceWorker` instance with the prepared data.",
                        "Return the results from the worker."
                    ],
                    "output": {
                        "shape": "A list of `CompletionOutput` objects."
                    },
                    "dependencies": [
                        "OfflineEngine.prepare_data",
                        "InferenceWorker.run_inference",
                        "CompletionOutput"
                    ],
                    "notes": [
                        "This is the main entry point for running inference with the engine. The input can be a list of `InputData` objects, JAX arrays, or NumPy arrays."
                    ]
                },
                "prepare_data": {
                    "purpose": "Converts various input formats to a standardized list of `InputData` objects, pads them, and optionally sorts them by length.",
                    "input": {
                        "shape": "A list of `InputData`, `jax.Array`, or `np.ndarray` objects.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Convert `jax.Array` inputs to `np.ndarray`.",
                        "Convert `np.ndarray` inputs to `InputData` objects, assuming no padding.",
                        "Validate that all `InputData` objects have unique IDs.",
                        "Call `pad_data` to pad all inputs to appropriate bucket lengths.",
                        "If `enable_batch_prefill` is True, sort the padded data by token length."
                    ],
                    "output": {
                        "shape": "A list of prepared `InputData` objects."
                    },
                    "dependencies": [
                        "InputData",
                        "numpy",
                        "OfflineEngine.pad_data"
                    ],
                    "notes": [
                        "This method prepares the raw input data for efficient processing by the `InferenceWorker`."
                    ]
                },
                "pad_data": {
                    "purpose": "Pads or truncates each input sequence to the nearest valid prefill length defined during initialization.",
                    "input": {
                        "shape": "A list of `InputData` objects.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Iterate through each `InputData` item.",
                        "Find the smallest prefill length from `self.prefill_lengths` that is greater than or equal to the item's true length.",
                        "If no suitable length is found, use the maximum prefill length.",
                        "Create a new zero-padded numpy array of the target length.",
                        "Copy the original tokens into the new array, truncating if necessary.",
                        "Create a new `InputData` object with the padded/truncated tokens."
                    ],
                    "output": {
                        "shape": "A list of padded `InputData` objects."
                    },
                    "dependencies": [
                        "InputData",
                        "numpy"
                    ],
                    "notes": [
                        "This bucketing strategy is crucial for efficient prefill, especially when prefill packing is enabled."
                    ]
                },
                "create_mesh": {
                    "purpose": "A static method that creates a JAX device mesh based on the provided configuration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determine ICI parallelism using `max_utils.fill_unspecified_mesh_axes`.",
                        "Create a device array using `mesh_utils.create_device_mesh`.",
                        "Instantiate and return a `jax.sharding.Mesh` object."
                    ],
                    "output": {
                        "shape": "A `jax.sharding.Mesh` object."
                    },
                    "dependencies": [
                        "jax.sharding.Mesh",
                        "max_utils.fill_unspecified_mesh_axes",
                        "jax.experimental.mesh_utils.create_device_mesh"
                    ],
                    "notes": [
                        "This is a helper method to abstract away the mesh creation logic."
                    ]
                },
                "_validate_config": {
                    "purpose": "An internal method to validate the engine's configuration and check for incompatible settings.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if `config.return_log_prob` is True.",
                        "Check if `enable_batch_prefill` and `config.scan_layers` are not both True.",
                        "Check if the calculated `max_decode_length` is greater than 0.",
                        "Log a warning if `config.scan_layers` is True."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "max_logging"
                    ],
                    "notes": [
                        "This method is called during initialization to ensure a valid state and raises a `ValueError` on failure."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#PageState",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "class PageState:\n  \"\"\"Represents the global state of memory pages managed by the `PageManager`.\n\n  This dataclass tracks the allocation status of each page across the entire system,\n  the mapping of pages to page groups (requests), and the current position within\n  each sequence's pages. State is managed globally, providing a single view\n  across all potential layers using this manager.\n\n  Attributes:\n    page_status: A `jnp.ndarray` of shape `[num_pages]`. Each element\n      indicates whether the corresponding page in the global pool is free (0)\n      or allocated (1).\n    page_map: A `jnp.ndarray` of shape `[max_page_groups, max_pages_per_group]`.\n      This array maps each page group to the indices (within the global pool)\n      of its allocated pages. Entries beyond `num_pages_used` for a group are invalid.\n    num_pages_used: A `jnp.ndarray` of shape `[max_page_groups]`. This array\n      tracks the number of pages currently allocated to each page group. This\n      determines the valid entries in `page_map` for each group.\n    sequence_lengths: A `jnp.ndarray` of shape `[max_page_groups]`. This array\n      stores the current true length of each sequence (in tokens) associated\n      with a page group.\n    active_page: A `jnp.ndarray` of shape `[max_page_groups]`. This array\n      stores the global index of the *currently active* page (the page where the\n      next token will be written) for each page group. Only valid if the\n      corresponding `has_active_page` is True.\n    has_active_page: A `jnp.ndarray` of shape `[max_page_groups]`. Boolean mask\n      indicating whether a page group currently represents an active sequence\n      and thus whether its `active_page` and `active_page_position` entries\n      are meaningful.\n    active_page_position: A `jnp.ndarray` of shape `[max_page_groups]`. This array\n      stores the index (offset, 0 to tokens_per_page-1) of the next available\n      token *within the `active_page`* for each page group. Only valid if\n      `has_active_page` is True.\n  \"\"\"\n\n  page_status: PagesInt1d\n  page_map: GroupsPagesInt2d\n  num_pages_used: GroupsInt1d\n  sequence_lengths: GroupsInt1d\n  active_page: GroupsInt1d\n  has_active_page: GroupsBool1d\n  active_page_position: GroupsInt1d",
        "analysis": {
            "functionality": "A Flax dataclass that represents the global state of memory pages for a paged attention mechanism. It tracks the allocation status of each page, the mapping of pages to page groups (requests), and the current position within each sequence's pages.",
            "usage": "This class is used as a data container to hold and pass around the state of the paged memory system. It is instantiated and manipulated by the `PageManager` class and its associated functions. It is not meant to be used directly for computation but rather to store state as a JAX-compatible pytree."
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#initialize_page_state",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def initialize_page_state(\n    num_pages: int,\n    max_page_groups: int,\n    max_pages_per_group: int,\n) -> PageState:\n  \"\"\"Creates and initializes a global `PageState` object.\n\n  All pages in the global pool are initially marked as free (status 0), except\n  for page 0 which is marked used as a workaround. No pages are assigned to any\n  page group. Sequence lengths and page usage counts are initialized to zero.\n  Active page tracking is also reset.\n\n  Args:\n    num_pages: The total number of available pages in the global pool.\n    max_page_groups: The maximum number of page groups (concurrent sequences/requests)\n      the system can track.\n    max_pages_per_group: The maximum number of pages that can be allocated to\n      a single page group (determines the size of the second dimension of `page_map`).\n\n  Returns:\n    An initialized `PageState` object with all values set to their defaults (zeros/False).\n  \"\"\"\n  # TODO(patemotter): Produces garbage output for any request that uses page 0\n  initial_page_status = jnp.zeros((num_pages,), dtype=jnp.int32)\n  initial_page_status = initial_page_status.at[0].set(1)  # Workaround page 0\n  return PageState(\n      page_status=initial_page_status,\n      page_map=jnp.zeros((max_page_groups, max_pages_per_group), dtype=jnp.int32),\n      num_pages_used=jnp.zeros((max_page_groups,), dtype=jnp.int32),\n      sequence_lengths=jnp.zeros((max_page_groups,), dtype=jnp.int32),\n      active_page=jnp.zeros((max_page_groups,), dtype=jnp.int32),\n      has_active_page=jnp.zeros((max_page_groups,), dtype=jnp.bool_),\n      active_page_position=jnp.zeros((max_page_groups,), dtype=jnp.int32),\n  )",
        "analysis": {
            "module_type": "state_initializer",
            "purpose": "Creates and initializes a global `PageState` object for managing paged attention memory, setting all states to their default (zero/False) values.",
            "input": {
                "shape": "N/A",
                "dtype": "int"
            },
            "processing_steps": [
                "Create a 1D integer array `initial_page_status` of size `num_pages`, initialized to zeros.",
                "Set the status of the first page (index 0) to 1 (used) as a workaround.",
                "Instantiate and return a `PageState` object with `initial_page_status` and several other arrays initialized to zeros or False, with dimensions based on `max_page_groups` and `max_pages_per_group`."
            ],
            "output": {
                "shape": "An instance of the `PageState` dataclass, containing arrays with shapes determined by the input parameters (e.g., `page_status` has shape [num_pages], `page_map` has shape [max_page_groups, max_pages_per_group])."
            },
            "dependencies": [
                "jax.numpy",
                "PageState"
            ],
            "parameters": {
                "num_pages": "The total number of available pages in the global pool.",
                "max_page_groups": "The maximum number of page groups (concurrent sequences/requests) the system can track.",
                "max_pages_per_group": "The maximum number of pages that can be allocated to a single page group."
            },
            "notes": [
                "The function intentionally marks page 0 as 'used' as a workaround. The comment notes this may produce garbage output for requests that use page 0.",
                "This function establishes the clean, initial state for the entire paged attention system before any sequences are processed."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#_find_next_free_page_index",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def _find_next_free_page_index(page_status: PagesInt1d) -> ScalarInt:\n  \"\"\"Finds the index of the next available free page in the global pool.\n\n  Searches the `page_status` array for the first occurrence of 0 (indicating\n  a free page), skipping index 0 due to potential issues.\n\n  Args:\n    page_status: A 1D `jnp.ndarray` representing the global status of pages\n      (0 for free, 1 for allocated). Should have shape [num_pages].\n\n  Returns:\n    A scalar `jnp.int32` array containing the index of the next free page\n    (the lowest index >= 1 where `page_status` is 0).\n    Returns -1 if no free pages (at index >= 1) are found.\n  \"\"\"\n  # TODO(patemotter): Produces garbage output for any request that uses page 0\n  search_status = page_status[1:]\n  overall_free_mask = search_status == 0\n\n  # argmax returns the index of the *first* True. If none are True, it returns 0.\n  next_free_relative = jnp.argmax(overall_free_mask)\n  # Add 1 to compensate for the slice [1:]\n  next_free_overall = next_free_relative + 1\n  # Check if a free page exists\n  has_free_overall = jnp.any(overall_free_mask)\n  # If a free page exists, return its index, otherwise return -1\n  return jnp.where(has_free_overall, next_free_overall, -1)",
        "analysis": {
            "module_type": "free_page_finder",
            "purpose": "Finds the index of the first available page in the global page pool, returning -1 if none are free.",
            "input": {
                "shape": "[num_pages]",
                "dtype": "int32"
            },
            "processing_steps": [
                "Slice the input `page_status` array to exclude the first element (index 0).",
                "Create a boolean mask `overall_free_mask` where `True` indicates a free page (value is 0).",
                "Use `jnp.argmax` on the mask to find the relative index of the first `True` value.",
                "Add 1 to the relative index to get the absolute index in the original array.",
                "Use `jnp.any` to check if any free pages were found in the sliced array.",
                "Use `jnp.where` to return the calculated absolute index if a free page exists, otherwise return -1."
            ],
            "output": {
                "shape": "[] (scalar)"
            },
            "dependencies": [
                "jax.numpy.argmax",
                "jax.numpy.any",
                "jax.numpy.where"
            ],
            "parameters": {},
            "notes": [
                "The function intentionally skips searching page index 0, as noted by a TODO comment.",
                "It returns -1 if no free pages are found at or after index 1.",
                "It relies on the behavior of `jnp.argmax` which returns 0 for an all-False input. A separate `jnp.any` check is used to distinguish this case from finding a free page at index 1."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#_release_pages_for_group",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def _release_pages_for_group(\n    page_state: PageState,\n    page_group_id: ScalarInt,\n    max_pages_per_group: int,\n) -> PageState:\n  \"\"\"Releases all pages associated with a given page group.\n\n  This function iterates through the potential pages allocated to the specified\n  `page_group_id` (up to `max_pages_per_group`). For each page index actually\n  used by the group (determined by `num_pages_used`), it retrieves the global\n  page index from `page_map` and resets its status to 0 (free) in the global\n  `page_status` array. It also resets all state fields related to the\n  `page_group_id` (length, count, active status, etc.) to their initial values.\n\n  Args:\n    page_state: The current global `PageState`.\n    page_group_id: The index of the page group whose pages are to be released.\n    max_pages_per_group: The maximum number of pages a group can hold (used as\n      the loop bound).\n\n  Returns:\n    A new `PageState` object where the specified group's pages are marked as free\n    in `page_status`, and the group's specific state entries are reset.\n  \"\"\"\n  current_page_status = page_state.page_status\n  current_page_map = page_state.page_map\n  num_valid_pages = page_state.num_pages_used[page_group_id]\n\n  def release_page(i: int, status: PagesInt1d) -> PagesInt1d:\n    is_valid = i < num_valid_pages\n    page_idx = current_page_map[page_group_id, i]\n    # Only release if index 'i' points to a valid allocated page\n    should_release = jnp.logical_and(is_valid, page_idx > 0)\n\n    return jax.lax.cond(should_release, lambda s: s.at[page_idx].set(0), lambda s: s, status)\n\n  new_page_status = jax.lax.fori_loop(0, max_pages_per_group, release_page, current_page_status)\n\n  return page_state.replace(\n      page_status=new_page_status,\n      num_pages_used=page_state.num_pages_used.at[page_group_id].set(0),\n      sequence_lengths=page_state.sequence_lengths.at[page_group_id].set(0),\n      active_page=page_state.active_page.at[page_group_id].set(0),\n      has_active_page=page_state.has_active_page.at[page_group_id].set(False),\n      active_page_position=page_state.active_page_position.at[page_group_id].set(0),\n  )",
        "analysis": {
            "module_type": "page_group_release",
            "purpose": "Releases all memory pages associated with a specific page group and resets the group's metadata.",
            "input": {
                "shape": "page_state: PageState object, page_group_id: scalar, max_pages_per_group: Python int",
                "dtype": "int32, bool_"
            },
            "processing_steps": [
                "Extract `page_status`, `page_map`, and `num_pages_used` from the input `page_state`.",
                "Iterate from 0 to `max_pages_per_group` using `jax.lax.fori_loop`.",
                "Inside the loop, define a `release_page` function that checks if a page slot is valid (i.e., index < `num_pages_used`) and allocated (`page_idx > 0`).",
                "Use `jax.lax.cond` to conditionally set the global status of the valid page to 0 (free) in the `page_status` array.",
                "After the loop, create a new `PageState` object using the `.replace()` method.",
                "Update the `page_status` with the result from the loop.",
                "Reset all state fields for the given `page_group_id` (`num_pages_used`, `sequence_lengths`, `active_page`, `has_active_page`, `active_page_position`) to their initial values (0 or False)."
            ],
            "output": {
                "shape": "Returns a `PageState` object with the same shape as the input `page_state`."
            },
            "dependencies": [
                "PageState",
                "jax.lax.fori_loop",
                "jax.lax.cond",
                "jax.numpy.logical_and"
            ],
            "parameters": {
                "max_pages_per_group": "The maximum number of pages a single group can hold, used as the loop boundary. It is a static argument for JIT compilation."
            },
            "notes": [
                "The function is JIT-compiled with `max_pages_per_group` as a static argument, which means the loop length is fixed at compile time.",
                "It safely handles cases where a group uses fewer pages than `max_pages_per_group` by checking against `num_pages_used`.",
                "In addition to freeing pages in the global status array, this function completely resets all metadata for the specified `page_group_id`, effectively making the group inactive and empty."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#_reserve_pages_for_group",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def _reserve_pages_for_group(\n    released_state: PageState,\n    page_group_id: ScalarInt,\n    true_length: ScalarInt,\n    tokens_per_page: int,\n    max_pages_per_group: int,\n) -> PageState:\n  \"\"\"Reserves pages for a specific group, assuming true_length > 0.\n\n  PRECONDITION: `true_length` must be > 0. This function assumes the caller\n  (e.g., `PageManager.update_prefill_pages`) has validated this.\n\n  Calculates the number of pages required for `true_length`. Checks if enough\n  free pages exist globally and if the group has capacity based on the state\n  provided in `released_state`. If resources are sufficient, it iteratively\n  finds free pages, marks them allocated, records them in the map, and updates\n  the group's state fields. If resources are insufficient, it returns the\n  `released_state` unchanged (effectively leaving the group empty).\n\n  Args:\n      released_state: The global `PageState` after pages for `page_group_id`\n          have already been released.\n      page_group_id: The index of the page group to allocate pages for.\n      true_length: The target sequence length for the prefill. MUST BE > 0.\n      tokens_per_page: The capacity of each page.\n      max_pages_per_group: The maximum number of pages the group can hold.\n\n  Returns:\n      A new `PageState` with pages allocated for the group and its state updated,\n      or the input `released_state` if allocation failed due to resource limits.\n  \"\"\"\n  num_pages_needed = (true_length + tokens_per_page - 1) // tokens_per_page\n  last_token_abs_idx = true_length - 1\n  last_page_position_idx = last_token_abs_idx % tokens_per_page\n  next_write_position = (last_page_position_idx + 1) % tokens_per_page\n\n  current_page_status = released_state.page_status\n  current_page_map = released_state.page_map\n  current_num_pages_used = released_state.num_pages_used\n\n  num_free_pages = jnp.sum(current_page_status == 0)\n  group_has_capacity = jax.lax.le(num_pages_needed, max_pages_per_group)\n  sufficient_free_pages = jax.lax.ge(num_free_pages, num_pages_needed)\n  has_enough_resources = jnp.logical_and(sufficient_free_pages, group_has_capacity)\n\n  def allocate_and_update_state(initial_state_tuple: tuple[PagesInt1d, GroupsPagesInt2d, GroupsInt1d]) -> PageState:\n    \"\"\"Allocates pages iteratively if resources are sufficient.\"\"\"\n    initial_status, initial_map, initial_num_used = initial_state_tuple\n\n    def allocate_one_page(\n        page_idx_in_group: ScalarInt, loop_state_tuple: tuple[PagesInt1d, GroupsPagesInt2d, GroupsInt1d]\n    ) -> tuple[PagesInt1d, GroupsPagesInt2d, GroupsInt1d]:\n      \"\"\"Allocates a single page within the fori_loop.\"\"\"\n      current_loop_status, current_loop_map, current_loop_num_used = loop_state_tuple\n      next_free_page_global = _find_next_free_page_index(current_loop_status)\n      page_allocated = jax.lax.ge(next_free_page_global, 0)\n\n      new_loop_status = jax.lax.cond(\n          page_allocated,\n          lambda s: s.at[next_free_page_global].set(1),\n          lambda s: s,\n          current_loop_status,\n      )\n      new_loop_map = jax.lax.cond(\n          page_allocated,\n          lambda m: m.at[page_group_id, page_idx_in_group].set(next_free_page_global),\n          lambda m: m,\n          current_loop_map,\n      )\n      new_loop_num_used = jax.lax.cond(\n          page_allocated,\n          lambda n: n.at[page_group_id].add(1),\n          lambda n: n,\n          current_loop_num_used,\n      )\n      return new_loop_status, new_loop_map, new_loop_num_used\n\n    final_page_status, final_page_map, final_num_pages_used = jax.lax.fori_loop(\n        0,\n        num_pages_needed,\n        allocate_one_page,\n        (initial_status, initial_map, initial_num_used),\n    )\n    active_page_global_index = final_page_map[page_group_id, num_pages_needed - 1]\n\n    return released_state.replace(\n        page_status=final_page_status,\n        page_map=final_page_map,\n        num_pages_used=final_num_pages_used,\n        sequence_lengths=released_state.sequence_lengths.at[page_group_id].set(true_length),\n        active_page=released_state.active_page.at[page_group_id].set(active_page_global_index),\n        has_active_page=released_state.has_active_page.at[page_group_id].set(True),\n        active_page_position=released_state.active_page_position.at[page_group_id].set(next_write_position),\n    )\n\n  # Conditionally perform allocation or return the released state\n  final_state = jax.lax.cond(\n      has_enough_resources,\n      allocate_and_update_state,\n      lambda _: released_state,\n      operand=(current_page_status, current_page_map, current_num_pages_used),\n  )\n  return final_state",
        "analysis": {
            "module_type": "page_reservation_helper",
            "purpose": "Conditionally allocates a set of memory pages for a specific sequence (page group) if sufficient global pages and group capacity are available.",
            "input": {
                "shape": "released_state: PageState object, page_group_id: ScalarInt, true_length: ScalarInt, tokens_per_page: int, max_pages_per_group: int",
                "dtype": "PageState contains jnp.int32 and jnp.bool_ arrays. Other inputs are int or JAX scalar int."
            },
            "processing_steps": [
                "Calculate the number of pages required (`num_pages_needed`) for the given `true_length`.",
                "Calculate the position of the next token to be written in the last page.",
                "Check if there are enough free pages globally and if the group has capacity for `num_pages_needed`.",
                "Use `jax.lax.cond` to execute allocation only if resources are sufficient.",
                "If allocating, define and call an inner function `allocate_and_update_state`.",
                "Inside `allocate_and_update_state`, use `jax.lax.fori_loop` to iterate `num_pages_needed` times.",
                "In each loop iteration, call `_find_next_free_page_index` to find a free page.",
                "Conditionally update the `page_status`, `page_map`, and `num_pages_used` arrays to reflect the new page allocation.",
                "After the loop, create a new `PageState` by updating the sequence length, active page index, active status, and next write position for the specified `page_group_id`.",
                "If resources are insufficient, return the input `released_state` without modification."
            ],
            "output": {
                "shape": "A PageState object with pages allocated and state updated for the specified group, or the original PageState if allocation failed."
            },
            "dependencies": [
                "PageState",
                "_find_next_free_page_index",
                "jax.lax.cond",
                "jax.lax.fori_loop",
                "jax.numpy"
            ],
            "parameters": {
                "tokens_per_page": "The capacity of each memory page in tokens. This is a static argument for JIT compilation.",
                "max_pages_per_group": "The maximum number of pages that can be allocated to a single page group. This is a static argument for JIT compilation."
            },
            "notes": [
                "This function is JIT-compiled using `partial(jax.jit, ...)`.",
                "It operates under the precondition that `true_length` is greater than 0.",
                "The input `released_state` is expected to be the global state after the target `page_group_id` has already had its pages freed.",
                "Allocation is an all-or-nothing operation; if resources are insufficient, the function has no effect on the state."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#_release_and_reserve_for_group",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def _release_and_reserve_for_group(\n    page_state: PageState,\n    page_group_id: ScalarInt,\n    true_length: ScalarInt,\n    tokens_per_page: int,\n    max_pages_per_group: int,\n) -> PageState:\n  \"\"\"Releases existing pages and reserves new pages for a group during prefill.\n\n  Assumes true_length > 0. Caller MUST validate inputs.\n  \"\"\"\n  released_state = _release_pages_for_group(page_state, page_group_id, max_pages_per_group)\n  final_state = _reserve_pages_for_group(released_state, page_group_id, true_length, tokens_per_page, max_pages_per_group)\n  return final_state",
        "analysis": {
            "module_type": "page_group_management",
            "purpose": "Releases all existing pages for a specific group and reserves a new set of pages to accommodate a given sequence length, typically used during prefill.",
            "input": {
                "shape": "page_state: PageState object, page_group_id: ScalarInt, true_length: ScalarInt",
                "dtype": "PageState object contains int32 and bool arrays. Other inputs are int32."
            },
            "processing_steps": [
                "Calls `_release_pages_for_group` to free all pages currently allocated to the specified `page_group_id`.",
                "Calls `_reserve_pages_for_group` on the resulting state to allocate a new set of pages based on the `true_length`.",
                "Returns the final, updated PageState."
            ],
            "output": {
                "shape": "Returns a PageState object with the same structure and shape as the input."
            },
            "dependencies": [
                "PageState",
                "_release_pages_for_group",
                "_reserve_pages_for_group",
                "jax.jit"
            ],
            "parameters": {
                "tokens_per_page": "The capacity of each page in tokens. This is a static argument for JIT compilation.",
                "max_pages_per_group": "The maximum number of pages that can be allocated to a single group. This is a static argument for JIT compilation."
            },
            "notes": [
                "This function is JIT-compiled using `jax.jit`.",
                "It assumes `true_length > 0` and relies on the caller to validate inputs.",
                "This operation is atomic from the caller's perspective: it first clears the group's resources and then attempts to re-allocate them."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#_update_decode_pages_global",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def _update_decode_pages_global(\n    page_state: PageState,\n    tokens_per_page: ScalarInt,\n    max_pages_per_group: ScalarInt,\n) -> PageState:\n  \"\"\"Updates pages globally for one step of autoregressive decoding.\n\n  This function performs the following steps for all page groups simultaneously:\n  1. Increments `sequence_lengths` for groups marked as `has_active_page`.\n  2. Calculates the new `active_page_position` based on the incremented length.\n  3. Determines which active groups now require a new page because their sequence\n     length has crossed a page boundary (`required_pages > num_pages_used`) and\n     they still have capacity (`required_pages <= max_pages_per_group`).\n  4. For each group identified in step 3, it attempts to find a free page globally and\n     allocate it, updating `page_status`, `page_map`, `num_pages_used`, and\n     `active_page` for that group.\n\n  Args:\n    page_state: The current global `PageState`.\n    tokens_per_page: The capacity of each page.\n    max_pages_per_group: The maximum number of pages allowed per group.\n\n  Returns:\n    A new `PageState` object reflecting the state after the decode step, potentially\n    with new pages allocated to groups that crossed page boundaries.\n  \"\"\"\n  max_page_groups = page_state.sequence_lengths.shape[0]\n\n  seq_len_increment = jnp.where(page_state.has_active_page, 1, 0)\n  new_sequence_lengths = page_state.sequence_lengths + seq_len_increment\n\n  new_active_page_position = jnp.where(\n      page_state.has_active_page,\n      (new_sequence_lengths - 1) % tokens_per_page,\n      page_state.active_page_position,\n  )\n\n  required_pages_per_group = (new_sequence_lengths + tokens_per_page - 1) // tokens_per_page\n  needs_new_page_mask = jnp.logical_and(page_state.has_active_page, required_pages_per_group > page_state.num_pages_used)\n  has_capacity_mask = required_pages_per_group <= max_pages_per_group\n  needs_allocation_mask = jnp.logical_and(needs_new_page_mask, has_capacity_mask)\n\n  def allocate_for_group_if_needed(group_idx: ScalarInt, current_state: PageState) -> PageState:\n    \"\"\"Inner function for fori_loop to conditionally allocate a page.\"\"\"\n    current_status = current_state.page_status\n    current_map = current_state.page_map\n    current_num_used = current_state.num_pages_used\n    current_active_page = current_state.active_page\n\n    needs_alloc = needs_allocation_mask[group_idx]\n    next_free_page_global = _find_next_free_page_index(current_status)\n    can_allocate = jnp.logical_and(needs_alloc, next_free_page_global >= 0)\n\n    new_status = jax.lax.cond(can_allocate, lambda s: s.at[next_free_page_global].set(1), lambda s: s, current_status)\n\n    page_map_index = current_num_used[group_idx]\n    new_map = jax.lax.cond(\n        can_allocate, lambda m: m.at[group_idx, page_map_index].set(next_free_page_global), lambda m: m, current_map\n    )\n    new_num_used = jax.lax.cond(can_allocate, lambda n: n.at[group_idx].add(1), lambda n: n, current_num_used)\n    new_active_page = jax.lax.cond(\n        can_allocate, lambda a: a.at[group_idx].set(next_free_page_global), lambda a: a, current_active_page\n    )\n\n    # Reconstruct state for loop carry/return\n    return current_state.replace(\n        page_status=new_status,\n        page_map=new_map,\n        num_pages_used=new_num_used,\n        active_page=new_active_page,\n    )\n\n  # Initialize loop state with pre-calculated lengths and positions\n  initial_loop_state = page_state.replace(\n      sequence_lengths=new_sequence_lengths,\n      active_page_position=new_active_page_position,\n  )\n\n  # Apply conditional allocation across all groups\n  final_state = jax.lax.fori_loop(0, max_page_groups, allocate_for_group_if_needed, initial_loop_state)\n  return final_state",
        "analysis": {
            "functionality": "Updates the global page state for a single step of autoregressive decoding. It increments the sequence length for active sequences and allocates a new page if a sequence's length crosses a page boundary and resources are available.",
            "usage": "This function is called during each step of autoregressive decoding. It takes the current `PageState`, the number of tokens per page, and the maximum pages per group as input, and returns an updated `PageState` reflecting the new token generation."
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#PageManager",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "class PageManager:\n  \"\"\"Manages the global allocation and release of pages for paged attention.\n\n  This class provides an interface for reserving pages during prefill and\n  decoding, and for releasing pages when a sequence (page group) is complete.\n  It encapsulates the logic for tracking page allocation globally and managing\n  the `PageState`. It uses the concept of page groups, where each group typically\n  corresponds to a single request or sequence being processed.\n\n  Example:\n    ```python\n    # Initialize a PageManager from configuration\n    config = YourConfig(...) # Set pagedattn_num_pages, etc.\n    page_manager = PageManager(config)\n\n    # Get initial page state (all pages free, except potentially page 0)\n    state = page_manager.get_initial_page_state()\n\n    # Update pages for prefill of a sequence in group 0 with length 16\n    state = page_manager.update_prefill_pages(\n        page_state=state,\n        page_group_id=0,\n        true_length=16\n    )\n\n    # Update pages for a single decode step (increments lengths, allocates if needed)\n    state = page_manager.update_decode_pages(state)\n\n    # Release pages associated with group 0 when the sequence is finished\n    state = page_manager.release_pages(\n        page_state=state,\n        page_group_id=0\n    )\n    ```\n  \"\"\"\n\n  def __init__(self, config: Config):\n    \"\"\"Initializes the `PageManager` from a configuration object.\n\n    Args:\n      config: A `Config` object containing the necessary parameters:\n        * `max_target_length`: The maximum sequence length supported.\n        * `pagedattn_num_pages`: The total number of pages available globally.\n        * `pagedattn_tokens_per_page`: The number of tokens each page can hold.\n        * `global_batch_size_to_load`: Used to determine the maximum number of concurrent\n          page groups (`max_page_groups`) the system can manage.\n        * `pagedattn_max_pages_per_group`: The maximum number of pages that can be\n          allocated to a single page group.\n\n    Raises:\n      ValueError: If the configuration parameters are invalid (e.g., non-positive\n        values, insufficient pages per group for max length).\n    \"\"\"\n    self.num_pages: int = config.pagedattn_num_pages\n    self.tokens_per_page: int = config.pagedattn_tokens_per_page\n    self.max_target_length: int = config.max_target_length\n    self.max_page_groups: int = config.global_batch_size_to_load\n    self.max_pages_per_group: int = config.pagedattn_max_pages_per_group\n    self._validate_init_params()\n\n  def _validate_init_params(self) -> None:\n    \"\"\"Validates initialization parameters for logical consistency.\"\"\"\n    if self.max_pages_per_group <= 0:\n      raise ValueError(\"`pagedattn_max_pages_per_group` must be positive.\")\n    min_required = (self.max_target_length + self.tokens_per_page - 1) // self.tokens_per_page\n    if self.max_pages_per_group < min_required:\n      raise ValueError(\n          f\"`pagedattn_max_pages_per_group` ({self.max_pages_per_group}) is insufficient for `max_target_length` \"\n          f\"({self.max_target_length}). Needs {min_required}.\"\n      )\n    # Check > 1 due to potential page 0 workaround\n    if self.num_pages <= 1:\n      raise ValueError(\"`pagedattn_num_pages` must be greater than 1.\")\n    if self.tokens_per_page <= 0:\n      raise ValueError(\"`pagedattn_tokens_per_page` must be positive.\")\n    if self.max_page_groups <= 0:\n      raise ValueError(\"`pagedattn_max_page_groups` must be positive.\")\n\n  def update_prefill_pages(self, page_state: PageState, page_group_id: int, true_length: int) -> PageState:\n    \"\"\"Reserves pages for a specific page group during prefill (global state).\n\n    This method first releases any pages currently allocated to the given\n    `page_group_id`. It then attempts to allocate the necessary number of pages\n    from the global pool to accommodate a sequence of `true_length`. If successful,\n    it updates the `PageState` to reflect the new allocation and marks the group\n    as active. If there are not enough free pages globally or the group exceeds\n    its `max_pages_per_group` limit, the group's state remains cleared (as after\n    the initial release). Input validation ensures `page_group_id` and `true_length`\n    are within valid ranges.\n\n    Args:\n      page_state: The current global `PageState`.\n      page_group_id: The ID of the page group (request) to allocate pages for. Must\n        be between 0 and `max_page_groups - 1`.\n      true_length: The sequence length to allocate pages for. Must be between 0\n        and `max_target_length`.\n\n    Returns:\n      The updated `PageState`. If allocation fails due to resource limits, the\n      returned state will have the specified `page_group_id` cleared.\n\n    Raises:\n      ValueError: If `page_group_id` or `true_length` are outside their valid ranges.\n\n    Example:\n      ```python\n      # Reserve pages for a 16-token sequence in group 0\n      state = page_manager.update_prefill_pages(\n          page_state=state,\n          page_group_id=0,\n          true_length=16\n      )\n      ```\n    \"\"\"\n    if page_group_id < 0 or page_group_id >= self.max_page_groups:\n      raise ValueError(f\"PageManager: page_group_id ({page_group_id}) out of range [0, {self.max_page_groups})\")\n    if true_length <= 0 or true_length > self.max_target_length:\n      raise ValueError(f\"PageManager: true_length ({true_length}) out of range (0, {self.max_target_length}]\")\n\n    return _release_and_reserve_for_group(\n        page_state, page_group_id, true_length, self.tokens_per_page, self.max_pages_per_group\n    )\n\n  def update_decode_pages(self, page_state: PageState) -> PageState:\n    \"\"\"Updates pages globally for one step of autoregressive decoding.\n\n    This method advances the state for all active page groups. It increments\n    their sequence lengths by one and updates their position within the current\n    active page. If this increment causes a sequence to cross a page boundary\n    (i.e., it needs more pages than currently allocated), this method attempts\n    to allocate a new page from the global pool, provided the group has not\n    reached its `max_pages_per_group` limit and free pages are available.\n\n    Args:\n      page_state: The current global `PageState`.\n\n    Returns:\n      The updated `PageState` reflecting the state after the decode step.\n      Sequence lengths and active positions are updated for all active groups.\n      Groups that required and successfully obtained a new page will have their\n      `num_pages_used`, `page_map`, and `active_page` updated.\n\n    Example:\n      ```python\n      # Advance state for all active sequences by one decode step\n      state = page_manager.update_decode_pages(state)\n      ```\n    \"\"\"\n    return _update_decode_pages_global(page_state, self.tokens_per_page, self.max_pages_per_group)\n\n  def release_pages(self, page_state: PageState, page_group_id: int) -> PageState:\n    \"\"\"Releases all pages associated with a given page group (global state).\n\n    This method identifies all pages currently allocated to the specified\n    `page_group_id` using the `page_map` and `num_pages_used`. It marks these\n    pages as free (status 0) in the global `page_status` array. It also resets\n    all state information specific to the `page_group_id` (sequence length,\n    page count, active status, etc.) to their initial zero/False values.\n    Input validation ensures the `page_group_id` is within the valid range.\n\n    Args:\n      page_state: The current global `PageState`.\n      page_group_id: The ID of the page group (request) to release. Must be\n        between 0 and `max_page_groups - 1`.\n\n    Returns:\n      The updated `PageState` after releasing the pages and resetting the group's\n      state.\n\n    Raises:\n      ValueError: If `page_group_id` is outside its valid range.\n\n    Example:\n      ```python\n      # Release all pages currently held by group 0\n      state = page_manager.release_pages(\n          page_state=state,\n          page_group_id=0\n      )\n      ```\n    \"\"\"\n    if page_group_id < 0 or page_group_id >= self.max_page_groups:\n      raise ValueError(f\"PageManager: page_group_id ({page_group_id}) out of range [0, {self.max_page_groups})\")\n    return _release_pages_for_group(page_state, page_group_id, self.max_pages_per_group)\n\n  def get_initial_page_state(self) -> PageState:\n    \"\"\"Creates and returns an initial global `PageState`.\n\n    This is a convenience method that calls `initialize_page_state` with\n    the parameters (`num_pages`, `max_page_groups`, `max_pages_per_group`)\n    stored during the `PageManager` initialization.\n\n    Returns:\n      An initialized `PageState` object where all pages are free (except possibly 0)\n      and no groups are active.\n\n    Example:\n      ```python\n      # Get a fresh, empty page state\n      initial_state = page_manager.get_initial_page_state()\n      ```\n    \"\"\"\n    return initialize_page_state(\n        num_pages=self.num_pages,\n        max_page_groups=self.max_page_groups,\n        max_pages_per_group=self.max_pages_per_group,\n    )",
        "analysis": {
            "module_type": "paged_attention_manager",
            "purpose": "Manages the global allocation and release of memory pages for paged attention, providing an interface to reserve pages for prefill, update them during decoding, and release them when a sequence is complete.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "Config",
                "PageState",
                "initialize_page_state",
                "_release_and_reserve_for_group",
                "_update_decode_pages_global",
                "_release_pages_for_group"
            ],
            "parameters": {
                "pagedattn_num_pages": "The total number of pages available globally.",
                "pagedattn_tokens_per_page": "The number of tokens each page can hold.",
                "max_target_length": "The maximum sequence length supported.",
                "global_batch_size_to_load": "Used to determine the maximum number of concurrent page groups.",
                "pagedattn_max_pages_per_group": "The maximum number of pages that can be allocated to a single page group."
            },
            "notes": [
                "This class encapsulates the logic for tracking page allocation globally and managing the `PageState`.",
                "It uses the concept of page groups, where each group typically corresponds to a single request or sequence being processed."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the PageManager from a configuration object, setting up parameters for page management.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Extracts page management parameters from the input config object.",
                        "Stores parameters as instance attributes.",
                        "Calls `_validate_init_params` to ensure parameter validity."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "self._validate_init_params"
                    ],
                    "notes": [
                        "Raises ValueError if the configuration parameters are invalid (e.g., non-positive values, insufficient pages per group for max length)."
                    ]
                },
                "_validate_init_params": {
                    "purpose": "Validates initialization parameters for logical consistency.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks if `max_pages_per_group` is positive.",
                        "Calculates and checks if `max_pages_per_group` is sufficient for `max_target_length`.",
                        "Checks if `num_pages` is greater than 1.",
                        "Checks if `tokens_per_page` is positive.",
                        "Checks if `max_page_groups` is positive."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": [
                        "Raises a ValueError if any of the validation checks fail."
                    ]
                },
                "update_prefill_pages": {
                    "purpose": "Reserves pages for a specific page group during the prefill stage.",
                    "input": {
                        "shape": "Inputs are a PageState object and two integer scalars (page_group_id, true_length).",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Validate `page_group_id` and `true_length` are within valid ranges.",
                        "Call `_release_and_reserve_for_group` to perform the state update."
                    ],
                    "output": {
                        "shape": "Returns an updated PageState object."
                    },
                    "dependencies": [
                        "PageState",
                        "_release_and_reserve_for_group"
                    ],
                    "notes": [
                        "This method first releases any pages currently allocated to the group before reserving new ones.",
                        "If allocation fails due to resource limits, the returned state will have the specified group's state cleared.",
                        "Raises ValueError if input arguments are out of range."
                    ]
                },
                "update_decode_pages": {
                    "purpose": "Updates the global page state for a single step of autoregressive decoding.",
                    "input": {
                        "shape": "Input is a PageState object.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `_update_decode_pages_global` to advance the state for all active page groups."
                    ],
                    "output": {
                        "shape": "Returns an updated PageState object."
                    },
                    "dependencies": [
                        "PageState",
                        "_update_decode_pages_global"
                    ],
                    "notes": [
                        "This method increments sequence lengths for active groups and allocates a new page if a page boundary is crossed."
                    ]
                },
                "release_pages": {
                    "purpose": "Releases all pages associated with a given page group, making them available for reallocation.",
                    "input": {
                        "shape": "Inputs are a PageState object and an integer scalar (page_group_id).",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Validate `page_group_id` is within the valid range.",
                        "Call `_release_pages_for_group` to free the pages and reset the group's state."
                    ],
                    "output": {
                        "shape": "Returns an updated PageState object."
                    },
                    "dependencies": [
                        "PageState",
                        "_release_pages_for_group"
                    ],
                    "notes": [
                        "Raises ValueError if `page_group_id` is out of range."
                    ]
                },
                "get_initial_page_state": {
                    "purpose": "Creates and returns an initial global PageState with all pages free.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `initialize_page_state` with the manager's stored configuration parameters."
                    ],
                    "output": {
                        "shape": "Returns an initialized PageState object."
                    },
                    "dependencies": [
                        "initialize_page_state"
                    ],
                    "notes": [
                        "This is a convenience method to get a fresh, empty page state."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention.py#paged_attention_op_as_linen",
        "file_path": "src/MaxText/inference/paged_attention.py",
        "code_block": "def paged_attention_op_as_linen(\n    *,\n    mesh: Mesh,\n    num_pages: int,\n    tokens_per_page: int,\n    max_pages_per_slot: int,\n    max_pages_per_prefill: int,\n    pages_per_compute_block: int,\n    num_kv_heads: int,\n    kv_head_dim_size: int,\n    dtype: DType = jnp.float32,\n    attn_logits_soft_cap: float | None = None,\n    query_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV),\n    kv_pages_axis_names: AxisNames = (\n        \"paged_kv_heads\",\n        \"num_pages\",\n        \"tokens_per_page\",\n        \"paged_kv_head_dim_size\",\n    ),\n):\n  \"\"\"A factory function to create a PagedAttentionOp as a Linen module.\n\n  This function serves as a bridge to use the NNX-based `PagedAttentionOp`\n  within a Linen model. It wraps the `PagedAttentionOp` module using\n  `nnx.bridge.to_linen`, making it compatible with the Linen API. This is\n  useful for gradual migration of a codebase from Linen to NNX.\n\n  Args:\n    mesh: The device mesh for sharding.\n    num_pages: The total number of pages in the KV cache.\n    tokens_per_page: The number of tokens each page can hold.\n    max_pages_per_slot: The maximum number of pages a single sequence can use.\n    max_pages_per_prefill: The maximum number of pages for a prefill sequence.\n    pages_per_compute_block: The number of pages processed in one kernel block.\n    num_kv_heads: The number of key/value heads.\n    kv_head_dim_size: The dimension of each key/value head.\n    dtype: The data type for computations.\n    attn_logits_soft_cap: The soft cap for attention logits.\n    query_axis_names: The logical axis names for the query tensor.\n    kv_pages_axis_names: The logical axis names for the KV cache pages.\n\n  Returns:\n    A Linen module that wraps the NNX `PagedAttentionOp` module.\n  \"\"\"\n\n  return nnx.bridge.to_linen(\n      PagedAttentionOp,\n      mesh=mesh,\n      num_pages=num_pages,\n      tokens_per_page=tokens_per_page,\n      max_pages_per_slot=max_pages_per_slot,\n      max_pages_per_prefill=max_pages_per_prefill,\n      pages_per_compute_block=pages_per_compute_block,\n      num_kv_heads=num_kv_heads,\n      kv_head_dim_size=kv_head_dim_size,\n      dtype=dtype,\n      attn_logits_soft_cap=attn_logits_soft_cap,\n      query_axis_names=query_axis_names,\n      kv_pages_axis_names=kv_pages_axis_names,\n      metadata_fn=variable_to_logically_partitioned,\n  )",
        "analysis": {
            "functionality": "This is a factory function that creates a Flax Linen module by wrapping the NNX-based `PagedAttentionOp` class. It serves as a compatibility bridge to allow the use of an NNX module within a Linen model.",
            "usage": "Call this function with configuration parameters for paged attention (e.g., `mesh`, `num_pages`, `num_kv_heads`). It returns a Linen module instance that can be used in a Linen model's `setup` method. The returned module itself will then process query, key, and value tensors during the forward pass."
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention.py#PagedAttentionOp",
        "file_path": "src/MaxText/inference/paged_attention.py",
        "code_block": "class PagedAttentionOp(nnx.Module):\n  \"\"\"An NNX module for paged attention.\n\n  This module implements the paged attention mechanism, which is an efficient\n  method for handling attention in autoregressive models with long sequences.\n  It divides the KV cache into fixed-size \"pages\" to manage memory dynamically.\n  \"\"\"\n\n  def __init__(\n      self,\n      mesh: Mesh,\n      num_pages: int,\n      tokens_per_page: int,\n      max_pages_per_slot: int,\n      max_pages_per_prefill: int,\n      pages_per_compute_block: int,\n      num_kv_heads: int,\n      kv_head_dim_size: int,\n      dtype: DType = jnp.float32,\n      attn_logits_soft_cap: float | None = None,\n      query_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV),\n      kv_pages_axis_names: AxisNames = (\n          \"paged_kv_heads\",\n          \"num_pages\",\n          \"tokens_per_page\",\n          \"paged_kv_head_dim_size\",\n      ),\n      *,\n      # Not used in Embed but passed in by nnx.bridge.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs,\n  ):\n    \"\"\"Initializes the PagedAttentionOp module.\n\n    Args:\n      mesh: The device mesh for sharding.\n      num_pages: The total number of pages in the KV cache.\n      tokens_per_page: The number of tokens each page can hold.\n      max_pages_per_slot: The maximum number of pages a single sequence can use.\n      max_pages_per_prefill: The maximum number of pages for a prefill sequence.\n      pages_per_compute_block: The number of pages processed in one kernel block.\n      num_kv_heads: The number of key/value heads.\n      kv_head_dim_size: The dimension of each key/value head.\n      dtype: The data type for computations.\n      attn_logits_soft_cap: The soft cap for attention logits.\n      query_axis_names: The logical axis names for the query tensor.\n      kv_pages_axis_names: The logical axis names for the KV cache pages.\n      rngs: The random number generators for initialization (required by NNX).\n    \"\"\"\n\n    self.mesh = mesh\n    self.num_pages = num_pages\n    self.tokens_per_page = tokens_per_page\n    self.max_pages_per_slot = max_pages_per_slot\n    self.max_pages_per_prefill = max_pages_per_prefill\n    self.pages_per_compute_block = pages_per_compute_block\n    self.num_kv_heads = num_kv_heads\n    self.kv_head_dim_size = kv_head_dim_size\n    self.dtype = dtype\n    self.attn_logits_soft_cap = attn_logits_soft_cap\n    self.query_axis_names = query_axis_names\n    self.kv_pages_axis_names = kv_pages_axis_names\n\n    self.kv_pages_shape = (\n        self.num_kv_heads,\n        self.num_pages,\n        self.tokens_per_page,\n        self.kv_head_dim_size,\n    )\n\n    self.key_pages = nnx.Cache(\n        jnp.zeros(self.kv_pages_shape, dtype=self.dtype),\n        sharding=self.kv_pages_axis_names,\n    )\n    self.value_pages = nnx.Cache(\n        jnp.zeros(self.kv_pages_shape, dtype=self.dtype),\n        sharding=self.kv_pages_axis_names,\n    )\n\n  def _maybe_materialize_cache(self, cache: nnx.Cache) -> nnx.Cache:\n    \"\"\"Materializes the cache if it's currently a ShapeDtypeStruct.\"\"\"\n    if isinstance(cache.value, jax.ShapeDtypeStruct):\n      # This is needed because the Linen bridge lazily creates this state. We\n      # need to ensure the cache state is accessible at runtime.\n      # TODO: Delete this function when the to_linen bridge is no longer needed.\n      return nnx.Cache(\n          jnp.zeros(self.kv_pages_shape, dtype=self.dtype),\n          sharding=cache.sharding,\n      )\n    return cache\n\n  def get_kv_pages(self):\n    \"\"\"Retrieves the key and value page caches.\n\n    This method ensures the KV cache pages are materialized (if they are abstract\n    ShapeDtypeStructs, a temporary state during Linen bridge initialization) and\n    applies the necessary sharding constraints.\n\n    Returns:\n      A tuple containing the key pages and value pages caches (`nnx.Cache`).\n    \"\"\"\n\n    # TODO: Remove once to_linen bridge is no longer needed\n    self.key_pages = self._maybe_materialize_cache(self.key_pages)\n    self.value_pages = self._maybe_materialize_cache(self.value_pages)\n\n    self.key_pages.value = nn.with_logical_constraint(self.key_pages.value, self.kv_pages_axis_names)\n    self.value_pages.value = nn.with_logical_constraint(self.value_pages.value, self.kv_pages_axis_names)\n    return self.key_pages, self.value_pages\n\n  def pad_qkv(self, *qkv):\n    \"\"\"Pad input to kv_head_dim_size\"\"\"\n\n    def pad_to_kv_head_dim_size(x):\n      if x.shape[-1] != self.kv_head_dim_size:\n        return jnp.pad(\n            x,\n            ((0, 0), (0, 0), (0, 0), (0, self.kv_head_dim_size - x.shape[-1])),\n            mode=\"constant\",\n            constant_values=0.0,\n        )\n      else:\n        return x\n\n    # Align Q, K, V to the same head dim. This is required by the kernel.\n    return tuple(pad_to_kv_head_dim_size(x) for x in qkv)\n\n  def paged_dot_product_attention_with_max_and_sum(self, query, key, value):\n    \"\"\"paged dot product attention with max & sum\"\"\"\n    b, t, n, d = query.shape\n    _, s, n_kv, _ = key.shape\n    query = jnp.reshape(query, (b, t, n_kv, n // n_kv, d))\n\n    attn_weights = jnp.einsum(\"btkgd,bskd->bkgts\", query, key)\n\n    causal_mask = jnp.triu(jnp.ones((t, s)), k=1)\n    causal_mask = jnp.reshape(causal_mask, (1, 1, 1, t, s))\n    masked_weights = jnp.where(causal_mask, jnp.full_like(attn_weights, -1e10), attn_weights)\n\n    local_max = jnp.max(masked_weights, axis=-1, keepdims=True)\n    local_exps = jnp.exp(masked_weights - local_max)\n    local_sums = jnp.sum(local_exps, axis=-1, keepdims=True)\n\n    attn = jnp.einsum(\"bkgts,bskd->btkgd\", local_exps, value)\n    attn = jnp.reshape(attn, (b, t, n, d))\n\n    local_max = jnp.moveaxis(local_max, -2, 1)\n    local_max = jnp.reshape(local_max, (b, t, n, 1))\n\n    local_sums = jnp.moveaxis(local_sums, -2, 1)\n    local_sums = jnp.reshape(local_sums, (b, t, n, 1))\n\n    return attn, local_max, local_sums\n\n  # TODO(rupliu): add sharding when SPMD is fully supported\n  def paged_attention_v2_prefill(\n      self,\n      query: Array,\n      key_pages_cache: nnx.Cache,\n      value_pages_cache: nnx.Cache,\n      page_state: page_manager.PageState,\n  ) -> Array:\n    \"\"\"Apply ragged input Paged Attention in prefill only. The assumption\n    is the batch_size is only 1\n    \"\"\"\n    assert query.shape[0] == 1  # ensure the batch size is 0\n    # shape of key_pages_cache.value is [num_kv_heads, num_pages, tokens_per_page, head_dim]\n    k_p = jnp.permute_dims(key_pages_cache.value, (1, 2, 0, 3))\n    v_p = jnp.permute_dims(value_pages_cache.value, (1, 2, 0, 3))\n    c_q_l = jnp.array([0, page_state.sequence_lengths[0]])  # [0, prefill_true_length]\n    num_seqs = jnp.array([1])\n    query = query[0]  # [batch_size, max_num_tokens, num_kv_heads, head_dim] to [max_num_tokens, num_kv_heads, head_dim]\n    result = paged_attention_kernel_v2.ragged_paged_attention(\n        q=query,\n        k_pages=k_p,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n        v_pages=v_p,\n        kv_lens=jnp.array([query.shape[0]]),  # max_prefill_length\n        cu_q_lens=c_q_l,  # the accumulative real lengths of requests, starting from 0\n        page_indices=page_state.page_map,\n        num_seqs=num_seqs,\n        # TODO(rupliu) debug: repeated response when enabled below\n        # num_kv_pages_per_block=self.pages_per_compute_block,\n    )\n    return jnp.expand_dims(result, axis=0)  # [batch_size, seq_len, n_kv_head, head_dim] and batch_size is 1 for now\n\n  # TODO(rupliu): add sharding when SPMD is fully supported\n  def paged_attention_v2_decode(\n      self,\n      query: Array,\n      key_pages_cache: nnx.Cache,\n      value_pages_cache: nnx.Cache,\n      page_state: page_manager.PageState,\n  ) -> Array:\n    \"\"\"Apply ragged input Paged Attention in decode only.\"\"\"\n    batch_size = query.shape[0]\n    query = jnp.squeeze(query, axis=1)  # [batch_size, seq_len, n_kv_head, head_dim] to [batch_size, n_kv_head, head_dim]\n    k_p = jnp.permute_dims(key_pages_cache.value, (1, 2, 0, 3))\n    v_p = jnp.permute_dims(value_pages_cache.value, (1, 2, 0, 3))\n    c_q_l = jnp.arange(batch_size + 1)  # one token per sequence\n    num_seqs = jnp.array([batch_size])  # real number of requests, set it to batch_size\n    result = paged_attention_kernel_v2.ragged_paged_attention(\n        q=query,  # [max_batched_num_tokens, num_kv_heads, head_dim]\n        k_pages=k_p,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n        v_pages=v_p,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n        kv_lens=page_state.sequence_lengths,  # [max_num_seqs]\n        cu_q_lens=c_q_l,  # [max_num_seqs+1]\n        page_indices=page_state.page_map,  # [max_num_seqs, pages_per_seq]\n        num_seqs=num_seqs,\n        num_kv_pages_per_block=self.pages_per_compute_block,\n    )\n    return jnp.expand_dims(\n        result, axis=1\n    )  # [batch_size, n_kv_head, head_dim] to [batch_size, seq_len, n_kv_head, head_dim]\n\n  # v1 kernel has around 20% performance gain than v2 kernel in decode only task\n  def paged_attention_v1_decode(\n      self,\n      query: Array,\n      key_pages_cache: nnx.Cache,\n      value_pages_cache: nnx.Cache,\n      page_state: page_manager.PageState,\n  ) -> Array:\n    \"\"\"Apply Paged Attention v1 in decode only.\"\"\"\n    kv_pages_pspec = nn.logical_to_mesh_axes((\"paged_kv_heads\", None, None, None))\n    q_pspec = nn.logical_to_mesh_axes((None, None, \"paged_kv_heads\", None))\n\n    @functools.partial(\n        shard_map,\n        mesh=self.mesh,\n        in_specs=(\n            q_pspec,\n            kv_pages_pspec,\n            kv_pages_pspec,\n            P(None),\n            P(None, None),\n            None,\n        ),\n        out_specs=q_pspec,\n        check_rep=False,\n    )\n    def wrap_paged_attention(q, k_pages, v_pages, lengths, page_indices, pages_per_compute_block):\n      q = jnp.squeeze(q, axis=1)\n      result = paged_attention_kernel.paged_attention(\n          q=q,  # [batch_size, num_heads, head_dim]\n          k_pages=k_pages,\n          v_pages=v_pages,\n          lengths=lengths,\n          page_indices=page_indices,\n          pages_per_compute_block=pages_per_compute_block,\n      )\n      return jnp.expand_dims(result, axis=1)  # [batch_size, n_kv_head, head_dim] to [batch_size, 1, n_kv_head, head_dim]\n\n    return wrap_paged_attention(\n        query,\n        key_pages_cache.value,\n        value_pages_cache.value,\n        page_state.sequence_lengths,\n        page_state.page_map,\n        self.pages_per_compute_block,\n    )\n\n  def __call__(\n      self,\n      query: Array,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array,\n      model_mode: str,\n      previous_chunk=None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n  ):\n    \"\"\"Applies the paged attention mechanism.\n\n    This is the main entry point for the module. It takes query, key, and value\n    tensors and performs paged attention based on the current model mode\n    (prefill or autoregressive).\n\n    Args:\n      query: The query tensor.\n      key: The key tensor for the current step.\n      value: The value tensor for the current step.\n      decoder_segment_ids: Segment IDs for the decoder, used for masking.\n      model_mode: The current operational mode, either 'prefill' or\n        'autoregressive'.\n      previous_chunk: Information about previously processed chunks, used for\n        chunked prefill.\n      slot: The batch slot index for the current request.\n      page_state: The current state of the page manager.\n\n    Returns:\n        A tuple (output, exponentials_max, exponentials_sum) containing:\n        - The attention output tensor.\n        - The max of the exponentials (for prefill mode with dot-product attention).\n        - The sum of the exponentials (for prefill mode with dot-product attention).\n        The latter two are None for autoregressive mode, as this is handled\n        internally by the paged attention kernel.\n    \"\"\"\n\n    key_pages_cache, value_pages_cache = self.get_kv_pages()\n    query, key, value = self.pad_qkv(query, key, value)\n\n    # update kv pages and call page attention kernel\n    if model_mode == MODEL_MODE_PREFILL:\n      self.update_prefill_step_pages(key_pages_cache, value_pages_cache, key, value, slot, page_state)\n      if _use_kernel_v2:\n        return (\n            self.paged_attention_v2_prefill(query, key_pages_cache, value_pages_cache, page_state),\n            None,\n            None,\n        )\n      return self.paged_dot_product_attention_with_max_and_sum(query, key, value)\n    elif model_mode == MODEL_MODE_AUTOREGRESSIVE and page_state is not None:\n      self.update_decode_step_pages(key_pages_cache, value_pages_cache, key, value, page_state)\n      if _use_kernel_v2:\n        return (\n            self.paged_attention_v2_decode(query, key_pages_cache, value_pages_cache, page_state),\n            None,\n            None,\n        )\n      return (\n          self.paged_attention_v1_decode(query, key_pages_cache, value_pages_cache, page_state),\n          None,\n          None,\n      )\n    else:\n      raise NotImplementedError(model_mode)\n\n  def update_prefill_step_pages(\n      self,\n      key_pages_cache: nnx.Cache,  # [num_kv_heads, num_pages, tokens_per_page, head_dim]\n      value_pages_cache: nnx.Cache,\n      key: Array,\n      value: Array,\n      slot: int,\n      page_state: page_manager.PageState,\n  ) -> None:\n    \"\"\"Update pages for prefill step.\"\"\"\n    assert (\n        key.shape == value.shape\n    ), f\"prefill_step key/value should have the same shape, but getting {key.shape=} and {value.shape=} instead\"\n    batch_size, seq_len, n_kv_head, head_dim = key.shape\n    assert seq_len % self.tokens_per_page == 0, f\"seq_length {seq_len} and  tokens_per_page {self.tokens_per_page}\"\n    assert key_pages_cache.value.shape == value_pages_cache.value.shape, (\n        f\"prefill_step key/value_pages_cache should have the same shape, but \"\n        f\"getting {key_pages_cache.shape=} and {value_pages_cache.shape=} instead\"\n    )\n\n    v_n_kv, _, v_p, v_d = key_pages_cache.value.shape\n    assert v_n_kv == n_kv_head, f\"{v_n_kv=} {n_kv_head=}\"\n    assert v_p == self.tokens_per_page, f\"{v_p=} {self.tokens_per_page=}\"\n    assert v_d == head_dim, f\"{v_d=} {head_dim=}\"\n    assert page_state.page_map.shape == (\n        page_state.num_pages_used.shape[0],\n        self.max_pages_per_slot,\n    )\n\n    # Handle both init (b>1) and runtime (b=1) cases\n    if batch_size == 1:\n      key = jnp.squeeze(key)  # [batch_size, seq_len, n_kv_head, head_dim] to [seq_len, n_kv_head, head_dim]\n      value = jnp.squeeze(value)\n    else:\n      key = key[0]\n      value = value[0]\n\n    key = jnp.transpose(key, axes=(1, 0, 2))\n    value = jnp.transpose(value, axes=(1, 0, 2))\n\n    key = jnp.reshape(\n        key,\n        shape=(\n            n_kv_head,\n            max(1, seq_len // self.tokens_per_page),\n            self.tokens_per_page,\n            head_dim,\n        ),\n    )\n    value = jnp.reshape(\n        value,\n        shape=(\n            n_kv_head,\n            max(1, seq_len // self.tokens_per_page),\n            self.tokens_per_page,\n            head_dim,\n        ),\n    )\n\n    key_pages_cache.value = nn.with_logical_constraint(key, self.kv_pages_axis_names)\n    value_pages_cache.value = nn.with_logical_constraint(value, self.kv_pages_axis_names)\n\n  def update_decode_step_pages(self, key_pages_cache, value_pages_cache, key, value, page_state):\n    \"\"\"Update decode-step pages\"\"\"\n    key_pages = key_pages_cache.value\n    value_pages = value_pages_cache.value\n\n    batch_size, _, kv_heads, head_dim = key.shape\n    kv_heads, _, _, head_dim = key_pages.shape\n\n    new_key = key.reshape(batch_size, kv_heads, head_dim)[:, :, :]\n    new_key = jnp.transpose(new_key, (1, 0, 2))  # [n_kv_heads, batch_size, head_dim]\n    new_value = value.reshape(batch_size, kv_heads, head_dim)[:, :, :]\n    new_value = jnp.transpose(new_value, (1, 0, 2))  # [n_kv_heads, batch_size, head_dim]\n\n    broadcast_pages = jnp.tile(page_state.active_page, (kv_heads, 1))  # [n_kv_heads, batch_size]\n    broadcast_pos = jnp.tile(page_state.active_page_position, (kv_heads, 1))  # [n_kv_heads, batch_size]\n\n    kv_indices = jnp.arange(kv_heads)[:, None]  # [n_kv_heads, 1]\n    kv_indices = jnp.tile(kv_indices, (1, batch_size))  # [n_kv_heads, batch_size]\n\n    # [num_kv_heads, num_pages, tokens_per_page, head_dim]\n    key_pages_updated = key_pages.at[kv_indices, broadcast_pages, broadcast_pos].set(new_key)\n    value_pages_updated = value_pages.at[kv_indices, broadcast_pages, broadcast_pos].set(new_value)\n\n    key_pages_cache.value = key_pages_updated\n    value_pages_cache.value = value_pages_updated\n    return key_pages_cache, value_pages_cache",
        "analysis": {
            "module_type": "paged_attention",
            "purpose": "Implements the paged attention mechanism for efficient handling of long sequences in autoregressive models by dynamically managing a paged KV cache.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes configuration parameters such as page sizes, head dimensions, and mesh.",
                "Creates and manages `key_pages` and `value_pages` as `nnx.Cache` objects.",
                "Dispatches to different attention implementations based on the `model_mode` ('prefill' or 'autoregressive') and a global kernel version flag.",
                "Updates the KV cache pages with new key/value data for each step."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx.Module",
                "nnx.Cache",
                "jax.numpy",
                "jax.sharding.Mesh",
                "page_manager.PageState",
                "paged_attention_kernel",
                "paged_attention_kernel_v2"
            ],
            "parameters": {
                "mesh": "The device mesh for sharding.",
                "num_pages": "The total number of pages in the KV cache.",
                "tokens_per_page": "The number of tokens each page can hold.",
                "max_pages_per_slot": "The maximum number of pages a single sequence can use.",
                "num_kv_heads": "The number of key/value heads.",
                "kv_head_dim_size": "The dimension of each key/value head."
            },
            "notes": [
                "The module manages a key-value (KV) cache divided into fixed-size 'pages'.",
                "It supports both 'prefill' and 'autoregressive' modes for generation.",
                "It contains logic to switch between different paged attention kernel implementations (v1, v2, and a pure JAX dot-product version).",
                "Includes a temporary workaround (`_maybe_materialize_cache`) for compatibility with the `nnx.bridge.to_linen`."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the PagedAttentionOp module, setting up configuration and creating the paged KV cache.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters (mesh, page sizes, head dimensions, etc.).",
                        "Calculate the shape of the KV pages tensor.",
                        "Initialize `self.key_pages` and `self.value_pages` as `nnx.Cache` objects filled with zeros."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.Cache",
                        "jnp.zeros"
                    ],
                    "notes": [
                        "The `rngs` parameter is required for compatibility with `nnx.bridge.to_linen` but is not used internally."
                    ]
                },
                "_maybe_materialize_cache": {
                    "purpose": "Materializes the KV cache from a `jax.ShapeDtypeStruct` to a concrete `jnp.ndarray` of zeros, a workaround for lazy initialization when using the Linen bridge.",
                    "input": {
                        "shape": "N/A (takes an nnx.Cache object)",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if the `cache.value` is an instance of `jax.ShapeDtypeStruct`.",
                        "If it is, create a new `nnx.Cache` with a zero-initialized tensor of the correct shape and dtype.",
                        "Return the original or the newly created cache."
                    ],
                    "output": {
                        "shape": "N/A (returns an nnx.Cache object)"
                    },
                    "dependencies": [
                        "jax.ShapeDtypeStruct",
                        "nnx.Cache",
                        "jnp.zeros"
                    ],
                    "notes": [
                        "This method is a temporary solution and is intended to be removed when the `to_linen` bridge is no longer needed."
                    ]
                },
                "get_kv_pages": {
                    "purpose": "Retrieves the key and value page caches, ensuring they are materialized and have the correct sharding constraints applied.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `_maybe_materialize_cache` for both `self.key_pages` and `self.value_pages`.",
                        "Apply logical sharding constraints to the materialized cache values using `nn.with_logical_constraint`.",
                        "Return the key and value page caches."
                    ],
                    "output": {
                        "shape": "A tuple of two `nnx.Cache` objects."
                    },
                    "dependencies": [
                        "self._maybe_materialize_cache",
                        "nn.with_logical_constraint"
                    ],
                    "notes": [
                        "The materialization step is a temporary workaround for the Linen bridge."
                    ]
                },
                "pad_qkv": {
                    "purpose": "Pads the query, key, and value tensors along the last dimension to match `self.kv_head_dim_size` if necessary.",
                    "input": {
                        "shape": "Tuple of tensors, each typically [batch_size, sequence_length, num_heads, head_dim].",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Iterate through the input tensors (q, k, v).",
                        "For each tensor, check if its last dimension's size equals `self.kv_head_dim_size`.",
                        "If not, pad it with zeros to match the required size using `jnp.pad`.",
                        "Return the (potentially padded) tensors as a tuple."
                    ],
                    "output": {
                        "shape": "Tuple of tensors, each with shape [batch_size, sequence_length, num_heads, kv_head_dim_size]."
                    },
                    "dependencies": [
                        "jnp.pad"
                    ],
                    "notes": [
                        "This padding is required by the underlying paged attention kernels."
                    ]
                },
                "paged_dot_product_attention_with_max_and_sum": {
                    "purpose": "Computes a standard dot-product attention for the prefill phase as a fallback when not using a custom kernel, returning the attention output, max logits, and sum of exponentials.",
                    "input": {
                        "shape": "query: [b, t, n, d], key: [b, s, n_kv, d], value: [b, s, n_kv, d]",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Reshape query for grouped-query attention.",
                        "Compute attention weights using `jnp.einsum`.",
                        "Apply a causal mask.",
                        "Calculate the max of the masked weights for stable softmax.",
                        "Compute exponentials of weights minus the max.",
                        "Calculate the sum of the exponentials.",
                        "Compute the final attention output using `jnp.einsum`.",
                        "Reshape output, max, and sum to match the original query head dimension."
                    ],
                    "output": {
                        "shape": "A tuple: (attn: [b, t, n, d], local_max: [b, t, n, 1], local_sums: [b, t, n, 1])"
                    },
                    "dependencies": [
                        "jnp.einsum",
                        "jnp.triu",
                        "jnp.where",
                        "jnp.max",
                        "jnp.exp",
                        "jnp.sum"
                    ],
                    "notes": [
                        "This is a pure JAX implementation used for prefill when `_use_kernel_v2` is False."
                    ]
                },
                "paged_attention_v2_prefill": {
                    "purpose": "Applies the v2 ragged paged attention kernel for a single prefill sequence.",
                    "input": {
                        "shape": "query: [1, max_num_tokens, num_kv_heads, head_dim]",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Assert that the batch size of the query is 1.",
                        "Permute dimensions of the key and value page caches.",
                        "Prepare kernel arguments: `cu_q_lens`, `num_seqs`, and squeeze the query batch dimension.",
                        "Call `paged_attention_kernel_v2.ragged_paged_attention`.",
                        "Expand the batch dimension of the result before returning."
                    ],
                    "output": {
                        "shape": "[1, seq_len, n_kv_head, head_dim]"
                    },
                    "dependencies": [
                        "paged_attention_kernel_v2.ragged_paged_attention",
                        "jnp.permute_dims"
                    ],
                    "notes": [
                        "Assumes a batch size of 1 for the prefill operation."
                    ]
                },
                "paged_attention_v2_decode": {
                    "purpose": "Applies the v2 ragged paged attention kernel for a batch of decode sequences.",
                    "input": {
                        "shape": "query: [batch_size, 1, n_kv_head, head_dim]",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Squeeze the sequence length dimension from the query.",
                        "Permute dimensions of the key and value page caches.",
                        "Prepare kernel arguments: `cu_q_lens` and `num_seqs` based on the batch size.",
                        "Call `paged_attention_kernel_v2.ragged_paged_attention`.",
                        "Expand the sequence length dimension of the result before returning."
                    ],
                    "output": {
                        "shape": "[batch_size, 1, n_kv_head, head_dim]"
                    },
                    "dependencies": [
                        "paged_attention_kernel_v2.ragged_paged_attention",
                        "jnp.permute_dims"
                    ],
                    "notes": [
                        "Handles batched decoding where each sequence generates one token."
                    ]
                },
                "paged_attention_v1_decode": {
                    "purpose": "Applies the v1 paged attention kernel for a batch of decode sequences, wrapped in a `shard_map` for SPMD.",
                    "input": {
                        "shape": "query: [batch_size, 1, n_kv_head, head_dim]",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Define input and output partition specs for `shard_map`.",
                        "Define a wrapper function that squeezes the query, calls `paged_attention_kernel.paged_attention`, and expands the result.",
                        "Apply `shard_map` to the wrapper function with the provided inputs (query, caches, page state)."
                    ],
                    "output": {
                        "shape": "[batch_size, 1, n_kv_head, head_dim]"
                    },
                    "dependencies": [
                        "jax.experimental.shard_map.shard_map",
                        "paged_attention_kernel.paged_attention"
                    ],
                    "notes": [
                        "This method is specifically designed for SPMD execution using `shard_map`."
                    ]
                },
                "__call__": {
                    "purpose": "Main entry point that orchestrates the paged attention mechanism, routing to the correct implementation based on the model's operational mode ('prefill' or 'autoregressive').",
                    "input": {
                        "shape": "query: [batch, q_len, heads, head_dim], key: [batch, kv_len, kv_heads, head_dim], value: [batch, kv_len, kv_heads, head_dim]",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Retrieve materialized and sharded KV page caches using `get_kv_pages`.",
                        "Pad query, key, and value tensors using `pad_qkv`.",
                        "If `model_mode` is 'prefill', update prefill pages and call a prefill attention implementation.",
                        "If `model_mode` is 'autoregressive', update decode pages and call a decode attention implementation.",
                        "Return the attention output and potentially max/sum values."
                    ],
                    "output": {
                        "shape": "A tuple: (output_tensor, exponentials_max, exponentials_sum). The last two are `None` in decode mode."
                    },
                    "dependencies": [
                        "self.get_kv_pages",
                        "self.pad_qkv",
                        "self.update_prefill_step_pages",
                        "self.update_decode_step_pages",
                        "self.paged_attention_v2_prefill",
                        "self.paged_dot_product_attention_with_max_and_sum",
                        "self.paged_attention_v2_decode",
                        "self.paged_attention_v1_decode"
                    ],
                    "notes": [
                        "The control flow depends heavily on the `model_mode` string and the global `_use_kernel_v2` flag."
                    ]
                },
                "update_prefill_step_pages": {
                    "purpose": "Updates the KV page caches with new key/value data during the prefill phase.",
                    "input": {
                        "shape": "key/value: [batch_size, seq_len, n_kv_head, head_dim]",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Perform several assertions to check input shapes.",
                        "Squeeze or select the first element of the batch dimension for key and value.",
                        "Transpose and reshape the key and value tensors to match the page layout.",
                        "Overwrite the `key_pages_cache.value` and `value_pages_cache.value` with the reshaped key and value, applying logical constraints."
                    ],
                    "output": {
                        "shape": "N/A (modifies cache state in-place)"
                    },
                    "dependencies": [
                        "jnp.squeeze",
                        "jnp.transpose",
                        "jnp.reshape",
                        "nn.with_logical_constraint"
                    ],
                    "notes": [
                        "This method directly overwrites the cache content, assuming the prefill data fits perfectly into a set of pages."
                    ]
                },
                "update_decode_step_pages": {
                    "purpose": "Updates the KV page caches with a single new key/value token for each sequence in the batch during the decode phase.",
                    "input": {
                        "shape": "key/value: [batch_size, 1, kv_heads, head_dim]",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Reshape and transpose the new key and value tensors.",
                        "Get the target page indices and positions within pages from `page_state`.",
                        "Use `jnp.ndarray.at[...].set(...)` to insert the new key and value data into the correct locations in the `key_pages` and `value_pages` tensors.",
                        "Update the `nnx.Cache` objects with the modified tensors."
                    ],
                    "output": {
                        "shape": "N/A (modifies cache state in-place)"
                    },
                    "dependencies": [
                        "jnp.transpose",
                        "jnp.tile",
                        "jnp.arange"
                    ],
                    "notes": [
                        "This is an efficient scatter update operation to add one token's KV data per sequence to the paged cache."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#MultiPageAsyncCopyDescriptor",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "class MultiPageAsyncCopyDescriptor:\n  \"\"\"Descriptor for async copy of multiple K/V pages from HBM.\"\"\"\n\n  def __init__(\n      self,\n      pages_hbm_ref,  # [total_num_pages, page_size, num_kv_heads_per_blk, head_dim]\n      vmem_buf,  # [num_kv_pages_per_blk, page_size, num_kv_heads_per_blk, head_dim]\n      sem,\n      page_indices_ref,  # i32[max_num_seqs, pages_per_seq]\n      offset,  # [seq_idx, kv_pages_start]\n  ):\n    self._vmem_buf = vmem_buf\n    seq_id, kv_pages_start = offset\n    self._async_copies = [\n        pltpu.make_async_copy(\n            pages_hbm_ref.at[page_indices_ref[seq_id, kv_pages_start + i]],\n            vmem_buf.at[i],\n            sem,\n        )\n        for i in range(vmem_buf.shape[0])\n    ]\n\n  def start(self):\n    \"\"\"Starts the async copies.\"\"\"\n    for async_copy in self._async_copies:\n      async_copy.start()\n\n  def wait(self):\n    for async_copy in self._async_copies:\n      async_copy.wait()\n    return self._vmem_buf",
        "analysis": {
            "functionality": "The `MultiPageAsyncCopyDescriptor` class manages a set of asynchronous memory copy operations. It is designed to transfer multiple key/value (K/V) cache pages from High Bandwidth Memory (HBM) to Vector Memory (VMEM) on a TPU, based on specified page indices for a particular sequence.",
            "usage": "Instantiate the class with references to the source HBM buffer (`pages_hbm_ref`), the destination VMEM buffer (`vmem_buf`), a semaphore (`sem`), page indices (`page_indices_ref`), and an offset (`offset`). Call the `start()` method to begin the non-blocking data transfer. Call the `wait()` method to block until the transfer is complete and retrieve the populated destination buffer."
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#ref_ragged_paged_attention",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def ref_ragged_paged_attention(\n    queries: jax.Array,  # [max_num_batched_tokens, num_q_heads, head_dim]\n    k_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    v_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    kv_lens: jax.Array,  # i32[max_num_seqs]\n    page_indices: jax.Array,  # i32[max_num_seqs, pages_per_seq]\n    cu_q_lens: jax.Array,  # i32[max_num_seqs + 1]\n    num_seqs: jax.Array,  # i32[1],\n    *,\n    sm_scale: float = 1.0,\n    mask_value: float = DEFAULT_MASK_VALUE,\n):\n  \"\"\"Ref ragged paged attention.\"\"\"\n  _, _, num_kv_heads, head_dim = k_pages.shape\n  num_q_heads = queries.shape[1]\n  assert num_q_heads % num_kv_heads == 0\n  num_query_per_kv = num_q_heads // num_kv_heads\n  outputs = []\n  for i in range(num_seqs[0]):\n    q_start = cu_q_lens[i]\n    q_end = cu_q_lens[i + 1]\n    q_len = q_end - q_start\n    kv_len = kv_lens[i]\n    indices = page_indices[i]\n    q = queries[q_start:q_end]\n    k = k_pages[indices, :, :, :].reshape(-1, num_kv_heads, head_dim)[:kv_len]\n    v = v_pages[indices, :, :, :].reshape(-1, num_kv_heads, head_dim)[:kv_len]\n    k = jnp.repeat(k, num_query_per_kv, axis=1)\n    v = jnp.repeat(v, num_query_per_kv, axis=1)\n    attn = jnp.einsum(\"qhd,khd->hqk\", q, k, preferred_element_type=jnp.float32)\n    attn *= sm_scale\n    q_span = (kv_len - q_len) + jax.lax.broadcasted_iota(jnp.int32, attn.shape, 1)\n    kv_span = jax.lax.broadcasted_iota(jnp.int32, attn.shape, 2)\n    attn += jnp.where(q_span < kv_span, mask_value, 0.0)\n    attn = jax.nn.softmax(attn, axis=-1).astype(v.dtype)\n    out = jnp.einsum(\"hqk,khd->qhd\", attn, v).astype(queries.dtype)\n    outputs.append(out)\n\n  return jnp.concatenate(outputs, axis=0)",
        "analysis": {
            "functionality": "Performs a reference implementation of ragged paged attention. It computes attention for a batch of sequences with varying lengths, where keys and values are stored in a paged memory format. This function processes each sequence individually in a loop.",
            "usage": "Call this function with query tensors, paged key/value caches, and metadata arrays that describe the layout of sequences within the batch. It is primarily used as a reference for correctness against more optimized, parallel implementations. Input `queries` has shape [max_num_batched_tokens, num_q_heads, head_dim], `k_pages` and `v_pages` have shape [total_num_pages, page_size, num_kv_heads, head_dim]. Other inputs (`kv_lens`, `page_indices`, `cu_q_lens`, `num_seqs`) provide the necessary indexing and length information. The function returns an output tensor with the same shape as the input `queries`."
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#validate_inputs_on_runtime",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def validate_inputs_on_runtime(\n    q: jax.Array,  # [max_num_batched_tokens, num_q_heads, head_dim]\n    k_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    v_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    kv_lens: jax.Array,  # i32[max_num_seqs]\n    page_indices: jax.Array,  # i32[max_num_seqs, pages_per_seq]\n    cu_q_lens: jax.Array,  # i32[max_num_seqs + 1]\n    num_seqs,  # i32[1]\n):\n  \"\"\"validate inputs on runtime\"\"\"\n  check_inputs_shapes(q, k_pages, v_pages, kv_lens, page_indices, cu_q_lens, num_seqs)\n  max_num_batched_tokens = q.shape[0]\n  page_size = k_pages.shape[1]\n  max_num_seqs, pages_per_seq = page_indices.shape\n  if num_seqs[0] > max_num_seqs:\n    raise ValueError(f\"{num_seqs[0]=} must be less or equal to {max_num_seqs=}\")\n  max_kv_len = jnp.max(kv_lens)\n  min_pages_per_seq = ceil_div(max_kv_len, page_size)\n  if pages_per_seq < min_pages_per_seq:\n    raise ValueError(\n        f\"{pages_per_seq=} must be greater or equal to\" f\" {min_pages_per_seq=} given {max_kv_len=} and {page_size=}.\"\n    )\n  if cu_q_lens[num_seqs[0]] > max_num_batched_tokens:\n    raise ValueError(f\"Total q tokens {cu_q_lens[num_seqs[0]]} must be less or equal to\" f\" {max_num_batched_tokens=}.\")\n  for i in range(num_seqs[0]):\n    q_len = cu_q_lens[i + 1] - cu_q_lens[i]\n    kv_len = kv_lens[i]\n    if q_len > kv_len:\n      raise ValueError(f\"{q_len=} must be less or equal to {kv_len=} at sequence {i}.\")",
        "analysis": {
            "module_type": "runtime_input_validator",
            "purpose": "Performs runtime validation of input tensors for a ragged paged attention kernel, checking for value-based inconsistencies that cannot be caught at compile time.",
            "input": {
                "shape": "q: [max_num_batched_tokens, num_q_heads, head_dim], k_pages: [total_num_pages, page_size, num_kv_heads, head_dim], v_pages: [total_num_pages, page_size, num_kv_heads, head_dim], kv_lens: [max_num_seqs], page_indices: [max_num_seqs, pages_per_seq], cu_q_lens: [max_num_seqs + 1], num_seqs: [1]",
                "dtype": "q, k_pages, v_pages are float types. kv_lens, page_indices, cu_q_lens, num_seqs are int32."
            },
            "processing_steps": [
                "Call `check_inputs_shapes` to validate static shapes and dtypes.",
                "Verify that the dynamic number of sequences (`num_seqs`) does not exceed the maximum capacity (`max_num_seqs`).",
                "Calculate the maximum key-value length (`max_kv_len`) from `kv_lens`.",
                "Ensure the allocated pages per sequence (`pages_per_seq`) is sufficient for the `max_kv_len`.",
                "Check that the total number of query tokens (derived from `cu_q_lens`) does not exceed the query buffer size (`max_num_batched_tokens`).",
                "Iterate through each active sequence to ensure its query length (`q_len`) is not greater than its key-value length (`kv_len`).",
                "Raise a `ValueError` if any check fails."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "check_inputs_shapes",
                "jnp.max",
                "ceil_div"
            ],
            "parameters": {},
            "notes": [
                "This function is intended to be called at runtime, as it validates the values within the input tensors, not just their static shapes.",
                "The function has no return value; successful execution implies the inputs are valid, otherwise it raises a `ValueError`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#check_inputs_shapes",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def check_inputs_shapes(\n    q: jax.Array,  # [max_num_batched_tokens, num_q_heads, head_dim]\n    k_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    v_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    kv_lens: jax.Array,  # i32[max_num_seqs]\n    page_indices: jax.Array,  # i32[max_num_seqs, pages_per_seq]\n    cu_q_lens: jax.Array,  # i32[max_num_seqs + 1]\n    num_seqs,  # i32[1]\n):\n  \"\"\"check shapes of inputs\"\"\"\n  _, num_q_heads, head_dim = q.shape\n  _, _, num_kv_heads, head_dim_k = k_pages.shape\n  max_num_seqs, _ = page_indices.shape\n  if num_seqs.shape != (1,):\n    raise ValueError(f\"{num_seqs.shape=} must be (1,)\")\n  if k_pages.shape != v_pages.shape:\n    raise ValueError(f\"{k_pages.shape=} and {v_pages.shape=} must have the same shape.\")\n  if head_dim_k != head_dim:\n    raise ValueError(f\"Q head_dim {head_dim} must be the same as that of K/V {head_dim_k}.\")\n  if kv_lens.shape != (max_num_seqs,):\n    raise ValueError(\n        f\"Expected {kv_lens.shape=} to be ({max_num_seqs},) where\" \" `max_num_seqs` is `page_indices.shape[0]`.\"\n    )\n  if cu_q_lens.shape != (max_num_seqs + 1,):\n    raise ValueError(\n        f\"Expected {cu_q_lens.shape=} to be ({max_num_seqs + 1},)  where\" \" `max_num_seqs` is `page_indices.shape[0]`.\"\n    )\n  if kv_lens.dtype != jnp.int32 or page_indices.dtype != jnp.int32 or cu_q_lens.dtype != jnp.int32:\n    raise ValueError(\n        \"The dtype of `kv_lens`, `page_indices`, and `cu_q_lens` must be\"\n        f\" int32. Got {kv_lens.dtype=}, {page_indices.dtype=},\"\n        f\" {cu_q_lens.dtype=}.\"\n    )\n  if num_q_heads % num_kv_heads != 0:\n    raise ValueError(f\"{num_q_heads=} must be divisible by {num_kv_heads=}\")",
        "analysis": {
            "functionality": "This function validates the shapes, dtypes, and dimensional relationships of input tensors for a ragged paged attention operation. It raises a `ValueError` with a descriptive message if any of the predefined checks fail.",
            "usage": "Call this function with the query (q), key pages (k_pages), value pages (v_pages), key/value lengths (kv_lens), page indices (page_indices), cumulative query lengths (cu_q_lens), and number of sequences (num_seqs) tensors. The function will either complete successfully (returning None) if the inputs are valid, or it will raise a `ValueError` halting execution if they are not."
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#ragged_paged_attention_kernel",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def ragged_paged_attention_kernel(\n    # Prefetch\n    kv_lens_ref,  # [max_num_seqs]\n    page_indices_ref,  # [max_num_seqs, pages_per_seq]\n    cu_q_lens_ref,  # [max_num_seqs + 1]\n    seq_buf_idx_ref,\n    # TODO(jevinjiang): if OOM in SMEM, consider pack to other scalar refs.\n    num_seqs_ref,\n    # Input\n    q_ref,  # [num_q_per_blk, num_q_heads_per_blk, head_dim]\n    k_pages_hbm_ref,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    v_pages_hbm_ref,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    # Output\n    o_ref,  # [num_q_per_blk, num_q_heads_per_blk, head_dim]\n    # Scratch\n    k_bufs,  # [2, num_kv_pages_per_blk, page_size, num_kv_heads_per_blk, head_dim]\n    v_bufs,  # [2, num_kv_pages_per_blk, page_size, num_kv_heads_per_blk, head_dim]\n    sems,  # [2, 2]\n    l_ref,  # [num_kv_heads_per_blk, num_q_per_blk * num_q_heads_per_kv_head, 128]\n    m_ref,  # [num_kv_heads_per_blk, num_q_per_blk * num_q_heads_per_kv_head, 128]\n    *,\n    sm_scale: float,\n    mask_value: float,\n):\n  \"\"\"ragged paged-attention kernel\"\"\"\n  num_q_per_blk, num_q_heads_per_blk, head_dim = q_ref.shape\n  num_seqs = num_seqs_ref[0]\n  _, num_kv_pages_per_blk, page_size, num_kv_heads_per_blk, _ = k_bufs.shape\n  num_kv_per_blk = num_kv_pages_per_blk * page_size\n  num_q_heads_per_kv_head = num_q_heads_per_blk // num_kv_heads_per_blk\n  heads_blk_idx, q_blk_idx = (\n      pl.program_id(0),\n      pl.program_id(1),\n  )\n  num_heads_blks = pl.num_programs(0)\n  init_seq_idx = seq_buf_idx_ref[0]\n  init_buf_idx = seq_buf_idx_ref[1]\n  q_len_start = q_blk_idx * num_q_per_blk\n  q_len_end = q_len_start + num_q_per_blk\n\n  def create_kv_async_copy_descriptors(heads_blk_idx, seq_idx, kv_blk_idx, buf_idx):\n    offset = (seq_idx, kv_blk_idx * num_kv_pages_per_blk)\n    heads_start = heads_blk_idx * num_kv_heads_per_blk\n    async_copy_k = MultiPageAsyncCopyDescriptor(\n        k_pages_hbm_ref.at[:, :, pl.ds(heads_start, num_kv_heads_per_blk), :],\n        k_bufs.at[buf_idx],\n        sems.at[buf_idx, 0],\n        page_indices_ref,\n        offset,\n    )\n    async_copy_v = MultiPageAsyncCopyDescriptor(\n        v_pages_hbm_ref.at[:, :, pl.ds(heads_start, num_kv_heads_per_blk), :],\n        v_bufs.at[buf_idx],\n        sems.at[buf_idx, 1],\n        page_indices_ref,\n        offset,\n    )\n    return async_copy_k, async_copy_v\n\n  # TODO(jevinjiang): Add these to Mosaic:\n  # 1. Support arbitrary strided load/store for any dtype.\n  # 2. Support arbitrary strided load/store for any last dimension.\n  def strided_load_kv(ref, start, step):\n    if ref.dtype == jnp.float32:\n      return ref[start::step, :]\n    packing = get_dtype_packing(ref.dtype)\n    assert ref.dtype == jnp.bfloat16\n    assert step % packing == 0\n    b_start = start // packing\n    b_offset = start % packing\n    b_step = step // packing\n    b_ref = ref.bitcast(jnp.int32)\n    b = b_ref[b_start::b_step, :]\n    bw = 32 // packing\n    b = jnp.right_shift(b, bw * b_offset)\n    b = jnp.left_shift(b, bw * (packing - 1))\n    return pltpu.bitcast(b, jnp.float32).astype(jnp.bfloat16)\n\n  def fold_on_2nd_minor(vec):\n    assert vec.dtype in (jnp.bfloat16, jnp.float32)\n    assert len(vec.shape) >= 2\n    last_dim = vec.shape[-1]\n    packing = get_dtype_packing(vec.dtype)\n    if vec.shape[-2] % packing != 0:\n      vec = vec.astype(jnp.float32)\n    return vec.reshape(-1, last_dim)\n\n  @pl.when(heads_blk_idx + q_blk_idx == 0)\n  def prefetch_first_kv_blk():\n    async_copy_k, async_copy_v = create_kv_async_copy_descriptors(heads_blk_idx, init_seq_idx, 0, init_buf_idx)\n    async_copy_k.start()\n    async_copy_v.start()\n\n  def is_cur_q_blk_needed(q_states):\n    done, cur_seq_idx, _ = q_states\n    return jnp.logical_and(done == 0, cur_seq_idx < num_seqs)\n\n  def compute_with_cur_q_blk(q_states):\n    done, cur_seq_idx, cur_buf_idx = q_states\n    q_start = cu_q_lens_ref[cur_seq_idx]\n    q_end = cu_q_lens_ref[cur_seq_idx + 1]\n    q_len = q_end - q_start\n    kv_len = kv_lens_ref[cur_seq_idx]\n\n    def get_next_prefetch_ids(heads_blk_idx, cur_seq_idx, kv_blk_idx, cur_buf_idx):\n      next_kv_blk_idx = kv_blk_idx + 1\n      is_last_kv_blk = next_kv_blk_idx * num_kv_per_blk >= kv_len\n      next_kv_blk_idx = lax.select(\n          is_last_kv_blk,\n          0,\n          next_kv_blk_idx,\n      )\n      is_cur_seq_end_in_cur_q_blk = q_end <= q_len_end\n      next_seq_idx = lax.select(\n          is_last_kv_blk,\n          lax.select(is_cur_seq_end_in_cur_q_blk, cur_seq_idx + 1, cur_seq_idx),\n          cur_seq_idx,\n      )\n      is_last_seq = next_seq_idx == num_seqs\n      next_seq_idx = lax.select(\n          is_last_seq,\n          0,\n          next_seq_idx,\n      )\n      next_heads_blk_idx = lax.select(\n          is_last_seq,\n          heads_blk_idx + 1,\n          heads_blk_idx,\n      )\n      next_buf_idx = lax.select(cur_buf_idx == 0, 1, 0)\n      return next_heads_blk_idx, next_seq_idx, next_kv_blk_idx, next_buf_idx\n\n    def flash_attention(\n        q,  # [num_q_per_blk * num_q_heads_per_kv_head, head_dim]\n        k,  # [num_kv_per_blk, head_dim]\n        v,  # [num_kv_per_blk, head_dim]\n        head_l_ref,  # [num_q_per_blk * num_q_heads_per_kv_head, 128]\n        head_m_ref,  # [num_q_per_blk * num_q_heads_per_kv_head, 128]\n        head_o_ref,  # [num_q_per_blk, num_q_heads_per_kv_head, head_dim]\n        *,\n        kv_blk_idx,\n    ):\n      assert q.shape == (\n          num_q_per_blk * num_q_heads_per_kv_head,\n          head_dim,\n      )\n      assert k.shape == (\n          num_kv_per_blk,\n          head_dim,\n      ), f\"{k.shape=}, {(num_kv_per_blk, head_dim)=} {k.dtype=}\"\n      assert v.shape == (num_kv_per_blk, head_dim)\n      assert head_m_ref.shape == (\n          num_q_per_blk * num_q_heads_per_kv_head,\n          128,\n      )\n      assert head_l_ref.shape == (\n          num_q_per_blk * num_q_heads_per_kv_head,\n          128,\n      )\n      assert head_o_ref.shape == (\n          num_q_per_blk,\n          num_q_heads_per_kv_head,\n          head_dim,\n      )\n      kv_len_start = kv_blk_idx * num_kv_per_blk\n\n      def masked_store(ref, val, start, end, group=1):\n        iota = lax.broadcasted_iota(jnp.int32, ref.shape, 0) // group\n        mask = jnp.logical_and(iota >= start, iota < end)\n        pl.store(ref, tuple(slice(None) for _ in ref.shape), val, mask=mask)\n\n      qk = jnp.einsum(\"nd,md->nm\", q, k, preferred_element_type=jnp.float32) * sm_scale\n      store_start = jnp.maximum(q_start - q_len_start, 0)\n      store_end = jnp.minimum(q_end - q_len_start, num_q_per_blk)\n\n      @pl.when(kv_blk_idx == 0)\n      def init_scratch_ref():\n        masked_store(\n            head_m_ref,\n            jnp.full_like(head_m_ref, -jnp.inf),\n            store_start,\n            store_end,\n            num_q_heads_per_kv_head,\n        )\n        masked_store(\n            head_l_ref,\n            jnp.zeros_like(head_l_ref),\n            store_start,\n            store_end,\n            num_q_heads_per_kv_head,\n        )\n        masked_store(\n            head_o_ref,\n            jnp.zeros_like(head_o_ref),\n            store_start,\n            store_end,\n        )\n\n      row_ids = (\n          (kv_len - q_len)\n          + q_len_start\n          - q_start\n          + jax.lax.broadcasted_iota(\n              jnp.int32,\n              (num_q_per_blk * num_q_heads_per_kv_head, num_kv_per_blk),\n              0,\n          )\n          // num_q_heads_per_kv_head\n      )\n      col_ids = kv_len_start + jax.lax.broadcasted_iota(\n          jnp.int32,\n          (num_q_per_blk * num_q_heads_per_kv_head, num_kv_per_blk),\n          1,\n      )\n      causal_mask = row_ids < col_ids\n      qk += jnp.where(causal_mask, mask_value, 0.0)\n      m_curr = jnp.max(qk, axis=1, keepdims=True)\n      s_curr = jnp.exp(qk - m_curr)\n      qkv = jnp.dot(s_curr, v, preferred_element_type=jnp.float32)\n      lm_store_shape = head_m_ref.shape\n      m_curr = jnp.broadcast_to(m_curr, lm_store_shape)\n      l_curr = jnp.broadcast_to(s_curr.sum(axis=1, keepdims=True), lm_store_shape)\n      m_prev = head_m_ref[...]\n      l_prev = head_l_ref[...]\n      m_next = jnp.maximum(m_prev, m_curr)\n      masked_store(head_m_ref, m_next, store_start, store_end, num_q_heads_per_kv_head)\n      alpha = jnp.exp(m_prev - m_next)\n      beta = jnp.exp(m_curr - m_next)\n      l_alpha = alpha * l_prev\n      l_next = l_alpha + beta * l_curr\n      l_next_safe = jnp.where(l_next == 0.0, 1.0, l_next)\n      masked_store(\n          head_l_ref,\n          l_next_safe,\n          store_start,\n          store_end,\n          num_q_heads_per_kv_head,\n      )\n\n      def broadcast_to_shape(arr, shape):\n        if arr.shape == shape:\n          return arr\n        assert len(arr.shape) == len(shape)\n        assert arr.shape[0] == shape[0]\n        assert shape[1] % arr.shape[1] == 0\n        # no-op concatenation.\n        return jnp.concatenate([arr for _ in range(shape[1] // arr.shape[1])], axis=1)\n\n      o_curr = head_o_ref[...].reshape(-1, head_dim)\n      l_alpha = broadcast_to_shape(l_alpha, qkv.shape)\n      beta = broadcast_to_shape(beta, qkv.shape)\n      l_next_safe = broadcast_to_shape(l_next_safe, qkv.shape)\n      out = lax.div(\n          l_alpha * o_curr + beta * qkv,\n          l_next_safe,\n      ).astype(head_o_ref.dtype)\n      masked_store(\n          head_o_ref,\n          out.reshape(head_o_ref.shape),\n          store_start,\n          store_end,\n      )\n\n    def is_valid_kv_blk_in_cur_seq(kv_states):\n      kv_blk_idx, _ = kv_states\n      return kv_blk_idx * num_kv_per_blk < kv_len\n\n    def compute_with_kv_blk_in_cur_seq(kv_states):\n      kv_blk_idx, cur_buf_idx = kv_states\n      next_heads_blk_idx, next_seq_idx, next_kv_blk_idx, next_buf_idx = get_next_prefetch_ids(\n          heads_blk_idx, cur_seq_idx, kv_blk_idx, cur_buf_idx\n      )\n\n      @pl.when(next_heads_blk_idx < num_heads_blks)\n      def prefetch_next_kv_blk():\n        # TODO(jevinjiang): reuse the same buffer if it is already prefetched!\n        # TODO(jevinjiang): only fetch effective dynamic size to hold kv_len and\n        # DMA to fixed size buffer!\n        next_async_copy_k, next_async_copy_v = create_kv_async_copy_descriptors(\n            next_heads_blk_idx, next_seq_idx, next_kv_blk_idx, next_buf_idx\n        )\n        next_async_copy_k.start()\n        next_async_copy_v.start()\n\n      cur_async_copy_k, cur_async_copy_v = create_kv_async_copy_descriptors(\n          heads_blk_idx, cur_seq_idx, kv_blk_idx, cur_buf_idx\n      )\n      kv_to_load_shape = (\n          num_kv_pages_per_blk * page_size * num_kv_heads_per_blk,\n          head_dim,\n      )\n      k_ref = cur_async_copy_k.wait().reshape(kv_to_load_shape)\n      v_ref = cur_async_copy_v.wait().reshape(kv_to_load_shape)\n      for kv_head_idx in range(num_kv_heads_per_blk):\n        q_head_idx = kv_head_idx * num_q_heads_per_kv_head\n        # TODO(jevinjiang): extra handling for packed type that can start at\n        # unaligned position!\n        q = fold_on_2nd_minor(q_ref[:, q_head_idx : q_head_idx + num_q_heads_per_kv_head, :])\n        k = strided_load_kv(k_ref, kv_head_idx, num_kv_heads_per_blk)\n        v = strided_load_kv(v_ref, kv_head_idx, num_kv_heads_per_blk)\n        flash_attention(\n            q,\n            k,\n            v,\n            l_ref.at[kv_head_idx],\n            m_ref.at[kv_head_idx],\n            o_ref.at[:, q_head_idx : q_head_idx + num_q_heads_per_kv_head, :],\n            kv_blk_idx=kv_blk_idx,\n        )\n      return kv_blk_idx + 1, next_buf_idx\n\n    _, next_buf_idx = lax.while_loop(\n        is_valid_kv_blk_in_cur_seq,\n        compute_with_kv_blk_in_cur_seq,\n        (0, cur_buf_idx),  # (kv_blk_idx, buf_idx)\n    )\n    next_seq_idx = lax.select(q_end <= q_len_end, cur_seq_idx + 1, cur_seq_idx)\n    done = lax.select(q_end < q_len_end, done, 1)\n    return done, next_seq_idx, next_buf_idx\n\n  _, seq_idx, buf_idx = lax.while_loop(\n      is_cur_q_blk_needed,\n      compute_with_cur_q_blk,\n      (0, init_seq_idx, init_buf_idx),  # (done, seq_idx, buf_idx)\n  )\n  # Reset seq_idx for next kv_heads_blk if run out of seqs!\n  seq_buf_idx_ref[0] = lax.select(seq_idx < num_seqs, seq_idx, 0)\n  seq_buf_idx_ref[1] = buf_idx",
        "analysis": {
            "functionality": "Performs a ragged paged attention operation as a Pallas kernel on a TPU. It processes blocks of queries against key-value pages fetched from HBM, using an online softmax algorithm (FlashAttention style) and double-buffering to handle variable-length sequences efficiently.",
            "usage": "This function is a low-level Pallas kernel and is not intended to be called directly. It is launched by the `ragged_paged_attention` JIT-compiled function. It takes references to query blocks, paged K/V caches in HBM, and metadata about sequence lengths and page indices. It computes the attention output for its assigned block of queries and writes the result to the output reference `o_ref`."
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#ceil_div",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def ceil_div(a, b):\n  assert b != 0\n  return (a + b - 1) // b",
        "analysis": {
            "module_type": "integer_ceiling_division",
            "purpose": "Computes the integer ceiling of the division of 'a' by 'b' using integer arithmetic.",
            "input": {
                "shape": "N/A",
                "dtype": "integer"
            },
            "processing_steps": [
                "Assert that the divisor 'b' is not zero.",
                "Calculate the result using the formula `(a + b - 1) // b`."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [],
            "parameters": {},
            "notes": [
                "This function implements ceiling division using only integer arithmetic, which is efficient and avoids floating-point operations."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#get_dtype_packing",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def get_dtype_packing(dtype):\n  if dtype == jnp.float32:\n    return 1\n  if dtype == jnp.bfloat16:\n    return 2\n  if dtype == jnp.int8:\n    return 4\n  if dtype == jnp.int4:\n    return 8\n  raise ValueError(f\"Not implemented: unsupported {dtype=}\")",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Returns an integer representing the packing factor for a given JAX numpy data type, indicating how many values of that type can fit into a 32-bit word.",
            "input": {
                "shape": "N/A",
                "dtype": "jax.numpy.dtype"
            },
            "processing_steps": [
                "Check if the input dtype is jnp.float32 and return 1.",
                "Check if the input dtype is jnp.bfloat16 and return 2.",
                "Check if the input dtype is jnp.int8 and return 4.",
                "Check if the input dtype is jnp.int4 and return 8.",
                "If the dtype is not supported, raise a ValueError."
            ],
            "output": {
                "shape": "Scalar integer"
            },
            "dependencies": [
                "jax.numpy as jnp"
            ],
            "parameters": {},
            "notes": [
                "This function is used to determine how data types can be packed for efficient memory access or processing, likely on specialized hardware like TPUs.",
                "The function only supports a specific subset of JAX dtypes."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#get_min_heads_per_blk",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def get_min_heads_per_blk(num_q_heads, num_kv_heads, q_dtype, kv_dtype):\n  \"\"\"get min heads per block\"\"\"\n  q_packing = get_dtype_packing(q_dtype)\n  kv_packing = get_dtype_packing(kv_dtype)\n\n  def can_be_xla_fully_tiled(x, packing):\n    if x % packing != 0:\n      return False\n    x //= packing\n    return x in (1, 2, 4, 8) or x % 8 == 0\n\n  # TODO(jevinjiang): support unaligned number of heads!\n  if not can_be_xla_fully_tiled(num_kv_heads, kv_packing):\n    raise ValueError(f\"Not implemented: {num_kv_heads=} can not be XLA fully tiled.\")\n  assert num_q_heads % num_kv_heads == 0\n  ratio = num_q_heads // num_kv_heads\n  # TODO(jevinjiang): we can choose smaller tiling for packed type if large\n  # second minor tiling is not on.\n  max_kv_tiling = 8 * kv_packing\n  min_kv_heads = max_kv_tiling if num_kv_heads % max_kv_tiling == 0 else num_kv_heads\n  min_q_heads = min_kv_heads * ratio\n  if can_be_xla_fully_tiled(min_q_heads, q_packing):\n    return min_q_heads, min_kv_heads\n  return num_q_heads, num_kv_heads",
        "analysis": {
            "module_type": "attention_head_block_size_calculator",
            "purpose": "Calculates the minimum number of query and key/value heads per processing block that satisfy XLA tiling constraints for efficient computation.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Get data type packing factors for query and key/value dtypes using `get_dtype_packing`.",
                "Define and use a helper function `can_be_xla_fully_tiled` to check if a number of heads is compatible with XLA tiling requirements.",
                "Validate that the input `num_kv_heads` can be fully tiled, raising a ValueError if not.",
                "Calculate the ratio of query heads to key/value heads.",
                "Determine a potential minimum block size for KV heads (`min_kv_heads`) based on a maximum tiling size (`max_kv_tiling`).",
                "Calculate the corresponding minimum block size for Q heads (`min_q_heads`).",
                "If the calculated `min_q_heads` is compatible with XLA tiling, return the calculated minimum block sizes.",
                "Otherwise, return the original total number of heads."
            ],
            "output": {
                "shape": "A tuple of two integers: (min_q_heads_per_block, min_kv_heads_per_block)."
            },
            "dependencies": [
                "get_dtype_packing"
            ],
            "parameters": {
                "num_q_heads": "The total number of query heads.",
                "num_kv_heads": "The total number of key/value heads.",
                "q_dtype": "The data type of the query tensor.",
                "kv_dtype": "The data type of the key/value tensor."
            },
            "notes": [
                "The function is designed to find an optimal block size for attention head computation on hardware with specific tiling constraints, likely TPUs.",
                "It raises a ValueError if the number of KV heads is not compatible with XLA's full tiling capabilities.",
                "It assumes `num_q_heads` is a multiple of `num_kv_heads`.",
                "If a smaller, optimal block size for Q heads doesn't meet tiling constraints, it defaults to using the full number of heads as the block size."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#ragged_paged_attention",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def ragged_paged_attention(\n    q: jax.Array,  # [max_num_batched_tokens, num_q_heads, head_dim]\n    # TODO(jevinjiang): create a write_to_kv_cache kernel!\n    k_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    v_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    kv_lens: jax.Array,  # i32[max_num_seqs]\n    page_indices: jax.Array,  # i32[max_num_seqs, pages_per_seq]\n    cu_q_lens: jax.Array,  # i32[max_num_seqs + 1]\n    num_seqs: jax.Array,  # i32[1]\n    *,\n    sm_scale: float = 1.0,\n    mask_value: float = DEFAULT_MASK_VALUE,\n    num_kv_pages_per_block: int = 16,\n    num_queries_per_block: int = 128,\n    vmem_limit_bytes: int | None = None,\n):\n  \"\"\"Ragged paged attention that supports mixed prefill and decode.\n\n  Args:\n    q: concatenated all sequences' queries.\n    k_pages: paged K cache. Normally in HBM.\n    v_pages: paged V cache. Normally in HBM.\n    kv_lens: padded kv lengths. Only the first num_seqs values are valid.\n    page_indices: the first index indicates which page to use in the kv cache\n      for each sequence. Only the first num_seqs values are valid.\n    cu_q_lens: the cumulative sum of the effective query lengths. Similar to\n      kv_lens, only the first num_seqs+1 values are valid.\n    num_seqs: the dynamic number of sequences.\n    sm_scale: the softmax scale which will be applied to the Q@K^T.\n    mask_value: mask value for causal mask.\n    num_kv_pages_per_block: number of kv pages to be processed in one flash\n      attention block in the pallas kernel.\n    num_queries_per_block: number of kv pages to be processed in one flash\n      attention block in the pallas kernel.\n    vmem_limit_bytes: the vmem limit for the pallas kernel.\n\n  Returns:\n    The output of the attention.\n  \"\"\"\n  check_inputs_shapes(q, k_pages, v_pages, kv_lens, page_indices, cu_q_lens, num_seqs)\n  _, num_q_heads, head_dim = q.shape\n  _, page_size, num_kv_heads, _ = k_pages.shape\n  num_q_per_blk = num_queries_per_block\n  num_kv_pages_per_blk = num_kv_pages_per_block\n  num_q_heads_per_kv_head = num_q_heads // num_kv_heads\n  num_q_blks = ceil_div(cu_q_lens[num_seqs[0]], num_q_per_blk)\n  num_q_heads_per_blk, num_kv_heads_per_blk = get_min_heads_per_blk(num_q_heads, num_kv_heads, q.dtype, k_pages.dtype)\n  assert num_q_heads_per_blk % num_q_heads_per_kv_head == 0\n  num_heads_blks = num_q_heads // num_q_heads_per_blk\n  grid = (num_heads_blks, num_q_blks)\n\n  def q_index_map(heads_blk_idx, q_blk_idx, *_):\n    return (q_blk_idx, heads_blk_idx, 0)\n\n  q_block_spec = pl.BlockSpec(\n      (num_q_per_blk, num_q_heads_per_blk, head_dim),\n      q_index_map,\n  )\n  in_specs = [\n      q_block_spec,\n      pl.BlockSpec(memory_space=pl.MemorySpace.ANY),\n      pl.BlockSpec(memory_space=pl.MemorySpace.ANY),\n  ]\n  out_specs = q_block_spec\n  lm_scratch = pltpu.VMEM(\n      # TODO(jevinjiang): use 128 instead of 1 is due to Mosaic does not support\n      # unaligned slicing!\n      (num_kv_heads_per_blk, num_q_per_blk * num_q_heads_per_kv_head, 128),\n      jnp.float32,\n  )\n  double_buf_scratch = pltpu.VMEM(\n      (\n          2,  # For double buffering during DMA copies.\n          num_kv_pages_per_blk,\n          page_size,\n          num_kv_heads_per_blk,\n          head_dim,\n      ),\n      k_pages.dtype,\n  )\n  scratch_shapes = [\n      double_buf_scratch,  # k_bufs\n      double_buf_scratch,  # v_bufs\n      pltpu.SemaphoreType.DMA((2, 2)),  # [double_buffers, k_sem/v_sem]\n      lm_scratch,  # l_ref\n      lm_scratch,  # m_ref\n  ]\n  scalar_prefetches = (\n      kv_lens,\n      page_indices,\n      cu_q_lens,\n      jnp.array((0, 0), jnp.int32),  # seq_idx, buf_idx\n      num_seqs,\n  )\n  kernel = pl.pallas_call(\n      functools.partial(\n          ragged_paged_attention_kernel,\n          sm_scale=sm_scale,\n          mask_value=mask_value,\n      ),\n      grid_spec=pltpu.PrefetchScalarGridSpec(\n          num_scalar_prefetch=len(scalar_prefetches),\n          in_specs=in_specs,\n          out_specs=out_specs,\n          grid=grid,\n          scratch_shapes=scratch_shapes,\n      ),\n      compiler_params=pltpu.CompilerParams(\n          dimension_semantics=(\n              \"arbitrary\",\n              \"arbitrary\",\n          ),\n          vmem_limit_bytes=vmem_limit_bytes,\n      ),\n      out_shape=jax.ShapeDtypeStruct(shape=q.shape, dtype=jnp.float32),\n      name=\"ragged_paged_attention_kernel\",\n  )\n  # TODO(jevinjiang): Use f32 acc scratch for output! So we only need\n  # to transfer output with desired dtype back to HBM.\n  return kernel(*scalar_prefetches, q, k_pages, v_pages).astype(q.dtype)",
        "analysis": {
            "functionality": "Performs ragged paged attention, supporting mixed prefill and decode workloads, by launching a custom Pallas kernel on TPU.",
            "usage": "This function is JIT-compiled and takes query tensors (`q`), paged key-value caches (`k_pages`, `v_pages`), and metadata tensors (`kv_lens`, `page_indices`, `cu_q_lens`, `num_seqs`) as input. It computes the attention output, which has the same shape as the input query tensor `q`."
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/decode_multi.py#_validate_config",
        "file_path": "src/MaxText/inference/scripts/decode_multi.py",
        "code_block": "def _validate_config(config):\n  \"\"\"Validate configuration settings.\"\"\"\n  assert config.load_full_state_path == \"\", (\n      \"Decode doesn't operate on full states! Convert to parameter checkpoint first.\"\n      \"Using generate_param_only_checkpoint.\"\n  )\n  assert (\n      0 < _INITIAL_PREFILL_STREAMS <= _NUM_STREAMS\n  ), f\"_INITIAL_PREFILL_STREAMS ({_INITIAL_PREFILL_STREAMS}) must be > 0 and <= _NUM_STREAMS ({_NUM_STREAMS})\"",
        "analysis": {
            "module_type": "validation_function",
            "purpose": "Validates specific configuration settings for the interleaved inference script, raising an error if conditions are not met.",
            "input": {
                "shape": "N/A",
                "dtype": "pyconfig.Config object"
            },
            "processing_steps": [
                "Assert that `config.load_full_state_path` is an empty string, as decoding operates on parameter checkpoints, not full states.",
                "Assert that the number of initial prefill streams is greater than 0 and less than or equal to the total number of streams."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "_INITIAL_PREFILL_STREAMS",
                "_NUM_STREAMS"
            ],
            "parameters": {
                "config.load_full_state_path": "The path to a full training state checkpoint. This must be empty for the script to run.",
                "_INITIAL_PREFILL_STREAMS": "A global constant defining how many streams to prefill before starting generation.",
                "_NUM_STREAMS": "A global constant defining the total number of streams to process."
            },
            "notes": [
                "This function does not return a value. It will raise an `AssertionError` if a validation check fails, halting execution."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/decode_multi.py#main",
        "file_path": "src/MaxText/inference/scripts/decode_multi.py",
        "code_block": "def main(argv: Sequence[str]) -> None:\n  \"\"\"Main function to run interleaved inference.\"\"\"\n  jax.config.update(\"jax_default_prng_impl\", \"unsafe_rbg\")\n  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"0\"\n\n  config = pyconfig.initialize(argv)\n  _validate_config(config)\n  max_utils.print_system_information()\n\n  engine = maxengine.MaxEngine(config)\n  rng = jax.random.PRNGKey(1234)\n  rng, rng_load_params = jax.random.split(rng)\n  params = engine.load_params(rng=rng_load_params)\n\n  text = config.prompt\n  metadata = engine.get_tokenizer()\n  tokenizer_model = engine.build_tokenizer(metadata)\n  tokens, true_length = tokenizer_model.encode(text, is_bos=True, prefill_lengths=[config.max_prefill_predict_length])\n  assert true_length <= config.max_prefill_predict_length, \"Prompt too long for prefill length\"\n\n  batch_size = int(config.per_device_batch_size * jax.device_count())\n  assert (\n      0 < _NUM_STREAMS <= batch_size\n  ), f\"The number of streams {_NUM_STREAMS} must be > 0 and <= batch size {batch_size}\"\n\n  # Initialize decode state\n  rng, rng_init_decode = jax.random.split(rng)\n  decode_state = engine.init_decode_state(rng=rng_init_decode)\n  print(\"Initial decode state initialized.\")\n\n  # Keep track of results per stream (slot)\n  streams_results: dict[int, list[int]] = {i: [] for i in range(_NUM_STREAMS)}\n  streams_active: list[bool] = [False] * _NUM_STREAMS  # Track which slots are active\n  streams_finished: list[bool] = [False] * _NUM_STREAMS  # Track finished streams\n  streams_prefilled_count = 0\n  streams_inserted_count = 0\n\n  # --- Initial Prefill Phase ---\n  print(f\"Starting initial prefill for {_INITIAL_PREFILL_STREAMS} streams...\")\n  prefill_results_to_insert = {}  # Store prefill results before inserting\n  for i in range(_INITIAL_PREFILL_STREAMS):\n    slot_idx = i\n    print(f\"  Prefilling stream for slot {slot_idx}...\")\n    rng, rng_prefill = jax.random.split(rng)\n    request_id = uuid.uuid4()\n    prefill_result, first_token = engine.prefill(\n        params=params,\n        padded_tokens=tokens,\n        true_length=true_length,\n        rng=rng_prefill,\n        slot=slot_idx,\n        request_id=request_id,\n    )\n    prefill_results_to_insert[slot_idx] = prefill_result\n    streams_results[slot_idx].append(first_token.get_result_at_slot(0).tokens.item())\n    streams_prefilled_count += 1\n    print(f\"After prefill stream {slot_idx}\")\n\n  # --- Insert Initial Prefills ---\n  print(\"Inserting initial prefill results...\")\n  for slot_idx, prefill_result in prefill_results_to_insert.items():\n    request_id = uuid.uuid4()\n    decode_state = engine.insert(\n        prefix=prefill_result,\n        decode_state=decode_state,\n        slot=slot_idx,\n        request_id=request_id,  # Pass request_id\n    )\n    streams_active[slot_idx] = True  # Mark stream as active\n    streams_inserted_count += 1\n    print(f\"  Inserted prefill for slot {slot_idx}\")\n\n  print(\"Starting interleaved generation loop...\")\n  total_steps = config.max_target_length - config.max_prefill_predict_length\n  for step in range(total_steps):\n    print(f\"\\n--- Step {step + 1} / {total_steps} ---\")\n\n    # Generate step for all active streams\n    active_stream_indices = [i for i, active in enumerate(streams_active) if active and not streams_finished[i]]\n    if active_stream_indices:\n      print(f\"  Generating for active slots: {active_stream_indices}\")\n      rng, rng_generate = jax.random.split(rng)\n      decode_state, sampled_tokens = engine.generate(params, decode_state, rng=rng_generate)\n\n      # Store the generated token and check for finished streams\n      for slot_idx in active_stream_indices:\n        # Check if the stream finished this step\n        current_len = config.max_prefill_predict_length + step + 1  # Includes prefill + current step\n        finished_this_step = False\n        if current_len >= config.max_target_length:\n          print(f\"    Stream in slot {slot_idx} reached max target length.\")\n          streams_finished[slot_idx] = True\n          streams_active[slot_idx] = False\n          finished_this_step = True\n\n        # Store token if it wasn't already finished before this step or if it finished on this step\n        if not streams_finished[slot_idx] or finished_this_step:\n          # Ensure we don't try to access results for a slot that might not exist\n          if slot_idx < sampled_tokens.data.shape[0]:\n            token_for_slot = sampled_tokens.get_result_at_slot(slot_idx).tokens.item()\n            streams_results[slot_idx].append(token_for_slot)\n          else:\n            print(f\"Warning: Tried to get token for slot {slot_idx}, but batch size seems smaller.\")\n\n        # Call release_pages if finished this step\n        if finished_this_step:\n          print(f\"    Calling engine to release pages for finished slot {slot_idx}...\")\n          engine.release_pages(slot=slot_idx)\n\n    else:\n      print(\"  No active streams to generate for.\")\n\n    # 2. Check if all streams are finished (can exit loop early)\n    if all(streams_finished):\n      print(\"\\nAll streams finished generation.\")\n      break\n\n    # 3. Prefill and Insert new streams if capacity allows\n    num_active_not_finished = sum(1 for i in range(_NUM_STREAMS) if streams_active[i] and not streams_finished[i])\n    available_slots = batch_size - num_active_not_finished\n    can_prefill_more = streams_prefilled_count < _NUM_STREAMS\n\n    if can_prefill_more and available_slots > 0:\n      try:\n        next_available_slot = streams_active.index(False)\n        print(f\"  Prefilling new stream for slot {next_available_slot}...\")\n        rng, rng_prefill = jax.random.split(rng)\n        request_id = uuid.uuid4()\n        prefill_result, first_token = engine.prefill(\n            params=params,\n            padded_tokens=tokens,\n            true_length=true_length,\n            rng=rng_prefill,\n            slot=next_available_slot,\n            request_id=request_id,\n        )\n        streams_prefilled_count += 1\n\n        # Insert the new prefill\n        print(f\"  Inserting new stream into slot {next_available_slot}...\")\n        request_id_insert = uuid.uuid4()\n        decode_state = engine.insert(\n            prefix=prefill_result,\n            decode_state=decode_state,\n            slot=next_available_slot,\n            request_id=request_id_insert,\n        )\n        streams_active[next_available_slot] = True\n        streams_inserted_count += 1\n        streams_results[next_available_slot].append(first_token.get_result_at_slot(0).tokens.item())\n\n      except ValueError:\n        print(\"  Warning: Available slots detected but couldn't find an inactive one.\")\n    elif can_prefill_more:\n      print(\"  Generate step finished, but no available slots to prefill new stream.\")\n    else:\n      print(\"  Generate step finished, all streams already prefilled.\")\n\n  print(\"\\n--- Final Results ---\")\n  for i in range(_NUM_STREAMS):\n    if streams_results[i]:\n      output = tokenizer_model.decode(streams_results[i])\n      print(f\"Stream {i}: Input=`{text}` -> Output=`{output}`\")\n\n      if i == 0:  # Check first stream as an example\n        assert output.startswith(\n            config.autoregressive_decode_assert\n        ), f\"Stream {i} generated text mismatch: `{output}` vs expected start `{config.autoregressive_decode_assert}`\"\n    else:\n      print(f\"Stream {i}: Was not activated.\")",
        "analysis": {
            "module_type": "interleaved_inference_runner",
            "purpose": "Runs a multi-stream, interleaved inference loop, demonstrating continuous batching by prefilling new requests while generating tokens for active requests.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes configuration, JAX, and the MaxEngine from the provided command-line arguments.",
                "Loads model parameters and builds the tokenizer.",
                "Tokenizes the input prompt specified in the configuration.",
                "Initializes the engine's decode state.",
                "Prefills an initial number of streams using `engine.prefill`.",
                "Inserts the KV caches from the prefilled streams into the main decode state using `engine.insert`.",
                "Enters a generation loop that runs for `max_target_length - max_prefill_predict_length` steps.",
                "Inside the loop, it generates one token for all active streams using `engine.generate`.",
                "It checks for streams that have reached their target length, marks them as finished, and releases their resources using `engine.release_pages`.",
                "If there are available slots and more streams to process, it pre-fills and inserts a new stream into the batch.",
                "After the loop finishes, it decodes the generated token sequences for all streams and prints them.",
                "Finally, it asserts that the output of the first stream matches an expected value from the config."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "jax",
                "maxengine.MaxEngine",
                "pyconfig",
                "max_utils",
                "_validate_config",
                "uuid"
            ],
            "parameters": {
                "argv": "A sequence of strings representing command-line arguments for configuration.",
                "config.prompt": "The input text prompt for generation.",
                "config.max_prefill_predict_length": "The maximum length of the input prompt.",
                "config.max_target_length": "The total desired output length (prompt + generated tokens).",
                "config.per_device_batch_size": "Batch size per device, used to calculate the total available slots for streams.",
                "config.autoregressive_decode_assert": "A string used to validate the beginning of the generated output for the first stream."
            },
            "notes": [
                "This function simulates a server environment with continuous batching, where new requests (streams) are added as old ones complete.",
                "It manages the lifecycle of multiple generation streams: prefilling, inserting, generating, and releasing.",
                "The total number of streams and the number of initial streams are controlled by the global constants `_NUM_STREAMS` and `_INITIAL_PREFILL_STREAMS`.",
                "The function is intended to be the main entry point for a command-line application, executed via `absl.app.run(main)`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/sharding_utils.py#latency_bound_comms",
        "file_path": "src/MaxText/inference/scripts/sharding_utils.py",
        "code_block": "def latency_bound_comms(comm: float, latency=1e-6):\n  return max(comm, latency)",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Calculates communication time, ensuring it is at least a minimum latency value.",
            "input": {
                "shape": "N/A",
                "dtype": "float"
            },
            "processing_steps": [
                "Calculate the maximum value between the input communication time (`comm`) and the minimum latency (`latency`)."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [],
            "parameters": {
                "comm": "The calculated communication time based on bandwidth and data volume.",
                "latency": "The minimum latency overhead for a communication operation, defaults to 1e-6."
            },
            "notes": [
                "This function models the physical constraint that any communication operation has a non-zero latency, regardless of the data volume."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/sharding_utils.py#calculate_matmul_resources",
        "file_path": "src/MaxText/inference/scripts/sharding_utils.py",
        "code_block": "def calculate_matmul_resources(\n    activations_shape: tuple[int, ...],\n    weights_shape: tuple[int, ...],\n    ici_bandwidth: float,\n    peak_flops: float,\n    sD: int = 1,\n    sK: int = 1,\n    sW: int = 1,\n    sF: int = 1,\n    sE: int = 1,\n    activation_size_bytes: int = 2,\n    weight_size_bytes: int = 2,\n    ici_latency: float = 1e-6,\n    all_gather_axes: Sequence[str] = tuple(),\n    debug=True,\n) -> dict[str, float]:\n  \"\"\"\n  Calculates estimated FLOPs, communication volume, and memory for a distributed matrix multiplication.\n\n  The multiplication is A @ W.\n  A (activations) has shape (M, K).\n  W (weights) has shape (G, K, F).\n\n  Sharding strategy assumed:\n  - Data Parallelism: `sD` shards the M dim of A.\n  - Embedding Parallelism: `sK` shards on the embedding dim of A.\n  - Tensor Parallelism for W dim: `sK` shards the W dimension of W.\n  - Tensor Parallelism for F dim: `sF` shards the second weight dim of W.\n\n  Args:\n      activations_shape: Shape of the activations tensor (M, K).\n      weights_shape: Shape of the weights tensor (G, K, F).\n                     G is the number of groups if this is a GMM (e.g in MoE layer).\n      sD: Number of data parallel shards (sD). Must be >= 1.\n      sK: Sharding factor for the activation embedding dimension.\n      sW: Sharding factor for the first weight dimension.\n      sF: Sharding factor for the second weight dimension.\n      sE: Sharding factor to split up expert weights.\n      activation_size_bytes: Size of a single element in bytes for the activations.\n      weight_size_bytes: Size of a single element in bytes for the weights.\n      ici_latency: The latency overhead of communicating between TPUs.\n      all_gather_axes: Optional additional output axes that need to be all-gathered (e.g. \"M\", \"F\").\n      debug: Whether to print intermediate resource calculations.\n\n  Returns:\n      A dictionary with keys:\n          \"t_flops\": Estimated FLOPs latency.\n          \"t_comms\": Estimated communication latency.\n          \"memory\": Estimated memory footprint per device for storing\n                                   local shards of activations, weights, and output (bytes).\n  \"\"\"\n\n  M, K_act = activations_shape[0], activations_shape[-1]\n  # Intermediate activation shape\n  I = np.prod(np.array(activations_shape[1:-1]))\n  if len(weights_shape) == 3:\n    G, K_w, F = weights_shape\n  elif len(weights_shape) == 2:\n    K_w, F = weights_shape\n    G = 1\n  else:\n    raise ValueError(f\"weights_shape={weights_shape} is not supported!.\")\n\n  def _gather_dim_to_shard():\n    # # Used to map all-gather arguments to the respective shardings.\n    return {\"D\": sD, \"K\": sK, \"W\": sW, \"F\": sF, \"E\": sE}\n\n  gather_dim_to_shard = _gather_dim_to_shard()\n\n  def _validate_shardings_and_shapes():\n    if not (sD >= 1 and sK >= 1 and sW >= 1 and sF >= 1 and sE >= 1):\n      raise ValueError(\"All sharding amounts must be >= 1.\")\n    if sK > 1 and sF > 1:\n      raise ValueError(\"Cannot have both sK & sF > 1!\")\n    if K_act != K_w:\n      raise ValueError(f\"K dimension of activations ({K_act}) must match K dimension of weights ({K_w})\")\n    if sK > 1 and sW > 1 and sK != sW:\n      raise ValueError(\"Sharding amounts between embedding dim and first weight matricx dim are different!.\")\n    # Warnings for non-divisibility. Calculations proceed with float division,\n    # implying an average or approximation if not perfectly divisible.\n    if M % sD != 0:\n      print(\n          f\"Warning: Activations M dimension ({M}) is not perfectly divisible by sharding amount {sD}.\",\n          \"Results are approximate.\",\n      )\n    if K_act % sK != 0:\n      print(\n          f\"Warning: Common K dimension ({K_act}) is not perfectly divisible by sharding amount {sK}.\",\n          \"Results are approximate.\",\n      )\n    if K_w % sW != 0:\n      print(\n          f\"Warning: Common W dimension ({K_w}) is not perfectly divisible by sharding amount {sW}. Results are approximate.\"\n      )\n    if F % sF != 0:\n      print(\n          f\"Warning: Weights F dimension ({F}) is not perfectly divisible by sharding amount {sF}. Results are approximate.\"\n      )\n    if G % sE != 0:\n      print(\n          f\"Warning: Experts G dimension ({G}) is not perfectly divisible by sharding amount {sE}. Results are approximate.\"\n      )\n\n  _validate_shardings_and_shapes()\n  K = K_act\n\n  # Implied all-gather flags\n  is_fsdp_act = sK > 1 and sW == 1\n  is_fsdp_weight = sK == 1 and sW > 1\n\n  # Local device dimensions\n  local_M_dim = M // sD\n  local_K_dim = K // sK\n  local_W_dim = K // sW\n  local_G_dim = G // sE\n  local_F_dim = F // sF\n\n  # 1. Total FLOPs\n  # For A(M,K) @ W(K,F), FLOPs = 2 * M * K * F\n  total_flops = 2.0 * np.prod(activations_shape) * G * F / (sF * sE * sD * sK * sW)\n  if debug:\n    print(f\"Total GFlops = {total_flops/1e9}\")\n  if is_fsdp_act:\n    total_flops *= sK\n    if debug:\n      print(f\"Total GFlops after activation all-gather = {total_flops/1e9}\")\n  elif is_fsdp_weight:\n    total_flops *= sW\n    if debug:\n      print(f\"Total GFlops after weights all-gather = {total_flops/1e9}\")\n  t_flops = total_flops / peak_flops\n\n  # 2. Memory per device\n  # A_local: (M/sD, K/sK)\n  # W_local: (G/gE, K/sK, N/sF)\n  # Out_local: (M/sD, N/sF) (buffer for local output)\n  mem_activations_bytes = local_M_dim * I * local_K_dim * activation_size_bytes\n  mem_weights_bytes = local_G_dim * local_W_dim * local_F_dim * weight_size_bytes\n  if debug:\n    print(f\"Activation memory (GB): {mem_activations_bytes/1e9}\")\n    print(f\"Weights memory (GB): {mem_weights_bytes/1e9}\")\n  # All-gather\n  if is_fsdp_act:\n    mem_activations_bytes *= sK\n    if debug:\n      print(f\"Activation memory (GB) after all-gather: {mem_activations_bytes/1e9}\")\n  elif is_fsdp_weight:\n    mem_weights_bytes *= sW\n    if debug:\n      print(f\"Weight memory (GB) after all-gather: {mem_weights_bytes/1e9}\")\n\n  local_output_bytes = local_M_dim * I * local_G_dim * local_F_dim * max(activation_size_bytes, weight_size_bytes)\n  if debug:\n    print(f\"Output memory (GB): {local_output_bytes/1e9}\")\n\n  gathered_output_bytes = local_output_bytes * np.prod([gather_dim_to_shard[axis] for axis in all_gather_axes])\n  if debug:\n    print(f\"Output memory (GB) after additional axes gathers: {gathered_output_bytes/1e9}\")\n  memory_per_TPU_bytes = mem_activations_bytes + mem_weights_bytes + gathered_output_bytes\n  if debug:\n    print(f\"Total memory (GB): {memory_per_TPU_bytes/1e9}\")\n\n  # 3. Communication Volume per TPU\n  t_comms = 0.0\n\n  # For FSDP-style comms, all-gather the tensor.\n  if is_fsdp_act:\n    communication_volume_per_TPU_bytes = np.prod(np.array(activations_shape)) / sK * activation_size_bytes\n    t_comms += latency_bound_comms(communication_volume_per_TPU_bytes / ici_bandwidth, ici_latency) * (sK - 1)\n    if debug:\n      print(f\"Per-TPU comms for activation all-gather (GB): {communication_volume_per_TPU_bytes/1e9}\")\n\n  elif is_fsdp_weight:\n    communication_volume_per_TPU_bytes = np.prod(np.array(weights_shape)) / sW * weight_size_bytes\n    t_comms += latency_bound_comms(communication_volume_per_TPU_bytes / ici_bandwidth, ici_latency) * (sW - 1)\n    if debug:\n      print(f\"Per-TPU comms for weights all-gather (GB): {communication_volume_per_TPU_bytes/1e9}\")\n\n  elif sK > 1 and sW > 1:\n    # Perform reduce-scatter on the output.\n    t_comms = latency_bound_comms(local_output_bytes / ici_bandwidth, ici_latency) * (sK - 1)\n    if debug:\n      print(f\"Per-TPU comms for all-reduce (GB): {local_output_bytes/1e9}\")\n\n  # All-to-all on the output during expert parallelism (assuming equal loads. i.e. 1/4 * comms(all-gather))\n  if sE > 1:\n    t_comms += latency_bound_comms(local_output_bytes / ici_bandwidth, ici_latency) * (sE - 1) / 4\n    if debug:\n      print(f\"Per-TPU comms for all-to-all (GB): {local_output_bytes/1e9}\")\n\n  for axis in all_gather_axes:\n    current_output_bytes = local_output_bytes\n    current_sharding = gather_dim_to_shard[axis]\n    t_comms += latency_bound_comms(current_output_bytes / ici_bandwidth, ici_latency) * (current_sharding - 1)\n    if debug:\n      print(f\"Per-TPU comms for axis {axis} all-gather (GB): {current_output_bytes/1e9}\")\n    current_output_bytes *= current_sharding\n\n  return {\n      \"t_flops\": t_flops,\n      \"t_comms\": t_comms,\n      \"memory_per_TPU_bytes\": memory_per_TPU_bytes,\n  }",
        "analysis": {
            "module_type": "distributed_matmul_resource_calculator",
            "purpose": "Calculates the estimated latency from FLOPs, communication, and the per-device memory footprint for a distributed matrix multiplication based on various sharding strategies.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Parse and validate input tensor shapes and sharding parameters.",
                "Determine local device tensor dimensions based on the provided sharding factors (sD, sK, sW, sF, sE).",
                "Calculate the total FLOPs per device, accounting for potential all-gathers in FSDP-style parallelism.",
                "Estimate FLOPs latency (t_flops) by dividing total FLOPs by peak_flops.",
                "Calculate the per-device memory required for local shards of activations, weights, and output, including memory expansion from all-gathers.",
                "Estimate communication latency (t_comms) based on the sharding strategy (FSDP all-gather, reduce-scatter, expert parallelism all-to-all, and optional final all-gathers).",
                "Return a dictionary containing the calculated t_flops, t_comms, and memory_per_TPU_bytes."
            ],
            "output": {
                "shape": "A dictionary with keys 't_flops', 't_comms', and 'memory_per_TPU_bytes'."
            },
            "dependencies": [
                "numpy",
                "latency_bound_comms"
            ],
            "parameters": {
                "activations_shape": "Shape of the input activation tensor, typically (M, ..., K).",
                "weights_shape": "Shape of the weight tensor, typically (G, K, F) for MoE or (K, F).",
                "ici_bandwidth": "The inter-chip interconnect bandwidth in bytes/second.",
                "peak_flops": "The peak floating-point operations per second of a single device.",
                "sD": "The number of data parallel shards.",
                "sK": "The sharding factor for the activation's embedding dimension (K).",
                "sW": "The sharding factor for the weight's first dimension (K).",
                "sF": "The sharding factor for the weight's second dimension (F).",
                "sE": "The sharding factor for the number of experts (G).",
                "all_gather_axes": "A sequence of dimension names ('D', 'K', 'W', 'F', 'E') indicating which dimensions of the output tensor should be all-gathered."
            },
            "notes": [
                "The function models a matrix multiplication A @ W, where A is activations and W is weights.",
                "It handles different parallelism strategies including Data Parallelism (DP), Fully Sharded Data Parallelism (FSDP), Tensor Parallelism (TP), and Expert Parallelism (EP).",
                "The function raises ValueError for unsupported or conflicting sharding configurations (e.g., sharding both K and F dimensions simultaneously).",
                "It prints warnings if tensor dimensions are not perfectly divisible by their corresponding sharding factors, but proceeds with float division for an approximate result."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/sharding_utils.py#plot_sharding_scheme_comparison",
        "file_path": "src/MaxText/inference/scripts/sharding_utils.py",
        "code_block": "def plot_sharding_scheme_comparison(\n    calc_resource_func,\n    activations_shape,\n    weights_shape,\n    sharding_schemes: list[dict],\n):\n  \"\"\"\n  Generates plots comparing different sharding schemes:\n  1. Communication latency vs. FLOPs latency\n  2. Communication latency / memory per device\n  3. Memory & Communication Latency\n\n  Args:\n      activations_shape: Shape of the activations tensor (M, K).\n      weights_shape: Shape of the weights tensor (G, K, F).\n      sharding_schemes: A list of dictionaries. Each dictionary must contain:\n          - \"label\": A string label for the scheme (e.g., \"DP=8\").\n          - \"shard_settings\": A dictionary with sharding parameters used for calc_resource_func().\n          E.g:\n          [\n              {\n                  \"label\": \"DP=8\", # Pure Data Parallelism\n                  \"shard_settings\": {\n                      \"sD\": 8,\n                      \"all_gather_axes\": (\"D\",)\n                  }\n              },\n          ]\n      element_size_bytes: Size of a single element in bytes.\n  \"\"\"\n  results = []\n  valid_schemes_labels = []\n\n  print(\"Calculating resources for sharding schemes...\")\n  for scheme in sharding_schemes:\n    label = scheme.get(\"label\", \"Unknown Scheme\")\n    shard_settings = scheme.get(\"shard_settings\")\n\n    print(f\"\\n--- Scheme: {label} ---\")\n    try:\n      # Clear previous warnings for divisibility for cleaner output per iteration\n      with warnings.catch_warnings(record=True) as caught_warnings:\n        del caught_warnings\n        warnings.simplefilter(\"always\")  # Catch all warnings\n\n        # Call the resource calculation function\n        res = calc_resource_func(activations_shape, weights_shape, **shard_settings)\n        print(\"Workload stats:\\n\")\n        pprint.PrettyPrinter(indent=4).pprint(res)\n\n      results.append(res)\n      valid_schemes_labels.append(label)\n    except ValueError as e:\n      print(f\"Error calculating resources for scheme '{label}': {e}. Skipping.\")\n    except (TypeError, KeyError, ZeroDivisionError, AttributeError) as e:\n      print(f\"An unexpected error occurred for scheme '{label}': {e}. Skipping.\")\n\n  if not results:\n    print(\"No valid data points generated. Cannot create plots.\")\n    return\n\n  # Extract data for plotting\n  t_flops_list = np.array([r[\"t_flops\"] for r in results])\n  t_comms_list = np.array([r[\"t_comms\"] for r in results])\n  mem_list = np.array([r[\"memory_per_TPU_bytes\"] for r in results]) / (1024**3)  # GB\n  title_suffix_context = f\": A{activations_shape} @ W{weights_shape}\"\n  num_schemes = len(valid_schemes_labels)  # Number of successfully processed schemes\n  colors = plt.cm.viridis(np.linspace(0, 1, num_schemes)) if num_schemes > 0 else []\n\n  # Calculate FLOPs/Communication ratio\n  flops_per_comm_ratio = np.zeros(num_schemes)\n  has_infinite_ratio = [False] * num_schemes\n  for i in range(num_schemes):\n    if t_comms_list[i] > 1e-9:  # Threshold to avoid near-zero division issues\n      flops_per_comm_ratio[i] = t_flops_list[i] / t_comms_list[i]\n    elif t_flops_list[i] > 1e-9:  # Positive FLOPs and zero/tiny communication\n      flops_per_comm_ratio[i] = np.inf\n      has_infinite_ratio[i] = True\n    else:  # Zero FLOPs and zero/tiny communication\n      flops_per_comm_ratio[i] = 0\n\n  finite_ratios = flops_per_comm_ratio[np.isfinite(flops_per_comm_ratio)]\n  placeholder_for_inf = 0\n  if finite_ratios.size > 0:\n    placeholder_for_inf = np.max(finite_ratios) * 1.5 if np.max(finite_ratios) > 0 else 1000\n  elif np.any(has_infinite_ratio):\n    placeholder_for_inf = 1000\n\n  plot_ratios = np.array(\n      [placeholder_for_inf if r_inf else r_val for r_val, r_inf in zip(flops_per_comm_ratio, has_infinite_ratio)]\n  )\n  plot_ratios = np.nan_to_num(plot_ratios, nan=0.0, posinf=placeholder_for_inf, neginf=-placeholder_for_inf)\n\n  # --- Create Plots ---\n  categorical_x = np.arange(num_schemes)  # For categorical x-axis\n\n  # Plot 1: FLOPs & Communication (Grouped Bar Plot)\n  grouped_bar_width_fc = 0.35\n  fig_flops_comm_grouped, ax_flops_comm_grouped = plt.subplots(figsize=(max(10, num_schemes * 1.7), 7))\n\n  rects_flops = ax_flops_comm_grouped.bar(\n      categorical_x - grouped_bar_width_fc / 2,\n      t_flops_list,\n      grouped_bar_width_fc,\n      label=\"T_flops\",\n      color=\"mediumseagreen\",\n  )\n  rects_comms_grouped = ax_flops_comm_grouped.bar(\n      categorical_x + grouped_bar_width_fc / 2, t_comms_list, grouped_bar_width_fc, label=\"T_comms\", color=\"deepskyblue\"\n  )\n\n  ax_flops_comm_grouped.set_xlabel(\"Sharding Scheme\")\n  ax_flops_comm_grouped.set_ylabel(\"Seconds\")\n  ax_flops_comm_grouped.set_title(f\"T_flops & T_comms by Sharding Scheme{title_suffix_context}\", fontsize=14)\n  ax_flops_comm_grouped.set_xticks(categorical_x)\n  ax_flops_comm_grouped.set_xticklabels(valid_schemes_labels, rotation=45, ha=\"right\", fontsize=10)\n  if num_schemes > 0:\n    ax_flops_comm_grouped.legend(fontsize=10)\n\n  ax_flops_comm_grouped.bar_label(rects_flops, padding=3, fmt=\"%.2e\", fontsize=9)\n  ax_flops_comm_grouped.bar_label(rects_comms_grouped, padding=3, fmt=\"%.2e\", fontsize=9)\n\n  ax_flops_comm_grouped.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n\n  max_y_val_fc = 0\n  if t_flops_list.size > 0:\n    max_y_val_fc = max(max_y_val_fc, np.max(t_flops_list))\n  if t_comms_list.size > 0:\n    max_y_val_fc = max(max_y_val_fc, np.max(t_comms_list))\n  print(f\"max_y_val_fc = {max_y_val_fc}\")\n  ax_flops_comm_grouped.set_ylim(0, max_y_val_fc * 1.15)\n\n  fig_flops_comm_grouped.tight_layout()\n  plt.show()\n\n  # Plot 2: FLOPs/Communication Ratio\n  fig_ratio, ax_ratio = plt.subplots(figsize=(max(10, num_schemes * 1.1), 7))\n\n  bars_ratio = ax_ratio.bar(categorical_x, plot_ratios, width=0.6, color=colors, alpha=0.9)\n\n  ax_ratio.set_xlabel(\"Sharding Scheme\")\n  ax_ratio.set_ylabel(\"T_flops / T_comms\")\n  ax_ratio.set_title(f\"Roofline (T_flops vs. T_comms) for {title_suffix_context}\", fontsize=14)\n  ax_ratio.set_xticks(categorical_x)\n  ax_ratio.set_xticklabels(valid_schemes_labels, rotation=45, ha=\"right\", fontsize=10)\n  ax_ratio.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n\n  for i, bar in enumerate(bars_ratio):\n    yval = bar.get_height()\n    label_text = f\"{yval:.2f}\"\n    if has_infinite_ratio[i] and yval == placeholder_for_inf:\n      label_text = f\"> {np.max(finite_ratios):.2f}\\n(Effectively Inf)\" if finite_ratios.size > 0 else \"Very High\"\n    ax_ratio.text(bar.get_x() + bar.get_width() / 2.0, yval, label_text, va=\"bottom\", ha=\"center\", fontsize=9)\n\n  if plot_ratios.size > 0:\n    max_ratio_plot_val = np.max(plot_ratios)\n    ax_ratio.set_ylim(0, max_ratio_plot_val * 1.15)\n\n  fig_ratio.tight_layout()\n  plt.show()\n\n  # Plot 3: Memory vs. Communication (Bars positioned by Communication Volume)\n  fig_mem, ax_mem = plt.subplots(figsize=(max(10, num_schemes * 1.3), 7))  # Slightly wider for labels\n  bar_width_mem = 0.6\n\n  ax_mem.bar(\n      categorical_x,\n      mem_list,\n      width=bar_width_mem,\n      color=colors,\n      alpha=0.85,\n      edgecolor=[np.array(c[:3]) * 0.6 for c in colors],\n  )\n\n  ax_mem.set_xlabel(\"Sharding Scheme\")\n  ax_mem.set_ylabel(\"Memory per TPU (GB)\")\n  ax_mem.set_title(f\"Memory & Comm. by Sharding Scheme{title_suffix_context}\", fontsize=14)  # Updated title\n  ax_mem.set_xticks(categorical_x)\n  ax_mem.set_xticklabels(valid_schemes_labels, rotation=45, ha=\"right\", fontsize=10)\n  ax_mem.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n\n  # Add custom labels in scientific notation\n  for i in range(num_schemes):\n    mem_val = mem_list[i]\n    comm_val = t_comms_list[i]  # This is assumed to be in MB\n\n    # Format the label string as requested\n    # Using \\n for a new line to make it more readable on the plot\n    label_text = f\"mem: {mem_val:.2e} GB\\nt_comms: {comm_val:.2e} sec\"\n\n    ax_mem.text(\n        categorical_x[i],  # x-position: center of the bar\n        mem_val,  # y-position: top of the bar\n        label_text,\n        ha=\"center\",  # Horizontal alignment\n        va=\"bottom\",  # Vertical alignment (anchor at bottom of text, so text is above y)\n        fontsize=8,\n        rotation=0,\n        bbox={\"facecolor\": \"white\", \"alpha\": 0.6, \"pad\": 2, \"boxstyle\": \"round,pad=0.3\"},  # Added bbox\n    )\n\n  if mem_list.size > 0:\n    max_mem_val = np.max(mem_list)\n    # Adjust y-limit to accommodate multi-line labels; factor might need tuning\n    ax_mem.set_ylim(0, max_mem_val * 1.35)  # Increased padding for multi-line labels\n  else:\n    ax_mem.set_ylim(0, 1)\n\n  fig_mem.tight_layout()\n  plt.show()",
        "analysis": {
            "module_type": "sharding_scheme_visualizer",
            "purpose": "Calculates and plots a comparison of resource usage (FLOPs latency, communication latency, memory) for various tensor sharding schemes.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Iterate through each provided sharding scheme in the `sharding_schemes` list.",
                "For each scheme, call the `calc_resource_func` to get FLOPs latency, communication latency, and memory usage.",
                "Handle and report any errors during resource calculation for a given scheme, skipping failed ones.",
                "Extract and process the calculated results into numpy arrays for plotting.",
                "Calculate the ratio of FLOPs latency to communication latency, handling potential division by zero and infinite ratios.",
                "Generate and display a grouped bar plot comparing FLOPs latency (T_flops) and communication latency (T_comms).",
                "Generate and display a bar plot of the T_flops / T_comms ratio (roofline).",
                "Generate and display a bar plot of memory usage per device, annotated with communication latency values.",
                "Display each plot sequentially using `plt.show()`."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "numpy",
                "matplotlib.pyplot",
                "pprint",
                "warnings"
            ],
            "parameters": {
                "calc_resource_func": "A function that takes activation shape, weight shape, and sharding settings to calculate performance metrics like t_flops, t_comms, and memory.",
                "activations_shape": "A tuple representing the shape of the activation tensor (e.g., (M, K)).",
                "weights_shape": "A tuple representing the shape of the weight tensor (e.g., (G, K, F)).",
                "sharding_schemes": "A list of dictionaries, where each defines a sharding strategy with a 'label' and 'shard_settings'."
            },
            "notes": [
                "The function does not return any value; its primary output is displaying plots using matplotlib.",
                "It includes error handling to skip schemes that cause calculation failures, allowing the plotting of valid schemes to proceed.",
                "Special logic is included to handle and visualize infinite FLOPs/communication ratios, which occur when communication latency is zero or near-zero."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/test_sharding_utils.py#ShardingTests",
        "file_path": "src/MaxText/inference/scripts/test_sharding_utils.py",
        "code_block": "class ShardingTests(unittest.TestCase):\n  \"\"\"Test suite for sharding resource calculation utilities.\"\"\"\n\n  def test_no_sharding(self):\n    \"\"\"Tests the basic case with no sharding.\"\"\"\n    sD, sK, sW, sF, sE = 1, 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    # Total FLOPs = 2 * M * K * F\n    expected_t_flops = (2.0 * M * K * F) / peak_flops_val\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    expected_t_comms = 0.0\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_output_feature_parallelism_sF(self):\n    \"\"\"Tests sharding on the F dimension of weights (sF > 1).\"\"\"\n    sF = 4\n    sD, sK, sW, sE = 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,  # (K, F)\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * (F / sF)) / peak_flops_val\n    assert (\n        abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n    ), f\"FLOPs mismatch: got {result['t_flops']}, expected {expected_t_flops}\"\n\n    # Expected comms\n    expected_t_comms = 0.0\n    assert (\n        abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n    ), f\"Comms mismatch: got {result['t_comms']}, expected {expected_t_comms}\"\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * (F / sF) * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * (F / sF) * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert (\n        abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n    ), f\"Memory mismatch: got {result['memory_per_TPU_bytes']}, expected {expected_memory_per_TPU}\"\n\n  def test_data_parallelism_sD(self):\n    \"\"\"Tests sharding on the M dimension of activations (sD).\"\"\"\n    sD = 4\n    sK, sW, sF, sE = 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs:\n    expected_t_flops = (2.0 * M * K * F) / (peak_flops_val * sD)\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    expected_t_comms = 0.0\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = (M / sD) * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = (M / sD) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_fsdp_activation_sharding_sK(self):\n    \"\"\"Tests FSDP-style sharding on the K dimension of activations (sK).\n\n    In this scenario, the weights are not sharded (sW=1).\n    \"\"\"\n    sK = 4\n    sD, sW, sF, sE = 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * F) / peak_flops_val\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    comm_data_size = (M * K / sK) * activation_size_bytes_val\n    # t_comms\n    expected_t_comms = latency_bound_comms(comm_data_size / ici_bandwidth_val, ici_latency_val) * (sK - 1)\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_fsdp_weight_sharding_sW(self):\n    \"\"\"Tests FSDP-style sharding on the W dimension of weights (sW).\n\n    In this scenario, the activations are not sharded (sK=1).\n    \"\"\"\n    sW = 4\n    sD, sK, sF, sE = 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * F) / peak_flops_val\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    comm_data_size = (K * F / sW) * weight_size_bytes_val\n    # t_comms\n    expected_t_comms = latency_bound_comms(comm_data_size / ici_bandwidth_val, ici_latency_val) * (sW - 1)\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_tensor_parallel_sK_sW(self):\n    \"\"\"Tests tensor parallelism where both sK and sW are used.\n\n    This test assumes sK == sW and a reduce-scatter operation for partial\n    results.\n    \"\"\"\n    sK = 2\n    sW = 2  # Must be equal to sK for this path\n    sD, sF, sE = 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * (K / (sK * sW)) * F) / peak_flops_val\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    local_output_bytes = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # t_comms\n    expected_t_comms = latency_bound_comms(local_output_bytes / ici_bandwidth_val, ici_latency_val) * (sK - 1)\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * (K / sK) * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = (K / sW) * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_output_feature_parallelism_sF_with_all_gather_F(self):\n    \"\"\"Tests sF sharding with a subsequent all-gather on the F dimension.\"\"\"\n    sF = 4  # Shard the output feature dimension\n    sD, sK, sW, sE = 1, 1, 1, 1  # Isolate sF effect\n    all_gather_axes = [\"F\"]\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,  # (K, F)\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        all_gather_axes=all_gather_axes,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * (F / sF)) / peak_flops_val\n    assert (\n        abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n    ), f\"FLOPs mismatch: got {result['t_flops']}, expected {expected_t_flops}\"\n\n    # Expected comms\n    # per TPU\n    local_output_bytes_for_gather = M * (F / sF) * max(activation_size_bytes_val, weight_size_bytes_val)\n    # t_comms\n    expected_t_comms = latency_bound_comms(local_output_bytes_for_gather / ici_bandwidth_val, ici_latency_val) * (sF - 1)\n    assert (\n        abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n    ), f\"Comms mismatch: got {result['t_comms']}, expected {expected_t_comms}\"\n\n    # Expected Memory per TPU:\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * (F / sF) * weight_size_bytes_val\n    # Outputs\n    expected_mem_output_gathered = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output_gathered\n    assert (\n        abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n    ), f\"Memory mismatch: got {result['memory_per_TPU_bytes']}, expected {expected_memory_per_TPU}\"\n\n  def test_expert_parallelism_sE(self):\n    \"\"\"Tests expert parallelism sharding on the G dimension (sE).\"\"\"\n    G_val = 8\n    weights_shape_3d = (G_val, K, F)\n    sE = 4\n    sD, sK, sW, sF = 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_3d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * F * G_val) / (peak_flops_val * sE)\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    local_output_bytes = M * (G_val / sE) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # t_comms\n    expected_t_comms = latency_bound_comms(local_output_bytes / ici_bandwidth_val, ici_latency_val) * (sE - 1) / 4\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = (G_val / sE) * K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * (G_val / sE) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_mixed_sharding_sD_sK_sW(self):\n    \"\"\"Tests a mix of data and tensor parallelism (reduce-scatter).\"\"\"\n    sD = 2\n    sK = 2\n    sW = 2  # sK == sW\n    sF, sE = 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * (M / sD) * (K / (sK * sW)) * F) / peak_flops_val\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    local_output_bytes = (M / sD) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # t_comms\n    expected_t_comms = latency_bound_comms(local_output_bytes / ici_bandwidth_val, ici_latency_val) * (sK - 1)\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = (M / sD) * (K / sK) * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = (K / sW) * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = (M / sD) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_additional_all_gather_axes_D(self):\n    \"\"\"Tests an additional all-gather on the 'D' dimension of the output.\"\"\"\n    sD = 2\n    sK, sW, sF, sE = 1, 1, 1, 1\n    all_gather_axes = [\"D\"]\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        all_gather_axes=all_gather_axes,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * F) / (peak_flops_val * sD)\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    local_output_bytes_base = (M / sD) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # t_comms\n    expected_t_comms = latency_bound_comms(local_output_bytes_base / ici_bandwidth_val, ici_latency_val) * (sD - 1)\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = (M / sD) * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE",
        "analysis": {
            "module_type": "unit_test_case",
            "purpose": "A test suite for verifying the correctness of sharding resource calculation utilities, specifically the `calculate_matmul_resources` function.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "unittest.TestCase",
                "calculate_matmul_resources",
                "latency_bound_comms"
            ],
            "parameters": {},
            "notes": [
                "This class inherits from `unittest.TestCase`.",
                "It uses a set of predefined global constants for tensor shapes (M, K, F), hardware parameters (bandwidth, flops, latency), and data types to create consistent test environments.",
                "Each test method validates the calculated FLOPs, communication time, and memory per TPU against manually computed expected values for a specific sharding strategy."
            ],
            "methods": {
                "test_no_sharding": {
                    "purpose": "Tests the baseline case where no sharding is applied (all sharding factors are 1).",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set all sharding factors (sD, sK, sW, sF, sE) to 1.",
                        "Call `calculate_matmul_resources` with predefined parameters.",
                        "Calculate expected FLOPs time, communication time (zero), and memory per TPU.",
                        "Assert that the function's results match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources"
                    ],
                    "notes": []
                },
                "test_output_feature_parallelism_sF": {
                    "purpose": "Tests output feature parallelism by sharding the 'F' dimension of the weights.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set the feature sharding factor `sF` to 4.",
                        "Call `calculate_matmul_resources`.",
                        "Calculate expected FLOPs time, communication time (zero), and memory per TPU, accounting for the sharded 'F' dimension.",
                        "Assert that the function's results match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources"
                    ],
                    "notes": []
                },
                "test_data_parallelism_sD": {
                    "purpose": "Tests data parallelism by sharding the 'M' dimension (e.g., batch size) of the activations.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set the data sharding factor `sD` to 4.",
                        "Call `calculate_matmul_resources`.",
                        "Calculate expected FLOPs time, communication time (zero), and memory per TPU, accounting for the sharded 'M' dimension.",
                        "Assert that the function's results match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources"
                    ],
                    "notes": []
                },
                "test_fsdp_activation_sharding_sK": {
                    "purpose": "Tests FSDP-style sharding on the contracting dimension 'K' of the activations, which requires an all-gather communication.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set the activation sharding factor `sK` to 4.",
                        "Call `calculate_matmul_resources`.",
                        "Calculate expected FLOPs time (unsharded), communication time (for all-gather), and memory per TPU (unsharded due to gather).",
                        "Assert that the function's results match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": []
                },
                "test_fsdp_weight_sharding_sW": {
                    "purpose": "Tests FSDP-style sharding on the contracting dimension 'K' of the weights, which requires an all-gather communication.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set the weight sharding factor `sW` to 4.",
                        "Call `calculate_matmul_resources`.",
                        "Calculate expected FLOPs time (unsharded), communication time (for all-gather), and memory per TPU (unsharded due to gather).",
                        "Assert that the function's results match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": []
                },
                "test_tensor_parallel_sK_sW": {
                    "purpose": "Tests tensor parallelism where both activations and weights are sharded on the contracting dimension 'K', requiring a reduce-scatter communication.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set both `sK` and `sW` to 2.",
                        "Call `calculate_matmul_resources`.",
                        "Calculate expected FLOPs time, communication time (for reduce-scatter), and memory per TPU, accounting for sharded inputs and output.",
                        "Assert that the function's results match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": [
                        "This test assumes `sK` is equal to `sW`."
                    ]
                },
                "test_output_feature_parallelism_sF_with_all_gather_F": {
                    "purpose": "Tests feature parallelism (`sF`) combined with a subsequent explicit all-gather on the output 'F' dimension.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set `sF` to 4 and specify `all_gather_axes=['F']`.",
                        "Call `calculate_matmul_resources`.",
                        "Calculate expected FLOPs time (sharded), communication time (for all-gather), and memory per TPU (output is unsharded due to gather).",
                        "Assert that the function's results match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": []
                },
                "test_expert_parallelism_sE": {
                    "purpose": "Tests expert parallelism by sharding the 'G' (experts) dimension of a 3D weight tensor.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Define a 3D weight shape `(G, K, F)`.",
                        "Set the expert sharding factor `sE` to 4.",
                        "Call `calculate_matmul_resources`.",
                        "Calculate expected FLOPs time, communication time (for all-to-all), and memory per TPU, accounting for the sharded 'G' dimension.",
                        "Assert that the function's results match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": []
                },
                "test_mixed_sharding_sD_sK_sW": {
                    "purpose": "Tests a combination of data parallelism (`sD`) and tensor parallelism (`sK`, `sW`).",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set `sD`, `sK`, and `sW` to 2.",
                        "Call `calculate_matmul_resources`.",
                        "Calculate expected FLOPs time, communication time, and memory per TPU based on the combined sharding effects.",
                        "Assert that the function's results match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": []
                },
                "test_additional_all_gather_axes_D": {
                    "purpose": "Tests data parallelism (`sD`) combined with a subsequent explicit all-gather on the output 'D' dimension.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set `sD` to 2 and specify `all_gather_axes=['D']`.",
                        "Call `calculate_matmul_resources`.",
                        "Calculate expected FLOPs time (sharded), communication time (for all-gather), and memory per TPU (output is unsharded due to gather).",
                        "Assert that the function's results match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": []
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/common_types.py#DecoderBlockType",
        "file_path": "src/MaxText/common_types.py",
        "code_block": "class DecoderBlockType(enum.Enum):\n  \"\"\"Decoder block types.\"\"\"\n\n  DEFAULT = \"default\"\n  LLAMA2 = \"llama2\"\n  MISTRAL = \"mistral\"\n  MIXTRAL = \"mixtral\"\n  DEEPSEEK = \"deepseek\"\n  GEMMA = \"gemma\"\n  GEMMA2 = \"gemma2\"\n  GEMMA3 = \"gemma3\"\n  QWEN3 = \"qwen3\"\n  QWEN3_MOE = \"qwen3_moe\"\n  GPT3 = \"gpt3\"\n  GPT_OSS = \"gpt_oss\"\n  SIMPLE = \"simple\"\n  SIMPLE_MLP = \"simple_mlp\"\n  LLAMA4 = \"llama4\"",
        "analysis": {
            "module_type": "decoder_block_type_enum",
            "purpose": "Defines an enumeration of supported decoder block architectures for transformer models.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "enum.Enum"
            ],
            "parameters": {},
            "notes": [
                "This class provides a set of named constants for different decoder block types, allowing for type-safe configuration.",
                "Available members are: DEFAULT, LLAMA2, MISTRAL, MIXTRAL, DEEPSEEK, GEMMA, GEMMA2, GEMMA3, QWEN3, QWEN3_MOE, GPT3, GPT_OSS, SIMPLE, SIMPLE_MLP, LLAMA4.",
                "Each member corresponds to a specific string value (e.g., DecoderBlockType.LLAMA2.value is 'llama2')."
            ]
        }
    },
    {
        "block_name": "src/MaxText/common_types.py#AttentionType",
        "file_path": "src/MaxText/common_types.py",
        "code_block": "class AttentionType(enum.Enum):\n  GLOBAL = \"global\"  # default, with causality\n  LOCAL_SLIDING = \"local_sliding\"\n  CHUNK = \"chunk\"\n  MLA = \"mla\"\n  FULL = \"full\"",
        "analysis": {
            "module_type": "enumeration",
            "purpose": "Defines a set of named constants for different types of attention mechanisms.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "enum.Enum"
            ],
            "parameters": {},
            "notes": [
                "This class provides symbolic names for various attention strategies: GLOBAL, LOCAL_SLIDING, CHUNK, MLA, and FULL.",
                "The 'GLOBAL' member is commented as the default and includes causality."
            ],
            "methods": {}
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_input_data_sharding",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_input_data_sharding(config, mesh):\n  \"\"\"Get the input data sharding for the model\"\"\"\n  return nn.logical_to_mesh_sharding(P(*config.input_data_sharding_logical_axes), mesh, config.logical_axis_rules)",
        "analysis": {
            "module_type": "sharding_utility",
            "purpose": "Calculates and returns the JAX sharding configuration for the input data based on logical axis rules defined in a configuration object.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Creates a `jax.sharding.PartitionSpec` from the logical axes specified in `config.input_data_sharding_logical_axes`.",
                "Calls `nn.logical_to_mesh_sharding` to map the logical `PartitionSpec` to a physical `MeshSharding` using the provided `mesh` and `config.logical_axis_rules`.",
                "Returns the resulting `MeshSharding` object."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.linen.partitioning.logical_to_mesh_sharding",
                "jax.sharding.PartitionSpec"
            ],
            "parameters": {
                "config.input_data_sharding_logical_axes": "A tuple of strings defining the logical axes for sharding the input data.",
                "config.logical_axis_rules": "A set of rules that map logical axis names to physical mesh axis names.",
                "mesh": "The JAX device mesh object."
            },
            "notes": [
                "The function translates a high-level, logical sharding specification into a concrete physical sharding object (`MeshSharding`) that JAX can use to distribute data across devices.",
                "The output is a `jax.sharding.Sharding` object, not a tensor."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_functional_train_with_signature",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_functional_train_with_signature(train_step, data_sharding, state_mesh_shardings, model, config):\n  \"\"\"Get the shardings (both state and data) for `train_step`.\"\"\"\n  functional_train = functools.partial(train_step, model, config, state_mesh_shardings)\n  functional_train.__name__ = \"train_step\"\n  in_shardings = (state_mesh_shardings, data_sharding, None)  # State, batch, rng\n  out_shardings = (state_mesh_shardings, None)  # State, metrics\n  static_argnums = ()  # We partial out the static argnums of model and config\n  donate_argnums = 0  # This is the index of the state - we allow the compiler to make use of this memory.\n  return functional_train, in_shardings, out_shardings, static_argnums, donate_argnums",
        "analysis": {
            "module_type": "pjit_signature_generator",
            "purpose": "Wraps a training step function and generates the necessary sharding annotations and pjit configuration (input/output shardings, static/donated arguments) for distributed execution.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Create a partially applied `train_step` function named `functional_train` with `model`, `config`, and `state_mesh_shardings` as fixed arguments using `functools.partial`.",
                "Define the input sharding tuple `in_shardings` for the remaining arguments (state, batch, rng).",
                "Define the output sharding tuple `out_shardings` for the results (state, metrics).",
                "Define `static_argnums` as an empty tuple because the static arguments have been baked into the function via `functools.partial`.",
                "Define `donate_argnums` as 0 to indicate that the memory of the first input (the state) can be reused for the output state, optimizing memory usage.",
                "Return the partially applied function and the generated pjit configuration tuples."
            ],
            "output": {
                "shape": "A tuple containing (functional_train, in_shardings, out_shardings, static_argnums, donate_argnums)."
            },
            "dependencies": [
                "functools.partial"
            ],
            "parameters": {
                "train_step": "The core training step function to be wrapped.",
                "data_sharding": "The sharding specification for the input data batch.",
                "state_mesh_shardings": "The sharding specification for the model's training state.",
                "model": "The model object, which is partially applied to the train_step.",
                "config": "The configuration object, which is partially applied to the train_step."
            },
            "notes": [
                "This function is a helper to prepare a training step for compilation with `jax.pjit` by explicitly defining how its inputs and outputs are distributed across devices.",
                "The `donate_argnums=0` is a key memory optimization, allowing the JAX compiler to modify the input state in-place."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_functional_eval_with_signature",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_functional_eval_with_signature(eval_step, data_sharding, state_mesh_shardings, model, config):\n  \"\"\"Get the shardings (both state and data) for `eval_step`.\"\"\"\n  functional_eval = functools.partial(eval_step, model, config)\n  functional_eval.__name__ = \"eval_step\"\n  in_shardings = (state_mesh_shardings, data_sharding, None)  # State, batch, rng\n  out_shardings = None  # metrics\n  static_argnums = ()  # We partial out the static argnums of model, config\n  donate_argnums = ()  # state will be kept instead of being donated in eval_step\n  return functional_eval, in_shardings, out_shardings, static_argnums, donate_argnums",
        "analysis": {
            "module_type": "evaluation_step_wrapper",
            "purpose": "Wraps an evaluation step function with its corresponding JAX sharding annotations and pjit configuration for parallel execution.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Creates a partial function `functional_eval` from `eval_step` with `model` and `config` arguments pre-filled using `functools.partial`.",
                "Sets the `__name__` of the partial function to 'eval_step'.",
                "Defines the input sharding tuple for state, batch, and RNG as `(state_mesh_shardings, data_sharding, None)`.",
                "Defines the output sharding for metrics as `None` (fully replicated).",
                "Defines empty tuples for `static_argnums` and `donate_argnums`.",
                "Returns the partial function along with the sharding and pjit configurations."
            ],
            "output": {
                "shape": "A tuple containing: (partial_function, in_shardings, out_shardings, static_argnums, donate_argnums)."
            },
            "dependencies": [
                "functools.partial"
            ],
            "parameters": {
                "eval_step": "The evaluation function to be wrapped.",
                "data_sharding": "The sharding specification for the input data batch.",
                "state_mesh_shardings": "The sharding specification for the model state.",
                "model": "The model object, which is partially applied to the `eval_step`.",
                "config": "The configuration object, which is partially applied to the `eval_step`."
            },
            "notes": [
                "This function prepares the `eval_step` to be compiled by `jax.pjit` by providing the necessary sharding metadata.",
                "The `donate_argnums` is explicitly set to an empty tuple, indicating that the model state will be kept and not donated for in-place modification during evaluation.",
                "The `static_argnums` is empty because the static arguments (`model`, `config`) are handled via `functools.partial` before compilation."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_shaped_batch",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_shaped_batch(config):\n  \"\"\"Return the shape of the batch - this is what eval_shape would return for the\n  output of create_data_iterator, but eval_shape doesn't work, see b/306901078.\"\"\"\n  batch_shape = (config.global_batch_size_to_load, config.max_target_length)\n  shaped_batch = {}\n  shaped_batch[\"inputs\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  shaped_batch[\"inputs_position\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  shaped_batch[\"inputs_segmentation\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  shaped_batch[\"targets\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  shaped_batch[\"targets_position\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  shaped_batch[\"targets_segmentation\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  if config.use_multimodal:\n    image_shape = multimodal_utils.get_dummy_image_shape_for_init(\n        config.model_name, batch_size=config.micro_batch_size_to_train_on\n    )\n    shaped_batch[\"images\"] = jax.ShapeDtypeStruct(image_shape, jnp.int32)\n  return shaped_batch",
        "analysis": {
            "module_type": "data_shape_generator",
            "purpose": "Creates a dictionary of JAX ShapeDtypeStruct objects representing the shape and data type of a model input batch, used for JAX transformations like `jax.eval_shape`.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Define a `batch_shape` tuple from `config.global_batch_size_to_load` and `config.max_target_length`.",
                "Initialize a dictionary `shaped_batch`.",
                "Populate `shaped_batch` with keys for text data ('inputs', 'inputs_position', 'targets', etc.) using `jax.ShapeDtypeStruct` with the defined `batch_shape` and `jnp.int32` dtype.",
                "If `config.use_multimodal` is true, determine the `image_shape` by calling `multimodal_utils.get_dummy_image_shape_for_init`.",
                "If multimodal, add an 'images' key to `shaped_batch` with a `jax.ShapeDtypeStruct` using the `image_shape`.",
                "Return the `shaped_batch` dictionary."
            ],
            "output": {
                "shape": "A dictionary where keys are strings (e.g., 'inputs', 'targets', 'images') and values are `jax.ShapeDtypeStruct` objects."
            },
            "dependencies": [
                "jax.ShapeDtypeStruct",
                "jax.numpy",
                "multimodal_utils.get_dummy_image_shape_for_init"
            ],
            "parameters": {
                "config": "A configuration object containing parameters like `global_batch_size_to_load`, `max_target_length`, `use_multimodal`, `model_name`, and `micro_batch_size_to_train_on`."
            },
            "notes": [
                "This function serves as a workaround because `jax.eval_shape` does not work directly on the data iterator, as noted by the internal bug reference b/306901078.",
                "The output is an abstract representation of a batch's shape and type, not actual data."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#load_compiled",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def load_compiled(config, partial_train, state):\n  \"\"\"# Loading a serialized compiled train step function.\"\"\"\n\n  # Currently partial_train and state  are needed to reconstruct\n  # input/output shapes to construct the in_trees and out_trees for load API\n  # Parker is working on a serializing these\n  def load_serialized_compiled(save_name):\n    with open(save_name, \"rb\") as f:\n      serialized_compiled = pickle.load(f)\n    return serialized_compiled\n\n  def get_train_input_output_trees(func, input_args, input_kwargs):\n    _, in_tree_recreated = jax.tree_util.tree_flatten((input_args, input_kwargs))\n    out_shaped = jax.eval_shape(func, *input_args, **input_kwargs)\n    _, out_tree_recreated = jax.tree_util.tree_flatten(out_shaped)\n    return in_tree_recreated, out_tree_recreated\n\n  serialized_compiled = load_serialized_compiled(config.compiled_trainstep_file)\n  shaped_batch = get_shaped_batch(config)\n  example_rng = jax.random.PRNGKey(0)\n  shaped_input_args = (state, shaped_batch, example_rng)\n  shaped_input_kwargs = {}\n  in_tree, out_tree = get_train_input_output_trees(partial_train, shaped_input_args, shaped_input_kwargs)\n  p_train_step = deserialize_and_load(serialized_compiled, in_tree, out_tree)\n  return p_train_step",
        "analysis": {
            "functionality": "Loads a pre-compiled and serialized JAX training step function from a file.",
            "usage": "Call this function with the configuration object, a partially applied training function, and the initial training state. It returns a deserialized, ready-to-use JAX pjit-compiled training function, which avoids the need for recompilation at startup. The `config.compiled_trainstep_file` must point to a valid serialized file created by JAX's serialization utilities."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_tokens_training_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_tokens_training_per_device(config):\n  \"\"\"Calculate training Tokens per device\"\"\"\n  return config.max_target_length * config.per_device_batch_size * config.gradient_accumulation_steps",
        "analysis": {
            "module_type": "training_token_calculator",
            "purpose": "Calculates the total number of training tokens processed per device for a single effective training step, accounting for gradient accumulation.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Multiplies `config.max_target_length`, `config.per_device_batch_size`, and `config.gradient_accumulation_steps`."
            ],
            "output": {
                "shape": "Scalar"
            },
            "dependencies": [],
            "parameters": {
                "config": "A configuration object containing `max_target_length`, `per_device_batch_size`, and `gradient_accumulation_steps`."
            },
            "notes": [
                "The result represents the effective number of tokens per device per optimizer update."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_gemma2_tflops_training_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_gemma2_tflops_training_per_device(config, total_ffn_flops, qkv_flops, projection_flops, embedding_flops):\n  \"\"\"\n  Calculate training TFLOP for Gemma2 as in Gemma2 we combine [local_attention, global_attention] into one decoder\n  layer and we use sliding window attention in local_attention\n  \"\"\"\n  noncausal_attention_flops = (\n      # global attention\n      4 * config.per_device_batch_size * config.max_target_length**2 * config.num_query_heads * config.head_dim\n      +\n      # local attention\n      4\n      * config.per_device_batch_size\n      * config.max_target_length\n      * min(config.sliding_window_size, config.max_target_length)\n      * config.num_query_heads\n      * config.head_dim\n  )\n  causal_attention_flops = noncausal_attention_flops / 2\n  attention_tflops = causal_attention_flops * config.num_decoder_layers * 3 / 10**12\n\n  # multiply num_decoder_layers by 2 because we combine [local_attention, global_attention] into one decoder layer\n  learnable_weight_tflops = (\n      ((total_ffn_flops + qkv_flops + projection_flops) * config.num_decoder_layers * 2 + embedding_flops) * 3 / 10**12\n  )\n\n  return attention_tflops, learnable_weight_tflops",
        "analysis": {
            "module_type": "gemma2_tflops_calculator",
            "purpose": "Calculates the training TeraFLOPs (TFLOPs) per device for a Gemma2 model, separating the contributions from attention and learnable weights.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calculate non-causal attention FLOPs by summing the FLOPs for global attention and local sliding window attention.",
                "Halve the non-causal attention FLOPs to get causal attention FLOPs.",
                "Convert causal attention FLOPs to TFLOPs, accounting for all decoder layers and the forward/backward passes (multiplied by 3).",
                "Calculate total learnable weight FLOPs from FFN, QKV, projection, and embedding layers.",
                "Convert learnable weight FLOPs to TFLOPs, accounting for the combined local/global decoder structure (multiplied by `num_decoder_layers * 2`) and the forward/backward passes (multiplied by 3).",
                "Return the attention TFLOPs and learnable weight TFLOPs as a tuple."
            ],
            "output": {
                "shape": "(scalar, scalar)"
            },
            "dependencies": [],
            "parameters": {
                "config": "A configuration object containing model and training hyperparameters such as per_device_batch_size, max_target_length, num_query_heads, head_dim, sliding_window_size, and num_decoder_layers.",
                "total_ffn_flops": "Total floating-point operations for the FFN layers.",
                "qkv_flops": "Total floating-point operations for the QKV projection layers.",
                "projection_flops": "Total floating-point operations for the attention output projection layer.",
                "embedding_flops": "Total floating-point operations for the embedding layer."
            },
            "notes": [
                "This function is specific to the Gemma2 architecture, which combines local (sliding window) and global attention into a single decoder layer.",
                "The calculation for learnable weights multiplies `num_decoder_layers` by 2 to account for this combined structure.",
                "A factor of 3 is used to estimate total training FLOPs from the forward pass FLOPs (1x for forward, 2x for backward/optimizer)."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_mixed_attention_model_tflops_training_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_mixed_attention_model_tflops_training_per_device(\n    config, total_ffn_flops, qkv_flops, projection_flops, embedding_flops, attention_pattern_length\n):\n  \"\"\"\n  Calculate training TFLOPs for models with a mixed attention pattern of local\n  and global attention layers, like Gemma3 and GPT-OSS.\n  \"\"\"\n  num_layers = config.num_decoder_layers\n\n  num_global_layers = num_layers // attention_pattern_length\n  num_local_layers = num_layers - num_global_layers\n\n  # FLOPs for a single global attention layer (full attention)\n  # Formula: 4 * batch_size * seq_len^2 * num_heads * head_dim\n  global_attention_flops_per_layer = (\n      4 * config.per_device_batch_size * config.max_target_length**2 * config.num_query_heads * config.head_dim\n  )\n\n  # FLOPs for a single local attention layer (sliding window)\n  # Formula: 4 * batch_size * seq_len * window_size * num_heads * head_dim\n  local_attention_flops_per_layer = (\n      4\n      * config.per_device_batch_size\n      * config.max_target_length\n      * min(config.sliding_window_size, config.max_target_length)\n      * config.num_query_heads\n      * config.head_dim\n  )\n\n  # Total attention FLOPs = (num_global_layers * FLOPs_per_global) + (num_local_layers * FLOPs_per_local)\n  noncausal_attention_flops = (\n      num_global_layers * global_attention_flops_per_layer + num_local_layers * local_attention_flops_per_layer\n  )\n  causal_attention_flops = noncausal_attention_flops / 2\n\n  # Convert to TFLOPs and multiply by 3 for fwd/bwd pass\n  attention_tflops = causal_attention_flops * 3 / 10**12\n\n  # Learnable weights (FFN, QKV, Projections) are present in every layer.\n  learnable_weight_tflops = ((total_ffn_flops + qkv_flops + projection_flops) * num_layers + embedding_flops) * 3 / 10**12\n\n  return attention_tflops, learnable_weight_tflops",
        "analysis": {
            "functionality": "Calculates the training TeraFLOPs per device for transformer models that use a mixed pattern of global (full) and local (sliding window) attention layers.",
            "usage": "Provide a configuration object with model hyperparameters, pre-calculated FLOPs for various model components (FFN, QKV, projection, embedding), and the length of the repeating attention pattern. The function returns a tuple of floats: (`attention_tflops`, `learnable_weight_tflops`), representing the computational cost for attention and learnable weights, respectively, during a training step (forward and backward pass)."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#_calculate_chunked_attention_flops_per_layer",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def _calculate_chunked_attention_flops_per_layer(config, seq_len, chunk_size):\n  \"\"\"Calculates the non-causal FLOPs for a single layer of chunked attention.\"\"\"\n  num_chunks = seq_len // chunk_size\n  rem_chunk_size = seq_len % chunk_size\n  # The complexity of chunked attention is the sum of squares of chunk lengths.\n  chunked_complexity = (num_chunks * chunk_size**2) + (rem_chunk_size**2)\n  # The formula for non-causal attention FLOPs is 4 * B * complexity * H * D,\n  # where B=batch_size, H=num_heads, D=head_dim.\n  return 4 * config.per_device_batch_size * chunked_complexity * config.num_query_heads * config.head_dim",
        "analysis": {
            "functionality": "Calculates the non-causal Floating Point Operations (FLOPs) for a single layer of chunked attention.",
            "usage": "To use this function, provide a configuration object (`config`) containing model parameters (`per_device_batch_size`, `num_query_heads`, `head_dim`), the sequence length (`seq_len`), and the attention chunk size (`chunk_size`). It returns the total non-causal FLOPs as a numerical value."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_llama4_attention_tflops",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_llama4_attention_tflops(config):\n  \"\"\"\n  Calculates attention-only training TFLOPs for Llama4's specific architecture,\n  which has an alternating pattern of global and chunked attention layers.\n  \"\"\"\n  num_layers = config.num_decoder_layers\n  seq_len = config.max_target_length\n  chunk_size = config.chunk_attn_window_size\n\n  # Determine number of global vs. chunked layers based on the NoPE interval.\n  # A \"NoPE\" layer uses global attention.\n  num_global_layers = num_layers // config.nope_layer_interval\n  num_chunked_layers = num_layers - num_global_layers\n\n  # FLOPs for a single global attention layer (full attention, non-causal)\n  global_attention_flops_per_layer = (\n      4 * config.per_device_batch_size * seq_len**2 * config.num_query_heads * config.head_dim\n  )\n\n  # FLOPs for a single chunked attention layer (non-causal)\n  chunked_attention_flops_per_layer = _calculate_chunked_attention_flops_per_layer(config, seq_len, chunk_size)\n\n  # Total non-causal attention FLOPs is the sum of all global and all chunked layers\n  noncausal_attention_flops = (num_global_layers * global_attention_flops_per_layer) + (\n      num_chunked_layers * chunked_attention_flops_per_layer\n  )\n\n  # Apply causal mask and convert to TFLOPs (multiply by 3 for fwd/bwd pass)\n  causal_attention_flops = noncausal_attention_flops / 2\n  attention_tflops = causal_attention_flops * 3 / 10**12\n\n  return attention_tflops",
        "analysis": {
            "module_type": "tflops_calculator",
            "purpose": "Calculates the attention-only training TFLOPs for a Llama4-style model architecture, which features an alternating pattern of global and chunked attention layers.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Extract model dimensions and layer counts from the input config object.",
                "Determine the number of global and chunked attention layers based on the `nope_layer_interval`.",
                "Calculate the non-causal FLOPs for a single global (full) attention layer.",
                "Call `_calculate_chunked_attention_flops_per_layer` to get the FLOPs for a single chunked attention layer.",
                "Sum the FLOPs from all global and chunked layers to get the total non-causal attention FLOPs.",
                "Divide the total by 2 to account for the causal attention mask.",
                "Multiply by 3 (for forward and backward passes) and divide by 10^12 to convert the final value to TFLOPs.",
                "Return the calculated attention TFLOPs."
            ],
            "output": {
                "shape": "scalar"
            },
            "dependencies": [
                "_calculate_chunked_attention_flops_per_layer"
            ],
            "parameters": {
                "config": "A configuration object containing model parameters such as `num_decoder_layers`, `max_target_length`, `chunk_attn_window_size`, `nope_layer_interval`, `per_device_batch_size`, `num_query_heads`, and `head_dim`."
            },
            "notes": [
                "This function is specific to models like Llama4 where attention layers alternate between global (full) and chunked attention.",
                "A 'NoPE' (No Position Embedding) layer is assumed to use global attention.",
                "The final TFLOPs value represents a full training step (forward pass, backward pass, and optimizer update), which is estimated by multiplying the causal FLOPs by 3."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_mla_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_mla_tflops_per_device(config):\n  \"\"\"Calculate Multi-Head Latent Attention TFLOP\"\"\"\n  batch_len = config.per_device_batch_size * config.max_target_length\n  qk_head_dim_sum = config.qk_nope_head_dim + config.qk_rope_head_dim\n  # calculate mla query projection\n  if config.q_lora_rank == 0:\n    q_flops = 2 * batch_len * config.emb_dim * config.num_query_heads * qk_head_dim_sum\n  else:\n    # calculate query down and up flops\n    q_flops = (\n        2\n        * batch_len\n        * (config.emb_dim * config.q_lora_rank + config.q_lora_rank * config.num_query_heads * qk_head_dim_sum)\n    )\n  # calculate mla kv projection with down and up flops\n  kv_flops = (\n      2\n      * batch_len\n      * (\n          config.emb_dim * (config.kv_lora_rank + config.qk_rope_head_dim)\n          + config.kv_lora_rank * config.num_query_heads * (config.qk_nope_head_dim + config.v_head_dim)\n      )\n  )\n  qkv_flops = q_flops + kv_flops\n\n  attention_flops = (\n      2 * batch_len * config.max_target_length * config.num_query_heads * (qk_head_dim_sum + config.v_head_dim)\n  )\n  projection_flops = 2 * batch_len * config.emb_dim * config.num_query_heads * config.v_head_dim\n  return qkv_flops, attention_flops, projection_flops",
        "analysis": {
            "functionality": "Calculates the theoretical floating-point operations (FLOPs) for the QKV projection, attention mechanism, and output projection of a Multi-Head Latent Attention (MLA) layer on a per-device basis.",
            "usage": "This function is used for performance modeling. Call it with a configuration object that specifies the model's dimensions and architecture. It returns a tuple containing the FLOP counts for QKV projection, the attention mechanism, and the final output projection."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_ffn_mamtul_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_ffn_mamtul_tflops_per_device(config, mlp_dim):\n  \"\"\"Helper function to calculate matmul TFLOP in ffn based on MLP dimension.\n\n  Applies to:\n    - Dense FFN layers (mlp_dim = config.mlp_dim).\n    - MoE FFN layers (mlp_dim = config.moe_mlp_dim),\n      need to scale by shared_experts or num_experts_per_tok.\n  \"\"\"\n  ffn1_flops = (\n      2 * config.per_device_batch_size * config.max_target_length * mlp_dim * config.emb_dim * len(config.mlp_activations)\n  )\n  ffn2_flops = 2 * config.per_device_batch_size * config.max_target_length * mlp_dim * config.emb_dim\n  return ffn1_flops + ffn2_flops",
        "analysis": {
            "functionality": "Calculates the total matrix multiplication Floating Point Operations (FLOPs) for a Feed-Forward Network (FFN) layer on a single device.",
            "usage": "Call this function with a configuration object and the MLP dimension to get the total FLOPs for the FFN layer. The result is a scalar float. This function is a helper used for calculating performance metrics for both standard dense FFNs and individual experts in Mixture-of-Experts (MoE) layers."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_routed_and_shared_ffn_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_routed_and_shared_ffn_tflops_per_device(config):\n  \"\"\"Helper function to calculate DeepSeek-style ffn TFLOP\"\"\"\n  gate_flops = 2 * config.per_device_batch_size * config.max_target_length * config.emb_dim * config.num_experts\n  # Due to the mixed decoder layers, the flops is multiplied by num of layers for both dense and moe\n  num_dense_layers, num_moe_layers = get_dense_moe_layers(config)\n  dense_ffn_flops = calculate_ffn_mamtul_tflops_per_device(config, config.mlp_dim) * num_dense_layers\n  shared_experts_flops = calculate_ffn_mamtul_tflops_per_device(config, config.moe_mlp_dim) * config.shared_experts\n  routed_experts_flops = calculate_ffn_mamtul_tflops_per_device(config, config.moe_mlp_dim) * config.num_experts_per_tok\n  moe_ffn_flops = (gate_flops + shared_experts_flops + routed_experts_flops) * num_moe_layers\n  total_ffn_flops = dense_ffn_flops + moe_ffn_flops\n  return total_ffn_flops",
        "analysis": {
            "module_type": "deepseek_ffn_tflops_calculator",
            "purpose": "Calculates the total TeraFLOPs for a model with a mix of standard dense FFN layers and DeepSeek-style Mixture-of-Experts (MoE) FFN layers that include both shared and routed experts.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calculate the TFLOPs for the MoE gating mechanism.",
                "Determine the number of dense and MoE layers by calling `get_dense_moe_layers`.",
                "Calculate the TFLOPs for all dense FFN layers using `calculate_ffn_mamtul_tflops_per_device` and scaling by the number of dense layers.",
                "Calculate the TFLOPs for the shared experts in the MoE layers.",
                "Calculate the TFLOPs for the routed (activated) experts in the MoE layers.",
                "Sum the gate, shared, and routed expert TFLOPs and scale by the number of MoE layers to get the total MoE FFN TFLOPs.",
                "Sum the TFLOPs from the dense and MoE FFN layers to get the final total.",
                "Return the total FFN TFLOPs."
            ],
            "output": {
                "shape": "[1]"
            },
            "dependencies": [
                "get_dense_moe_layers",
                "calculate_ffn_mamtul_tflops_per_device"
            ],
            "parameters": {
                "config": "A configuration object containing model and training parameters.",
                "config.per_device_batch_size": "Batch size per device.",
                "config.max_target_length": "Maximum sequence length of the input.",
                "config.emb_dim": "Embedding dimension of the model.",
                "config.num_experts": "Total number of experts in the MoE layers.",
                "config.mlp_dim": "The dimension of the MLP for dense FFN layers.",
                "config.moe_mlp_dim": "The dimension of the MLP for MoE FFN layers.",
                "config.shared_experts": "The number of experts that are shared and processed by all tokens.",
                "config.num_experts_per_tok": "The number of experts to route each token to.",
                "config.decoder_block": "The type of decoder block, which determines the mix of dense and MoE layers."
            },
            "notes": [
                "This function is specifically designed to calculate TFLOPs for models with a hybrid FFN architecture, like DeepSeek, which combines standard dense layers with MoE layers featuring both shared and routed experts.",
                "The calculation assumes a mixed-layer architecture where the total number of layers is split between dense and MoE types, determined by the `get_dense_moe_layers` helper function."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_dense_moe_layers",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_dense_moe_layers(config):\n  \"\"\"Helper function to calculate number of dense and moe layers\"\"\"\n  if config.decoder_block == DecoderBlockType.DEEPSEEK:\n    num_dense_layers = config.first_num_dense_layers\n    num_moe_layers = config.num_decoder_layers - config.first_num_dense_layers\n    return num_dense_layers, num_moe_layers\n  elif config.decoder_block == DecoderBlockType.LLAMA4:\n    num_moe_layers = config.num_decoder_layers // config.interleave_moe_layer_step\n    num_dense_layers = config.num_decoder_layers - num_moe_layers\n  else:\n    raise ValueError(\"Currently we only support DeepSeek and Llama4 calculation.\")\n\n  return num_dense_layers, num_moe_layers",
        "analysis": {
            "functionality": "Calculates the number of dense and Mixture-of-Experts (MoE) layers for specific model architectures (DeepSeek, Llama4) based on a configuration object.",
            "usage": "Input a configuration object with attributes like `decoder_block` and `num_decoder_layers`. The function returns a tuple of two integers: `(number_of_dense_layers, number_of_moe_layers)`."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_gemma3_vision_layers_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_gemma3_vision_layers_tflops_per_device(config):\n  \"\"\"\n  Estimate TFLOPs for Gemma3 vision encoder (ViT-style).\n  Returns:\n      total_tflops: Total TFLOPs (counts for fwd + bwd + optimizer)\n      learnable_weight_tflops: TFLOPs from learnable weights (patch embedding, qkv, MLP, projections)\n      attention_tflops: TFLOPs from attention multiplications\n  \"\"\"\n  # Config values\n  B = config.per_device_batch_size\n  C = config.num_channels_for_vit\n  H = W = config.image_size_for_vit  # Gemma3 default 896\n  embed_dim = config.emb_dim  # text embedding dim after projection\n  # Values below are hardcoded in Gemma3VisionEncoderLayer\n  patch_size = 14\n  hidden_dim = 1152\n  intermediate_dim = 4304\n  num_layers = 27\n  vision_exit_pooling_window = 4\n\n  # 1. Patch embedding (Conv2D)\n  num_patches_h = H // patch_size\n  num_patches_w = W // patch_size\n  seq_len = num_patches_h * num_patches_w  # 64*64=4096\n  patch_embed_flops = 2 * B * seq_len * (C * patch_size * patch_size) * hidden_dim\n\n  # 2. gemma3.Encoder: num_layers * gemma3.Encoder1DBlock\n  qkv_flops_per_layer = 3 * (2 * B * seq_len * hidden_dim * hidden_dim)\n  attn_flops_per_layer = 4 * B * seq_len * seq_len * hidden_dim\n  projection_flops_per_layer = 2 * B * seq_len * hidden_dim * hidden_dim  # projection after attention multiplication\n  mlp_flops_per_layer = 2 * (2 * B * seq_len * hidden_dim * intermediate_dim)  # two fc layers\n  total_attn_flops = attn_flops_per_layer * num_layers\n  encoder_flops = (+qkv_flops_per_layer + projection_flops_per_layer + mlp_flops_per_layer) * num_layers\n\n  # 4. VisionEmbedder\n  seq_len_after_pooling = (num_patches_h // vision_exit_pooling_window) * (num_patches_w // vision_exit_pooling_window)\n  vision_embedder_flops = 2 * B * seq_len_after_pooling * hidden_dim * embed_dim  # One linear projection\n\n  # Learnable weights summation\n  learnable_weight_flops = patch_embed_flops + encoder_flops + vision_embedder_flops\n\n  if config.freeze_vision_encoder_params:\n    learnable_weight_flops += 2 * vision_embedder_flops  # only projector is learnable, add fwd+optimizer\n  else:\n    learnable_weight_flops *= 3  # multiply by 3 for fwd + bwd + optimizer\n\n  # Convert to TFLOPs\n  learnable_weight_tflops = learnable_weight_flops / 1e12\n  total_attn_tflops = total_attn_flops / 1e12\n  total_tflops = learnable_weight_tflops + total_attn_tflops\n\n  return total_tflops, learnable_weight_tflops, total_attn_tflops",
        "analysis": {
            "functionality": "Estimates the training TFLOPs per device for a Gemma3-style vision encoder, breaking down the calculation into contributions from learnable weights (like convolutions and linear layers) and attention matrix multiplications.",
            "usage": "Call this function with a configuration object that contains parameters like `per_device_batch_size`, `image_size_for_vit`, `emb_dim`, and `freeze_vision_encoder_params`. It returns a tuple of three floats: (total_tflops, learnable_weight_tflops, total_attn_tflops)."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_llama4_vision_layers_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_llama4_vision_layers_tflops_per_device(config):\n  \"\"\"\n  Estimate TFLOPs for Llama4 vision encoder (ViT-style).\n  Returns:\n      total_tflops: Total TFLOPs (counts for fwd + bwd + optimizer)\n      learnable_weight_tflops: TFLOPs from learnable weights (patch embedding, qkv, MLP, projections)\n      attention_tflops: TFLOPs from attention multiplications\n  \"\"\"\n  # Config values\n  B = config.per_device_batch_size\n  C = config.num_channels_for_vit\n  H = W = config.tile_size_for_vit\n  patch_size = config.patch_size_for_vit\n  hidden_dim = config.hidden_size_for_vit\n  intermediate_dim = config.intermediate_size_for_vit\n  num_layers = config.num_hidden_layers_for_vit\n  pixel_shuffle_fc1_out_dim = config.projector_input_dim_for_vit  # 4096\n  pixel_shuffle_fc2_out_dim = config.projector_output_dim_for_vit  # 4096\n  base_emb_dim = config.base_emb_dim\n  pixel_shuffle_ratio = config.pixel_shuffle_ratio_for_vit  # 0.5\n  num_patches = (H // patch_size) * (W // patch_size)  # 24*24 = 576\n  pixel_shuffle_tokens = num_patches * pixel_shuffle_ratio**2  # 144\n\n  # 1. Llama4UnfoldConvolution (flops by linear projection)\n  # lax.conv_general_dilated_patches extracts patches through reshaping/indexing without flops\n  # Each patch: C * patch_size * patch_size -> hidden_dim\n  patch_embed_flops = 2 * B * num_patches * (C * patch_size * patch_size) * hidden_dim\n\n  # 2. Llama4VisionEncoder: num_layers * (qkv + att_projection + mlp)\n  seq_len = num_patches + 1  # +1 for class token, so 577\n  qkv_flops_per_layer = 3 * (2 * B * seq_len * hidden_dim * hidden_dim)  # Q, K, V projections\n  attn_flops_per_layer = 4 * B * seq_len * seq_len * hidden_dim  # Attention scores and weighted sum\n  projection_flops_per_layer = 2 * B * seq_len * hidden_dim * hidden_dim  # projection after attention multiplication\n  mlp_flops_per_layer = 2 * (2 * B * seq_len * hidden_dim * intermediate_dim)  # two fc layers\n  total_attn_flops = attn_flops_per_layer * num_layers\n  vision_encoder_flops = (+qkv_flops_per_layer + projection_flops_per_layer + mlp_flops_per_layer) * num_layers\n\n  # 3. Llama4VisionPixelShuffleMLP\n  # (B, 144, 5632) -> (B, 144, 4096) -> (B, 144, 4096)\n  pixel_shuffle_fc1_flops = 2 * B * pixel_shuffle_tokens * intermediate_dim * pixel_shuffle_fc1_out_dim\n  pixel_shuffle_fc2_flops = 2 * B * pixel_shuffle_tokens * pixel_shuffle_fc1_out_dim * pixel_shuffle_fc2_out_dim\n  pixel_shuffle_total_flops = pixel_shuffle_fc1_flops + pixel_shuffle_fc2_flops\n\n  # 4. Llama4MultiModalProjector: (B, 144, 5120) x (5120, base_emb_dim)\n  projector_flops = 2 * B * pixel_shuffle_tokens * pixel_shuffle_fc1_out_dim * base_emb_dim\n\n  # Learnable weights: all matmuls above\n  learnable_weight_flops = patch_embed_flops + vision_encoder_flops + pixel_shuffle_total_flops + projector_flops\n\n  if config.freeze_vision_encoder_params:\n    learnable_weight_flops += 2 * projector_flops  # only projector is learnable, add fwd+optimizer\n  else:\n    learnable_weight_flops *= 3  # multiply by 3 for fwd + bwd + optimizer\n\n  # Convert to TFLOPs\n  learnable_weight_tflops = learnable_weight_flops / 1e12\n  total_attn_tflops = total_attn_flops / 1e12\n  total_tflops = learnable_weight_tflops + total_attn_tflops\n\n  return total_tflops, learnable_weight_tflops, total_attn_tflops",
        "analysis": {
            "functionality": "Estimates the training TeraFLOPs (TFLOPs) per device for a Llama4-style vision encoder. It breaks down the total TFLOPs into contributions from learnable weights (like convolutions and linear layers) and attention computations.",
            "usage": "Call this function with a configuration object containing the vision model's architectural parameters (e.g., batch size, image dimensions, hidden sizes, number of layers). It returns a tuple of three float values: (total_tflops, learnable_weight_tflops, attention_tflops)."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_vision_encoder_tflops",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_vision_encoder_tflops(config):\n  \"\"\"Calculate vision encoder TFLOPs per prefill step per device.\"\"\"\n  if config.model_name.startswith(\"gemma3\"):\n    mm_total_tflops, mm_learnable_weight_tflops, mm_attention_tflops = calculate_gemma3_vision_layers_tflops_per_device(\n        config\n    )\n  elif config.model_name.startswith(\"llama4\"):\n    mm_total_tflops, mm_learnable_weight_tflops, mm_attention_tflops = calculate_llama4_vision_layers_tflops_per_device(\n        config\n    )\n  else:\n    max_logging.log(\n        f\"Vision encoder TFLOPs calculation not implemented for model {config.model_name}, counting as 0 for now.\"\n    )\n    mm_total_tflops = mm_learnable_weight_tflops = mm_attention_tflops = 0\n\n  return mm_total_tflops, mm_learnable_weight_tflops, mm_attention_tflops",
        "analysis": {
            "module_type": "vision_encoder_tflops_calculator",
            "purpose": "Calculates the TFLOPs (Tera Floating Point Operations Per Second) for a vision encoder during a prefill step on a single device, dispatching to the appropriate model-specific calculation function.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check if `config.model_name` starts with 'gemma3' and call `calculate_gemma3_vision_layers_tflops_per_device` if true.",
                "Check if `config.model_name` starts with 'llama4' and call `calculate_llama4_vision_layers_tflops_per_device` if true.",
                "If the model name is not supported, log a message and set all TFLOPs values to 0.",
                "Return the calculated total, learnable weight, and attention TFLOPs."
            ],
            "output": {
                "shape": "A tuple of three scalar values: (mm_total_tflops, mm_learnable_weight_tflops, mm_attention_tflops)."
            },
            "dependencies": [
                "calculate_gemma3_vision_layers_tflops_per_device",
                "calculate_llama4_vision_layers_tflops_per_device",
                "max_logging.log"
            ],
            "parameters": {
                "config.model_name": "The name of the model, which determines which specific TFLOPs calculation function to use."
            },
            "notes": [
                "This function acts as a dispatcher based on the model name provided in the configuration.",
                "For models not explicitly handled (e.g., not 'gemma3' or 'llama4'), it defaults to returning 0 for all TFLOPs values."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_tflops_training_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_tflops_training_per_device(config, log=True):\n  \"\"\"Calculate training TFLOP\"\"\"\n  # MLP flops\n  if config.num_experts > 1:\n    # calculation based on dropless implementation\n    if config.decoder_block in (DecoderBlockType.DEEPSEEK, DecoderBlockType.LLAMA4):\n      total_ffn_flops = calculate_routed_and_shared_ffn_tflops_per_device(config)\n    else:\n      gate_flops = 2 * config.per_device_batch_size * config.max_target_length * config.emb_dim * config.num_experts\n      total_ffn_flops = (\n          gate_flops + calculate_ffn_mamtul_tflops_per_device(config, config.mlp_dim) * config.num_experts_per_tok\n      )\n  else:\n    total_ffn_flops = calculate_ffn_mamtul_tflops_per_device(config, config.mlp_dim)\n\n  # Attention flops\n  if config.attention_type == \"mla\":\n    qkv_flops, noncausal_attention_flops, projection_flops = calculate_mla_tflops_per_device(config)\n  else:\n    qkv_flops = (\n        2\n        * config.per_device_batch_size\n        * config.max_target_length\n        * config.emb_dim\n        * (config.num_query_heads + 2 * config.num_kv_heads)\n        * config.head_dim\n    )\n    noncausal_attention_flops = (\n        4 * config.per_device_batch_size * config.max_target_length**2 * config.num_query_heads * config.head_dim\n    )\n    projection_flops = (\n        2\n        * config.per_device_batch_size\n        * config.max_target_length\n        * config.emb_dim\n        * config.num_query_heads\n        * config.head_dim\n    )\n\n  # Divide attention flops by 2 due to causal mask\n  # References:\n  # NVIDIA/Megatron-LM (2025 March): https://github.com/NVIDIA/Megatron-LM/blob/250b79415dcc4b660521273c87f15334c804eeae/megatron/training/training.py#L361-L362\n  # NVIDIA/NeMo (2025 April): https://github.com/NVIDIA/NeMo/blob/ba4d6d116463de512ff0cfc14641aa6cf4577a42/nemo/utils/flops_formulas.py#L259-L272\n  causal_attention_flops = noncausal_attention_flops / 2\n\n  # Embedding flops\n  embedding_flops = 2 * config.per_device_batch_size * config.max_target_length * config.emb_dim * config.vocab_size\n\n  # Combine flops with number of decoder layers\n  if config.decoder_block == DecoderBlockType.GEMMA2:\n    attention_tflops, learnable_weight_tflops = calculate_gemma2_tflops_training_per_device(\n        config, total_ffn_flops, qkv_flops, projection_flops, embedding_flops\n    )\n  elif config.decoder_block == DecoderBlockType.GEMMA3:\n    attention_tflops, learnable_weight_tflops = calculate_mixed_attention_model_tflops_training_per_device(\n        config, total_ffn_flops, qkv_flops, projection_flops, embedding_flops, attention_pattern_length=6\n    )\n  elif config.decoder_block == DecoderBlockType.GPT_OSS:\n    attention_tflops, learnable_weight_tflops = calculate_mixed_attention_model_tflops_training_per_device(\n        config, total_ffn_flops, qkv_flops, projection_flops, embedding_flops, attention_pattern_length=2\n    )\n  elif config.decoder_block == DecoderBlockType.LLAMA4:\n    # Use the new helper to calculate attention TFLOPs correctly.\n    attention_tflops = calculate_llama4_attention_tflops(config)\n    # The learnable weight calculation remains the same as it correctly handles Llama4's MoE structure.\n    learnable_weight_tflops = (\n        (total_ffn_flops + (qkv_flops + projection_flops) * config.num_decoder_layers + embedding_flops) * 3 / 10**12\n    )\n  elif config.decoder_block == DecoderBlockType.DEEPSEEK:\n    learnable_weight_tflops = (\n        (total_ffn_flops + (qkv_flops + projection_flops) * config.num_decoder_layers + embedding_flops) * 3 / 10**12\n    )\n    attention_tflops = causal_attention_flops * config.num_decoder_layers * 3 / 10**12\n  else:\n    # multiply by 3 for both feed forward and back propagation flops\n    learnable_weight_tflops = (\n        ((total_ffn_flops + qkv_flops + projection_flops) * config.num_decoder_layers + embedding_flops) * 3 / 10**12\n    )\n    attention_tflops = causal_attention_flops * config.num_decoder_layers * 3 / 10**12\n\n  learnable_weight_tflops = learnable_weight_tflops * config.gradient_accumulation_steps\n  attention_tflops = attention_tflops * config.gradient_accumulation_steps\n\n  # DPO includes one additional forward pass per gradient accumulation step\n  if config.use_dpo:\n    reference_model_tflops = learnable_weight_tflops / 3  # additional forward pass\n    reference_model_attention_tflops = attention_tflops / 3\n    attention_tflops = attention_tflops + reference_model_attention_tflops\n  else:\n    reference_model_tflops = 0\n\n  total_tflops = learnable_weight_tflops + attention_tflops + reference_model_tflops\n\n  if config.use_multimodal:\n    # Add vision layers TFLOPs for multimodal models\n    mm_total_tflops, mm_learnable_weight_tflops, mm_attention_tflops = calculate_vision_encoder_tflops(config)\n    if log:\n      print(\n          f\"{config.model_name} vision layers per train step:\\n\",\n          f\"Total TFLOPs: {mm_total_tflops:.2f} \\n\",\n          f\"split as {100 * mm_learnable_weight_tflops/mm_total_tflops:.2f}% learnable weight flops\",\n          f\"and {100 * mm_attention_tflops/mm_total_tflops:.2f}% attention flops;\\n\",\n          f\"learnable weight {mm_learnable_weight_tflops:.2f} TFLOPs, attention {mm_attention_tflops:.2f} TFLOPs\",\n      )\n    total_tflops += mm_total_tflops\n    learnable_weight_tflops += mm_learnable_weight_tflops\n    attention_tflops += mm_attention_tflops\n\n  if log:\n    print(\n        \"Per train step:\\n\",\n        f\"Total TFLOPs: {total_tflops:.2f} \\n\",\n        f\"split as {100 * learnable_weight_tflops/total_tflops:.2f}% learnable weight flops\",\n        f\"and {100 * attention_tflops/total_tflops:.2f}% attention flops\",\n    )\n  return total_tflops, learnable_weight_tflops, attention_tflops",
        "analysis": {
            "functionality": "Calculates the training TeraFLOPs (TFLOPs) per device for a single training step, breaking it down into learnable weight FLOPs and attention FLOPs.",
            "usage": "This function is used for performance analysis and estimation. It takes a configuration object (`config`) detailing the model architecture and training parameters. It returns a tuple of three floats: `(total_tflops, learnable_weight_tflops, attention_tflops)`."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_prefill_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_prefill_tflops_per_device(num_model_parameters, prefill_length, config, log=True):\n  \"\"\"Calculate training TFLOP\"\"\"\n  learnable_weight_tflops = 2 * num_model_parameters * prefill_length / jax.device_count() / 1e12\n  noncausal_attention_flops = (\n      4\n      * config.num_query_heads\n      * config.num_decoder_layers\n      * config.head_dim\n      * prefill_length**2\n      / jax.device_count()\n      / 1e12\n  )\n  causal_attention_tflops = noncausal_attention_flops / 2  # due to causality in attention\n  total_tflops = learnable_weight_tflops + causal_attention_tflops\n\n  if log:\n    print(\n        \"Per prefill step per device: \\n\",\n        f\"\\tTotal TFLOPs: {total_tflops:.2f} \\n\",\n        f\"\\t\\tLearnable weight TFLOPs: {learnable_weight_tflops:.2f} \",\n        f\"({100 * learnable_weight_tflops/total_tflops:.2f})% of Total\\n\",\n        f\"\\t\\tCausal attention TFLOPs: {causal_attention_tflops:.2f} \",\n        f\"({100 * causal_attention_tflops/total_tflops:.2f})% of Total\",\n    )\n  return total_tflops, learnable_weight_tflops, causal_attention_tflops",
        "analysis": {
            "module_type": "tflops_calculator",
            "purpose": "Calculates the theoretical TFLOPs per device for a single prefill step of a transformer model, breaking it down into learnable weight and causal attention components.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calculate TFLOPs from learnable weights using the total number of model parameters and the prefill sequence length.",
                "Calculate the TFLOPs for a non-causal attention mechanism based on model configuration and prefill length.",
                "Halve the non-causal attention TFLOPs to account for the causal mask.",
                "Sum the learnable weight TFLOPs and causal attention TFLOPs to get the total.",
                "Optionally print a detailed breakdown of the TFLOPs.",
                "Return the total, learnable weight, and causal attention TFLOPs."
            ],
            "output": {
                "shape": "A tuple containing three scalar float values: (total_tflops, learnable_weight_tflops, causal_attention_tflops)."
            },
            "dependencies": [
                "jax"
            ],
            "parameters": {
                "num_model_parameters": "Total number of learnable parameters in the model.",
                "prefill_length": "The length of the input sequence for the prefill computation.",
                "config": "A configuration object containing model architecture details like `num_query_heads`, `num_decoder_layers`, and `head_dim`.",
                "log": "A boolean flag to enable or disable printing the TFLOPs breakdown."
            },
            "notes": [
                "The calculation is based on the formula from Appendix B of the PaLM paper (https://arxiv.org/pdf/2204.02311.pdf).",
                "The result is per device, as the total FLOPs are divided by `jax.device_count()`.",
                "This function calculates FLOPs for a single forward pass (prefill), not a full training step that includes backward propagation."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_mesh_axes_used_by_tensor_spec",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_mesh_axes_used_by_tensor_spec(tensor_sharding_spec):\n  \"\"\"\n  Extracts the set of mesh axis names that a tensor's PartitionSpec uses.\n\n  This function inspects a tensor's sharding specification (PartitionSpec) and\n  identifies which mesh axes are actively used for sharding. If a tensor is not\n  sharded (i.e., fully replicated), the resulting set will be empty.\n\n  Args:\n    tensor_sharding_spec: The PartitionSpec of a tensor, which defines how it's partitioned across the mesh.\n    It can be None or contain strings and iterables representing the mesh axes.\n    all_mesh_axis_names: A collection of all available mesh axis names in the current device mesh.\n\n  Returns:\n    A set of strings, where each string is a mesh axis name used by the\n    tensor's sharding spec. Returns an empty set for unsharded tensors.\n  \"\"\"\n  # Flatten the sharding spec, as it can contain nested iterables (e.g., ('data', 'mdl')).\n  tensor_sharding_spec = sum(\n      [\n          [axis] if isinstance(axis, str) else list(axis) if isinstance(axis, Iterable) else []\n          for axis in tensor_sharding_spec\n      ],\n      [],\n  )\n  return tensor_sharding_spec",
        "analysis": {
            "functionality": "Flattens a JAX PartitionSpec or a similar nested iterable structure into a single list of mesh axis names.",
            "usage": "Call this function with a tensor's sharding specification (e.g., a `jax.sharding.PartitionSpec`) to get a flat list of all mesh axis strings it contains. The input can be an iterable containing strings and other iterables. The output is a list of strings."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#_get_nontrival_mesh_axes",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def _get_nontrival_mesh_axes(mesh):\n  \"\"\"\n  Returns mesh axes from config that are valid and have more than one shard.\n\n  This function identifies which of the predefined potential sharding axes are\n  actually present in the current device mesh and are configured with a size\n  greater than one (i.e., are actually sharded).\n\n  Args:\n    mesh: The device mesh object, which contains information about the mesh topology, including axis names and their sizes.\n\n  Returns:\n    A set of strings, where each string is a mesh axis name that is both\n    pre-configured as a target for sharding and has more than one shard in the mesh.\n  \"\"\"\n\n  target_sharding_axes_config = [\n      \"fsdp\",\n      \"fsdp_transpose\",\n      \"sequence\",\n      \"context\",\n      \"context_autoregressive\",\n      \"tensor\",\n      \"tensor_transpose\",\n      \"tensor_sequence\",\n      \"stage\",\n      \"expert\",\n  ]\n\n  # Filter the target axes to find those that exist in the current mesh\n  # and have a size greater than 1, meaning they are actually used for sharding.\n  return {axis for axis in target_sharding_axes_config if axis in mesh.axis_names and mesh.shape[axis] > 1}",
        "analysis": {
            "module_type": "mesh_axis_filter",
            "purpose": "Filters a predefined list of potential sharding axes to identify which ones are actively used (size > 1) in a given JAX device mesh.",
            "input": {
                "shape": "JAX Mesh object",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Define a hardcoded list of target sharding axis names.",
                "Use a set comprehension to iterate through the hardcoded list.",
                "Filter the list to include only axes that are present in the input `mesh.axis_names`.",
                "Further filter the list to include only axes whose size in `mesh.shape` is greater than 1.",
                "Return the resulting set of axis names."
            ],
            "output": {
                "shape": "A set of strings, where each string is a non-trivial mesh axis name."
            },
            "dependencies": [
                "JAX Mesh object"
            ],
            "parameters": {
                "mesh": "The device mesh object, which contains information about the mesh topology, including axis names and their sizes."
            },
            "notes": [
                "The list of potential sharding axes to check against is hardcoded inside the function.",
                "An axis is considered 'non-trivial' if its size in the mesh is greater than 1, indicating it is actually used for sharding."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#_analyze_sharding",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def _analyze_sharding(params, mesh, valid_target_mesh_axes):\n  \"\"\"\n  Analyzes parameters to find which are unsharded on any valid mesh axis.\n\n  This function iterates through all parameters in a model, checking their\n  sharding specifications. It identifies parameters that are not sharded along any\n  of the provided valid target axes (i.e., they are fully replicated across these axes).\n\n  Args:\n    params: A PyTree of model parameters.\n    mesh: The device mesh object.\n    valid_target_mesh_axes: A set of mesh axis names that are considered valid targets for sharding.\n\n  Returns:\n    A tuple containing:\n      - unsharded_params_total_size (int): The total size (number of elements) of all parameters found to be\n        unsharded on the target axes.\n      - problematic_tensors_details (list): A list of dictionaries, where each\n        dictionary contains details about a tensor that is not sharded on any of the target axes.\n  \"\"\"\n  unsharded_params_total_size = 0  # Initialize a counter for the size of unsharded parameters.\n  problematic_tensors_details = []  # Initialize a list to store details of problematic tensors.\n\n  # Get a flattened list of all parameters (leaves) in the PyTree, along with their paths.\n  all_params_leaves = jtu.tree_leaves_with_path(params)\n\n  for path, p_leaf in all_params_leaves:  # Iterate over each parameter leaf\n    param_name_str = jtu.keystr(path)  # Convert the tree path to a readable string\n\n    # Check that sharding and spec exist and are valid\n    sharding = getattr(p_leaf, \"sharding\", None)\n    spec = getattr(sharding, \"spec\", None)\n    assert sharding is not None and spec is not None and isinstance(spec, P), (\n        f\"Parameter '{param_name_str}' is missing a valid '.sharding.spec'.\"\n        \"Expected 'p_leaf.sharding.spec' to be a non-null 'partitionspec'.\"\n    )\n\n    current_sharding_spec = p_leaf.sharding.spec  # Extract the current tensor's sharding spec\n    # Identify axes used for sharding\n    mesh_axes_used = get_mesh_axes_used_by_tensor_spec(current_sharding_spec)\n    # Check if the parameter is sharded on all the valid target axes.\n    is_sharded_on_all_target_axis = all(axis in mesh_axes_used for axis in valid_target_mesh_axes)\n\n    # If the parameter is not sharded on all of the target axes, it's considered \"problematic.\"\n    if not is_sharded_on_all_target_axis:\n      unsharded_params_total_size += p_leaf.size  # Add to total unsharded parameter size\n      unsharded_axes = set(valid_target_mesh_axes) - set(mesh_axes_used)\n      # Add detailed info to list of problematic tensors\n      problematic_tensors_details.append(\n          {\n              \"name\": param_name_str,  # Tensor name\n              \"size\": p_leaf.size,  # tensor size\n              \"shape\": p_leaf.shape,  # tensor shape\n              \"spec\": str(current_sharding_spec),  # Tensor sharding spec as string\n              \"available_axes\": sorted(list(valid_target_mesh_axes)),  # Axes that could be used for sharding\n              \"unsharded_axes\": sorted(list(unsharded_axes)),  # Unsharded axes\n          }\n      )\n  # Return the total size of unsharded parameters and the list of problematic tensors.\n  return unsharded_params_total_size, problematic_tensors_details",
        "analysis": {
            "functionality": "Analyzes a PyTree of model parameters to identify tensors that are not sharded along a specified set of valid mesh axes. It calculates the total size of these unsharded parameters and returns detailed information about each one.",
            "usage": "Call this function with a PyTree of parameters, a JAX mesh object, and a set of target mesh axis names. It returns a tuple containing the total size of unsharded parameters and a list of dictionaries, where each dictionary details a parameter that is not sharded on all the target axes."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#_raise_if_unsharded_exceeds_tolerance",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def _raise_if_unsharded_exceeds_tolerance(unsharded_size, total_size, tolerance, problematic_tensors_details):\n  \"\"\"\n  Raises an AssertionError if the percentage of unsharded parameters exceeds the given tolerance.\n\n  This function calculates the proportion of model parameters that are unsharded\n  and compares it against a specified tolerance. If the tolerance is exceeded,\n  it constructs and raises a detailed error message.\n\n  Args:\n    unsharded_size: The total size of parameters not sharded on target axes.\n    total_size: The total size of all parameters in the model.\n    tolerance: A float (e.g., 0.05 for 5%) representing the maximum allowed percentage of unsharded parameters.\n    problematic_tensors_details: A list of details about the unsharded tensors,\n    used to generate an informative error message.\n\n  Raises:\n    AssertionError: If the percentage of unsharded parameters is greater than the tolerance.\n  \"\"\"\n  if total_size <= 0:\n    raise ValueError(\"Total size must be greater than zero.\")\n\n  # Calculate the percentage of unsharded parameters.\n  unsharded_param_perc = unsharded_size / total_size\n\n  # If the percentage is over the tolerance, prepare and raise an error.\n  if unsharded_param_perc > tolerance:\n    # Sort the problematic tensors by size to show the largest ones first.\n    problematic_tensors_details.sort(key=lambda x: x[\"size\"], reverse=True)\n\n    # Begin constructing the error message.\n    error_msg_lines = [\n        f\"Unsharded parameter percentage ({unsharded_param_perc:.2%})\" f\"exceeds tolerance ({tolerance:.2%}).\"\n    ]\n    # Add a header explaining the issue.\n    error_msg_lines.append(\n        \"The following large tensors are replicated (unsharded) but could be sharded on at \"\n        \"least one of the available axes:\"\n    )\n    # Add details for the top 5 largest problematic tensors.\n    for detail in problematic_tensors_details[:5]:  # Show top 5 largest problematic tensors\n      error_msg_lines.append(\n          f\" - Name: {detail['name']}(Size: {detail['size']}, Shape: {detail['spec']}, Spec: {detail['spec']}) \"\n          f\" is unsharded on axis: {detail['unsharded_axes']}\"\n          f\" could be sharded on: {detail['available_axes']}\"\n      )\n\n    # Raise the assertion error with the combined, formatted message.\n    raise AssertionError(\"\\n\".join(error_msg_lines))",
        "analysis": {
            "module_type": "unsharded_parameter_check",
            "purpose": "Raises an AssertionError with a detailed message if the percentage of unsharded model parameters exceeds a specified tolerance.",
            "input": {
                "shape": "unsharded_size (int), total_size (int), tolerance (float), problematic_tensors_details (list of dicts)",
                "dtype": "int, float, list"
            },
            "processing_steps": [
                "Validate that `total_size` is greater than zero, raising a ValueError if not.",
                "Calculate the percentage of unsharded parameters by dividing `unsharded_size` by `total_size`.",
                "Check if the calculated percentage is greater than the `tolerance`.",
                "If the tolerance is exceeded, sort `problematic_tensors_details` by tensor size in descending order.",
                "Construct a multi-line error message detailing the failure and listing the top 5 largest problematic tensors with their sharding information.",
                "Raise an AssertionError with the formatted error message."
            ],
            "output": {
                "shape": "N/A (The function either returns None or raises an AssertionError)."
            },
            "dependencies": [],
            "parameters": {
                "unsharded_size": "The total size of parameters not sharded on target axes.",
                "total_size": "The total size of all parameters in the model.",
                "tolerance": "A float (e.g., 0.05 for 5%) representing the maximum allowed percentage of unsharded parameters.",
                "problematic_tensors_details": "A list of dictionaries containing details about the unsharded tensors, used to generate an informative error message."
            },
            "notes": [
                "This is a helper function designed to fail loudly and provide actionable debugging information when a model's sharding configuration is suboptimal.",
                "The error message is specifically formatted to highlight the top 5 largest unsharded tensors to guide the user.",
                "The function's primary output is a side effect: raising an `AssertionError` or `ValueError`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#assert_params_sufficiently_sharded",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def assert_params_sufficiently_sharded(params, mesh, tolerance):\n  \"\"\"\n  Asserts that the total size of replicated parameters is within a given tolerance.\n\n  This is the main function that orchestrates the sharding analysis. It determines\n  the total number of parameters, identifies valid sharding axes, analyzes the\n  sharding of all parameters, and then raises an error if the amount of\n  unsharded parameters exceeds the specified tolerance.\n\n  Args:\n    params: A PyTree of model parameters.\n    mesh: The device mesh object.\n    tolerance: A float representing the maximum allowed percentage of unsharded parameters.\n  \"\"\"\n  # Calculate the total size of all parameters in the model.\n  total_num_params = max_utils.calculate_bytes_from_pytree(params)\n\n  # Get the set of nontrival mesh axes that can be used for sharding.\n  valid_target_mesh_axes = _get_nontrival_mesh_axes(mesh)\n  # If there are no valid axes to shard along, there's nothing to check, so we can exit.\n  if not valid_target_mesh_axes:\n    return  # Exit early\n\n  # Analyze the parameters to find the total size of unsharded parameters\n  # and get details on which tensors are problematic.\n  unsharded_params_total_size, problematic_tensors_details = _analyze_sharding(params, mesh, valid_target_mesh_axes)\n\n  # Check if the amount of unsharded parameters is within the tolerance and\n  # raise an exception if it is not.\n  _raise_if_unsharded_exceeds_tolerance(\n      unsharded_params_total_size, total_num_params, tolerance, problematic_tensors_details\n  )",
        "analysis": {
            "functionality": "Analyzes the sharding of model parameters and asserts that the total size of replicated (unsharded) parameters does not exceed a specified tolerance.",
            "usage": "Call this function to validate the sharding configuration of a model. Provide the model parameters as a PyTree (`params`), the JAX device mesh (`mesh`), and a float `tolerance` for the maximum allowed percentage of unsharded parameters. The function raises an AssertionError if the validation fails."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#apply_gradient_clipping",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def apply_gradient_clipping(raw_grads, state, clipping_threshold):\n  \"\"\"Applies gradient clipping to raw gradients, with special handing for FLAX fp8 stats.\n\n  Args:\n    raw_grads: A pytree of raw gradients.\n    state: The current optimizer state.\n    clipping_threshold: The gradient clipping threshold.\n\n  Returns:\n    A pytree of clipped gradients.\n  \"\"\"\n  gradient_clip_transformation = optax.clip_by_global_norm(clipping_threshold)\n  if OVERWRITE_WITH_GRADIENT in raw_grads:\n    # Scales + Amax History for Delayed Tensor Scaling SHOULD NOT be clipped or affect clipping\n    fp8_stats = raw_grads.pop(OVERWRITE_WITH_GRADIENT)\n    grads, _ = gradient_clip_transformation.update(raw_grads, state, None)\n    grads[OVERWRITE_WITH_GRADIENT] = fp8_stats  # pytype: disable=unsupported-operands\n    raw_grads[OVERWRITE_WITH_GRADIENT] = fp8_stats  # pytype: disable=unsupported-operands\n  else:\n    grads, _ = gradient_clip_transformation.update(raw_grads, state, None)\n\n  return grads",
        "analysis": {
            "functionality": "This function applies global norm gradient clipping to a pytree of gradients. It includes special handling to exclude FLAX fp8 statistics (identified by the `_overwrite_with_gradient` key) from the clipping calculation, ensuring they are preserved.",
            "usage": "To use this function, provide a pytree of raw gradients (`raw_grads`), the current optimizer state (`state`), and a scalar clipping threshold (`clipping_threshold`). The function returns a new pytree of clipped gradients with the same structure as the input."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_nested_value",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_nested_value(dictionary, nested_key, default=None):\n  \"\"\"\n  Retrieves a value from a nested key in a dictionary.\n\n  Args:\n      dictionary: The dictionary to search in.\n      nested_key: A tuple representing the nested key, e.g., ('level1', 'level2', 'key').\n      default: The value to return if the nested key is not found.\n\n  Returns:\n      The value associated with the nested key, or the default value if not found.\n  \"\"\"\n  current_level = dictionary\n\n  for key in nested_key:\n    if not isinstance(current_level, dict) or key not in current_level:\n      return default\n    current_level = current_level[key]\n  return current_level",
        "analysis": {
            "module_type": "nested_dictionary_accessor",
            "purpose": "Retrieves a value from a nested dictionary using a tuple of keys representing the path.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initialize a variable `current_level` with the input dictionary.",
                "Iterate through each key in the `nested_key` tuple.",
                "In each iteration, check if `current_level` is a dictionary and if the key exists within it.",
                "If the check fails, return the provided `default` value immediately.",
                "If the check succeeds, update `current_level` to the value associated with the key.",
                "After iterating through all keys, return the final `current_level`."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [],
            "parameters": {
                "dictionary": "The dictionary to search in.",
                "nested_key": "A tuple representing the nested key path, e.g., ('level1', 'level2', 'key').",
                "default": "The value to return if the nested key is not found."
            },
            "notes": [
                "The function short-circuits and returns the default value as soon as any key in the path is not found or an intermediate value is not a dictionary."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#init_decode_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def init_decode_state(apply_fn, params) -> train_state.TrainState:\n  \"\"\"Init train state with null opt state for decode.\"\"\"\n  state = train_state.TrainState(step=0, apply_fn=apply_fn, params=params, tx=None, opt_state={})  # type: ignore\n  return state",
        "analysis": {
            "module_type": "state_initializer",
            "purpose": "Initializes a Flax TrainState object specifically for decoding, with a null optimizer state.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Instantiates a `train_state.TrainState` object.",
                "Sets `step` to 0, `tx` (optimizer transformation) to None, and `opt_state` to an empty dictionary.",
                "Assigns the provided `apply_fn` and `params` to the state.",
                "Returns the newly created `TrainState` object."
            ],
            "output": {
                "shape": "An instance of `flax.training.train_state.TrainState`."
            },
            "dependencies": [
                "flax.training.train_state.TrainState"
            ],
            "parameters": {},
            "notes": [
                "This function is used to create a state for inference or decoding where optimizer state is not required.",
                "The `# type: ignore` comment indicates that the arguments might intentionally deviate from the standard `TrainState` creation signature, specifically by providing null optimizer components."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#init_training_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def init_training_state(apply_fn, params, tx):\n  \"\"\"Init train state with null opt state for decode.\"\"\"\n  state = train_state.TrainState.create(apply_fn=apply_fn, params=params, tx=tx)\n  return state",
        "analysis": {
            "functionality": "This function initializes a Flax `TrainState` object for training.",
            "usage": "Call this function with a model's apply function (`apply_fn`), its parameters (`params`), and an Optax optimizer transformation (`tx`). It returns a `train_state.TrainState` object that bundles these components and initializes the optimizer's state, ready for training."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#init_initial_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def init_initial_state(model, tx, config, is_training, key):\n  \"\"\"\n  We pass in \"static\" objects like model, tx, config as JAX compares them by\n  object hash, and instantiating them inside causes pjit top-level annotations\n  to fail to match as pytree prefixes if we re-instantiate.\n\n  Args: model, tx, config, is_training, key\n  \"\"\"\n  input_shape = (config.micro_batch_size_to_train_on, config.max_target_length)\n  image_shape = multimodal_utils.get_dummy_image_shape_for_init(\n      config.model_name, batch_size=config.micro_batch_size_to_train_on\n  )\n  model_vars = model.init(\n      {\"params\": key, \"dropout\": key, \"aqt\": key},\n      np.ones(input_shape, dtype=jnp.int32),\n      np.ones(input_shape, dtype=jnp.int32),\n      encoder_images=np.ones(image_shape, dtype=jnp.int32) if config.use_multimodal else None,\n      # nnx_method=\"no_op\",\n  )\n  if is_training:\n    return init_training_state(model.apply, model_vars, tx)\n  return init_decode_state(model.apply, model_vars)",
        "analysis": {
            "module_type": "model_state_initializer",
            "purpose": "Initializes model variables using dummy data and creates either a training state (with optimizer) or a decode state (without optimizer) based on a flag.",
            "input": {
                "shape": "N/A (Inputs are a model object, optimizer, config, boolean flag, and a JAX PRNGKey).",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Determine dummy input shapes for text and optionally images based on the configuration.",
                "Create dummy numpy arrays with the determined shapes.",
                "Initialize model variables by calling `model.init` with the dummy data and a PRNG key for params, dropout, and aqt.",
                "Conditionally check the `is_training` flag.",
                "If `is_training` is true, call `init_training_state` to create a `TrainState` object with an optimizer state.",
                "If `is_training` is false, call `init_decode_state` to create a `TrainState` object without an optimizer state.",
                "Return the created `TrainState` object."
            ],
            "output": {
                "shape": "A `flax.training.train_state.TrainState` PyTree containing the model parameters, apply function, and optionally the optimizer state."
            },
            "dependencies": [
                "multimodal_utils.get_dummy_image_shape_for_init",
                "init_training_state",
                "init_decode_state",
                "numpy",
                "jax.numpy"
            ],
            "parameters": {
                "config.micro_batch_size_to_train_on": "The batch size used for creating dummy input tensors for initialization.",
                "config.max_target_length": "The sequence length used for creating dummy input tensors for initialization.",
                "config.use_multimodal": "Boolean flag that determines whether to create dummy image inputs for multimodal model initialization.",
                "config.model_name": "Used to determine the correct dummy image shape for multimodal initialization."
            },
            "notes": [
                "The function is designed to be JIT-compiled. Static objects like `model`, `tx`, and `config` are passed as arguments to avoid re-instantiation issues with JAX's object hashing, which would break pjit annotations.",
                "It uses `np.ones` to create dummy tensors of the correct shape and dtype to trigger model parameter initialization via `model.init`.",
                "The returned state differs based on the `is_training` flag: a full training state includes optimizer state, while a decode state does not."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#setup_decode_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def setup_decode_state(model, config, rng, mesh, checkpoint_manager):\n  \"\"\"Setup decode state by loading params from a checkpoint.\n  Args:\n    model: the flax model to initialize\n    config: config object\n    rng: jax.prng key\n    mesh: jax.devices() mesh\n    checkpoint_manager: Checkpoint manager\n\n  Returns:\n    state: state with decode params loaded from the checkpoint\n    state_mesh_annotations: the mesh annotations for the state\n  \"\"\"\n  if not config.load_parameters_path:\n    # generate random params\n    max_logging.log(\"No decode checkpoint specified - generating random weights.\")\n    state, state_mesh_annotations, _, _ = setup_initial_state(\n        model, None, None, config, rng, mesh, checkpoint_manager, False\n    )\n  else:\n    # Load params from checkpoint\n    max_logging.log(f\"Loading decode params from {config.load_parameters_path}\")\n    unboxed_abstract_state, state_mesh_annotations, _ = get_abstract_state(model, None, config, rng, mesh, False)\n    with nn_partitioning.axis_rules(config.logical_axis_rules):\n      params = checkpointing.load_params_from_path(\n          config.load_parameters_path,\n          unboxed_abstract_state.params,\n          config.checkpoint_storage_concurrent_gb,\n          config.checkpoint_storage_use_ocdbt,\n          config.checkpoint_storage_use_zarr3,\n      )\n    state = init_decode_state(None, params)\n\n  state = max_utils.unbox_logicallypartioned(state)\n  return state, state_mesh_annotations",
        "analysis": {
            "module_type": "decode_state_setup",
            "purpose": "Initializes a model's state for decoding, either by loading parameters from a checkpoint or by generating random weights.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check if `config.load_parameters_path` is provided.",
                "If a path is not provided, call `setup_initial_state` to generate a state with random weights.",
                "If a path is provided, get the abstract state shape and sharding via `get_abstract_state`.",
                "Load parameters from the specified path using `checkpointing.load_params_from_path`.",
                "Create a `TrainState` object suitable for decoding using `init_decode_state` with the loaded parameters.",
                "Unbox the logically partitioned state using `max_utils.unbox_logicallypartioned`.",
                "Return the final state and its mesh annotations."
            ],
            "output": {
                "shape": "A tuple containing: (1) a PyTree representing the `TrainState` for decoding, and (2) a PyTree of mesh annotations for the state."
            },
            "dependencies": [
                "setup_initial_state",
                "get_abstract_state",
                "checkpointing.load_params_from_path",
                "init_decode_state",
                "max_utils.unbox_logicallypartioned",
                "flax.training.train_state.TrainState"
            ],
            "parameters": {
                "load_parameters_path": "Path to a checkpoint to load model parameters from. If not provided, random weights are generated.",
                "logical_axis_rules": "Rules for mapping logical tensor axes to physical mesh axes, used for sharding.",
                "checkpoint_storage_concurrent_gb": "Concurrent gigabytes to use for checkpoint loading.",
                "checkpoint_storage_use_ocdbt": "Boolean flag to indicate if OCDBT is used for checkpoint storage.",
                "checkpoint_storage_use_zarr3": "Boolean flag to indicate if Zarr3 is used for checkpoint storage."
            },
            "notes": [
                "The function's primary control flow depends on the presence of `config.load_parameters_path`.",
                "When loading from a checkpoint, it first determines the abstract shape and sharding of the state before loading the parameters.",
                "The returned state is specifically for decoding, meaning it's a `TrainState` with no optimizer state (`tx=None`, `opt_state={}`)."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#setup_training_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def setup_training_state(model, data_iterator, tx, config, rng, mesh, checkpoint_manager):\n  is_training = True\n  return setup_initial_state(\n      model,\n      data_iterator,\n      tx,\n      config,\n      rng,\n      mesh,\n      checkpoint_manager,\n      is_training,\n  )",
        "analysis": {
            "functionality": "This function initializes the model's state for training. It acts as a wrapper around `setup_initial_state`, explicitly setting the `is_training` flag to `True` before making the call.",
            "usage": "Call this function with the model, data iterator, optimizer, configuration, RNG key, device mesh, and checkpoint manager to get the initial training state, mesh annotations, sharding specifications, and the potentially updated data iterator. Example: `state, annotations, shardings, iterator = setup_training_state(model, data_iterator, tx, config, rng, mesh, checkpoint_manager)`."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#setup_initial_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def setup_initial_state(\n    model,\n    data_iterator,\n    tx,\n    config,\n    rng,\n    mesh,\n    checkpoint_manager,\n    is_training=True,\n):\n  \"\"\"We initialize the model and optimizer state, and optionally load from a\n  checkpoint as necessary.\n\n  Args:\n    model: the flax model to initialize\n    tx: the optax.GradientTransformation\n    config: config object\n    rng: jax.prng key\n    mesh: jax.devices() mesh\n    checkpoint_manager: an Orbax checkpointing.CheckpointManager object\n    is_training: True to initialize training state, False for decode state\n\n  Returns:\n    state: the initialized train state\n    state_mesh_annotations: the mesh annotations for the train state\n  \"\"\"\n\n  unboxed_abstract_state, state_mesh_annotations, state_mesh_shardings = get_abstract_state(\n      model, tx, config, rng, mesh, is_training\n  )\n\n  # Initialization\n  with nn_partitioning.axis_rules(config.logical_axis_rules):\n    restored, raw_params = checkpointing.load_state_if_possible(\n        checkpoint_manager,\n        data_iterator,\n        config.load_parameters_path,\n        config.load_full_state_path,\n        config.checkpoint_storage_concurrent_gb,\n        unboxed_abstract_state,\n        config.enable_single_replica_ckpt_restoring,\n        config.dataset_type,\n        use_ocdbt=config.checkpoint_storage_use_ocdbt,\n        use_zarr3=config.checkpoint_storage_use_zarr3,\n        enable_orbax_v1=config.enable_orbax_v1,\n        checkpoint_conversion_fn=config.checkpoint_conversion_fn,\n        source_checkpoint_layout=config.source_checkpoint_layout,\n    )\n\n    if restored:\n      if isinstance(\n          checkpoint_manager,\n          (\n              emergency_checkpoint_manager.CheckpointManager,\n              emergency_replicator_checkpoint_manager.ReplicatorCheckpointManager,\n          ),\n      ):\n        state = restored\n      else:\n        if \"iter\" in restored and restored[\"iter\"] is not None:\n          data_iterator.local_iterator = restored[\"iter\"]\n        state = restored[\"items\"]\n    else:\n      init_state_partial = functools.partial(init_initial_state, model, tx, config, is_training)\n      init_state_partial.__name__ = \"initialize_state\"\n      # pylint: disable=not-callable\n      state = jax.jit(\n          init_state_partial,\n          in_shardings=None,\n          out_shardings=state_mesh_shardings,\n      )(rng)\n      if raw_params:  # If we loaded a partial state, we need to merge it.\n        state = state.replace(params=raw_params)\n\n  state = max_utils.unbox_logicallypartioned(state)\n\n  return state, state_mesh_annotations, state_mesh_shardings, data_iterator",
        "analysis": {
            "module_type": "model_state_initializer",
            "purpose": "Initializes the model and optimizer state, optionally loading from a checkpoint if one is available.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Call `get_abstract_state` to determine the shape, sharding, and annotations for the model and optimizer state.",
                "Use `checkpointing.load_state_if_possible` to attempt to restore the state from a checkpoint.",
                "If a checkpoint is successfully restored, extract the state and update the data iterator's step.",
                "If no checkpoint is restored, initialize a new state from scratch using a JIT-compiled `init_initial_state` function.",
                "If partial parameters (`raw_params`) were loaded without a full state, merge them into the newly initialized state.",
                "Call `max_utils.unbox_logicallypartioned` to convert the state from logically partitioned arrays to regular JAX arrays.",
                "Return the final state, mesh annotations, mesh shardings, and the potentially updated data iterator."
            ],
            "output": {
                "shape": "Returns a tuple: (state, state_mesh_annotations, state_mesh_shardings, data_iterator)."
            },
            "dependencies": [
                "get_abstract_state",
                "checkpointing.load_state_if_possible",
                "init_initial_state",
                "max_utils.unbox_logicallypartioned",
                "jax.jit",
                "functools.partial",
                "nn_partitioning.axis_rules",
                "emergency_checkpoint_manager.CheckpointManager",
                "emergency_replicator_checkpoint_manager.ReplicatorCheckpointManager"
            ],
            "parameters": {
                "is_training": "A boolean flag that determines whether to initialize a full training state (with optimizer) or a simpler decode state (without optimizer).",
                "config.load_parameters_path": "Path to a checkpoint containing only model parameters to load.",
                "config.load_full_state_path": "Path to a checkpoint containing the full training state, including optimizer state and step."
            },
            "notes": [
                "The function's primary control flow depends on whether `checkpointing.load_state_if_possible` returns a restored state.",
                "It handles three scenarios: full state restoration, partial parameter restoration (initializing optimizer state but using loaded weights), and initialization from scratch.",
                "The data iterator object is passed through and may be modified in place if the checkpoint contains an iterator state."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_abstract_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_abstract_state(model, tx, config, rng, mesh, is_training=True):\n  \"\"\"Get a shaped abstraction of the state (including optimizer)\"\"\"\n  init_state_partial = functools.partial(init_initial_state, model, tx, config, is_training, rng)\n\n  with nn_partitioning.axis_rules(config.logical_axis_rules):\n    abstract_state = jax.eval_shape(init_state_partial)\n\n  state_logical_annotations = nn.get_partition_spec(abstract_state)\n\n  state_mesh_shardings = nn.logical_to_mesh_sharding(state_logical_annotations, mesh, config.logical_axis_rules)\n  if is_training and config.optimizer_memory_host_offload:\n    opt_state = jax.tree_util.tree_map(lambda x: x.with_memory_kind(kind=\"pinned_host\"), state_mesh_shardings.opt_state)\n    state_mesh_shardings = state_mesh_shardings.replace(opt_state=opt_state)\n  if is_training and config.parameter_memory_host_offload:\n    assert config.param_scan_axis == 0, \"You must set the scan axis 0 to enable parameter offloading.\"\n\n    def move(path, x):\n      max_logging.log(f\"max_utils.py: Moving {path} to host\")\n      return x.with_memory_kind(kind=\"pinned_host\")\n\n    params = jax.tree_util.tree_map_with_path(move, state_mesh_shardings.params)\n    state_mesh_shardings = state_mesh_shardings.replace(params=params)\n\n  abstract_sharded_state = jax.jit(init_state_partial, in_shardings=None, out_shardings=state_mesh_shardings).eval_shape()\n\n  unboxed_abstract_sharded_state = max_utils.unbox_logicallypartioned(abstract_sharded_state)\n  # Initialization\n  with mesh, nn_partitioning.axis_rules(config.logical_axis_rules):\n    state_mesh_annotations = nn.logical_to_mesh(state_logical_annotations)\n  return (\n      unboxed_abstract_sharded_state,\n      state_mesh_annotations,\n      state_mesh_shardings,\n  )",
        "analysis": {
            "functionality": "This function calculates the abstract shape, data type, and sharding specifications for a model's state, including parameters and optimizer state, without initializing the actual values. It determines how the state should be partitioned across a distributed device mesh and supports offloading parts of the state to host memory.",
            "usage": "Call this function with a model, optimizer, configuration, RNG key, and device mesh to get the necessary abstract state representations and sharding information required for JIT-compiling the actual state initialization. The returned values are used to set up the distributed training or decoding state. For example: `unboxed_abstract_state, state_mesh_annotations, state_mesh_shardings = get_abstract_state(model, tx, config, rng, mesh, is_training=True)`."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_prefill_kv_cache_annotations",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_prefill_kv_cache_annotations(model, config, rng, mesh, page_state: None | PageState = None):\n  \"\"\"Get a shaped abstraction of the state (including optimizer)\"\"\"\n\n  def init_kv_cache(model, config):\n    input_shape = (\n        config.micro_batch_size_to_train_on,\n        config.max_prefill_predict_length,\n    )\n    image_shape = multimodal_utils.get_dummy_image_shape_for_init(\n        config.model_name, batch_size=config.micro_batch_size_to_train_on\n    )\n\n    model_vars = model.init(\n        {\"params\": rng, \"dropout\": rng, \"aqt\": rng},\n        jnp.ones(input_shape),\n        jnp.ones(input_shape),\n        encoder_images=jnp.ones(image_shape) if config.use_multimodal else None,\n        model_mode=MODEL_MODE_PREFILL,\n        slot=0,\n        page_state=page_state,\n    )\n    return model_vars[\"cache\"]\n\n  with nn_partitioning.axis_rules(config.logical_axis_rules):\n    init_kv_cache_partial = functools.partial(init_kv_cache, model, config)\n    abstract_state = jax.eval_shape(init_kv_cache_partial)\n  state_logical_annotations = nn.get_partition_spec(abstract_state)\n  with mesh, nn_partitioning.axis_rules(config.logical_axis_rules):\n    state_mesh_annotations = nn.logical_to_mesh(state_logical_annotations)\n  return state_mesh_annotations",
        "analysis": {
            "functionality": "This function determines the sharding annotations for a model's Key-Value (KV) cache during the prefill phase of inference. It achieves this by performing a 'dry run' of the model's initialization with dummy data, using `jax.eval_shape` to get the abstract shape and sharding specification of the cache without allocating any actual device memory.",
            "usage": "To use this function, provide the model, configuration object, a JAX PRNG key, and the device mesh. It returns a PyTree of `jax.sharding.NamedSharding` objects that can be used to correctly initialize and partition the KV cache for the prefill step in a distributed environment. \n\n**Inputs**:\n- `model`: The Flax model instance.\n- `config`: The configuration object with model and sharding details.\n- `rng`: A JAX PRNG key.\n- `mesh`: The JAX device mesh.\n- `page_state` (optional): A `PageState` object for paged attention.\n\n**Output**:\nA PyTree of `jax.sharding.NamedSharding` objects representing the mesh-specific sharding annotations for the KV cache."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_kv_cache_annotations",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_kv_cache_annotations(model, config, rng, mesh, page_state: None | PageState = None):\n  \"\"\"Get a shaped abstraction of the state (including optimizer)\"\"\"\n\n  def init_kv_cache(model, config):\n    input_shape = (config.micro_batch_size_to_train_on, 1)\n    image_shape = multimodal_utils.get_dummy_image_shape_for_init(\n        config.model_name, batch_size=config.micro_batch_size_to_train_on\n    )\n\n    model_vars = model.init(\n        {\"params\": rng, \"dropout\": rng, \"aqt\": rng},\n        jnp.ones(input_shape),\n        jnp.ones(input_shape),\n        encoder_images=jnp.ones(image_shape) if config.use_multimodal else None,\n        model_mode=MODEL_MODE_AUTOREGRESSIVE,\n        slot=0,\n        page_state=page_state,\n    )\n    return model_vars[\"cache\"]\n\n  with nn_partitioning.axis_rules(config.logical_axis_rules):\n    init_kv_cache_partial = functools.partial(init_kv_cache, model, config)\n    abstract_state = jax.eval_shape(init_kv_cache_partial)\n  state_logical_annotations = nn.get_partition_spec(abstract_state)\n  with mesh, nn_partitioning.axis_rules(config.logical_axis_rules):\n    state_mesh_annotations = nn.logical_to_mesh(state_logical_annotations)\n  return state_mesh_annotations",
        "analysis": {
            "module_type": "kv_cache_annotation_generator",
            "purpose": "Computes the mesh sharding annotations for a model's key-value (KV) cache by performing a shape-only initialization for an autoregressive step.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Define a nested function `init_kv_cache` to initialize the model and extract the KV cache structure.",
                "Inside `init_kv_cache`, create dummy input tensors with a sequence length of 1.",
                "Call `model.init` in `MODEL_MODE_AUTOREGRESSIVE` with the dummy inputs to get the model variables.",
                "Extract the 'cache' PyTree from the initialized model variables.",
                "Use `functools.partial` to create a version of `init_kv_cache` with the model and config arguments fixed.",
                "Use `jax.eval_shape` on the partial function to get the abstract shape and dtype of the KV cache without allocating memory.",
                "Use `nn.get_partition_spec` to derive the logical sharding annotations from the abstract cache structure.",
                "Use `nn.logical_to_mesh` to convert the logical annotations into physical mesh sharding annotations.",
                "Return the final mesh annotations."
            ],
            "output": {
                "shape": "A PyTree of mesh sharding annotations (e.g., `jax.sharding.NamedSharding`) that matches the structure of the model's KV cache."
            },
            "dependencies": [
                "functools.partial",
                "jax.eval_shape",
                "flax.linen.partitioning.axis_rules",
                "flax.linen.get_partition_spec",
                "flax.linen.logical_to_mesh",
                "multimodal_utils.get_dummy_image_shape_for_init",
                "MODEL_MODE_AUTOREGRESSIVE"
            ],
            "parameters": {
                "model": "The Flax model instance to be initialized.",
                "config": "The configuration object containing model and sharding details like `logical_axis_rules` and `micro_batch_size_to_train_on`.",
                "rng": "A JAX PRNG key for initialization.",
                "mesh": "The JAX device mesh for determining physical sharding.",
                "page_state": "Optional PageState for paged attention KV cache initialization."
            },
            "notes": [
                "This function is a 'dry run' that determines sharding specifications without instantiating the actual large KV cache tensors.",
                "It specifically initializes the model for a single autoregressive step by using an input sequence length of 1.",
                "The function relies on Flax's partitioning utilities to translate the abstract model structure into a concrete sharding plan for a given device mesh."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#save_quantized_checkpoint_if_configured",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def save_quantized_checkpoint_if_configured(config, params):\n  \"\"\"Save quantized checkpoint if configured\"\"\"\n  assert config.quantization, \"quantization must be configured\"\n  if config.save_quantized_params_path:\n    checkpointing.save_params_to_path(\n        checkpoint_dir=config.save_quantized_params_path,\n        params=params,\n        use_ocdbt=config.checkpoint_storage_use_ocdbt,\n        use_zarr3=config.checkpoint_storage_use_zarr3,\n    )\n  else:\n    max_logging.log(\"Skipping saving quantized checkpoint as save_quantized_params_path is null.\")",
        "analysis": {
            "module_type": "checkpoint_saver",
            "purpose": "Saves quantized model parameters to a specified path if a path is provided in the configuration.",
            "input": {
                "shape": "config: Configuration object, params: PyTree of JAX arrays.",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Assert that `config.quantization` is enabled.",
                "Check if `config.save_quantized_params_path` is provided.",
                "If a path exists, call `checkpointing.save_params_to_path` with the parameters and storage options from the config.",
                "If no path is provided, log a message indicating that saving is skipped."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "checkpointing.save_params_to_path",
                "max_logging.log"
            ],
            "parameters": {
                "config.quantization": "A boolean flag that must be True to proceed with saving.",
                "config.save_quantized_params_path": "The directory path to save the quantized parameters. If None, saving is skipped.",
                "config.checkpoint_storage_use_ocdbt": "A boolean flag to control the use of OCDBT storage format.",
                "config.checkpoint_storage_use_zarr3": "A boolean flag to control the use of Zarr3 storage format."
            },
            "notes": [
                "This function has a side effect of writing files to the disk and does not return any value.",
                "The core operation is conditional on the `save_quantized_params_path` configuration value."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#add_config_to_summary_writer",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def add_config_to_summary_writer(config, summary_writer):\n  \"\"\"Writes config params to tensorboard\"\"\"\n  if jax.process_index() == 0:\n    for key, value in config.get_keys().items():\n      max_utils.add_text_to_summary_writer(key, str(value), summary_writer)",
        "analysis": {
            "module_type": "tensorboard_config_writer",
            "purpose": "Writes configuration parameters as text to a TensorBoard summary writer for experiment tracking.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check if the current JAX process index is 0.",
                "Iterate through key-value pairs from the `config.get_keys()` method.",
                "Call `max_utils.add_text_to_summary_writer` for each key-value pair, converting the value to a string."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "jax.process_index",
                "max_utils.add_text_to_summary_writer"
            ],
            "parameters": {
                "config": "A configuration object with a `get_keys()` method that returns a dictionary of its parameters.",
                "summary_writer": "A TensorBoard summary writer object."
            },
            "notes": [
                "The function only executes on the primary process (`jax.process_index() == 0`) to prevent redundant writes in a multi-process environment.",
                "All configuration values are cast to strings before being logged."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#logical_axis_rules_pp_act_as_dp",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def logical_axis_rules_pp_act_as_dp(logical_rules):\n  \"\"\"Add stage as a physical axes before data for each rule, so stage acts just like data instead of PP.\n  This is used when we want to pipeline only a subset of layers, and leave the rest like DP.\n  \"\"\"\n  new_rules = []\n  for key, physical_axes in logical_rules:\n    if isinstance(physical_axes, str):\n      physical_axes = (physical_axes,)\n    else:\n      physical_axes = tuple(physical_axes)\n    new_physical_axes = tuple(axis for axis in physical_axes if axis != \"stage\")\n    if \"data\" in new_physical_axes:\n      data_idx = new_physical_axes.index(\"data\")\n      new_physical_axes = new_physical_axes[0:data_idx] + (\"stage\",) + new_physical_axes[data_idx:]\n    new_rules.append((key, new_physical_axes))\n  return tuple(new_rules)",
        "analysis": {
            "functionality": "Modifies a list of logical axis partitioning rules by repositioning the 'stage' physical axis to be immediately before the 'data' axis. This makes pipeline parallelism ('stage') behave like data parallelism ('data') for partitioning.",
            "usage": "Input a list of tuples, where each tuple represents a logical-to-physical axis mapping rule (e.g., `[('embed', ('data', 'fsdp'))]`). The function returns a new tuple of tuples with the modified rules, where 'stage' is inserted before 'data' in the physical axis definitions."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#create_device_mesh",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def create_device_mesh(config, devices=None):\n  \"\"\"Creates a device mesh with each slice in its own data parallel group. If there is only one slice, uses two replicas\"\"\"\n  if devices is None:\n    devices = jax.devices()\n  if config.subslice_shape and config.enable_single_controller and config.num_slices == 1:\n    max_logging.log(f\"Trying to create a subslice with shape: {config.subslice_shape}\")\n    subslice_shape = tuple(int(x) for x in config.subslice_shape.split(\",\"))\n    device_coords = [device.coords for device in devices]\n    device_coords_np = np.array(device_coords)\n\n    # Find the minimum coordinates to start the subslice\n    min_coords = device_coords_np.min(axis=0)\n\n    subslice_devices = []\n    for device in devices:\n      coords = device.coords\n      if all(min_coords[i] <= coords[i] < min_coords[i] + subslice_shape[i] for i in range(len(subslice_shape))):\n        subslice_devices.append(device)\n    devices = subslice_devices\n\n  num_devices = len(devices)\n  num_slices = 1 if config.inference_benchmark_test else config.num_slices\n  num_devices_per_slice = num_devices // num_slices\n\n  multi_slice_env = num_slices > 1\n\n  # Find possible unspecified parallelisms\n  ici_parallelism = max_utils.fill_unspecified_mesh_axes(config.ici_parallelism.copy(), num_devices_per_slice, \"ICI\")\n\n  allow_split_physical_axes = config.allow_split_physical_axes if config.allow_split_physical_axes else False\n\n  if multi_slice_env:\n    dcn_parallelism = max_utils.fill_unspecified_mesh_axes(config.dcn_parallelism.copy(), num_slices, \"DCN\")\n    if max_utils.is_valid_custom_mesh(ici_parallelism, config.custom_mesh):\n      mesh = max_utils.create_custom_device_mesh(ici_parallelism, dcn_parallelism, devices, config.custom_mesh)\n    else:\n      mesh = mesh_utils.create_hybrid_device_mesh(\n          ici_parallelism,\n          dcn_parallelism,\n          devices,\n          allow_split_physical_axes=allow_split_physical_axes,\n      )\n  else:\n    if allow_split_physical_axes:\n      if max_utils.is_valid_custom_mesh(ici_parallelism, config.custom_mesh):\n        mesh = mesh_utils.create_device_mesh(\n            [16, 16],\n            devices,\n            contiguous_submeshes=False,\n            allow_split_physical_axes=False,\n        )\n        mesh = max_utils.reshape_mesh_to_rings(mesh, config.custom_mesh)\n        mesh = np.reshape(mesh, ici_parallelism)\n      else:\n        mesh = mesh_utils.create_device_mesh(\n            ici_parallelism,\n            devices,\n            contiguous_submeshes=False,\n            allow_split_physical_axes=allow_split_physical_axes,\n        )\n    else:\n      mesh = mesh_utils.create_device_mesh(\n          ici_parallelism,\n          devices,\n      )\n      if config.optimize_mesh_for_tpu_v6e:\n        mesh = max_utils.optimize_mesh_for_tpu_v6e(mesh, devices)\n\n  max_logging.log(f\"Num_devices: {num_devices}, shape {mesh.shape}\")\n\n  return mesh",
        "analysis": {
            "functionality": "Creates a JAX device mesh based on configuration, handling single-slice, multi-slice, sub-slicing, and custom topologies.",
            "usage": "Call this function with a configuration object and an optional list of JAX devices. It returns a JAX device mesh object (a numpy array of devices) shaped according to the parallelism settings in the config. The config should specify parameters like `num_slices`, `ici_parallelism`, `dcn_parallelism`, and optionally `custom_mesh` or `subslice_shape`."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#create_learning_rate_schedule",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def create_learning_rate_schedule(config):\n  \"\"\"Creates a warmup and cosine decay learning rate schedule:\n  We take inspiration from Llama2's learning rate (LR) schedule, see https://arxiv.org/pdf/2307.09288.pdf section 2.2\n  Learning rate schedule has either two or three parts:\n  1) Linear warmup from 0 to [learning_rate] over steps 0 to [learning_rate_schedule_steps * warmup_steps_fraction]\n  2) Cosine from [learning_rate] to [learning_rate * cosine_learning_rate_final_fraction] until learning_rate_schedule_steps\n  3) Constant learning rate of 0 from learning_rate_schedule_steps to steps.\n  The zero learning rate section can be used to more accurately measure the fully trained model's performance.\n  \"\"\"\n\n  def make_cos_schedule(init_lr, final_lr, len_steps):\n    def schedule(step):\n      pct = (step) / len_steps\n      a = 0.5 * (jnp.cos(jnp.pi * pct) + 1)\n      lr = init_lr * a + final_lr * (1 - a)\n      return lr\n\n    return schedule\n\n  lr = config.learning_rate\n  cos_final_lr = lr * config.cosine_learning_rate_final_fraction\n\n  warmup_steps = int(config.learning_rate_schedule_steps * config.warmup_steps_fraction)\n  cos_steps = config.learning_rate_schedule_steps - warmup_steps\n  constant_zero_steps = config.steps - config.learning_rate_schedule_steps\n\n  warmup_schedule = optax.linear_schedule(init_value=0.0, end_value=lr, transition_steps=warmup_steps)\n  cos_schedule = make_cos_schedule(lr, cos_final_lr, cos_steps)\n  constant_schedule = optax.constant_schedule(0.0)\n\n  pieces = [warmup_schedule, cos_schedule]\n  boundaries = [\n      warmup_steps,\n      warmup_steps + cos_steps,\n  ]\n\n  if constant_zero_steps > 0:\n    pieces.append(constant_schedule)\n    boundaries.append(warmup_steps + cos_steps + constant_zero_steps)\n\n  return optax.join_schedules(pieces, boundaries)",
        "analysis": {
            "module_type": "learning_rate_schedule",
            "purpose": "Creates a multi-part learning rate schedule consisting of a linear warmup phase, a cosine decay phase, and an optional final phase with a constant zero learning rate.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Define a nested function `make_cos_schedule` for the cosine decay part of the schedule.",
                "Retrieve learning rate parameters from the input `config` object.",
                "Calculate the number of steps for the warmup, cosine decay, and optional constant zero phases.",
                "Create the linear warmup schedule using `optax.linear_schedule`.",
                "Create the cosine decay schedule using the `make_cos_schedule` helper function.",
                "Create the constant zero learning rate schedule using `optax.constant_schedule`.",
                "Define the schedule pieces and their corresponding step boundaries.",
                "Conditionally append the constant zero schedule if its duration is greater than zero.",
                "Combine the individual schedules into a single schedule using `optax.join_schedules`."
            ],
            "output": {
                "shape": "Returns an Optax schedule, which is a function that takes a step count and returns a learning rate."
            },
            "dependencies": [
                "optax.linear_schedule",
                "optax.constant_schedule",
                "optax.join_schedules",
                "jax.numpy"
            ],
            "parameters": {
                "config.learning_rate": "The peak learning rate after the warmup phase.",
                "config.cosine_learning_rate_final_fraction": "The fraction of the peak learning rate to which the schedule decays during the cosine phase.",
                "config.learning_rate_schedule_steps": "The total number of steps for the combined warmup and cosine decay phases.",
                "config.warmup_steps_fraction": "The fraction of `learning_rate_schedule_steps` dedicated to the linear warmup.",
                "config.steps": "The total number of training steps, used to determine the length of the optional constant zero learning rate phase."
            },
            "notes": [
                "The schedule is inspired by the Llama2 paper's learning rate schedule.",
                "The final constant zero learning rate phase can be used to accurately measure the fully trained model's performance without further weight updates."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_formatted_sharding_annotations",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_formatted_sharding_annotations(params, mesh=None):\n  \"\"\"\n  Generates a readable string report of sharding annotations for all parameters.\n\n  This function iterates through a PyTree of model parameters and inspects the\n  sharding information attached to each parameter (leaf). It creates a\n  human-readable summary that is useful for debugging sharding configurations.\n\n  Args:\n    params: The PyTree of model parameters to inspect.\n    mesh: (Optional) The device mesh. If provided, its axis names and shape\n          are included in the report for additional context.\n\n  Returns:\n    A single string containing the formatted report of sharding annotations\n    for every parameter, with each entry on a new line.\n  \"\"\"\n  # Initialize a list to hold the lines of the report, starting with a title.\n  annotation_lines = [\"Comprehensice Weight Sharding Annotations:\"]\n\n  # If a mesh object is provided, add its details to the report header.\n  if mesh:\n    annotation_lines.append(f\"Mesh axes: {mesh.axis_names}, Mesh shape: {mesh.shape}\")\n    annotation_lines.append(\"-\" * 30)\n\n  # Get a flattened list of all parameters (leaves) and their corresponding paths in the PyTree.\n  all_params_leaves = jtu.tree_leaves_with_path(params)\n\n  # Loop through each parameter leaf in the flattened list.\n  for path, p_leaf in all_params_leaves:\n    # Convert the parameter's path (a sequence of keys) into a readable string name.\n    param_name_str = jtu.keystr(path)\n    # Get the shape of the parameter as a string.\n    shape_str = str(p_leaf.shape)\n    # Set a default description for sharding, in case none is found.\n    sharding_desc = \"N/A\"\n\n    # Check if the parameter leaf has a 'sharding' attribute.\n    if hasattr(p_leaf, \"sharding\"):\n      # Case 1: Standard JAX sharding with a PartitionSpec.\n      if hasattr(p_leaf.sharding, \"spec\") and p_leaf.sharding.spec is not None:\n        # The spec is a tuple (PartitionSpec), format it for readability.\n        spec_parts = []\n        for item in p_leaf.sharding.spec:\n          # Represent None as \"Replicated\" to make it explicit.\n          spec_parts.append(str(item) if item is not None else \"Replicated\")\n        sharding_desc = f\"PartitionSpec({', '.join(spec_parts)})\"\n      # Case 2: The parameter is explicitly marked as fully replicated.\n      elif hasattr(p_leaf.sharding, \"spec\") and p_leaf.sharding.spec is None:\n        sharding_desc = \"Fully Replicated (spec is None)\"\n      # Case 3: A generic fallback if a sharding object exists but has no recognized spec attribute.\n      else:\n        # Print the string representation of the sharding object itself.\n        sharding_desc = str(p_leaf.sharding)\n    # Case 4: The parameter has no .sharding attribute at all.\n    else:\n      sharding_desc = \"No .sharding attribute found\"\n\n    # Append the formatted details for the current parameter to our list of lines.\n    annotation_lines.append(f\" - Param: {param_name_str}\\n\" f\"   Shape: {shape_str}\\n\" f\"   Sharding: {sharding_desc}\")\n  # Join all the collected lines into a single string, separated by newlines.\n  return \"\\n\".join(annotation_lines)",
        "analysis": {
            "module_type": "sharding_annotation_formatter",
            "purpose": "Generates a human-readable string report detailing the sharding annotations for each parameter in a PyTree.",
            "input": {
                "shape": "A PyTree of tensors (e.g., model parameters).",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initialize a list of strings for the report, optionally adding mesh details if a mesh is provided.",
                "Flatten the input `params` PyTree into a list of (path, leaf) pairs using `jtu.tree_leaves_with_path`.",
                "Iterate through each parameter leaf in the flattened list.",
                "For each parameter, extract its string name, shape, and sharding description.",
                "Determine the sharding description by checking for the existence and value of the `.sharding` and `.sharding.spec` attributes.",
                "Format the `PartitionSpec` into a readable string, explicitly marking replicated axes.",
                "Append the formatted details for the current parameter to the list of report lines.",
                "Join all collected lines into a single string, separated by newlines."
            ],
            "output": {
                "shape": "A single string containing the formatted report."
            },
            "dependencies": [
                "jax.tree_util as jtu"
            ],
            "parameters": {
                "params": "The PyTree of model parameters to inspect.",
                "mesh": "(Optional) The device mesh. If provided, its axis names and shape are included in the report for context."
            },
            "notes": [
                "This function is primarily used for debugging sharding configurations.",
                "It handles various cases: standard JAX sharding with a `PartitionSpec`, explicitly replicated parameters where `spec` is None, and parameters with no `.sharding` attribute.",
                "The output string explicitly uses the term 'Replicated' for `None` values within a `PartitionSpec` to improve clarity."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_physical_spec_no_fsdp",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_physical_spec_no_fsdp(full_logical, mesh, logical_axis_rules):\n  \"\"\"\n  Generates a physical sharding spec for fully replicated weights.\n\n  This function computes a target sharding layout where model parameters are fully\n  replicated across the 'fsdp' mesh axis. It starts with the original logical\n  sharding and removes any rules that shard along the 'fsdp' or\n  'fsdp_transpose' axes.\n\n  Replacing a sharding axis with `None` in a PartitionSpec instructs JAX to\n  replicate the array data along that physical mesh dimension. The resulting\n  specification is used as a target layout for an all-gather operation.\n\n  Args:\n    full_logical: A PyTree of logical PartitionSpecs for the model parameters.\n    mesh: The JAX device mesh.\n    logical_axis_rules: Rules for converting logical axes to physical mesh axes.\n\n  Returns:\n    A PyTree of physical `jax.sharding.NamedSharding` objects that describe a\n    layout where parameters are fully gathered (replicated) across the 'fsdp'\n    mesh axis.\n  \"\"\"\n\n  def remove_fsdp_sharding(sharding_tree):\n    \"\"\"Recursively traverses the sharding tree to remove fsdp axes.\"\"\"\n\n    def _remove_fsdp_from_partition_spec(named_sharding):\n      \"\"\"Removes 'fsdp' and 'fsdp_transpose' from a PartitionSpec.\"\"\"\n      if isinstance(named_sharding, jax.sharding.NamedSharding):\n        new_spec = []\n        # Iterate through each axis in the original PartitionSpec.\n        for axis in named_sharding.spec:\n          if axis is None:\n            new_spec.append(None)\n          elif isinstance(axis, str):\n            # If the axis is 'fsdp', replace it with None to signify replication.\n            if axis not in (\"fsdp\", \"fsdp_transpose\"):\n              new_spec.append(axis)\n            else:\n              new_spec.append(None)\n          elif isinstance(axis, (list, tuple)):\n            # If the axis is a collection, filter out 'fsdp'.\n            new_axis = [a for a in axis if a not in (\"fsdp\", \"fsdp_transpose\")]\n            new_spec.append(tuple(new_axis))\n          else:\n            raise ValueError(f\"Unsupported_axis_type: {type(axis)}\")\n          # Return a new sharding object with the modified spec.\n        return jax.sharding.NamedSharding(named_sharding.mesh, jax.sharding.PartitionSpec(*new_spec))\n      return named_sharding\n\n    return jax.tree.map(_remove_fsdp_from_partition_spec, sharding_tree)\n\n  # Convert the high-level logical spec to a physical one using default rules.\n  physical = nn.logical_to_mesh_sharding(full_logical, mesh=mesh, rules=logical_axis_rules)\n  # Apply the function to remove the FSDP sharding, defining our target layout.\n  physical_no_fsdp = remove_fsdp_sharding(physical)\n  return physical_no_fsdp",
        "analysis": {
            "module_type": "physical_sharding_spec_generator",
            "purpose": "Computes a physical sharding specification (a PyTree of `jax.sharding.NamedSharding` objects) where arrays are fully replicated across the 'fsdp' and 'fsdp_transpose' mesh axes.",
            "input": {
                "shape": "A PyTree of logical `jax.sharding.PartitionSpec` objects.",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Convert the input logical sharding PyTree (`full_logical`) to a physical sharding PyTree of `jax.sharding.NamedSharding` objects using `nn.logical_to_mesh_sharding`.",
                "Define and apply a nested function `remove_fsdp_sharding` that traverses the physical sharding PyTree using `jax.tree.map`.",
                "Within the traversal, for each `NamedSharding` object, inspect its `spec` (a `PartitionSpec`).",
                "Create a new `PartitionSpec` by replacing any occurrences of 'fsdp' or 'fsdp_transpose' axes with `None`.",
                "Construct and return a new `jax.sharding.NamedSharding` object with the modified spec.",
                "Return the final modified PyTree of physical sharding specifications."
            ],
            "output": {
                "shape": "A PyTree of `jax.sharding.NamedSharding` objects with the same tree structure as the input `full_logical`."
            },
            "dependencies": [
                "flax.linen.logical_to_mesh_sharding",
                "jax.sharding.NamedSharding",
                "jax.sharding.PartitionSpec",
                "jax.tree.map"
            ],
            "parameters": {
                "full_logical": "A PyTree of logical PartitionSpecs for the model parameters.",
                "mesh": "The JAX device mesh.",
                "logical_axis_rules": "Rules for converting logical axes to physical mesh axes."
            },
            "notes": [
                "The primary use case is to generate a target layout for an all-gather operation, forcing JAX to replicate data that was previously sharded for FSDP.",
                "Replacing a sharding axis with `None` in a `PartitionSpec` is the key mechanism used to instruct JAX to replicate data along the corresponding physical mesh dimension.",
                "The function handles string, list, and tuple axis specifications within the `PartitionSpec`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#all_gather_over_fsdp",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def all_gather_over_fsdp(variables, sharding_info, mesh, logical_axis_rules):\n  \"\"\"Performs an all-gather on FSDP-sharded variables via a sharding constraint.\n  This function triggers an all-gather operation on the model's parameters.\n  It does so by applying a sharding constraint that specifies a fully\n  replicated layout.\n\n  The JAX compiler satisfies this constraint by automatically inserting the\n  necessary `all-gather` collective communication operations into the\n  computation graph, effectively gathering the sharded weights.\n\n  Args:\n    variables: The PyTree of model parameters, currently sharded across devices.\n    sharding_info: The logical partition spec of the currently sharded `variables`.\n    mesh: The JAX device mesh.\n    logical_axis_rules: Rules for converting logical axes to physical mesh axes.\n\n  Returns:\n    The model's variables with the all-gather operation applied, resulting\n    in the weights being fully replicated on all devices in the 'fsdp' mesh.\n  \"\"\"\n  # Get the target physical layout (weights fully replicated).\n  physical_constraint_no_fsdp = get_physical_spec_no_fsdp(sharding_info, mesh, logical_axis_rules)\n  # Apply the constraint to the model's current variables. This tells JAX to\n  # gather the weights into this layout.\n  return jax.lax.with_sharding_constraint(variables, physical_constraint_no_fsdp)",
        "analysis": {
            "module_type": "fsdp_all_gather",
            "purpose": "Performs an all-gather on FSDP-sharded variables by applying a sharding constraint that specifies a fully replicated layout.",
            "input": {
                "shape": "A PyTree of model parameters (variables).",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `get_physical_spec_no_fsdp` to compute a target physical sharding layout where parameters are fully replicated across the 'fsdp' mesh axis.",
                "Applies the computed sharding layout as a constraint to the input `variables` using `jax.lax.with_sharding_constraint`."
            ],
            "output": {
                "shape": "A PyTree of model parameters with the same tensor shapes as the input, but with a new sharding constraint applied, resulting in the weights being fully replicated on all devices in the 'fsdp' mesh."
            },
            "dependencies": [
                "get_physical_spec_no_fsdp",
                "jax.lax.with_sharding_constraint"
            ],
            "parameters": {
                "variables": "The PyTree of model parameters, currently sharded across devices.",
                "sharding_info": "The logical partition spec of the currently sharded `variables`.",
                "mesh": "The JAX device mesh.",
                "logical_axis_rules": "Rules for converting logical axes to physical mesh axes."
            },
            "notes": [
                "This function triggers an all-gather operation implicitly. The JAX compiler satisfies the sharding constraint by inserting the necessary `all-gather` collective communication operations into the computation graph."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#compute_loss_nnx",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def compute_loss_nnx(intermediate_outputs, logits, data, config, model, params, is_train):\n  one_hot_targets = jax.nn.one_hot(data[\"targets\"], config.vocab_size)\n  xent, _ = max_utils.cross_entropy_with_logits(logits, one_hot_targets, 0.0)\n  xent = nn.with_logical_constraint(xent, (\"activation_embed_and_logits_batch\", \"activation_length\"))\n  # Mask out paddings at the end of each example.\n  xent = xent * (data[\"targets_segmentation\"] != 0)\n  total_loss = jnp.sum(xent)\n  return total_loss",
        "analysis": {
            "functionality": "Calculates the total cross-entropy loss for a batch of logits and target labels, masking out padding tokens.",
            "usage": "This function is used as a loss function in a training loop. It takes the model's output logits, the ground truth data (including target token IDs and a segmentation mask), and a configuration object. It returns a single scalar value representing the total loss for the batch."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#compute_loss_linen",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def compute_loss_linen(intermediate_outputs, logits, data, config, model, params, is_train):\n  if config.num_vocab_tiling > 1:\n    hidden_state_key = (\"intermediates\", \"decoder\", \"hidden_states\")\n    hidden_states = get_nested_value(intermediate_outputs, hidden_state_key)[0]\n    total_loss = get_vocab_tiling_loss_linen(hidden_states, data, config, model, params, is_train)\n  else:\n    one_hot_targets = jax.nn.one_hot(data[\"targets\"], config.vocab_size)\n    xent, _ = max_utils.cross_entropy_with_logits(logits, one_hot_targets, 0.0)\n    xent = nn.with_logical_constraint(xent, (\"activation_embed_and_logits_batch\", \"activation_length\"))\n    # Mask out paddings at the end of each example.\n    xent = xent * (data[\"targets_segmentation\"] != 0)\n    total_loss = jnp.sum(xent)\n  return total_loss",
        "analysis": {
            "module_type": "loss_computation",
            "purpose": "Calculates the cross-entropy loss for a model, supporting both standard and vocabulary-tiled computation methods based on the configuration.",
            "input": {
                "shape": "Takes multiple arguments. Key tensors include: `logits` of shape [batch_size, sequence_length, vocab_size], `data['targets']` of shape [batch_size, sequence_length], and `intermediate_outputs` containing hidden states.",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check if vocabulary tiling is enabled via `config.num_vocab_tiling > 1`.",
                "If tiling is enabled, retrieve the decoder's hidden states from `intermediate_outputs` using `get_nested_value`.",
                "Call `get_vocab_tiling_loss_linen` to compute the loss with vocabulary tiling.",
                "If tiling is not enabled, create one-hot encoded targets from `data['targets']`.",
                "Compute cross-entropy loss between `logits` and one-hot targets using `max_utils.cross_entropy_with_logits`.",
                "Apply a logical constraint for sharding purposes using `nn.with_logical_constraint`.",
                "Mask out padding tokens from the loss using `data['targets_segmentation']`.",
                "Sum the masked cross-entropy values to get the final total loss.",
                "Return the total loss scalar."
            ],
            "output": {
                "shape": "[1] (scalar)"
            },
            "dependencies": [
                "get_nested_value",
                "get_vocab_tiling_loss_linen",
                "jax.nn.one_hot",
                "max_utils.cross_entropy_with_logits",
                "flax.linen.with_logical_constraint",
                "jax.numpy.sum"
            ],
            "parameters": {
                "num_vocab_tiling": "An integer from the config that determines whether to use the standard loss calculation or the memory-efficient vocabulary tiling method."
            },
            "notes": [
                "The function implements two distinct paths for loss calculation based on the `num_vocab_tiling` configuration.",
                "The standard path is used for smaller vocabularies, while the tiling path is a memory optimization for very large vocabularies.",
                "Padding tokens are explicitly masked out from the loss calculation to ensure they do not contribute to the final loss value."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_vocab_tiling_loss_linen",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_vocab_tiling_loss_linen(\n    hidden_states,\n    data,\n    config,\n    model,\n    params,\n    is_train,\n):\n  labels = data[\"targets\"]\n  segmentation = data[\"targets_segmentation\"]\n  deterministic = not config.enable_dropout if is_train else True\n\n  param_spec= nn.get_partition_spec(params)\n  hidden_spec = jax.sharding.NamedSharding(model.mesh, nn.logical_to_mesh_axes((\"activation_embed_and_logits_batch\", \"activation_length_no_exp\", \"activation_embed\")))\n  label_spec = jax.sharding.NamedSharding(model.mesh, nn.logical_to_mesh_axes((\"activation_embed_and_logits_batch\", \"activation_length_no_exp\")))\n  reshaped_hidden_spec = jax.sharding.NamedSharding(model.mesh, nn.logical_to_mesh_axes((\"num_tile\", 'activation_embed_and_logits_batch_sequence', \"activation_embed\")))\n  reshaped_data_spec = jax.sharding.NamedSharding(model.mesh, nn.logical_to_mesh_axes((\"num_tile\", 'activation_embed_and_logits_batch_sequence')))\n  chunked_hidden_spec = jax.sharding.NamedSharding(model.mesh, nn.logical_to_mesh_axes(('activation_embed_and_logits_batch_sequence', \"activation_embed\")))\n  chunked_data_spec = jax.sharding.NamedSharding(model.mesh, nn.logical_to_mesh_axes(('activation_embed_and_logits_batch_sequence',)))\n  chunked_logits_spec = jax.sharding.NamedSharding(model.mesh, nn.logical_to_mesh_axes(('activation_embed_and_logits_batch_sequence', \"activation_vocab\")))\n  \n  hidden_states = jax.lax.with_sharding_constraint(hidden_states, hidden_spec)\n  labels = jax.lax.with_sharding_constraint(labels, label_spec)\n  segmentation = jax.lax.with_sharding_constraint(segmentation, label_spec)\n  #TODO (chengnuojin) all gather only embedding table instead of all params after NNX module is enabled\n  gathered_params = all_gather_over_fsdp(params, param_spec, model.mesh, config.logical_axis_rules)\n\n  # Customized forward and backward maps for the embedding tiling\n  @jax.custom_vjp\n  def chunked_cross_entropy_loss(gathered_params, hidden_states, labels, segmentation):\n      \"\"\"\n      Calculates the total cross-entropy loss using vocab tiling.\n      \"\"\"\n      total_loss, _ = _chunked_cross_entropy_loss_fwd(gathered_params, hidden_states, labels, segmentation)\n      return total_loss\n\n\n  def _chunked_cross_entropy_loss_fwd(gathered_params, hidden_states, labels, segmentation):\n      batch_size, seq_len, emb_dim = hidden_states.shape\n      vocab_tile_size = (batch_size * seq_len) // config.num_vocab_tiling\n\n      reshaped_hidden_states = hidden_states.reshape((config.num_vocab_tiling, vocab_tile_size, emb_dim))\n      reshaped_hidden_states = jax.lax.with_sharding_constraint(reshaped_hidden_states, reshaped_hidden_spec)\n      reshaped_labels = labels.reshape((config.num_vocab_tiling, vocab_tile_size))\n      reshaped_labels = jax.lax.with_sharding_constraint(reshaped_labels, reshaped_data_spec)\n      reshaped_segmentation = segmentation.reshape((config.num_vocab_tiling, vocab_tile_size))\n      reshaped_segmentation = jax.lax.with_sharding_constraint(reshaped_segmentation, reshaped_data_spec)\n      \n      # Scan body accumulates loss from each tile given chunked hidden states and labels\n      def _fwd_scan_body(loss_accumulator, chunk_data):\n          hidden_chunk, label_chunk, segmentation_chunk = chunk_data\n          hidden_chunk = jax.lax.with_sharding_constraint(hidden_chunk, chunked_hidden_spec)\n          label_chunk = jax.lax.with_sharding_constraint(label_chunk, chunked_data_spec)\n          segmentation_chunk = jax.lax.with_sharding_constraint(segmentation_chunk, chunked_data_spec)\n\n          # Calculate logits for the current chunk\n          chunk_logits = model.apply(\n              {'params': gathered_params['params']},\n              hidden_chunk,\n              deterministic=deterministic,\n              method='logits_from_hidden_states'\n          )\n          chunk_logits = jax.lax.with_sharding_constraint(chunk_logits, chunked_logits_spec)\n          one_hot_label_chunk = jax.nn.one_hot(label_chunk, config.vocab_size)\n          chunk_xent, _ = max_utils.cross_entropy_with_logits(chunk_logits, one_hot_label_chunk, 0.0)\n          masked_xent = jnp.sum(chunk_xent * (segmentation_chunk != 0))\n          loss_accumulator += masked_xent\n          return loss_accumulator, None\n\n      initial_loss = 0.0\n      total_loss, _ = jax.lax.scan(\n          _fwd_scan_body,\n          initial_loss,\n          (reshaped_hidden_states, reshaped_labels, reshaped_segmentation)\n      )\n      residuals = (gathered_params, reshaped_hidden_states, reshaped_labels, reshaped_segmentation, batch_size, seq_len, emb_dim)\n\n      return total_loss, residuals\n\n  def _chunked_cross_entropy_loss_bwd(residuals, loss_cotangent):\n    gathered_params, reshaped_hidden_states, reshaped_labels, reshaped_segmentation, batch_size, seq_len, emb_dim = residuals\n    \n    def _single_chunk_loss_fn(input_params, input_hidden_chunk, input_label_chunk, input_segmentation_chunk):\n      chunk_logits = model.apply(\n          {'params': input_params['params']},\n          input_hidden_chunk,\n          deterministic=deterministic,\n          method='logits_from_hidden_states'\n      )\n      chunk_logits = jax.lax.with_sharding_constraint(chunk_logits, chunked_logits_spec)\n      one_hot_label_chunk = jax.nn.one_hot(input_label_chunk, config.vocab_size)\n      xent, _ = max_utils.cross_entropy_with_logits(chunk_logits, one_hot_label_chunk, 0.0)\n      return jnp.sum(xent * (input_segmentation_chunk != 0))\n\n    def _bwd_scan_body(grad_params_acc, chunk_data):\n      hidden_chunk, label_chunk, segmentation_chunk = chunk_data\n        \n      # Apply sharding constraints to the chunk data\n      hidden_chunk = jax.lax.with_sharding_constraint(hidden_chunk, chunked_hidden_spec)\n      label_chunk = jax.lax.with_sharding_constraint(label_chunk, chunked_data_spec)\n      segmentation_chunk = jax.lax.with_sharding_constraint(segmentation_chunk, chunked_data_spec)\n\n      # Create a loss function closure that captures the current chunk's labels and segmentation.\n      # This gives `jax.vjp` a function with the required signature: `loss(params, hidden_states)`.\n      loss_fn_for_vjp = lambda p, h: _single_chunk_loss_fn(p, h, label_chunk, segmentation_chunk)\n      \n      # Get the vector-Jacobian product function wrt both params and hidden states\n      _, vjp_fn = jax.vjp(loss_fn_for_vjp, gathered_params, hidden_chunk)\n      \n      # 1.0 since total_loss is sum of all individual chunked loss\n      (grad_params_update, grad_hidden_chunk) = vjp_fn(1.0)\n      grad_hidden_chunk = jax.lax.with_sharding_constraint(grad_hidden_chunk, chunked_hidden_spec)\n      \n      grad_params_acc = jax.tree_util.tree_map(\n          lambda acc, update: acc + update, grad_params_acc, grad_params_update,\n      )\n      return grad_params_acc, grad_hidden_chunk\n\n    initial_grad_params_acc = jax.tree_util.tree_map(jnp.zeros_like, gathered_params)\n\n    # The scan now returns the total gradients for the params in the final carry\n    grad_params, grad_reshaped_hidden_states = jax.lax.scan(\n        _bwd_scan_body,\n        initial_grad_params_acc,\n        (reshaped_hidden_states, reshaped_labels, reshaped_segmentation)\n    )\n    grad_reshaped_hidden_states = jax.lax.with_sharding_constraint(grad_reshaped_hidden_states, reshaped_hidden_spec)\n    # TODO (chengnuojin): we may want to convert grad_params to bf16 to save memory\n    # grad_params = jax.tree_util.tree_map(lambda x, y: y.astype(x.dtype), gathered_params, grad_params)\n    # Chain-rule to accumulate gradients\n    grad_params = jax.tree_util.tree_map(lambda g: g * loss_cotangent, grad_params)\n    # Give back sharding constraint\n    grad_reshaped_hidden_states = grad_reshaped_hidden_states.reshape((batch_size, seq_len, emb_dim))\n    grad_reshaped_hidden_states = jax.lax.with_sharding_constraint(grad_reshaped_hidden_states, hidden_spec)\n    return (\n      grad_params, # grad for params\n      grad_reshaped_hidden_states.astype(reshaped_hidden_states.dtype),\n      None, # grad for reshaped_labels\n      None, # grad for reshaped_segmentation\n    )\n\n  chunked_cross_entropy_loss.defvjp(_chunked_cross_entropy_loss_fwd, _chunked_cross_entropy_loss_bwd)\n\n  total_loss = chunked_cross_entropy_loss(\n    gathered_params,\n    hidden_states,\n    labels,\n    segmentation,\n  )\n\n  return total_loss",
        "analysis": {
            "functionality": "This function calculates the cross-entropy loss using a memory-saving technique called vocabulary tiling. It avoids materializing the full, potentially very large, logits matrix by computing logits and loss in sequential chunks. This is implemented using a custom vector-jacobian product (VJP) for efficient gradient calculation in a distributed setting.",
            "usage": "To use this function, provide the final hidden states from a model, a data dictionary containing target labels and segmentation masks, the model configuration, the model object itself, the model parameters, and a boolean indicating if it's in training mode. The function returns a single scalar value representing the total loss. It is specifically designed for large models where the full logits matrix (`[batch*seq_len, vocab_size]`) would not fit in memory. The degree of tiling is controlled by `config.num_vocab_tiling`."
        }
    }
]