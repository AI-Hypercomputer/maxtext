{
    "data": {
        "q": [],
        "processed_components": [
            "transformers/modeling_outputs.py#MoeModelOutputWithPast",
            "transformers/modeling_utils.py#get_parameter_dtype",
            "transformers/utils.py#ModelOutput",
            "transformers/cache_utils.py#Cache",
            "transformers/modeling_rope_utils.py#_compute_longrope_parameters",
            "transformers/masking_utils.py#create_sliding_window_causal_mask",
            "transformers/modeling_rope_utils.py#dynamic_rope_update",
            "transformers/modeling_utils.py#ModuleUtilsMixin",
            "transformers/activations.py#LinearActivation",
            "transformers/activations.py#ReLUSquaredActivation",
            "transformers/models/mixtral/modeling_mixtral.py#load_balancing_loss_func",
            "transformers/modeling_rope_utils.py#_compute_yarn_parameters",
            "transformers/models/mixtral/modeling_mixtral.py#MixtralExperts",
            "transformers/activations.py#ACT2FN",
            "transformers/models/mixtral/modeling_mixtral.py#MixtralDecoderLayer",
            "transformers/models/mixtral/modeling_mixtral.py#MixtralMLP",
            "transformers/models/mixtral/modeling_mixtral.py#MixtralPreTrainedModel",
            "transformers/masking_utils.py#_preprocess_mask_arguments",
            "transformers/masking_utils.py#and_masks",
            "transformers/configuration_utils.py#PreTrainedConfig",
            "transformers/activations.py#SiLUActivation",
            "transformers/models/mixtral/modeling_mixtral.py#MixtralSparseMoeBlock",
            "transformers/activations.py#LaplaceActivation",
            "transformers/masking_utils.py#sliding_window_causal_mask_function",
            "transformers/models/mixtral/modeling_mixtral.py#apply_rotary_pos_emb",
            "transformers/masking_utils.py#causal_mask_function",
            "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig",
            "transformers/modeling_rope_utils.py#rope_config_validation",
            "transformers/models/mixtral/modeling_mixtral.py#rotate_half",
            "transformers/masking_utils.py#sliding_window_overlay",
            "transformers/activations.py#GELUTanh",
            "transformers/modeling_utils.py#PreTrainedModel",
            "transformers/activations.py#XIELUActivation",
            "transformers/models/mixtral/modeling_mixtral.py#eager_attention_forward",
            "transformers/activations.py#ClassInstantier",
            "transformers/models/mixtral/modeling_mixtral.py#MixtralAttention",
            "transformers/modeling_utils.py#_init_weights",
            "transformers/modeling_rope_utils.py#_compute_llama3_parameters",
            "transformers/masking_utils.py#create_causal_mask",
            "transformers/modeling_rope_utils.py#standardize_rope_params",
            "transformers/activations.py#AccurateGELUActivation",
            "transformers/modeling_rope_utils.py#_compute_dynamic_ntk_parameters",
            "transformers/models/mixtral/modeling_mixtral.py#MixtralRMSNorm",
            "transformers/modeling_utils.py#EmbeddingAccessMixin",
            "transformers/modeling_rope_utils.py#RopeParameters",
            "transformers/activations.py#FastGELUActivation",
            "transformers/activations.py#MishActivation",
            "transformers/models/mixtral/modeling_mixtral.py#MixtralForCausalLM",
            "transformers/activations.py#GELUActivation",
            "transformers/modeling_rope_utils.py#ROPE_VALIDATION_FUNCTIONS",
            "transformers/models/mixtral/modeling_mixtral.py#MixtralRotaryEmbedding",
            "transformers/models/qwen3_moe/modular_qwen3_moe.py#Qwen3MoeModel",
            "transformers/models/mixtral/modeling_mixtral.py#repeat_kv",
            "transformers/modeling_rope_utils.py#_compute_linear_scaling_rope_parameters",
            "transformers/activations.py#QuickGELUActivation",
            "transformers/quantizers/quantizers_utils.py#get_module_from_name",
            "transformers/masking_utils.py#find_packed_sequence_indices",
            "transformers/modeling_outputs.py#MoeCausalLMOutputWithPast",
            "transformers/activations.py#NewGELUActivation",
            "transformers/models/mixtral/modeling_mixtral.py#MixtralModel",
            "transformers/models/qwen3_moe/modular_qwen3_moe.py#Qwen3MoeForCausalLM",
            "transformers/modeling_rope_utils.py#ROPE_INIT_FUNCTIONS",
            "transformers/cache_utils.py#CacheLayerMixin",
            "transformers/modeling_utils.py#get_parameter_device"
        ],
        "processed_count": 138,
        "file_analysis_cache": {
            "transformers/models/qwen3_moe/modular_qwen3_moe.py#Qwen3MoeForCausalLM": {
                "sorted_modules": {
                    "Qwen3MoeForCausalLM": "\n\nclass Qwen3MoeForCausalLM(MixtralForCausalLM):\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = Qwen3MoeModel(config)\n        self.num_experts = config.num_experts\n\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Cache] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_router_logits: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        logits_to_keep: Union[int, torch.Tensor] = 0,\n        **kwargs: Unpack[TransformersKwargs],\n    ) -> MoeCausalLMOutputWithPast:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, Qwen3MoeForCausalLM\n\n        >>> model = Qwen3MoeForCausalLM.from_pretrained(\"Qwen/Qwen3-MoE-15B-A2B\")\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-MoE-15B-A2B\")\n\n        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n        ```\"\"\"\n\n        output_router_logits = (\n            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n        )\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs: MoeModelOutputWithPast = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_router_logits=output_router_logits,\n            cache_position=cache_position,\n            **kwargs,\n        )\n\n        hidden_states = outputs.last_hidden_state\n        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n        logits = self.lm_head(hidden_states[:, slice_indices, :])\n\n        loss = None\n        if labels is not None:\n            loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)\n\n        aux_loss = None\n        if output_router_logits:\n            aux_loss = load_balancing_loss_func(\n                outputs.router_logits,\n                self.num_experts,\n                self.num_experts_per_tok,\n                attention_mask,\n            )\n            if labels is not None:\n                loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n\n        return MoeCausalLMOutputWithPast(\n            loss=loss,\n            aux_loss=aux_loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            router_logits=outputs.router_logits,\n        )"
                },
                "component_dependencies": {
                    "Qwen3MoeForCausalLM": [
                        "transformers/cache_utils.py#Cache",
                        "transformers/modeling_outputs.py#MoeCausalLMOutputWithPast",
                        "transformers/modeling_outputs.py#MoeModelOutputWithPast",
                        "transformers/models/mixtral/modeling_mixtral.py#MixtralForCausalLM",
                        "transformers/models/mixtral/modeling_mixtral.py#load_balancing_loss_func",
                        "transformers/models/qwen3_moe/modular_qwen3_moe.py#Qwen3MoeModel",
                        "transformers/processing_utils.py#Unpack",
                        "transformers/utils.py#TransformersKwargs"
                    ]
                },
                "warning": null
            },
            "transformers/cache_utils.py#Cache": {
                "sorted_modules": {
                    "Cache": "\n\nclass Cache:\n    \"\"\"\n    A `Cache` is mostly a list of `CacheLayerMixin` objects, one per model layer. It serves as a container for\n    the Cache of each layer.\n\n    Args:\n        layers (`Optional`, *optional*):\n            A list of pre-created `CacheLayerMixin`. If omitted (`None`), then `layer_class_to_replicate` will\n            be used.\n        layer_class_to_replicate (`type[CacheLayerMixin]`, *optional*):\n            Only used if `layers` is omitted (`None`), in which case it will be used as the base class for each layer,\n            and the layers will be added lazily as soon as `update` is called with a `layer_idx` greater than the current\n            list of layers.\n        offloading (`bool`, *optional*, defaults to `False`):\n            Whether to perform offloading of the layers to `cpu`, to save GPU memory.\n        offload_only_non_sliding (`bool`, *optional*, defaults to `True`):\n            If `offloading` is `True`, this further decides if only the non-sliding layers will be offloaded (because\n            usually the sliding layers are small in size, so there is no need to offload them, and skipping it is faster).\n    \"\"\"\n\n    def __init__(\n        self,\n        layers: Optional[list[CacheLayerMixin]] = None,\n        layer_class_to_replicate: Optional[type[CacheLayerMixin]] = None,\n        offloading: bool = False,\n        offload_only_non_sliding: bool = True,\n    ):\n        if layers is not None and layer_class_to_replicate is not None:\n            raise ValueError(\n                \"You can construct a Cache either from a list `layers` of all the predefined `CacheLayer`, or from a \"\n                \"`layer_class_to_replicate`, in which case the Cache will append a new layer corresponding to \"\n                \"`layer_class_to_replicate` for each new call to `update` with an idx not already in the Cache.\"\n            )\n        if layers is None and layer_class_to_replicate is None:\n            raise ValueError(\n                \"You should provide exactly one of `layers` or `layer_class_to_replicate` to initialize a Cache.\"\n            )\n        self.layers = layers if layers is not None else []\n        self.layer_class_to_replicate = layer_class_to_replicate\n        self.offloading = offloading\n        if self.offloading:\n            self.only_non_sliding = offload_only_non_sliding\n            self.prefetch_stream = torch.Stream() if _is_torch_greater_or_equal_than_2_7 else torch.cuda.Stream()\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}(layers={self.layers})\"\n\n    def prefetch(self, layer_idx: int, only_non_sliding: bool = True):\n        \"\"\"\n        Prefetch a given layer on its device. If `only_non_sliding` is True, it will try to prefetch only the layers\n        which are non-sliding. If the `layer_idx` is outside the range, this will circle back to the first layers.\n        Note that we use a non-default stream for this, to avoid blocking.\n        \"\"\"\n        if only_non_sliding:\n            # Try to find next non-sliding, starting at `layer_idx`\n            try:\n                layer_idx = layer_idx + self.is_sliding[layer_idx:].index(False)\n            # In this case, we need to circle back to the beginning\n            except ValueError:\n                layer_idx = self.is_sliding.index(False)\n        else:\n            layer_idx = layer_idx if layer_idx < len(self.layers) else 0\n\n        # Prefetch\n        with self.prefetch_stream if _is_torch_greater_or_equal_than_2_7 else torch.cuda.stream(self.prefetch_stream):\n            self.layers[layer_idx].prefetch()\n\n    def offload(self, layer_idx: int, only_non_sliding: bool = True):\n        \"\"\"\n        Offload a given `layer_idx`. If `only_non_sliding` is True, it will offload `layer_idx` only if it is a\n        non-sliding layer. Note that we do it on the default stream, so that we ensure all earlier\n        computation in the layer's `update` methods are finished.\n        \"\"\"\n        if not (only_non_sliding and self.is_sliding[layer_idx]):\n            self.layers[layer_idx].offload()\n\n    def update(\n        self,\n        key_states: torch.Tensor,\n        value_states: torch.Tensor,\n        layer_idx: int,\n        cache_kwargs: Optional[dict[str, Any]] = None,\n    ) -> tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n\n        Parameters:\n            key_states (`torch.Tensor`):\n                The new key states to cache.\n            value_states (`torch.Tensor`):\n                The new value states to cache.\n            layer_idx (`int`):\n                The index of the layer to cache the states for.\n            cache_kwargs (`dict[str, Any]`, *optional*):\n                Additional arguments for the cache subclass. These are specific to each subclass and allow new types of\n                cache to be created.\n\n        Return:\n            A tuple containing the updated key and value states.\n        \"\"\"\n        # In this case, the `layers` were not provided, and we must append as much as `layer_idx`\n        if self.layer_class_to_replicate is not None:\n            while len(self.layers) <= layer_idx:\n                self.layers.append(self.layer_class_to_replicate())\n\n        if self.offloading:\n            # Wait for the stream to finish if needed, and start prefetching the next layer\n            torch.cuda.default_stream(key_states.device).wait_stream(self.prefetch_stream)\n            self.prefetch(layer_idx + 1, self.only_non_sliding)\n\n        keys, values = self.layers[layer_idx].update(key_states, value_states, cache_kwargs)\n\n        if self.offloading:\n            self.offload(layer_idx, self.only_non_sliding)\n\n        return keys, values\n\n    def early_initialization(\n        self, batch_size: int, num_heads: int, head_dim: int, dtype: torch.dtype, device: torch.device\n    ):\n        \"\"\"\n        Initialize all the layers in advance (it's otherwise lazily initialized on the first `update` call).\n        This is useful for our `export` recipes, as `export` needs everything in advance.\n        \"\"\"\n        # Note that the initialization needs all dimensions (except -2), as well as device and dtype, so we use\n        # this fake tensor approach. It has size 0 on the -2 dimension, so it does not allocate any data (it only\n        # creates an empty tensor with correct shape, dtype and device), which is very efficient and practical\n        fake_keys_tensor = torch.zeros((batch_size, num_heads, 0, head_dim), dtype=dtype, device=device)\n        # Init all layers\n        for layer in self.layers:\n            layer.lazy_initialization(fake_keys_tensor)\n\n    def get_seq_length(self, layer_idx: int = 0) -> int:\n        \"\"\"Returns the sequence length of the cache for the given layer.\"\"\"\n        if layer_idx >= len(self.layers):\n            return 0\n        return self.layers[layer_idx].get_seq_length()\n\n    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n        \"\"\"\n        Return a tuple (kv_length, kv_offset) corresponding to the length and offset that will be returned for\n        the given layer at `layer_idx`.\n        The masks are then prepared according to the given lengths (kv_length, kv_offset) and patterns for each layer.\n        \"\"\"\n        # For DynamicCache, where the layers are created at runtime -> if it was not yet created, the size is\n        # simply the shape of `cache_position`\n        if layer_idx >= len(self.layers):\n            return cache_position.shape[0], 0\n        return self.layers[layer_idx].get_mask_sizes(cache_position)\n\n    def get_max_cache_shape(self, layer_idx: int = 0) -> int:\n        \"\"\"Returns maximum sequence length of the cache object. Dynamic caches do not have a maximum length.\"\"\"\n        # For DynamicCache, where the layers are created at runtime -> if it was not yet created, return -1\n        # as DynamicLayer does\n        if layer_idx >= len(self.layers):\n            return -1\n        return self.layers[layer_idx].get_max_cache_shape()\n\n    def reset(self):\n        \"\"\"Recursively reset all layers tensors\"\"\"\n        for layer_idx in range(len(self.layers)):\n            self.layers[layer_idx].reset()\n\n    def reorder_cache(self, beam_idx: torch.LongTensor):\n        \"\"\"Reorder the cache for beam search\"\"\"\n        for layer_idx in range(len(self.layers)):\n            self.layers[layer_idx].reorder_cache(beam_idx)\n\n    def crop(self, max_length: int):\n        \"\"\"Crop the cache to the given length\"\"\"\n        for layer_idx in range(len(self.layers)):\n            self.layers[layer_idx].crop(max_length)\n\n    def batch_repeat_interleave(self, repeats: int):\n        \"\"\"Repeat and interleave the cache\"\"\"\n        for layer_idx in range(len(self.layers)):\n            self.layers[layer_idx].batch_repeat_interleave(repeats)\n\n    def batch_select_indices(self, indices: torch.Tensor):\n        \"\"\"Select indices from the cache\"\"\"\n        for layer_idx in range(len(self.layers)):\n            self.layers[layer_idx].batch_select_indices(indices)\n\n    @property\n    def max_batch_size(self) -> int:\n        \"\"\"Return the maximum batch size of the cache\"\"\"\n        values = [layer.max_batch_size for layer in self.layers]\n        if len(set(values)) > 1:\n            raise ValueError(f\"Max batch size is not consistent across layers: {values}\")\n        return values[0]\n\n    @property\n    def max_cache_len(self) -> int:\n        \"\"\"Return the maximum cache length of the cache\"\"\"\n        values = [layer.max_cache_len for layer in self.layers]\n        return max(values)\n\n    @property\n    def is_compileable(self) -> bool:\n        \"\"\"Return whether the cache is compileable\"\"\"\n        # For DynamicCache dispatching the layers lazily (otherwise, all([]) is True)\n        if len(self.layers) == 0:\n            return False\n        return all(layer.is_compileable for layer in self.layers)\n\n    @property\n    def is_initialized(self) -> bool:\n        \"\"\"Return whether the cache data is initialized\"\"\"\n        return len(self.layers) > 0 and all(layer.is_initialized for layer in self.layers)\n\n    @property\n    def is_sliding(self) -> list[bool]:\n        \"\"\"Return whether the layers of the cache are sliding window\"\"\"\n        return [getattr(layer, \"is_sliding\", False) for layer in self.layers]\n\n    def __len__(self):\n        \"\"\"\n        This value corresponds to the number of layers in the model.\n        \"\"\"\n        # Note: for DynamicCache, layers are initialized lazily, so this will not be accurate before the first\n        # forward through all the layers\n        return len(self.layers)"
                },
                "component_dependencies": {
                    "Cache": [
                        "transformers/cache_utils.py#CacheLayerMixin",
                        "transformers/cache_utils.py#_is_torch_greater_or_equal_than_2_7"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_outputs.py#MoeCausalLMOutputWithPast": {
                "sorted_modules": {
                    "MoeCausalLMOutputWithPast": "\n\n@dataclass\nclass MoeCausalLMOutputWithPast(ModelOutput):\n    \"\"\"\n    Base class for causal language model (or autoregressive) with mixture of experts outputs.\n\n    Args:\n        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n            Language modeling loss (for next-token prediction).\n\n        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n\n        aux_loss (`torch.FloatTensor`, *optional*, returned when `labels` is provided):\n            aux_loss for the sparse modules.\n\n        router_logits (`tuple(torch.FloatTensor)`, *optional*, returned when `output_router_probs=True` and `config.add_router_probs=True` is passed or when `config.output_router_probs=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, sequence_length, num_experts)`.\n\n            Raw router logtis (post-softmax) that are computed by MoE routers, these terms are used to compute the auxiliary\n            loss for Mixture of Experts models.\n\n        past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n\n            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n            `past_key_values` input) to speed up sequential decoding.\n        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n            sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n    \"\"\"\n\n    loss: Optional[torch.FloatTensor] = None\n    aux_loss: Optional[torch.FloatTensor] = None\n    logits: Optional[torch.FloatTensor] = None\n    past_key_values: Optional[Cache] = None\n    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n    router_logits: Optional[tuple[torch.FloatTensor]] = None"
                },
                "component_dependencies": {
                    "MoeCausalLMOutputWithPast": [
                        "transformers/cache_utils.py#Cache",
                        "transformers/utils.py#ModelOutput"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_outputs.py#MoeModelOutputWithPast": {
                "sorted_modules": {
                    "MoeModelOutputWithPast": "\n\n@dataclass\nclass MoeModelOutputWithPast(ModelOutput):\n    \"\"\"\n    Base class for model's outputs, with potential hidden states and attentions.\n\n    Args:\n        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n            Sequence of hidden-states at the output of the last layer of the model.\n        past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n\n            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n            `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n            input) to speed up sequential decoding.\n        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n            sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n        router_logits (`tuple(torch.FloatTensor)`, *optional*, returned when `output_router_probs=True` and `config.add_router_probs=True` is passed or when `config.output_router_probs=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, sequence_length, num_experts)`.\n\n            Raw router logtis (post-softmax) that are computed by MoE routers, these terms are used to compute the auxiliary\n            loss for Mixture of Experts models.\n    \"\"\"\n\n    last_hidden_state: Optional[torch.FloatTensor] = None\n    past_key_values: Optional[Cache] = None\n    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n    router_logits: Optional[tuple[torch.FloatTensor]] = None"
                },
                "component_dependencies": {
                    "MoeModelOutputWithPast": [
                        "transformers/cache_utils.py#Cache",
                        "transformers/utils.py#ModelOutput"
                    ]
                },
                "warning": null
            },
            "transformers/models/mixtral/modeling_mixtral.py#MixtralForCausalLM": {
                "sorted_modules": {
                    "MixtralForCausalLM": "\n\n@auto_docstring\nclass MixtralForCausalLM(MixtralPreTrainedModel, GenerationMixin):\n    _tied_weights_keys = [\"lm_head.weight\"]\n    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = MixtralModel(config)\n        self.vocab_size = config.vocab_size\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        self.router_aux_loss_coef = config.router_aux_loss_coef\n        self.num_experts = config.num_local_experts\n        self.num_experts_per_tok = config.num_experts_per_tok\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @can_return_tuple\n    @auto_docstring\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Cache] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_router_logits: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        logits_to_keep: Union[int, torch.Tensor] = 0,\n        **kwargs: Unpack[TransformersKwargs],\n    ) -> MoeCausalLMOutputWithPast:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, MixtralForCausalLM\n\n        >>> model = MixtralForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n\n        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n        ```\"\"\"\n\n        output_router_logits = (\n            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n        )\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs: MoeModelOutputWithPast = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_router_logits=output_router_logits,\n            cache_position=cache_position,\n            **kwargs,\n        )\n\n        hidden_states = outputs.last_hidden_state\n        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n        logits = self.lm_head(hidden_states[:, slice_indices, :])\n\n        loss = None\n        if labels is not None:\n            loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)\n\n        aux_loss = None\n        if output_router_logits:\n            aux_loss = load_balancing_loss_func(\n                outputs.router_logits,\n                self.num_experts,\n                self.num_experts_per_tok,\n                attention_mask,\n            )\n            if labels is not None:\n                loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n\n        return MoeCausalLMOutputWithPast(\n            loss=loss,\n            aux_loss=aux_loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            router_logits=outputs.router_logits,\n        )"
                },
                "component_dependencies": {
                    "MixtralForCausalLM": [
                        "transformers/cache_utils.py#Cache",
                        "transformers/generation.py#GenerationMixin",
                        "transformers/modeling_outputs.py#MoeCausalLMOutputWithPast",
                        "transformers/modeling_outputs.py#MoeModelOutputWithPast",
                        "transformers/models/mixtral/modeling_mixtral.py#MixtralModel",
                        "transformers/models/mixtral/modeling_mixtral.py#MixtralPreTrainedModel",
                        "transformers/models/mixtral/modeling_mixtral.py#load_balancing_loss_func",
                        "transformers/processing_utils.py#Unpack",
                        "transformers/utils.py#TransformersKwargs",
                        "transformers/utils.py#auto_docstring",
                        "transformers/utils.py#can_return_tuple"
                    ]
                },
                "warning": null
            },
            "transformers/models/mixtral/modeling_mixtral.py#load_balancing_loss_func": {
                "sorted_modules": {
                    "load_balancing_loss_func": "\n\ndef load_balancing_loss_func(\n    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n    num_experts: Optional[int] = None,\n    top_k=2,\n    attention_mask: Optional[torch.Tensor] = None,\n) -> Union[torch.Tensor, int]:\n    r\"\"\"\n    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n\n    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n    experts is too unbalanced.\n\n    Args:\n        gate_logits:\n            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n            shape [batch_size X sequence_length, num_experts].\n        num_experts:\n            Number of experts\n        top_k:\n            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n            parameter.\n        attention_mask (`torch.Tensor`, *optional*):\n            The attention_mask used in forward function\n            shape [batch_size X sequence_length] if not None.\n\n    Returns:\n        The auxiliary loss.\n    \"\"\"\n    if gate_logits is None or not isinstance(gate_logits, tuple):\n        return 0\n\n    if isinstance(gate_logits, tuple):\n        compute_device = gate_logits[0].device\n        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n\n    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n\n    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n\n    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n\n    if attention_mask is None:\n        # Compute the percentage of tokens routed to each experts\n        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n\n        # Compute the average probability of routing to these experts\n        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n    else:\n        batch_size, sequence_length = attention_mask.shape\n        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n\n        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n        expert_attention_mask = (\n            attention_mask[None, :, :, None, None]\n            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n            .reshape(-1, top_k, num_experts)\n            .to(compute_device)\n        )\n\n        # Compute the percentage of tokens routed to each experts\n        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n            expert_attention_mask, dim=0\n        )\n\n        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n        router_per_expert_attention_mask = (\n            attention_mask[None, :, :, None]\n            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n            .reshape(-1, num_experts)\n            .to(compute_device)\n        )\n\n        # Compute the average probability of routing to these experts\n        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n            router_per_expert_attention_mask, dim=0\n        )\n\n    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n    return overall_loss * num_experts"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/models/qwen3_moe/modular_qwen3_moe.py#Qwen3MoeModel": {
                "sorted_modules": {
                    "Qwen3MoeModel": "\n\nclass Qwen3MoeModel(MixtralModel):\n    pass"
                },
                "component_dependencies": {
                    "Qwen3MoeModel": [
                        "transformers/models/mixtral/modeling_mixtral.py#MixtralModel"
                    ]
                },
                "warning": null
            },
            "transformers/cache_utils.py#CacheLayerMixin": {
                "sorted_modules": {
                    "CacheLayerMixin": "\n\nclass CacheLayerMixin(ABC):\n    \"\"\"Base, abstract class for a single layer's cache.\"\"\"\n\n    is_compileable = False\n\n    def __init__(self):\n        self.keys: Optional[torch.Tensor] = None\n        self.values: Optional[torch.Tensor] = None\n        self.is_initialized = False\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}\"\n\n    @abstractmethod\n    def lazy_initialization(self, key_states: torch.Tensor): ...\n\n    @abstractmethod\n    def update(\n        self, key_states: torch.Tensor, value_states: torch.Tensor, cache_kwargs: Optional[dict[str, Any]] = None\n    ) -> tuple[torch.Tensor, torch.Tensor]: ...\n\n    @abstractmethod\n    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]: ...\n\n    @abstractmethod\n    def get_seq_length(self) -> int: ...\n\n    @abstractmethod\n    def get_max_cache_shape(self) -> int: ...\n\n    def offload(self):\n        \"\"\"Offload this layer's data to CPU device.\"\"\"\n        if self.is_initialized:\n            self.keys = self.keys.to(\"cpu\", non_blocking=True)\n            self.values = self.values.to(\"cpu\", non_blocking=True)\n\n    def prefetch(self):\n        \"\"\"In case of layer offloading, this allows to move the data back to the layer's device ahead of time.\"\"\"\n        if self.is_initialized and self.keys.device != self.device:\n            self.keys = self.keys.to(self.device, non_blocking=True)\n            self.values = self.values.to(self.device, non_blocking=True)\n\n    def reset(self) -> None:\n        \"\"\"Resets the cache values while preserving the objects\"\"\"\n        if self.is_initialized:\n            self.keys.zero_()\n            self.values.zero_()\n        # This attribute is set on several Layers\n        if hasattr(self, \"cumulative_length\"):\n            self.cumulative_length = 0\n\n    def reorder_cache(self, beam_idx: torch.LongTensor) -> None:\n        \"\"\"Reorders this layer's cache for beam search.\"\"\"\n        if self.get_seq_length() > 0:\n            self.keys = self.keys.index_select(0, beam_idx.to(self.keys.device))\n            self.values = self.values.index_select(0, beam_idx.to(self.values.device))"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/models/mixtral/modeling_mixtral.py#MixtralModel": {
                "sorted_modules": {
                    "MixtralModel": "\n\n@auto_docstring\nclass MixtralModel(MixtralPreTrainedModel):\n    def __init__(self, config: MixtralConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList(\n            [MixtralDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n        )\n        self.norm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.rotary_emb = MixtralRotaryEmbedding(config=config)\n        self.gradient_checkpointing = False\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @check_model_inputs()\n    @auto_docstring\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Cache] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs: Unpack[TransformersKwargs],\n    ) -> MoeModelOutputWithPast:\n        if (input_ids is None) ^ (inputs_embeds is not None):\n            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n\n        if use_cache and past_key_values is None:\n            past_key_values = DynamicCache(config=self.config)\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        if cache_position is None:\n            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n            cache_position = torch.arange(\n                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n            )\n        if position_ids is None:\n            position_ids = cache_position.unsqueeze(0)\n\n        mask_function = create_causal_mask if self.config.sliding_window is None else create_sliding_window_causal_mask\n        causal_mask = mask_function(\n            config=self.config,\n            input_embeds=inputs_embeds,\n            attention_mask=attention_mask,\n            cache_position=cache_position,\n            past_key_values=past_key_values,\n            position_ids=position_ids,\n        )\n\n        hidden_states = inputs_embeds\n        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n\n        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n            hidden_states = decoder_layer(\n                hidden_states,\n                attention_mask=causal_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                use_cache=use_cache,\n                cache_position=cache_position,\n                position_embeddings=position_embeddings,\n                **kwargs,\n            )\n\n        hidden_states = self.norm(hidden_states)\n\n        return MoeModelOutputWithPast(  # only diff with Mistral is the output type, we need MoE\n            last_hidden_state=hidden_states,\n            past_key_values=past_key_values,\n        )"
                },
                "component_dependencies": {
                    "MixtralModel": [
                        "transformers/cache_utils.py#Cache",
                        "transformers/cache_utils.py#DynamicCache",
                        "transformers/masking_utils.py#create_causal_mask",
                        "transformers/masking_utils.py#create_sliding_window_causal_mask",
                        "transformers/modeling_outputs.py#MoeModelOutputWithPast",
                        "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig",
                        "transformers/models/mixtral/modeling_mixtral.py#MixtralDecoderLayer",
                        "transformers/models/mixtral/modeling_mixtral.py#MixtralPreTrainedModel",
                        "transformers/models/mixtral/modeling_mixtral.py#MixtralRMSNorm",
                        "transformers/models/mixtral/modeling_mixtral.py#MixtralRotaryEmbedding",
                        "transformers/processing_utils.py#Unpack",
                        "transformers/utils.py#TransformersKwargs",
                        "transformers/utils.py#auto_docstring",
                        "transformers/utils/generic.py#check_model_inputs"
                    ]
                },
                "warning": null
            },
            "transformers/models/mixtral/modeling_mixtral.py#MixtralPreTrainedModel": {
                "sorted_modules": {
                    "MixtralPreTrainedModel": "\n\n@auto_docstring\nclass MixtralPreTrainedModel(PreTrainedModel):\n    config: MixtralConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"MixtralDecoderLayer\"]\n    _skip_keys_device_placement = [\"past_key_values\"]\n    _supports_flash_attn = True\n    _supports_sdpa = True\n    _supports_flex_attn = True\n    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n    _supports_attention_backend = True\n    _can_record_outputs = {\n        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"block_sparse_moe.gate\", index=0),\n        \"hidden_states\": MixtralDecoderLayer,\n        \"attentions\": MixtralAttention,\n    }"
                },
                "component_dependencies": {
                    "MixtralPreTrainedModel": [
                        "transformers/modeling_utils.py#PreTrainedModel",
                        "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig",
                        "transformers/models/mixtral/modeling_mixtral.py#MixtralAttention",
                        "transformers/models/mixtral/modeling_mixtral.py#MixtralDecoderLayer",
                        "transformers/utils.py#auto_docstring",
                        "transformers/utils/generic.py#OutputRecorder"
                    ]
                },
                "warning": null
            },
            "transformers/masking_utils.py#create_causal_mask": {
                "sorted_modules": {
                    "create_causal_mask": "\n\ndef create_causal_mask(\n    config: PreTrainedConfig,\n    input_embeds: torch.Tensor,\n    attention_mask: Optional[torch.Tensor],\n    cache_position: torch.Tensor,\n    past_key_values: Optional[Cache],\n    position_ids: Optional[torch.Tensor] = None,\n    or_mask_function: Optional[Callable] = None,\n    and_mask_function: Optional[Callable] = None,\n) -> Optional[Union[torch.Tensor, BlockMask]]:\n    \"\"\"\n    Create a standard causal mask based on the attention implementation used (stored in the config). If `past_key_values`\n    has an hybrid cache structure, this function will return the mask corresponding to one of the \"full_attention\" layers (to align\n    to what is needed in the `modeling_xxx.py` files).\n\n    Args:\n        config (`PreTrainedConfig`):\n            The model config.\n        input_embeds (`torch.Tensor`):\n            The input embeddings of shape (batch_size, query_length, hidden_dim). This is used only to infer the\n            batch size, query length and dtype.\n        attention_mask (`torch.Tensor`, optional):\n            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length).\n            It can also be an already prepared 4D mask, in which case it is returned as-is.\n        cache_position (`torch.Tensor`):\n            A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n        past_key_values (`Cache`, optional):\n            The past key values, if we use a cache.\n        position_ids (`torch.Tensor`, optional)\n            A 2D tensor of shape (batch_size, query_length) indicating the positions of each token in the sequences.\n        or_mask_function (`Callable`, optional):\n            An optional mask function to combine with the causal mask function (by doing the union of both). This is\n            useful to easily overlay another mask on top of the causal one, for example for image tokens handling.\n        and_mask_function (`Callable`, optional):\n            An optional mask function to combine with the causal mask function (by doing the intersection of both). This is\n            useful to easily overlay another mask on top of the causal one, for example for image tokens handling.\n    \"\"\"\n    # If we have an hybrid cache structure, here we want to create the mask for the full layers\n    if hasattr(past_key_values, \"is_sliding\") and False in past_key_values.is_sliding:\n        layer_idx = past_key_values.is_sliding.index(False)\n    else:\n        layer_idx = 0\n\n    early_exit, attention_mask, packed_sequence_mask, kv_length, kv_offset = _preprocess_mask_arguments(\n        config, input_embeds, attention_mask, cache_position, past_key_values, position_ids, layer_idx\n    )\n    if early_exit:\n        return attention_mask\n\n    batch_size, dtype = input_embeds.shape[0], input_embeds.dtype\n    mask_factory_function = causal_mask_function\n    mask_interface = ALL_MASK_ATTENTION_FUNCTIONS[config._attn_implementation]\n\n    # Do not allow skip if we are compiling (this is to match BC)\n    # TODO: cyril -> probably revisit and remove this, but a lot of tests rely on it\n    if _is_torch_xpu_available:\n        # Do not allow skip if we are compiling for decoding, but for prefill, we still allow skip to optimization the perf of 1st token generation\n        allow_is_causal_skip = not (getattr(past_key_values, \"is_compileable\", False) and cache_position.shape[0] == 1)\n    else:\n        allow_is_causal_skip = not getattr(past_key_values, \"is_compileable\", False)\n\n    # Allow slight deviations from causal mask\n    # Note that it is very important to apply this before any other deviations of the mask (such as packed sequence mask,\n    # padding mask, etc) as the resulting mask may otherwise not be correct!\n    if or_mask_function is not None:\n        if not _is_torch_greater_or_equal_than_2_6:\n            raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6\")\n        mask_factory_function = or_masks(mask_factory_function, or_mask_function)\n        allow_is_causal_skip = False\n    if and_mask_function is not None:\n        if not _is_torch_greater_or_equal_than_2_6:\n            raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6\")\n        mask_factory_function = and_masks(mask_factory_function, and_mask_function)\n        allow_is_causal_skip = False\n\n    # If we detected packing format\n    if packed_sequence_mask is not None and _is_torch_greater_or_equal_than_2_6:\n        mask_factory_function = and_masks(mask_factory_function, packed_sequence_mask_function(packed_sequence_mask))\n        allow_is_causal_skip = False\n\n    # We now create the mask\n    causal_mask = mask_interface(\n        batch_size=batch_size,\n        cache_position=cache_position,\n        kv_length=kv_length,\n        kv_offset=kv_offset,\n        mask_function=mask_factory_function,\n        attention_mask=attention_mask,\n        allow_is_causal_skip=allow_is_causal_skip,  # additional kwarg for sdpa\n        dtype=dtype,  # Additional kwarg for eager\n        config=config,  # Pass the config as well, in case someone wants to easily have their own mask_interface\n    )\n    return causal_mask"
                },
                "component_dependencies": {
                    "create_causal_mask": [
                        "transformers/cache_utils.py#Cache",
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/masking_utils.py#_is_torch_greater_or_equal_than_2_6",
                        "transformers/masking_utils.py#_is_torch_xpu_available",
                        "transformers/masking_utils.py#_preprocess_mask_arguments",
                        "transformers/masking_utils.py#and_masks",
                        "transformers/masking_utils.py#causal_mask_function",
                        "transformers/masking_utils.py#or_masks",
                        "transformers/masking_utils.py#packed_sequence_mask_function"
                    ]
                },
                "warning": null
            },
            "transformers/masking_utils.py#create_sliding_window_causal_mask": {
                "sorted_modules": {
                    "create_sliding_window_causal_mask": "\n\ndef create_sliding_window_causal_mask(\n    config: PreTrainedConfig,\n    input_embeds: torch.Tensor,\n    attention_mask: Optional[torch.Tensor],\n    cache_position: torch.Tensor,\n    past_key_values: Optional[Cache],\n    position_ids: Optional[torch.Tensor] = None,\n    or_mask_function: Optional[Callable] = None,\n    and_mask_function: Optional[Callable] = None,\n) -> Optional[Union[torch.Tensor, BlockMask]]:\n    \"\"\"\n    Create a sliding window causal mask based on the attention implementation used (stored in the config). This type\n    of attention pattern was mostly democratized by Mistral. If `past_key_values` has an hybrid cache structure, this\n    function will return the mask corresponding to one of the \"sliding_attention\" layers (to align to what is needed in the\n    `modeling_xxx.py` files).\n\n    Args:\n        config (`PreTrainedConfig`):\n            The model config.\n        input_embeds (`torch.Tensor`):\n            The input embeddings of shape (batch_size, query_length, hidden_dim). This is used only to infer the\n            batch size, query length and dtype.\n        attention_mask (`torch.Tensor`, optional):\n            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length).\n            It can also be an already prepared 4D mask, in which case it is returned as-is.\n        cache_position (`torch.Tensor`):\n            A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n        past_key_values (`Cache`, optional):\n            The past key values, if we use a cache.\n        position_ids (`torch.Tensor`, optional)\n            A 2D tensor of shape (batch_size, query_length) indicating the positions of each token in the sequences.\n        or_mask_function (`Callable`, optional):\n            An optional mask function to combine with the sliding causal mask function (by doing the union of both). This is\n            useful to easily overlay another mask on top of the sliding causal one, for example for image tokens handling.\n        and_mask_function (`Callable`, optional):\n            An optional mask function to combine with the sliding causal mask function (by doing the intersection of both). This is\n            useful to easily overlay another mask on top of the sliding causal one, for example for image tokens handling.\n    \"\"\"\n    # If we have an hybrid cache structure, here we want to create the mask for the sliding layers\n    if hasattr(past_key_values, \"is_sliding\") and True in past_key_values.is_sliding:\n        layer_idx = past_key_values.is_sliding.index(True)\n    else:\n        layer_idx = 0\n\n    early_exit, attention_mask, packed_sequence_mask, kv_length, kv_offset = _preprocess_mask_arguments(\n        config, input_embeds, attention_mask, cache_position, past_key_values, position_ids, layer_idx\n    )\n    if early_exit:\n        return attention_mask\n\n    sliding_window = getattr(config, \"sliding_window\", None)\n    if sliding_window is None:\n        raise ValueError(\"Could not find a `sliding_window` argument in the config, or it is not set\")\n\n    batch_size, dtype = input_embeds.shape[0], input_embeds.dtype\n    mask_factory_function = sliding_window_causal_mask_function(sliding_window)\n    mask_interface = ALL_MASK_ATTENTION_FUNCTIONS[config._attn_implementation]\n\n    # Do not allow skip if we are compiling (this is to match BC)\n    # TODO: cyril -> probably revisit and remove this, but a lot of tests rely on it\n    allow_is_causal_skip = not getattr(past_key_values, \"is_compileable\", False)\n\n    # Allow slight deviations from causal mask\n    # Note that it is very important to apply this before any other deviations of the mask (such as packed sequence mask,\n    # padding mask, etc) as the resulting mask may otherwise not be correct!\n    if or_mask_function is not None:\n        if not _is_torch_greater_or_equal_than_2_6:\n            raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6\")\n        mask_factory_function = or_masks(mask_factory_function, or_mask_function)\n        allow_is_causal_skip = False\n    if and_mask_function is not None:\n        if not _is_torch_greater_or_equal_than_2_6:\n            raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6\")\n        mask_factory_function = and_masks(mask_factory_function, and_mask_function)\n        allow_is_causal_skip = False\n\n    # If we detected packing format\n    if packed_sequence_mask is not None and _is_torch_greater_or_equal_than_2_6:\n        mask_factory_function = and_masks(mask_factory_function, packed_sequence_mask_function(packed_sequence_mask))\n        allow_is_causal_skip = False\n\n    # We now create the mask\n    causal_mask = mask_interface(\n        batch_size=batch_size,\n        cache_position=cache_position,\n        kv_length=kv_length,\n        kv_offset=kv_offset,\n        mask_function=mask_factory_function,\n        attention_mask=attention_mask,\n        allow_is_causal_skip=allow_is_causal_skip,  # additional kwarg for sdpa\n        local_size=sliding_window,  # Additional kwarg for sdpa\n        dtype=dtype,  # Additional kwarg for eager\n        config=config,  # Pass the config as well, in case someone wants to easily have their own mask_interface\n    )\n    return causal_mask"
                },
                "component_dependencies": {
                    "create_sliding_window_causal_mask": [
                        "transformers/cache_utils.py#Cache",
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/masking_utils.py#_is_torch_greater_or_equal_than_2_6",
                        "transformers/masking_utils.py#_preprocess_mask_arguments",
                        "transformers/masking_utils.py#and_masks",
                        "transformers/masking_utils.py#or_masks",
                        "transformers/masking_utils.py#packed_sequence_mask_function",
                        "transformers/masking_utils.py#sliding_window_causal_mask_function"
                    ]
                },
                "warning": null
            },
            "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig": {
                "sorted_modules": {
                    "MixtralConfig": "\n\nclass MixtralConfig(PreTrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`MixtralModel`]. It is used to instantiate an\n    Mixtral model according to the specified arguments, defining the model architecture. Instantiating a configuration\n    with the defaults will yield a similar configuration to that of the Mixtral-7B-v0.1 or Mixtral-7B-Instruct-v0.1.\n\n    [mixtralai/Mixtral-8x7B](https://huggingface.co/mixtralai/Mixtral-8x7B)\n    [mixtralai/Mixtral-7B-Instruct-v0.1](https://huggingface.co/mixtralai/Mixtral-7B-Instruct-v0.1)\n\n    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PreTrainedConfig`] for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 32000):\n            Vocabulary size of the Mixtral model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`MixtralModel`]\n        hidden_size (`int`, *optional*, defaults to 4096):\n            Dimension of the hidden representations.\n        intermediate_size (`int`, *optional*, defaults to 14336):\n            Dimension of the MLP representations.\n        num_hidden_layers (`int`, *optional*, defaults to 32):\n            Number of hidden layers in the Transformer encoder.\n        num_attention_heads (`int`, *optional*, defaults to 32):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        num_key_value_heads (`int`, *optional*, defaults to 8):\n            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n            by meanpooling all the original heads within that group. For more details, check out [this\n            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to `8`.\n        head_dim (`int`, *optional*, defaults to `hidden_size // num_attention_heads`):\n            The attention head dimension.\n        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n            The non-linear activation function (function or string) in the decoder.\n        max_position_embeddings (`int`, *optional*, defaults to `4096*32`):\n            The maximum sequence length that this model might ever be used with. Mixtral's sliding window attention\n            allows sequence of up to 4096*32 tokens.\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n            The epsilon used by the rms normalization layers.\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return the last key/values attentions (not used by all models). Only\n            relevant if `config.is_decoder=True`.\n        pad_token_id (`int`, *optional*):\n            The id of the padding token.\n        bos_token_id (`int`, *optional*, defaults to 1):\n            The id of the \"beginning-of-sequence\" token.\n        eos_token_id (`int`, *optional*, defaults to 2):\n            The id of the \"end-of-sequence\" token.\n        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n            Whether the model's input and output word embeddings should be tied.\n        sliding_window (`int`, *optional*):\n            Sliding window attention window size. If not specified, will default to `4096`.\n        attention_dropout (`float`, *optional*, defaults to 0.0):\n            The dropout ratio for the attention probabilities.\n        num_experts_per_tok (`int`, *optional*, defaults to 2):\n            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n            parameter\n        num_local_experts (`int`, *optional*, defaults to 8):\n            Number of experts per Sparse MLP layer.\n        output_router_logits (`bool`, *optional*, defaults to `False`):\n            Whether or not the router logits should be returned by the model. Enabling this will also\n            allow the model to output the auxiliary loss. See [here]() for more details\n        router_aux_loss_coef (`float`, *optional*, defaults to 0.001):\n            The aux loss factor for the total loss.\n        router_jitter_noise (`float`, *optional*, defaults to 0.0):\n            Amount of noise to add to the router.\n        rope_parameters (`RopeParameters`, *optional*):\n            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n            with longer `max_position_embeddings`.\n\n    ```python\n    >>> from transformers import MixtralModel, MixtralConfig\n\n    >>> # Initializing a Mixtral 7B style configuration\n    >>> configuration = MixtralConfig()\n\n    >>> # Initializing a model from the Mixtral 7B style configuration\n    >>> model = MixtralModel(configuration)\n\n    >>> # Accessing the model configuration\n    >>> configuration = model.config\n    ```\"\"\"\n\n    model_type = \"mixtral\"\n    keys_to_ignore_at_inference = [\"past_key_values\"]\n    base_model_tp_plan = {\n        \"layers.*.self_attn.q_proj\": \"colwise\",\n        \"layers.*.self_attn.k_proj\": \"colwise\",\n        \"layers.*.self_attn.v_proj\": \"colwise\",\n        \"layers.*.self_attn.o_proj\": \"rowwise\",\n        \"layers.*.block_sparse_moe.gate\": \"colwise_rep\",  # we need to replicate here to correctly route experts\n        \"layers.*.block_sparse_moe.experts.*.w1\": \"colwise\",\n        \"layers.*.block_sparse_moe.experts.*.w2\": \"rowwise\",\n        \"layers.*.block_sparse_moe.experts.*.w3\": \"colwise\",\n    }\n    base_model_pp_plan = {\n        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n    }\n    attribute_map = {\n        \"num_experts\": \"num_local_experts\",\n    }\n\n    def __init__(\n        self,\n        vocab_size: Optional[int] = 32000,\n        hidden_size: Optional[int] = 4096,\n        intermediate_size: Optional[int] = 14336,\n        num_hidden_layers: Optional[int] = 32,\n        num_attention_heads: Optional[int] = 32,\n        num_key_value_heads: Optional[int] = 8,\n        head_dim: Optional[int] = None,\n        hidden_act: Optional[str] = \"silu\",\n        max_position_embeddings: Optional[int] = 4096 * 32,\n        initializer_range: Optional[float] = 0.02,\n        rms_norm_eps: Optional[int] = 1e-5,\n        use_cache: Optional[bool] = True,\n        pad_token_id: Optional[int] = None,\n        bos_token_id: Optional[int] = 1,\n        eos_token_id: Optional[int] = 2,\n        tie_word_embeddings: Optional[bool] = False,\n        sliding_window: Optional[int] = None,\n        attention_dropout: Optional[float] = 0.0,\n        num_experts_per_tok: Optional[int] = 2,\n        num_local_experts: Optional[int] = 8,\n        output_router_logits: Optional[bool] = False,\n        router_aux_loss_coef: Optional[float] = 0.001,\n        router_jitter_noise: Optional[float] = 0.0,\n        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n        **kwargs,\n    ):\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.sliding_window = sliding_window\n\n        # for backward compatibility\n        if num_key_value_heads is None:\n            num_key_value_heads = num_attention_heads\n\n        self.num_key_value_heads = num_key_value_heads\n        self.hidden_act = hidden_act\n        self.initializer_range = initializer_range\n        self.rms_norm_eps = rms_norm_eps\n        self.use_cache = use_cache\n        self.attention_dropout = attention_dropout\n        self.head_dim = head_dim\n\n        self.num_experts_per_tok = num_experts_per_tok\n        self.num_local_experts = num_local_experts\n        self.output_router_logits = output_router_logits\n        self.router_aux_loss_coef = router_aux_loss_coef\n        self.router_jitter_noise = router_jitter_noise\n        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n        self.rope_parameters = rope_scaling or rope_parameters\n\n        # Validate the correctness of rotary position embeddings parameters\n        rope_theta = kwargs.get(\"rope_theta\", 1000000.0)\n        standardize_rope_params(self, rope_theta=rope_theta)\n        rope_config_validation(self)\n\n        super().__init__(\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs,\n        )"
                },
                "component_dependencies": {
                    "MixtralConfig": [
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/modeling_rope_utils.py#RopeParameters",
                        "transformers/modeling_rope_utils.py#rope_config_validation",
                        "transformers/modeling_rope_utils.py#standardize_rope_params"
                    ]
                },
                "warning": null
            },
            "transformers/models/mixtral/modeling_mixtral.py#MixtralDecoderLayer": {
                "sorted_modules": {
                    "MixtralDecoderLayer": "\n\nclass MixtralDecoderLayer(GradientCheckpointingLayer):\n    def __init__(self, config: MixtralConfig, layer_idx: int):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n\n        self.self_attn = MixtralAttention(config, layer_idx)\n\n        self.block_sparse_moe = MixtralSparseMoeBlock(config)\n        self.input_layernorm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Cache] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs: Unpack[TransformersKwargs],\n    ) -> torch.Tensor:\n        residual = hidden_states\n        hidden_states = self.input_layernorm(hidden_states)\n        hidden_states, _ = self.self_attn(\n            hidden_states=hidden_states,\n            position_embeddings=position_embeddings,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            cache_position=cache_position,\n            **kwargs,\n        )\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.block_sparse_moe(hidden_states)\n        hidden_states = residual + hidden_states\n        return hidden_states"
                },
                "component_dependencies": {
                    "MixtralDecoderLayer": [
                        "transformers/cache_utils.py#Cache",
                        "transformers/modeling_layers.py#GradientCheckpointingLayer",
                        "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig",
                        "transformers/models/mixtral/modeling_mixtral.py#MixtralAttention",
                        "transformers/models/mixtral/modeling_mixtral.py#MixtralRMSNorm",
                        "transformers/models/mixtral/modeling_mixtral.py#MixtralSparseMoeBlock",
                        "transformers/processing_utils.py#Unpack",
                        "transformers/utils.py#TransformersKwargs"
                    ]
                },
                "warning": null
            },
            "transformers/models/mixtral/modeling_mixtral.py#MixtralRMSNorm": {
                "sorted_modules": {
                    "MixtralRMSNorm": "\n\n@use_kernel_forward_from_hub(\"RMSNorm\")\nclass MixtralRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        MixtralRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n    def extra_repr(self):\n        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\""
                },
                "component_dependencies": {
                    "MixtralRMSNorm": [
                        "transformers/integrations.py#use_kernel_forward_from_hub"
                    ]
                },
                "warning": null
            },
            "transformers/models/mixtral/modeling_mixtral.py#MixtralRotaryEmbedding": {
                "sorted_modules": {
                    "MixtralRotaryEmbedding": "\n\nclass MixtralRotaryEmbedding(nn.Module):\n    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n\n    def __init__(self, config: MixtralConfig, device=None):\n        super().__init__()\n        self.max_seq_len_cached = config.max_position_embeddings\n        self.original_max_seq_len = config.max_position_embeddings\n\n        self.config = config\n\n        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n        rope_init_fn: Callable = self.compute_default_rope_parameters\n        if self.rope_type != \"default\":\n            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        self.original_inv_freq = inv_freq\n\n    @staticmethod\n    def compute_default_rope_parameters(\n        config: Optional[MixtralConfig] = None,\n        device: Optional[\"torch.device\"] = None,\n        seq_len: Optional[int] = None,\n    ) -> tuple[\"torch.Tensor\", float]:\n        \"\"\"\n        Computes the inverse frequencies according to the original RoPE implementation\n        Args:\n            config ([`~transformers.PreTrainedConfig`]):\n                The model configuration.\n            device (`torch.device`):\n                The device to use for initialization of the inverse frequencies.\n            seq_len (`int`, *optional*):\n                The current sequence length. Unused for this type of RoPE.\n        Returns:\n            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n        \"\"\"\n        base = config.rope_parameters[\"rope_theta\"]\n        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n\n        attention_factor = 1.0  # Unused in this type of RoPE\n\n        # Compute the inverse frequencies\n        inv_freq = 1.0 / (\n            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n        )\n        return inv_freq, attention_factor\n\n    @torch.no_grad()\n    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n    def forward(self, x, position_ids):\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n        position_ids_expanded = position_ids[:, None, :].float()\n\n        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos() * self.attention_scaling\n            sin = emb.sin() * self.attention_scaling\n\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)"
                },
                "component_dependencies": {
                    "MixtralRotaryEmbedding": [
                        "transformers/modeling_rope_utils.py#ROPE_INIT_FUNCTIONS",
                        "transformers/modeling_rope_utils.py#dynamic_rope_update",
                        "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_utils.py#PreTrainedModel": {
                "sorted_modules": {
                    "PreTrainedModel": "\n\nclass PreTrainedModel(nn.Module, EmbeddingAccessMixin, ModuleUtilsMixin, PushToHubMixin, PeftAdapterMixin):\n    r\"\"\"\n    Base class for all models.\n\n    [`PreTrainedModel`] takes care of storing the configuration of the models and handles methods for loading,\n    downloading and saving models as well as a few methods common to all models to:\n\n        - resize the input embeddings\n\n    Class attributes (overridden by derived classes):\n\n        - **config_class** ([`PreTrainedConfig`]) -- A subclass of [`PreTrainedConfig`] to use as configuration class\n          for this model architecture.\n        - **base_model_prefix** (`str`) -- A string indicating the attribute associated to the base model in derived\n          classes of the same architecture adding modules on top of the base model.\n        - **main_input_name** (`str`) -- The name of the principal input to the model (often `input_ids` for NLP\n          models, `pixel_values` for vision models and `input_values` for speech models).\n        - **can_record_outputs** (dict):\n    \"\"\"\n\n    config_class = None\n    base_model_prefix = \"\"\n    main_input_name = \"input_ids\"\n    model_tags = None\n\n    _checkpoint_conversion_mapping = {}  # used for BC support in VLMs, not meant to be used by new models\n\n    _auto_class = None\n    _no_split_modules = None\n    _skip_keys_device_placement = None\n\n    _keep_in_fp32_modules = None\n    # the _keep_in_fp32_modules will avoid casting to anything other than float32, except bfloat16\n    # to also prevent bfloat16 casting, use the _keep_in_fp32_modules_strict flag\n    _keep_in_fp32_modules_strict = None\n\n    # a list of `re` patterns of `state_dict` keys that should be removed from the list of missing\n    # keys we find (keys inside the model but not in the checkpoint) and avoid unnecessary warnings.\n    _keys_to_ignore_on_load_missing = None\n    # a list of `re` patterns of `state_dict` keys that should be removed from the list of\n    # unexpected keys we find (keys inside the checkpoint but not the model) and avoid unnecessary\n    # warnings.\n    _keys_to_ignore_on_load_unexpected = None\n    # a list of `state_dict` keys to ignore when saving the model (useful for keys that aren't\n    # trained, but which are either deterministic or tied variables)\n    _keys_to_ignore_on_save = None\n    # a list of `state_dict` keys that are potentially tied to another key in the state_dict.\n    _tied_weights_keys = None\n\n    supports_gradient_checkpointing = False\n    _is_stateful = False\n\n    # Flash Attention support\n    _supports_flash_attn = False\n\n    # SDPA support\n    _supports_sdpa = False\n\n    # Flex Attention support\n    _supports_flex_attn = False\n\n    _can_compile_fullgraph = False\n\n    # A tensor parallel plan to be applied to the model when TP is enabled. For\n    # top-level models, this attribute is currently defined in respective model\n    # code. For base models, this attribute comes from\n    # `config.base_model_tp_plan` during `__init__`.\n    # It should identify the layers exactly: if you want to TP model.language_model.layers.fc1\n    # by passing `tp_plan` to the init, it should be {\"model.language_model.layers.fc1\":\"colwise\"}\n    # for example.\n    _tp_plan = None\n\n    # tensor parallel degree to which model is sharded to.\n    _tp_size = None\n\n    # A pipeline parallel plan specifying the layers which may not be present\n    # on all ranks when PP is enabled. For top-level models, this attribute is\n    # currently defined in respective model code. For base models, this\n    # attribute comes from `config.base_model_pp_plan` during `post_init`.\n    #\n    # The variable names for the inputs and outputs of the specified layers can\n    # be indexed using the `PipelineParallel` enum as follows:\n    # - `_pp_plan[\"layers\"][PipelineParallel.inputs]`\n    # - `_pp_plan[\"layers\"][PipelineParallel.outputs]`\n    _pp_plan = None\n\n    # This flag signal that the model can be used as an efficient backend in TGI and vLLM\n    # In practice, it means that they support attention (mask) interface functions, fully pass the kwargs\n    # through all modules up to the Attention layer, can slice logits with Tensor, and have a default TP plan\n    _supports_attention_backend = False\n    _can_record_outputs = None\n\n    # Attributes used mainly in multimodal LLMs, though all models contain a valid field for these\n    # Possible values are: text, image, video, audio and time\n    input_modalities: Union[str, list[str]] = \"text\"  # most models are text\n\n    @property\n    @torch._dynamo.allow_in_graph\n    def can_record_outputs(self) -> dict[str, OutputRecorder]:\n        \"\"\"\n         Maps output names (e.g., \"attentions\", \"hidden_states\")\n         to either:\n             - A module class (e.g., `LlamaDecoderLayer`), using default index conventions:\n                 * index=0 for \"hidden_states\"\n                 * index=1 for \"attentions\"\n             - Or an `OutputRecorder(...)` with `target_class`, optional `index`, and `layer_name`.\n\n         Examples:\n             These two are equivalent:\n\n         ```python\n             _can_record_outputs = {\n                 \"attentions\": LlamaAttention,\n                 \"hidden_states\": LlamaDecoderLayer\n             }\n\n             _can_record_outputs = {\n                 \"attentions\": OutputRecorder(LlamaAttention, index=1),\n                 \"hidden_states\": OutputRecorder(LlamaDecoderLayer, index=0)\n             }\n        ```\n\n         This means you can record outputs from the same class, by specifying a layer name. Before\n         collecting outputs, we check that they come from this layer.\n\n         If you have cross attention that come from `LlamaAttention` and self attention that also\n         come from `LlamaAttention` but from `self_attn` you can do this:\n\n         ```python\n         class LlamaModel(PreTrainedModel):\n             _can_record_outputs = {\n                 \"attentions\": OutputRecorder(LlamaAttention, index=1, layer-name=\"self_attn\"),\n                 \"cross_attentions\": OutputRecorder(LlamaAttention, index=1, layer_name=\"cross_attn\")\n             }\n\n        ```\n        \"\"\"\n        return self._can_record_outputs or {}\n\n    @property\n    def dummy_inputs(self) -> dict[str, torch.Tensor]:\n        \"\"\"\n        `dict[str, torch.Tensor]`: Dummy inputs to do a forward pass in the network.\n        \"\"\"\n        return {\"input_ids\": torch.tensor(DUMMY_INPUTS)}\n\n    def __init_subclass__(cls, **kwargs):\n        super().__init_subclass__(**kwargs)\n        # For BC we keep the original `config_class` definition in case\n        # there is a `config_class` attribute (e.g. remote code models),\n        # otherwise we derive it from the annotated `config` attribute.\n\n        # defined in this particular subclass\n        child_annotation = cls.__dict__.get(\"__annotations__\", {}).get(\"config\", None)\n        child_attribute = cls.__dict__.get(\"config_class\", None)\n\n        # defined in the class (this subclass or any parent class)\n        full_annotation = get_type_hints(cls).get(\"config\", None)\n        full_attribute = cls.config_class\n\n        # priority (child class_config -> child annotation -> global class_config -> global annotation)\n        if child_attribute is not None:\n            cls.config_class = child_attribute\n        elif child_annotation is not None:\n            cls.config_class = child_annotation\n        elif full_attribute is not None:\n            cls.config_class = full_attribute\n        elif full_annotation is not None:\n            cls.config_class = full_annotation\n\n    def __init__(self, config: PreTrainedConfig, *inputs, **kwargs):\n        super().__init__()\n        if not isinstance(config, PreTrainedConfig):\n            raise TypeError(\n                f\"Parameter config in `{self.__class__.__name__}(config)` should be an instance of class \"\n                \"`PreTrainedConfig`. To create a model from a pretrained model use \"\n                f\"`model = {self.__class__.__name__}.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n            )\n        self.config = config\n\n        # Check the attention implementation is supported, or set it if not yet set (on the internal attr, to avoid\n        # setting it recursively)\n        self.config._attn_implementation_internal = self._check_and_adjust_attn_implementation(\n            self.config._attn_implementation, is_init_check=True\n        )\n\n        # for initialization of the loss\n        loss_type = self.__class__.__name__\n        if loss_type not in LOSS_MAPPING:\n            loss_groups = f\"({'|'.join(LOSS_MAPPING)})\"\n            loss_type = re.findall(loss_groups, self.__class__.__name__)\n            if len(loss_type) > 0:\n                loss_type = loss_type[0]\n            else:\n                loss_type = None\n        self.loss_type = loss_type\n\n        self.name_or_path = config.name_or_path\n        self.warnings_issued = {}\n        self.generation_config = GenerationConfig.from_model_config(config) if self.can_generate() else None\n        # Overwrite the class attribute to make it an instance attribute, so models like\n        # `InstructBlipForConditionalGeneration` can dynamically update it without modifying the class attribute\n        # when a different component (e.g. language_model) is used.\n        self._keep_in_fp32_modules = copy.copy(self.__class__._keep_in_fp32_modules)\n        self._keep_in_fp32_modules_strict = copy.copy(self.__class__._keep_in_fp32_modules_strict)\n\n        self._no_split_modules = self._no_split_modules or []\n        _CAN_RECORD_REGISTRY[str(self.__class__)] = self._can_record_outputs  # added for executorch support only\n\n    def post_init(self):\n        \"\"\"\n        A method executed at the end of each Transformer model initialization, to execute code that needs the model's\n        modules properly initialized (such as weight initialization).\n\n        This is also used when the user is running distributed code. We add hooks to the modules here, according to\n        the model's tp_plan!\n        \"\"\"\n        self.init_weights()\n        self._backward_compatibility_gradient_checkpointing()\n\n        # Make sure the modules correctly exist if the flag is active\n        if self._keep_in_fp32_modules is not None or self._keep_in_fp32_modules_strict is not None:\n            all_parameters = {name for name, _ in self.named_parameters() if len(name) > 0}\n            unique_module_names = set()\n            # Get all unique module names in the module graph, without the prefixes\n            for param in all_parameters:\n                unique_module_names.update(\n                    [name for name in param.split(\".\") if not name.isnumeric() and name not in [\"weight\", \"bias\"]]\n                )\n            # Check that every module in the keep_in_fp32 list is part of the module graph\n            if self._keep_in_fp32_modules is not None:\n                for module in self._keep_in_fp32_modules:\n                    if module not in unique_module_names:\n                        raise ValueError(\n                            f\"{module} was specified in the `_keep_in_fp32_modules` list, but is not part of the modules in\"\n                            f\" {self.__class__.__name__}\"\n                        )\n\n            if self._keep_in_fp32_modules_strict is not None:\n                for module in self._keep_in_fp32_modules_strict:\n                    if module not in unique_module_names:\n                        raise ValueError(\n                            f\"{module} was specified in the `_keep_in_fp32_modules_strict` list, but is not part of the modules in\"\n                            f\" {self.__class__.__name__}\"\n                        )\n\n        self._tp_plan, self._ep_plan, self._pp_plan = {}, {}, {}\n        # If current model is a base model, attach `base_model_tp_plan` and `base_model_pp_plan` from config\n        if self.base_model is self:\n            self._pp_plan = self.config.base_model_pp_plan.copy() if self.config.base_model_pp_plan is not None else {}\n            self._tp_plan = self.config.base_model_tp_plan.copy() if self.config.base_model_tp_plan is not None else {}\n            self._ep_plan = self.config.base_model_ep_plan.copy() if self.config.base_model_ep_plan is not None else {}\n        for name, module in self.named_children():\n            if plan := getattr(module, \"_ep_plan\", None):\n                self._ep_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n            if plan := getattr(module, \"_tp_plan\", None):\n                self._tp_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n            if plan := getattr(module, \"_pp_plan\", None):\n                self._pp_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n\n    @property\n    def tp_plan(self) -> dict[str, str]:\n        \"\"\"\n        The full tp plan for the model's modules\n        \"\"\"\n        if hasattr(self.config, \"distributed_config\") and self.config.distributed_config.enable_expert_parallel:\n            return self._ep_plan\n        return self._tp_plan\n\n    @property\n    def pp_plan(self) -> dict[str, tuple[str, str]]:\n        return self._pp_plan\n\n    @tp_plan.setter\n    def tp_plan(self, plan: dict[str, str] | None):\n        if plan is None:\n            self._tp_plan = {}\n            return\n        if not isinstance(plan, dict):\n            raise ValueError(\"Can only set a dictionary as `tp_plan`\")\n\n        # Ensure the styles are all valid\n        for layer_pattern, parallel_style in plan.items():\n            if parallel_style not in ALL_PARALLEL_STYLES:\n                raise ValueError(\n                    f\"Unsupported tensor parallel style '{parallel_style}' for layer '{layer_pattern}'. \"\n                    f\"Supported styles are {list(ALL_PARALLEL_STYLES.keys())}\"\n                )\n\n        # Validate that the layer patterns match existing model structure. We check this by getting all parameter\n        # names and seeing if any match the patterns\n        model_param_names = [name for name, _ in self.named_parameters()]\n        for layer_pattern in plan.keys():\n            # Convert pattern to regex (replace * with .*)\n            regex_pattern = layer_pattern.replace(\"*\", r\"\\d+\")\n            pattern_matched = False\n            for param_name in model_param_names:\n                if re.match(regex_pattern, param_name):\n                    pattern_matched = True\n                    break\n            if not pattern_matched:\n                warnings.warn(\n                    f\"Layer pattern '{layer_pattern}' does not match any parameters in the model. This rule may not \"\n                    \"be applied during tensor parallelization, or may lead to dimension mismatches\"\n                )\n\n        # Set the plan\n        self._tp_plan = plan\n\n    @pp_plan.setter\n    def pp_plan(self, plan: dict[str, tuple[str, str]]):\n        self._pp_plan = plan\n\n    def dequantize(self):\n        \"\"\"\n        Potentially dequantize the model in case it has been quantized by a quantization method that support\n        dequantization.\n        \"\"\"\n        hf_quantizer = getattr(self, \"hf_quantizer\", None)\n\n        if hf_quantizer is None:\n            raise ValueError(\"You need to first quantize your model in order to dequantize it\")\n\n        return hf_quantizer.dequantize(self)\n\n    def _backward_compatibility_gradient_checkpointing(self):\n        if self.supports_gradient_checkpointing and getattr(self.config, \"gradient_checkpointing\", False):\n            self.gradient_checkpointing_enable()\n            # Remove the attribute now that is has been consumed, so it's no saved in the config.\n            delattr(self.config, \"gradient_checkpointing\")\n\n    def add_model_tags(self, tags: Union[list[str], str]) -> None:\n        r\"\"\"\n        Add custom tags into the model that gets pushed to the Hugging Face Hub. Will\n        not overwrite existing tags in the model.\n\n        Args:\n            tags (`Union[list[str], str]`):\n                The desired tags to inject in the model\n\n        Examples:\n\n        ```python\n        from transformers import AutoModel\n\n        model = AutoModel.from_pretrained(\"google-bert/bert-base-cased\")\n\n        model.add_model_tags([\"custom\", \"custom-bert\"])\n\n        # Push the model to your namespace with the name \"my-custom-bert\".\n        model.push_to_hub(\"my-custom-bert\")\n        ```\n        \"\"\"\n        if isinstance(tags, str):\n            tags = [tags]\n\n        if self.model_tags is None:\n            self.model_tags = []\n\n        for tag in tags:\n            if tag not in self.model_tags:\n                self.model_tags.append(tag)\n\n    @classmethod\n    @restore_default_dtype\n    def _from_config(cls, config, **kwargs):\n        \"\"\"\n        All context managers that the model should be initialized under go here.\n\n        Args:\n            dtype (`torch.dtype`, *optional*):\n                Override the default `dtype` and load the model under this dtype.\n        \"\"\"\n        # when we init a model from within another model (e.g. VLMs) and dispatch on FA2\n        # a warning is raised that dtype should be fp16. Since we never pass dtype from within\n        # modeling code, we can try to infer it here same way as done in `from_pretrained`\n        # For BC on the old `torch_dtype`\n        dtype = kwargs.pop(\"dtype\", config.dtype)\n        if (torch_dtype := kwargs.pop(\"torch_dtype\", None)) is not None:\n            logger.warning_once(\"`torch_dtype` is deprecated! Use `dtype` instead!\")\n            # if both kwargs are provided, use `dtype`\n            dtype = dtype if dtype != config.dtype else torch_dtype\n        if isinstance(dtype, str):\n            dtype = getattr(torch, dtype)\n\n        # override default dtype if needed\n        dtype_orig = None\n        if dtype is not None:\n            dtype_orig = cls._set_default_dtype(dtype)\n\n        # If passing `attn_implementation` as kwargs, respect it (it will be applied recursively on subconfigs)\n        if \"attn_implementation\" in kwargs:\n            config._attn_implementation = kwargs.pop(\"attn_implementation\")\n\n        if is_deepspeed_zero3_enabled() and not _is_quantized and not _is_ds_init_called:\n            logger.info(\"Detected DeepSpeed ZeRO-3: activating zero.init() for this model\")\n            # this immediately partitions the model across all gpus, to avoid the overhead in time\n            # and memory copying it on CPU or each GPU first\n            import deepspeed\n\n            init_contexts = [deepspeed.zero.Init(config_dict_or_path=deepspeed_config()), set_zero3_state()]\n            with ContextManagers(init_contexts):\n                model = cls(config, **kwargs)\n\n        else:\n            model = cls(config, **kwargs)\n\n        # restore default dtype if it was modified\n        if dtype_orig is not None:\n            torch.set_default_dtype(dtype_orig)\n\n        return model\n\n    @classmethod\n    def _set_default_dtype(cls, dtype: torch.dtype) -> torch.dtype:\n        \"\"\"\n        Change the default dtype and return the previous one. This is needed when wanting to instantiate the model\n        under specific dtype.\n\n        Args:\n            dtype (`torch.dtype`):\n                a floating dtype to set to.\n\n        Returns:\n            `torch.dtype`: the original `dtype` that can be used to restore `torch.set_default_dtype(dtype)` if it was\n            modified. If it wasn't, returns `None`.\n\n        Note `set_default_dtype` currently only works with floating-point types and asserts if for example,\n        `torch.int64` is passed. So if a non-float `dtype` is passed this functions will throw an exception.\n        \"\"\"\n        if not dtype.is_floating_point:\n            raise ValueError(\n                f\"Can't instantiate {cls.__name__} model under dtype={dtype} since it is not a floating point dtype\"\n            )\n\n        logger.info(f\"Instantiating {cls.__name__} model under default dtype {dtype}.\")\n        dtype_orig = torch.get_default_dtype()\n        torch.set_default_dtype(dtype)\n        return dtype_orig\n\n    @property\n    def base_model(self) -> nn.Module:\n        \"\"\"\n        `torch.nn.Module`: The main body of the model.\n        \"\"\"\n        return getattr(self, self.base_model_prefix, self)\n\n    @classmethod\n    def can_generate(cls) -> bool:\n        \"\"\"\n        Returns whether this model can generate sequences with `.generate()` from the `GenerationMixin`.\n\n        Under the hood, on classes where this function returns True, some generation-specific changes are triggered:\n        for instance, the model instance will have a populated `generation_config` attribute.\n\n        Returns:\n            `bool`: Whether this model can generate sequences with `.generate()`.\n        \"\"\"\n        # Directly inherits `GenerationMixin` -> can generate\n        if \"GenerationMixin\" in str(cls.__bases__):\n            return True\n        # The class inherits from a class that can generate (recursive check) -> can generate\n        for base in cls.__bases__:\n            if not hasattr(base, \"can_generate\"):\n                continue\n            if \"PreTrainedModel\" not in str(base) and base.can_generate():\n                return True\n        # Detects whether `prepare_inputs_for_generation` has been overwritten in the model. Prior to v4.45, this\n        # was how we detected whether a model could generate.\n        if hasattr(cls, \"prepare_inputs_for_generation\"):  # implicit: doesn't inherit `GenerationMixin`\n            logger.warning(\n                f\"{cls.__name__} has generative capabilities, as `prepare_inputs_for_generation` is explicitly \"\n                \"defined. However, it doesn't directly inherit from `GenerationMixin`. From \ud83d\udc49v4.50\ud83d\udc48 onwards, \"\n                \"`PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability \"\n                \"to call `generate` and other related functions.\"\n                \"\\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the \"\n                \"model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\"\n                \"\\n  - If you are the owner of the model architecture code, please modify your model class such that \"\n                \"it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\"\n                \"\\n  - If you are not the owner of the model architecture class, please contact the model code owner \"\n                \"to update it.\"\n            )\n        # Otherwise, can't generate\n        return False\n\n    def _flash_attn_2_can_dispatch(self, is_init_check: bool = False) -> bool:\n        \"\"\"\n        Check the availability of Flash Attention 2 for a given model.\n\n        Args:\n            is_init_check (`bool`, *optional*):\n                Whether this check is performed early, i.e. at __init__ time, or later when the model and its weights are\n                fully instantiated. This is needed as we also check the devices of the weights, which are only available\n                later after __init__. This allows to raise proper exceptions early before instantiating the full models\n                if we know that the model does not support the requested attention.\n        \"\"\"\n        dtype = self.config.dtype\n\n        # check `supports_flash_attn_2` for BC with custom code. TODO: remove after a few releases\n        if not (self._supports_flash_attn or getattr(self, \"_supports_flash_attn_2\", False)):\n            raise ValueError(\n                f\"{self.__class__.__name__} does not support Flash Attention 2.0 yet. Please request to add support where\"\n                f\" the model is hosted, on its model hub page: https://huggingface.co/{self.config._name_or_path}/discussions/new\"\n                \" or in the Transformers GitHub repo: https://github.com/huggingface/transformers/issues/new\"\n            )\n\n        if not is_flash_attn_2_available():\n            preface = \"FlashAttention2 has been toggled on, but it cannot be used due to the following error:\"\n            install_message = \"Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\"\n\n            # package `flash-attn` can not be installed on Ascend NPU, following validation logics can be ignored.\n            if is_torch_npu_available():\n                logger.info(\"Detect using FlashAttention2 on Ascend NPU.\")\n                return True\n\n            if importlib.util.find_spec(\"flash_attn\") is None:\n                raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n            else:\n                # Check FA2 installed version compatibility\n                flash_attention_version = version.parse(importlib.metadata.version(\"flash_attn\"))\n                if torch.version.cuda:\n                    if flash_attention_version < version.parse(\"2.1.0\"):\n                        raise ImportError(\n                            f\"{preface} you need flash_attn package version to be greater or equal than 2.1.0. Detected version {flash_attention_version}. {install_message}\"\n                        )\n                    elif not torch.cuda.is_available():\n                        raise ValueError(\n                            f\"{preface} Flash Attention 2 is not available on CPU. Please make sure torch can access a CUDA device.\"\n                        )\n                    else:\n                        raise ImportError(f\"{preface} Flash Attention 2 is not available. {install_message}\")\n                elif torch.version.hip:\n                    if flash_attention_version < version.parse(\"2.0.4\"):\n                        raise ImportError(\n                            f\"{preface} you need flash_attn package version to be greater or equal than 2.0.4. Detected version {flash_attention_version}. {install_message}\"\n                        )\n                    else:\n                        raise ImportError(f\"{preface} Flash Attention 2 is not available. {install_message}\")\n\n        if dtype is None:\n            logger.warning_once(\n                \"You are attempting to use Flash Attention 2 without specifying a torch dtype. This might lead to unexpected behaviour\"\n            )\n        elif dtype is not None and dtype not in [torch.float16, torch.bfloat16]:\n            logger.warning_once(\n                \"Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but\"\n                f\" the current dype in {self.__class__.__name__} is {dtype}. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator,\"\n                ' or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", dtype=torch.float16)`'\n            )\n\n        # With the early check, the parameters are not yet initialized correctly\n        if not is_init_check:\n            param_devices = list({param.device for param in self.parameters()})\n            if len(param_devices) == 1 and param_devices[0].type == \"cpu\":\n                if torch.cuda.is_available():\n                    logger.warning_once(\n                        \"You are attempting to use Flash Attention 2 with a model not initialized on GPU. Make sure to move the model to GPU\"\n                        \" after initializing it on CPU with `model.to('cuda')`.\"\n                    )\n                elif is_torch_mlu_available():\n                    logger.warning_once(\n                        \"You are attempting to use Flash Attention 2 with a model not initialized on MLU. Make sure to move the model to MLU\"\n                        \" after initializing it on CPU with `model.to('mlu')`.\"\n                    )\n                else:\n                    raise ValueError(\n                        \"You are attempting to use Flash Attention 2 with a model not initialized on GPU and with no GPU available. \"\n                        \"This is not supported yet. Please make sure to have access to a GPU and either initialise the model on a GPU by passing a device_map \"\n                        \"or initialising the model on CPU and then moving it to GPU.\"\n                    )\n\n        # If no error raise by this point, we can return `True`\n        return True\n\n    def _flash_attn_3_can_dispatch(self, is_init_check: bool = False) -> bool:\n        \"\"\"\n        Check the availability of Flash Attention 3 for a given model.\n\n        Args:\n            is_init_check (`bool`, *optional*):\n                Whether this check is performed early, i.e. at __init__ time, or later when the model and its weights are\n                fully instantiated. This is needed as we also check the devices of the weights, which are only available\n                later after __init__. This allows to raise proper exceptions early before instantiating the full models\n                if we know that the model does not support the requested attention.\n        \"\"\"\n        dtype = self.config.dtype\n\n        if not self._supports_flash_attn:\n            raise ValueError(\n                f\"{self.__class__.__name__} does not support Flash Attention 3 yet. Please request to add support where\"\n                f\" the model is hosted, on its model hub page: https://huggingface.co/{self.config._name_or_path}/discussions/new\"\n                \" or in the Transformers GitHub repo: https://github.com/huggingface/transformers/issues/new\"\n            )\n\n        if not is_flash_attn_3_available():\n            preface = \"FlashAttention3 has been toggled on, but it cannot be used due to the following error:\"\n\n            if importlib.util.find_spec(\"flash_attn_3\") is None:\n                raise ImportError(f\"{preface} the package flash_attn_3 seems to be not installed.\")\n\n            if torch.cuda.is_available():\n                major, _ = torch.cuda.get_device_capability()\n                if major < 9:\n                    raise ValueError(\n                        f\"{preface} Flash Attention 3 requires compute capability >= 9.0, but found {torch.cuda.get_device_capability()} with compute capability {major}.0.\"\n                    )\n                else:\n                    raise ImportError(f\"{preface} Flash Attention 3 is not available.\")\n            else:\n                raise ValueError(\n                    f\"{preface} Flash Attention 3 is not available on CPU. Please make sure torch can access a CUDA device.\"\n                )\n\n        if dtype is None:\n            logger.warning_once(\n                \"You are attempting to use Flash Attention 3 without specifying a torch dtype. This might lead to unexpected behaviour\"\n            )\n        elif dtype is not None and dtype not in [torch.float16, torch.bfloat16]:\n            logger.warning_once(\n                \"Flash Attention 3 only supports torch.float16 and torch.bfloat16 dtypes, but\"\n                f\" the current dype in {self.__class__.__name__} is {dtype}. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator,\"\n                ' or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained(\"meta-llama/Llama-3.2-1B\", attn_implementation=\"flash_attention_3\", dtype=torch.float16)`'\n            )\n\n        if getattr(self.config, \"alibi\", False) or getattr(self.config, \"use_alibi\", False):\n            raise ValueError(\"Model is configured to use ALiBi, which is not supported by Flash Attention 3.\")\n\n        # Check for attention dropout, which is incompatible with FA3\n        if hasattr(self.config, \"attention_dropout\") and self.config.attention_dropout > 0:\n            raise ValueError(\n                f\"Model has attention_dropout={self.config.attention_dropout}, which is not supported by Flash Attention 3.\"\n            )\n\n        # With the early check, the parameters are not yet initialized correctly\n        if not is_init_check:\n            param_devices = list({param.device for param in self.parameters()})\n            if len(param_devices) == 1 and param_devices[0].type == \"cpu\":\n                if torch.cuda.is_available():\n                    logger.warning_once(\n                        \"You are attempting to use Flash Attention 3 with a model not initialized on GPU. Make sure to move the model to GPU\"\n                        \" after initializing it on CPU with `model.to('cuda')`.\"\n                    )\n                else:\n                    raise ValueError(\n                        \"You are attempting to use Flash Attention 3 with a model not initialized on GPU and with no GPU available. \"\n                        \"This is not supported yet. Please make sure to have access to a GPU and either initialise the model on a GPU by passing a device_map \"\n                        \"or initialising the model on CPU and then moving it to GPU.\"\n                    )\n\n        return True\n\n    def _sdpa_can_dispatch(self, is_init_check: bool = False) -> bool:\n        \"\"\"\n        Check the availability of SDPA for a given model.\n\n        Args:\n            is_init_check (`bool`, *optional*):\n                Whether this check is performed early, i.e. at __init__ time, or later when the model and its weights are\n                fully instantiated. This is needed as we also check the devices of the weights, which are only available\n                later after __init__. This allows to raise proper exceptions early before instantiating the full models\n                if we know that the model does not support the requested attention.\n        \"\"\"\n        if not self._supports_sdpa:\n            raise ValueError(\n                f\"{self.__class__.__name__} does not support an attention implementation through torch.nn.functional.scaled_dot_product_attention yet.\"\n                \" Please request the support for this architecture: https://github.com/huggingface/transformers/issues/28005. If you believe\"\n                ' this error is a bug, please open an issue in Transformers GitHub repository and load your model with the argument `attn_implementation=\"eager\"` meanwhile. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"eager\")`'\n            )\n\n        if (\n            torch.version.hip is not None\n            and torch.cuda.device_count() > 1\n            and version.parse(torch.__version__) < version.parse(\"2.4.1\")\n        ):\n            logger.warning_once(\n                \"Using the `SDPA` attention implementation on multi-gpu setup with ROCM may lead to performance issues due to the FA backend. Disabling it to use alternative backends.\"\n            )\n            torch.backends.cuda.enable_flash_sdp(False)\n\n        return True\n\n    def _flex_attn_can_dispatch(self, is_init_check: bool = False) -> bool:\n        \"\"\"\n        Check the availability of Flex Attention for a given model.\n\n        Args:\n            is_init_check (`bool`, *optional*):\n                Whether this check is performed early, i.e. at __init__ time, or later when the model and its weights are\n                fully instantiated. This is needed as we also check the devices of the weights, which are only available\n                later after __init__. This allows to raise proper exceptions early before instantiating the full models\n                if we know that the model does not support the requested attention.\n        \"\"\"\n        if not self._supports_flex_attn:\n            raise ValueError(\n                f\"{self.__class__.__name__} does not support an attention implementation through torch's flex_attention.\"\n                \" Please request the support for this architecture: https://github.com/huggingface/transformers/issues/34809.\"\n                \" If you believe this error is a bug, please open an issue in Transformers GitHub repository\"\n                ' and load your model with the argument `attn_implementation=\"eager\"` meanwhile.'\n                ' Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"eager\")`'\n            )\n        if not is_torch_flex_attn_available():\n            raise ImportError(\n                \"PyTorch Flex Attention requirements in Transformers are not met. Please install torch>=2.5.0.\"\n            )\n\n        # If no error raise by this point, we can return `True`\n        return True\n\n    def _check_and_adjust_attn_implementation(\n        self, attn_implementation: Optional[str], is_init_check: bool = False\n    ) -> str:\n        \"\"\"\n        Check that the `attn_implementation` exists and is supported by the models, and try to get the kernel from hub if\n        it matches hf kernels pattern.\n\n        Args:\n            attn_implementation (`str` or `None`):\n                The attention implementation to check for existence/validity.\n            is_init_check (`bool`, *optional*):\n                Whether this check is performed early, i.e. at __init__ time, or later when the model and its weights are\n                fully instantiated. This is needed as we also check the devices of the weights, which are only available\n                later after __init__. This allows to raise proper exceptions early before instantiating the full models\n                if we know that the model does not support the requested attention.\n\n        Returns:\n            `str`: The final attention implementation to use, including potential fallbacks from sdpa to eager, or from\n            None to sdpa (to potentially eager).\n        \"\"\"\n        applicable_attn_implementation = attn_implementation\n\n        # If FA not installed, do not fail but use kernels instead\n        if (\n            attn_implementation is not None\n            and \"flash\" in attn_implementation\n            and self._supports_flash_attn\n            and not (is_flash_attn_2_available() or is_flash_attn_3_available())\n            and is_kernels_available()\n            and not is_torch_npu_available()\n        ):\n            if attn_implementation.endswith(\"2\"):\n                applicable_attn_implementation = \"kernels-community/flash-attn\"\n            else:\n                applicable_attn_implementation = \"kernels-community/vllm-flash-attn3\"\n\n        if is_kernel(applicable_attn_implementation):\n            try:\n                load_and_register_attn_kernel(applicable_attn_implementation)\n                # log that we used kernel fallback if successful\n                if \"flash_\" in attn_implementation:\n                    logger.warning_once(\n                        f\"You do not have `flash_attn` installed, using `{applicable_attn_implementation}` \"\n                        \"from the `kernels` library instead!\"\n                    )\n            except Exception as e:\n                # raise the proper exception for requested flash attention\n                if attn_implementation.startswith(\"flash_\"):\n                    if attn_implementation.endswith(\"2\"):\n                        self._flash_attn_2_can_dispatch()\n                    else:\n                        self._flash_attn_3_can_dispatch()\n\n                # error properly out if a kernel was specifically requested\n                raise e\n        else:\n            applicable_attn_implementation = self.get_correct_attn_implementation(\n                applicable_attn_implementation, is_init_check\n            )\n            # preload flash attention here to allow compile with fullgraph\n            if applicable_attn_implementation.startswith(\"flash_\"):\n                lazy_import_flash_attention(applicable_attn_implementation, force_import=True)\n        return applicable_attn_implementation\n\n    def get_correct_attn_implementation(self, requested_attention: Optional[str], is_init_check: bool = False) -> str:\n        applicable_attention = \"sdpa\" if requested_attention is None else requested_attention\n        if applicable_attention not in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys():\n            message = (\n                f'Specified `attn_implementation=\"{applicable_attention}\"` is not supported. The only possible arguments are '\n                '`attn_implementation=\"eager\"`, `\"paged|eager\"`'\n            )\n            # check `supports_flash_attn_2` for BC with custom code. TODO: remove after a few releases\n            if self._supports_flash_attn or getattr(self, \"_supports_flash_attn_2\", False):\n                message += ', `\"attn_implementation=flash_attention_3\"`, `\"attn_implementation=flash_attention_2\"`, `\"attn_implementation=paged|flash_attention_2\"`'\n            if self._supports_sdpa:\n                message += ', `\"attn_implementation=sdpa\"`, `\"attn_implementation=paged|spda\"`'\n            if self._supports_flex_attn:\n                message += ', `\"attn_implementation=flex_attention\"`'\n            raise ValueError(message + \".\")\n\n        # Perform relevant checks\n        if \"flash_attention_2\" in applicable_attention:\n            self._flash_attn_2_can_dispatch(is_init_check)\n        elif \"flash_attention_3\" in applicable_attention:\n            self._flash_attn_3_can_dispatch(is_init_check)\n        elif \"flex_attention\" in applicable_attention:\n            self._flex_attn_can_dispatch(is_init_check)\n        elif \"sdpa\" in applicable_attention:\n            # Sdpa is the default, so we try it and fallback to eager otherwise when not possible\n            try:\n                self._sdpa_can_dispatch(is_init_check)\n            except (ValueError, ImportError) as e:\n                if requested_attention is not None and \"sdpa\" in requested_attention:\n                    raise e\n                applicable_attention = \"eager\"\n\n        return applicable_attention\n\n    @classmethod\n    def _can_set_attn_implementation(cls) -> bool:\n        \"\"\"Detect whether the class supports setting its attention implementation dynamically. It is an ugly check based on\n        opening the file, but avoids maintaining yet another property flag.\n        \"\"\"\n        class_file = sys.modules[cls.__module__].__file__\n        with open(class_file, \"r\") as f:\n            code = f.read()\n        # heuristic -> if we find those patterns, the model uses the correct interface\n        if re.search(r\"class \\w+Attention\\(nn.Module\\)\", code):\n            return (\n                \"eager_attention_forward\" in code\n                and \"ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\" in code\n            )\n        else:\n            # If no attention layer, assume `True`. Most probably a multimodal model or inherits from existing models\n            return True\n\n    def set_attn_implementation(self, attn_implementation: Union[str, dict]):\n        \"\"\"\n        Set the requested `attn_implementation` for this model.\n\n        Args:\n            attn_implementation (`str` or `dict`):\n                The attention implementation to set for this model. It can be either a `str`, in which case it will be\n                dispatched to all submodels if relevant, or a `dict` where keys are the sub_configs name, in which case each\n                submodel will dispatch the corresponding value.\n        \"\"\"\n        requested_implementation = (\n            attn_implementation\n            if not isinstance(attn_implementation, dict)\n            else attn_implementation.get(\"\", self.config._attn_implementation)\n        )\n\n        if requested_implementation != self.config._attn_implementation:\n            # In this case, raise\n            if not self._can_set_attn_implementation():\n                logger.warning(\n                    f\"{self.__class__.__name__} does not support setting its attention implementation dynamically, because it \"\n                    \"does not follow the functional approach based on AttentionInterface \"\n                    \"(see https://huggingface.co/docs/transformers/en/attention_interface)\"\n                )\n            else:\n                requested_implementation = self._check_and_adjust_attn_implementation(\n                    requested_implementation, is_init_check=False\n                )\n                # Apply the change (on the internal attr, to avoid setting it recursively)\n                self.config._attn_implementation_internal = requested_implementation\n\n        # Apply it to all submodels as well\n        for submodule in self.modules():\n            # We found a submodel (which is not self) with a different config (otherwise, it may be the same \"actual model\",\n            # e.g. ForCausalLM has a Model inside, but no need to check it again)\n            if (\n                submodule is not self\n                and isinstance(submodule, PreTrainedModel)\n                and submodule.config.__class__ != self.config.__class__\n                # If it was already changed, no need to do it again\n                and not hasattr(submodule.config, \"_attn_was_changed\")\n            ):\n                # In this case, warn and skip\n                if not submodule._can_set_attn_implementation():\n                    logger.warning(\n                        f\"{submodule.__class__.__name__} does not support setting its attention implementation dynamically, because it \"\n                        \"does not follow the functional approach based on AttentionInterface \"\n                        \"(see https://huggingface.co/docs/transformers/en/attention_interface)\"\n                    )\n                # Set the attn on the submodule\n                else:\n                    sub_implementation = requested_implementation\n                    if isinstance(attn_implementation, dict):\n                        for subconfig_key in self.config.sub_configs:\n                            # We need to check for exact object match here, with `is`\n                            if getattr(self.config, subconfig_key) is submodule.config:\n                                sub_implementation = attn_implementation.get(\n                                    subconfig_key, submodule.config._attn_implementation\n                                )\n                                break\n                    # Check the module can use correctly, otherwise we raise an error if requested attention can't be set for submodule\n                    sub_implementation = submodule.get_correct_attn_implementation(sub_implementation)\n                    submodule.config._attn_implementation_internal = sub_implementation\n\n                # Still add it as \"changed\" even if it was skipped, as we would otherwise try to set it in the dark afterwards\n                # We need to set it on the config itself, to differentiate 2 subconfigs of the same __class__ potentially\n                submodule.config._attn_was_changed = True\n\n        # We need this as some old and badly designed models use subconfigs without declaring the corresponding modules as PreTrainedModel\n        for subconfig_key in self.config.sub_configs:\n            if (subconfig := getattr(self.config, subconfig_key)) is not None:\n                sub_implementation = (\n                    requested_implementation\n                    if not isinstance(attn_implementation, dict)\n                    else attn_implementation.get(subconfig_key, subconfig._attn_implementation)\n                )\n                # This means we did not perform any check above for this particular subconfig -> set it in the dark if it is registered\n                if (\n                    not hasattr(subconfig, \"_attn_was_changed\")\n                    # If it's already the same, then no need to enter here and raise warnings\n                    and sub_implementation != subconfig._attn_implementation\n                ):\n                    if sub_implementation not in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys():\n                        raise ValueError(\n                            f'Specified `attn_implementation=\"{sub_implementation}\"` is not supported for {subconfig_key}. '\n                            'The only possible arguments are \"eager\" (manual attention implementation)'\n                            f\"or one of the following: {list(ALL_ATTENTION_FUNCTIONS.valid_keys())}\"\n                        )\n                    subconfig._attn_implementation_internal = sub_implementation\n                    logger.warning(\n                        f\"We set the attention implementation for the sub-config `{subconfig_key}` to `{sub_implementation}` \"\n                        \"without finding the associated sub-model. For this reason we could not check if the model supports it. \"\n                        \"You may encounter undefined behavior.\"\n                    )\n                # Unset the attribute in this case, to avoid issues in the future\n                else:\n                    if hasattr(subconfig, \"_attn_was_changed\"):\n                        del subconfig._attn_was_changed\n\n    def enable_input_require_grads(self):\n        \"\"\"\n        Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while keeping\n        the model weights fixed.\n        \"\"\"\n\n        def make_inputs_require_grads(module, input, output):\n            output.requires_grad_(True)\n\n        self._require_grads_hook = self.get_input_embeddings().register_forward_hook(make_inputs_require_grads)\n\n    def disable_input_require_grads(self):\n        \"\"\"\n        Removes the `_require_grads_hook`.\n        \"\"\"\n        self._require_grads_hook.remove()\n\n    def get_decoder(self):\n        \"\"\"\n        Best-effort lookup of the *decoder* module.\n\n        Order of attempts (covers ~85 % of current usages):\n\n        1. `self.decoder`\n        2. `self.model`                       (many wrappers store the decoder here)\n        3. `self.model.get_decoder()`         (nested wrappers)\n        4. fallback: raise for the few exotic models that need a bespoke rule\n        \"\"\"\n        if hasattr(self, \"decoder\"):\n            return self.decoder\n\n        if hasattr(self, \"model\"):\n            inner = self.model\n            # See: https://github.com/huggingface/transformers/issues/40815\n            if hasattr(inner, \"get_decoder\") and type(inner) is not type(self):\n                return inner.get_decoder()\n            return inner\n\n        # If this is a base transformer model (no decoder/model attributes), return self\n        # This handles cases like MistralModel which is itself the decoder\n        return self\n\n    def set_decoder(self, decoder):\n        \"\"\"\n        Symmetric setter. Mirrors the lookup logic used in `get_decoder`.\n        \"\"\"\n\n        if hasattr(self, \"decoder\"):\n            self.decoder = decoder\n            return\n\n        if hasattr(self, \"model\"):\n            inner = self.model\n            if hasattr(inner, \"set_decoder\"):\n                inner.set_decoder(decoder)\n            else:\n                self.model = decoder\n            return\n\n        return\n\n    def _init_weights(self, module):\n        \"\"\"\n        Initialize the weights. This is quite general on purpose, in the spirit of what we usually do. For more complex\n        initialization scheme, it should be overridden by the derived `PreTrainedModel` class. In case a model adds an explicit\n        `nn.Parameter`, this method should also be overridden in order to initialize it correctly.\n        \"\"\"\n        if hasattr(self.config, \"initializer_range\"):\n            std = self.config.initializer_range\n        else:\n            # 0.02 is the standard default value across the library\n            std = getattr(self.config.get_text_config(), \"initializer_range\", 0.02)\n\n        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose1d, nn.ConvTranspose2d)):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.MultiheadAttention):\n            # This uses torch's original init\n            module._reset_parameters()\n        # We cannot use `isinstance` on the RMSNorms or LayerNorms, as they usually are custom modules which change names\n        # between modelings (because they are prefixed with the model name)\n        elif (\n            isinstance(module, (nn.GroupNorm, nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d))\n            or \"LayerNorm\" in module.__class__.__name__\n            or \"RMSNorm\" in module.__class__.__name__\n        ):\n            # Norms can exist without weights (in which case they are None from torch primitives)\n            if hasattr(module, \"weight\") and module.weight is not None:\n                module.weight.data.fill_(1.0)\n            if hasattr(module, \"bias\") and module.bias is not None:\n                module.bias.data.zero_()\n\n    def _initialize_weights(self, module):\n        \"\"\"\n        Initialize the weights if they are not already initialized.\n        \"\"\"\n        if getattr(module, \"_is_hf_initialized\", False):\n            return\n        self._init_weights(module)\n        module._is_hf_initialized = True\n\n    @torch.no_grad()\n    def initialize_weights(self):\n        \"\"\"\n        This is equivalent to calling `self.apply(self._initialize_weights)`, but correctly handles composite models.\n        This function dynamically dispatches the correct `init_weights` function to the modules as we advance in the\n        module graph along the recursion. It can handle an arbitrary number of sub-models. Without it, every composite\n        model would have to recurse a second time on all sub-models explicitly in the outer-most `_init_weights`, which\n        is extremely error prone and inefficient.\n\n        Note that the `torch.no_grad()` decorator is very important as well, as most of our `_init_weights` do not use\n        `torch.nn.init` functions (which are all no_grad by default), but simply do in-place ops such as\n        `module.weight.data.zero_()`.\n        \"\"\"\n        if not hasattr(torch.nn.Module, \"smart_apply\"):\n            # This function is equivalent to `torch.nn.Module.apply`, except that it dynamically adjust the function\n            # to apply as we go down the graph\n            def smart_apply(self, fn):\n                for module in self.children():\n                    # We found a sub-model: recursively dispatch its own init function now!\n                    if isinstance(module, PreTrainedModel):\n                        module.smart_apply(module._initialize_weights)\n                    else:\n                        module.smart_apply(fn)\n                fn(self)\n                return self\n\n            torch.nn.Module.smart_apply = smart_apply\n\n        # Let the magic happen with this simple call\n        self.smart_apply(self._initialize_weights)\n\n    def tie_embeddings_and_encoder_decoder(self):\n        \"\"\"\n        If set in the config, tie the weights between the input embeddings and the output embeddings,\n        and the encoder and decoder.\n        \"\"\"\n        if getattr(self.config.get_text_config(decoder=True), \"tie_word_embeddings\", True):\n            output_embeddings = self.get_output_embeddings()\n            if output_embeddings is not None:\n                self._tie_embedding_weights(output_embeddings, self.get_input_embeddings())\n\n        if getattr(self.config, \"is_encoder_decoder\", False) and getattr(self.config, \"tie_encoder_decoder\", False):\n            if hasattr(self, self.base_model_prefix):\n                self = getattr(self, self.base_model_prefix)\n            tied_weights = self._tie_encoder_decoder_weights(\n                self.encoder, self.decoder, self.base_model_prefix, \"encoder\"\n            )\n            # Setting a dynamic variable instead of `_tied_weights_keys` because it's a class\n            # attributed not an instance member, therefore modifying it will modify the entire class\n            # Leading to issues on subsequent calls by different tests or subsequent calls.\n            self._dynamic_tied_weights_keys = tied_weights\n\n    def tie_weights(self):\n        \"\"\"\n        Recursively (for all submodels) tie all the weights of the model.\n        \"\"\"\n        # Note that `self` is included in `self.modules` so we also apply to current PreTrainedModel with this call\n        for module in self.modules():\n            # If it's a PreTrainedModel, may need to tie the embeddings and/or encoder/decoder weights\n            if isinstance(module, PreTrainedModel):\n                module.tie_embeddings_and_encoder_decoder()\n            # Additionally, if it has a custom `_tie_weights`, honor it\n            if hasattr(module, \"_tie_weights\"):\n                module._tie_weights()\n\n    @staticmethod\n    def _tie_encoder_decoder_weights(\n        encoder: nn.Module, decoder: nn.Module, base_model_prefix: str, base_encoder_name: str\n    ):\n        uninitialized_encoder_weights: list[str] = []\n        tied_weights: list[str] = []\n        if decoder.__class__ != encoder.__class__:\n            logger.info(\n                f\"{decoder.__class__} and {encoder.__class__} are not equal. In this case make sure that all encoder\"\n                \" weights are correctly initialized.\"\n            )\n\n        def tie_encoder_to_decoder_recursively(\n            decoder_pointer: nn.Module,\n            encoder_pointer: nn.Module,\n            module_name: str,\n            base_encoder_name: str,\n            uninitialized_encoder_weights: list[str],\n            depth=0,\n            total_decoder_name=\"\",\n            total_encoder_name=\"\",\n        ):\n            assert isinstance(decoder_pointer, nn.Module) and isinstance(encoder_pointer, nn.Module), (\n                f\"{decoder_pointer} and {encoder_pointer} have to be of type nn.Module\"\n            )\n            if hasattr(decoder_pointer, \"weight\"):\n                assert hasattr(encoder_pointer, \"weight\")\n                encoder_pointer.weight = decoder_pointer.weight\n                tied_weights.append(f\"{base_encoder_name}{total_encoder_name}.weight\")\n                if hasattr(decoder_pointer, \"bias\"):\n                    assert hasattr(encoder_pointer, \"bias\")\n                    tied_weights.append(f\"{base_encoder_name}{total_encoder_name}.bias\")\n                    encoder_pointer.bias = decoder_pointer.bias\n                return\n\n            encoder_modules = encoder_pointer._modules\n            decoder_modules = decoder_pointer._modules\n            if len(decoder_modules) > 0:\n                assert len(encoder_modules) > 0, (\n                    f\"Encoder module {encoder_pointer} does not match decoder module {decoder_pointer}\"\n                )\n\n                all_encoder_weights = {module_name + \"/\" + sub_name for sub_name in encoder_modules}\n                encoder_layer_pos = 0\n                for name in decoder_modules:\n                    if name.isdigit():\n                        encoder_name = str(int(name) + encoder_layer_pos)\n                        decoder_name = name\n                        if not isinstance(decoder_modules[decoder_name], type(encoder_modules[encoder_name])) and len(\n                            encoder_modules\n                        ) != len(decoder_modules):\n                            # this can happen if the name corresponds to the position in a list module list of layers\n                            # in this case the decoder has added a cross-attention that the encoder does not have\n                            # thus skip this step and subtract one layer pos from encoder\n                            encoder_layer_pos -= 1\n                            continue\n                    elif name not in encoder_modules:\n                        continue\n                    elif depth > 500:\n                        raise ValueError(\n                            \"Max depth of recursive function `tie_encoder_to_decoder` reached. It seems that there is\"\n                            \" a circular dependency between two or more `nn.Modules` of your model.\"\n                        )\n                    else:\n                        decoder_name = encoder_name = name\n                    tie_encoder_to_decoder_recursively(\n                        decoder_modules[decoder_name],\n                        encoder_modules[encoder_name],\n                        module_name + \"/\" + name,\n                        base_encoder_name,\n                        uninitialized_encoder_weights,\n                        depth=depth + 1,\n                        total_encoder_name=f\"{total_encoder_name}.{encoder_name}\",\n                        total_decoder_name=f\"{total_decoder_name}.{decoder_name}\",\n                    )\n                    all_encoder_weights.remove(module_name + \"/\" + encoder_name)\n\n                uninitialized_encoder_weights += list(all_encoder_weights)\n\n        # tie weights recursively\n        tie_encoder_to_decoder_recursively(\n            decoder, encoder, base_model_prefix, base_encoder_name, uninitialized_encoder_weights\n        )\n\n        if len(uninitialized_encoder_weights) > 0:\n            logger.warning(\n                f\"The following encoder weights were not tied to the decoder {uninitialized_encoder_weights}\"\n            )\n        return tied_weights\n\n    def _tie_embedding_weights(self, output_embeddings, input_embeddings):\n        \"\"\"Tie weights, and add hooks and flags if using TP.\"\"\"\n        output_embeddings.weight = input_embeddings.weight\n\n        # Passing hooks over to the embeddings if needed\n        # (currently limited to tensor parallel hooks and flags only)\n        if hasattr(input_embeddings, \"_is_hooked\") and getattr(input_embeddings, \"_hf_tp_plan\", None):\n            output_embeddings._is_hooked = input_embeddings._is_hooked\n            output_embeddings._hf_tp_plan = input_embeddings._hf_tp_plan\n            output_embeddings._forward_hooks = input_embeddings._forward_hooks\n            output_embeddings._forward_pre_hooks = input_embeddings._forward_pre_hooks\n            output_embeddings.__repr__ = (\n                lambda: f\"{output_embeddings.__repr__()}\\nTP Plan: {output_embeddings._hf_tp_plan}\"\n            )\n\n        if getattr(output_embeddings, \"bias\", None) is not None:\n            output_embeddings.bias.data = nn.functional.pad(\n                output_embeddings.bias.data,\n                (0, output_embeddings.weight.shape[0] - output_embeddings.bias.shape[0]),\n                \"constant\",\n                0,\n            )\n        if hasattr(output_embeddings, \"out_features\") and hasattr(input_embeddings, \"num_embeddings\"):\n            output_embeddings.out_features = input_embeddings.num_embeddings\n\n    def _get_no_split_modules(self, device_map: str):\n        \"\"\"\n        Get the modules of the model that should not be spit when using device_map. We iterate through the modules to\n        get the underlying `_no_split_modules`.\n\n        Args:\n            device_map (`str`):\n                The device map value. Options are [\"auto\", \"balanced\", \"balanced_low_0\", \"sequential\"]\n\n        Returns:\n            `list[str]`: List of modules that should not be split\n        \"\"\"\n        _no_split_modules = set()\n        modules_to_check = [self]\n        while len(modules_to_check) > 0:\n            module = modules_to_check.pop(-1)\n            # if the module does not appear in _no_split_modules, we also check the children\n            if module.__class__.__name__ not in _no_split_modules:\n                if isinstance(module, PreTrainedModel):\n                    if module._no_split_modules is None:\n                        raise ValueError(\n                            f\"{module.__class__.__name__} does not support `device_map='{device_map}'`. To implement support, the model \"\n                            \"class needs to implement the `_no_split_modules` attribute.\"\n                        )\n                    else:\n                        _no_split_modules = _no_split_modules | set(module._no_split_modules)\n                modules_to_check += list(module.children())\n        return list(_no_split_modules)\n\n    def resize_token_embeddings(\n        self,\n        new_num_tokens: Optional[int] = None,\n        pad_to_multiple_of: Optional[int] = None,\n        mean_resizing: bool = True,\n    ) -> nn.Embedding:\n        \"\"\"\n        Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\n\n        Takes care of tying weights embeddings afterwards if the model class has a `tie_weights()` method.\n\n        Arguments:\n            new_num_tokens (`int`, *optional*):\n                The new number of tokens in the embedding matrix. Increasing the size will add newly initialized\n                vectors at the end. Reducing the size will remove vectors from the end. If not provided or `None`, just\n                returns a pointer to the input tokens `torch.nn.Embedding` module of the model without doing anything.\n            pad_to_multiple_of (`int`, *optional*):\n                If set will pad the embedding matrix to a multiple of the provided value.If `new_num_tokens` is set to\n                `None` will just pad the embedding to a multiple of `pad_to_multiple_of`.\n\n                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n                `>= 7.5` (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128. For more\n                details about this, or help on choosing the correct value for resizing, refer to this guide:\n                https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n            mean_resizing (`bool`):\n                Whether to initialize the added embeddings from a multivariate normal distribution that has old embeddings' mean and\n                covariance or to initialize them with a normal distribution that has a mean of zero and std equals `config.initializer_range`.\n\n                Setting `mean_resizing` to `True` is useful when increasing the size of the embeddings of causal language models,\n                where the generated tokens' probabilities won't be affected by the added embeddings because initializing the new embeddings with the\n                old embeddings' mean will reduce the kl-divergence between the next token probability before and after adding the new embeddings.\n                Refer to this article for more information: https://nlp.stanford.edu/~johnhew/vocab-expansion.html\n\n        Return:\n            `torch.nn.Embedding`: Pointer to the input tokens Embeddings Module of the model.\n        \"\"\"\n        model_embeds = self._resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n        if new_num_tokens is None and pad_to_multiple_of is None:\n            return model_embeds\n\n        # Since we are basically reusing the same old embeddings with new weight values, gathering is required\n        is_quantized = hasattr(self, \"hf_quantizer\") and self.hf_quantizer is not None\n        if is_deepspeed_zero3_enabled() and not is_quantized:\n            import deepspeed\n\n            with deepspeed.zero.GatheredParameters(model_embeds.weight, modifier_rank=None):\n                vocab_size = model_embeds.weight.shape[0]\n        else:\n            vocab_size = model_embeds.weight.shape[0]\n\n        # Update base model and current model config.\n        self.config.get_text_config().vocab_size = vocab_size\n        self.vocab_size = vocab_size\n\n        # Tie weights again if needed\n        self.tie_weights()\n\n        return model_embeds\n\n    def _resize_token_embeddings(self, new_num_tokens, pad_to_multiple_of=None, mean_resizing=True):\n        old_embeddings = self.get_input_embeddings()\n        new_embeddings = self._get_resized_embeddings(\n            old_embeddings, new_num_tokens, pad_to_multiple_of, mean_resizing\n        )\n        if hasattr(old_embeddings, \"_hf_hook\"):\n            hook = old_embeddings._hf_hook\n            add_hook_to_module(new_embeddings, hook)\n        old_embeddings_requires_grad = old_embeddings.weight.requires_grad\n        new_embeddings.requires_grad_(old_embeddings_requires_grad)\n        self.set_input_embeddings(new_embeddings)\n        is_quantized = hasattr(self, \"hf_quantizer\") and self.hf_quantizer is not None\n\n        # Update new_num_tokens with the actual size of new_embeddings\n        if pad_to_multiple_of is not None:\n            if is_deepspeed_zero3_enabled() and not is_quantized:\n                import deepspeed\n\n                with deepspeed.zero.GatheredParameters(new_embeddings.weight, modifier_rank=None):\n                    new_num_tokens = new_embeddings.weight.shape[0]\n            else:\n                new_num_tokens = new_embeddings.weight.shape[0]\n\n        # if word embeddings are not tied, make sure that lm head is resized as well\n        if (\n            self.get_output_embeddings() is not None\n            and not self.config.get_text_config(decoder=True).tie_word_embeddings\n        ):\n            old_lm_head = self.get_output_embeddings()\n            if isinstance(old_lm_head, torch.nn.Embedding):\n                new_lm_head = self._get_resized_embeddings(old_lm_head, new_num_tokens, mean_resizing=mean_resizing)\n            else:\n                new_lm_head = self._get_resized_lm_head(old_lm_head, new_num_tokens, mean_resizing=mean_resizing)\n            if hasattr(old_lm_head, \"_hf_hook\"):\n                hook = old_lm_head._hf_hook\n                add_hook_to_module(new_lm_head, hook)\n            old_lm_head_requires_grad = old_lm_head.weight.requires_grad\n            new_lm_head.requires_grad_(old_lm_head_requires_grad)\n            self.set_output_embeddings(new_lm_head)\n\n        return self.get_input_embeddings()\n\n    def _get_resized_embeddings(\n        self,\n        old_embeddings: nn.Embedding,\n        new_num_tokens: Optional[int] = None,\n        pad_to_multiple_of: Optional[int] = None,\n        mean_resizing: bool = True,\n    ) -> nn.Embedding:\n        \"\"\"\n        Build a resized Embedding Module from a provided token Embedding Module. Increasing the size will add newly\n        initialized vectors at the end. Reducing the size will remove vectors from the end\n\n        Args:\n            old_embeddings (`torch.nn.Embedding`):\n                Old embeddings to be resized.\n            new_num_tokens (`int`, *optional*):\n                New number of tokens in the embedding matrix.\n\n                Increasing the size will add newly initialized vectors at the end. Reducing the size will remove\n                vectors from the end. If not provided or `None`, just returns a pointer to the input tokens\n                `torch.nn.Embedding` module of the model without doing anything.\n            pad_to_multiple_of (`int`, *optional*):\n                If set will pad the embedding matrix to a multiple of the provided value. If `new_num_tokens` is set to\n                `None` will just pad the embedding to a multiple of `pad_to_multiple_of`.\n\n                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n                `>= 7.5` (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128. For more\n                details about this, or help on choosing the correct value for resizing, refer to this guide:\n                https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n            mean_resizing (`bool`):\n                Whether to initialize the added embeddings from a multivariate normal distribution that has old embeddings' mean and\n                covariance or to initialize them with a normal distribution that has a mean of zero and std equals `config.initializer_range`.\n\n                Setting `mean_resizing` to `True` is useful when increasing the size of the embeddings of causal language models,\n                where the generated tokens' probabilities will not be affected by the added embeddings because initializing the new embeddings with the\n                old embeddings' mean will reduce the kl-divergence between the next token probability before and after adding the new embeddings.\n                Refer to this article for more information: https://nlp.stanford.edu/~johnhew/vocab-expansion.html\n\n\n        Return:\n            `torch.nn.Embedding`: Pointer to the resized Embedding Module or the old Embedding Module if\n            `new_num_tokens` is `None`\n        \"\"\"\n\n        if pad_to_multiple_of is not None:\n            if not isinstance(pad_to_multiple_of, int):\n                raise ValueError(\n                    f\"Asking to pad the embedding matrix to a multiple of `{pad_to_multiple_of}`, which is not and integer. Please make sure to pass an integer\"\n                )\n            if new_num_tokens is None:\n                new_num_tokens = old_embeddings.weight.shape[0]\n            new_num_tokens = ((new_num_tokens + pad_to_multiple_of - 1) // pad_to_multiple_of) * pad_to_multiple_of\n        else:\n            logger.info(\n                \"You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding\"\n                f\" dimension will be {new_num_tokens}. This might induce some performance reduction as *Tensor Cores* will not be available.\"\n                \" For more details about this, or help on choosing the correct value for resizing, refer to this guide:\"\n                \" https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\"\n            )\n\n        if new_num_tokens is None:\n            return old_embeddings\n\n        is_quantized = hasattr(self, \"hf_quantizer\") and self.hf_quantizer is not None\n        if is_deepspeed_zero3_enabled() and not is_quantized:\n            import deepspeed\n\n            with deepspeed.zero.GatheredParameters(old_embeddings.weight, modifier_rank=None):\n                old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n        else:\n            old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n\n        if old_num_tokens == new_num_tokens and not is_deepspeed_zero3_enabled():\n            return old_embeddings\n\n        if not isinstance(old_embeddings, nn.Embedding):\n            raise TypeError(\n                f\"Old embeddings are of type {type(old_embeddings)}, which is not an instance of {nn.Embedding}. You\"\n                \" should either use a different resize function or make sure that `old_embeddings` are an instance of\"\n                f\" {nn.Embedding}.\"\n            )\n\n        # Build new embeddings\n\n        # When using DeepSpeed ZeRO-3, we shouldn't create new embeddings with DeepSpeed init\n        # because the shape of the new embedding layer is used across various modeling files\n        # as well as to update config vocab size. Shape will be 0 when using DeepSpeed init leading\n        # to errors when training.\n        new_embeddings = nn.Embedding(\n            new_num_tokens,\n            old_embedding_dim,\n            device=old_embeddings.weight.device,\n            dtype=old_embeddings.weight.dtype,\n        )\n\n        if new_num_tokens > old_num_tokens and not mean_resizing:\n            # initialize new embeddings (in particular added tokens) with a mean of 0 and std equals `config.initializer_range`.\n            self._init_weights(new_embeddings)\n\n        elif new_num_tokens > old_num_tokens and mean_resizing:\n            # initialize new embeddings  (in particular added tokens). The new embeddings will be initialized\n            # from a multivariate normal distribution that has old embeddings' mean and covariance.\n            # as described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html\n            logger.warning_once(\n                \"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. \"\n                \"As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. \"\n                \"To disable this, use `mean_resizing=False`\"\n            )\n\n            added_num_tokens = new_num_tokens - old_num_tokens\n            if is_deepspeed_zero3_enabled() and not is_quantized:\n                import deepspeed\n\n                with deepspeed.zero.GatheredParameters([old_embeddings.weight], modifier_rank=None):\n                    self._init_added_embeddings_weights_with_mean(\n                        old_embeddings, new_embeddings, old_num_tokens, added_num_tokens\n                    )\n            else:\n                self._init_added_embeddings_weights_with_mean(\n                    old_embeddings, new_embeddings, old_num_tokens, added_num_tokens\n                )\n\n        # Copy token embeddings from the previous weights\n\n        # numbers of tokens to copy\n        n = min(old_num_tokens, new_num_tokens)\n\n        if is_deepspeed_zero3_enabled() and not is_quantized:\n            import deepspeed\n\n            params = [old_embeddings.weight, new_embeddings.weight]\n            with deepspeed.zero.GatheredParameters(params, modifier_rank=0):\n                new_embeddings.weight.data[:n, :] = old_embeddings.weight.data[:n, :]\n        else:\n            new_embeddings.weight.data[:n, :] = old_embeddings.weight.data[:n, :]\n\n        # Replace weights in old_embeddings and return to maintain the same embedding type.\n        # This ensures correct functionality when a Custom Embedding class is passed as input.\n        # The input and output embedding types remain consistent. (c.f. https://github.com/huggingface/transformers/pull/31979)\n        if is_deepspeed_zero3_enabled() and not is_quantized:\n            import deepspeed\n\n            params = [old_embeddings.weight, new_embeddings.weight]\n            with deepspeed.zero.GatheredParameters(params, modifier_rank=0):\n                old_embeddings.weight = new_embeddings.weight\n                old_embeddings.num_embeddings = new_embeddings.weight.data.shape[0]\n\n                # If the new number of tokens is smaller than the original `padding_idx`, the `padding_idx`\n                # will be set to `None` in the resized embeddings.\n                if old_embeddings.padding_idx is not None and (new_num_tokens - 1) < old_embeddings.padding_idx:\n                    old_embeddings.padding_idx = None\n        else:\n            old_embeddings.weight.data = new_embeddings.weight.data\n            old_embeddings.num_embeddings = new_embeddings.weight.data.shape[0]\n            if old_embeddings.padding_idx is not None and (new_num_tokens - 1) < old_embeddings.padding_idx:\n                old_embeddings.padding_idx = None\n\n        return old_embeddings\n\n    def _get_resized_lm_head(\n        self,\n        old_lm_head: nn.Linear,\n        new_num_tokens: Optional[int] = None,\n        transposed: bool = False,\n        mean_resizing: bool = True,\n    ) -> nn.Linear:\n        \"\"\"\n        Build a resized Linear Module from a provided old Linear Module. Increasing the size will add newly initialized\n        vectors at the end. Reducing the size will remove vectors from the end\n\n        Args:\n            old_lm_head (`torch.nn.Linear`):\n                Old lm head liner layer to be resized.\n            new_num_tokens (`int`, *optional*):\n                New number of tokens in the linear matrix.\n\n                Increasing the size will add newly initialized vectors at the end. Reducing the size will remove\n                vectors from the end. If not provided or `None`, just returns a pointer to the input tokens\n                `torch.nn.Linear` module of the model without doing anything. transposed (`bool`, *optional*, defaults\n                to `False`): Whether `old_lm_head` is transposed or not. If True `old_lm_head.size()` is `lm_head_dim,\n                vocab_size` else `vocab_size, lm_head_dim`.\n            mean_resizing (`bool`):\n                Whether to initialize the added embeddings from a multivariate normal distribution that has old embeddings' mean and\n                covariance or to initialize them with a normal distribution that has a mean of zero and std equals `config.initializer_range`.\n\n                Setting `mean_resizing` to `True` is useful when increasing the size of the embeddings of causal language models,\n                where the generated tokens' probabilities will not be affected by the added embeddings because initializing the new embeddings with the\n                old embeddings' mean will reduce the kl-divergence between the next token probability before and after adding the new embeddings.\n                Refer to this article for more information: https://nlp.stanford.edu/~johnhew/vocab-expansion.html\n\n        Return:\n            `torch.nn.Linear`: Pointer to the resized Linear Module or the old Linear Module if `new_num_tokens` is\n            `None`\n        \"\"\"\n\n        if new_num_tokens is None:\n            return old_lm_head\n\n        is_quantized = hasattr(self, \"hf_quantizer\") and self.hf_quantizer is not None\n        if is_deepspeed_zero3_enabled() and not is_quantized:\n            import deepspeed\n\n            with deepspeed.zero.GatheredParameters(old_lm_head.weight, modifier_rank=None):\n                old_num_tokens, old_lm_head_dim = (\n                    old_lm_head.weight.size() if not transposed else old_lm_head.weight.t().size()\n                )\n        else:\n            old_num_tokens, old_lm_head_dim = (\n                old_lm_head.weight.size() if not transposed else old_lm_head.weight.t().size()\n            )\n\n        if old_num_tokens == new_num_tokens and not is_deepspeed_zero3_enabled():\n            return old_lm_head\n\n        if not isinstance(old_lm_head, nn.Linear):\n            raise TypeError(\n                f\"Old language model head is of type {type(old_lm_head)}, which is not an instance of {nn.Linear}. You\"\n                \" should either use a different resize function or make sure that `old_lm_head` are an instance of\"\n                f\" {nn.Linear}.\"\n            )\n\n        # Build new lm head\n        new_lm_head_shape = (old_lm_head_dim, new_num_tokens) if not transposed else (new_num_tokens, old_lm_head_dim)\n        has_new_lm_head_bias = old_lm_head.bias is not None\n\n        # When using DeepSpeed ZeRO-3, we shouldn't create new embeddings with DeepSpeed init\n        # because the shape of the new embedding layer is used across various modeling files\n        # as well as to update config vocab size. Shape will be 0 when using DeepSpeed init leading\n        # to errors when training.\n        new_lm_head = nn.Linear(\n            *new_lm_head_shape,\n            bias=has_new_lm_head_bias,\n            device=old_lm_head.weight.device,\n            dtype=old_lm_head.weight.dtype,\n        )\n\n        if new_num_tokens > old_num_tokens and not mean_resizing:\n            # initialize new embeddings (in particular added tokens) with a mean of 0 and std equals `config.initializer_range`.\n            self._init_weights(new_lm_head)\n\n        elif new_num_tokens > old_num_tokens and mean_resizing:\n            # initialize new lm_head weights (in particular added tokens). The new lm_head weights\n            # will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance.\n            # as described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html\n            logger.warning_once(\n                \"The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. \"\n                \"As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. \"\n                \"To disable this, use `mean_resizing=False`\"\n            )\n\n            added_num_tokens = new_num_tokens - old_num_tokens\n            if is_deepspeed_zero3_enabled() and not is_quantized:\n                import deepspeed\n\n                params = [old_lm_head.weight]\n                if has_new_lm_head_bias:\n                    params += [old_lm_head.bias]\n                with deepspeed.zero.GatheredParameters(params, modifier_rank=None):\n                    self._init_added_lm_head_weights_with_mean(\n                        old_lm_head, new_lm_head, old_lm_head_dim, old_num_tokens, added_num_tokens, transposed\n                    )\n                    if has_new_lm_head_bias:\n                        self._init_added_lm_head_bias_with_mean(old_lm_head, new_lm_head, added_num_tokens)\n\n            else:\n                self._init_added_lm_head_weights_with_mean(\n                    old_lm_head, new_lm_head, old_lm_head_dim, old_num_tokens, added_num_tokens, transposed\n                )\n                if has_new_lm_head_bias:\n                    self._init_added_lm_head_bias_with_mean(old_lm_head, new_lm_head, added_num_tokens)\n\n        num_tokens_to_copy = min(old_num_tokens, new_num_tokens)\n\n        if is_deepspeed_zero3_enabled() and not is_quantized:\n            import deepspeed\n\n            params = [old_lm_head.weight, old_lm_head.bias, new_lm_head.weight, new_lm_head.bias]\n            with deepspeed.zero.GatheredParameters(params, modifier_rank=0):\n                self._copy_lm_head_original_to_resized(\n                    new_lm_head, old_lm_head, num_tokens_to_copy, transposed, has_new_lm_head_bias\n                )\n        else:\n            self._copy_lm_head_original_to_resized(\n                new_lm_head, old_lm_head, num_tokens_to_copy, transposed, has_new_lm_head_bias\n            )\n\n        return new_lm_head\n\n    def _init_added_embeddings_weights_with_mean(\n        self, old_embeddings, new_embeddings, old_num_tokens, added_num_tokens\n    ):\n        old_embeddings_weight = old_embeddings.weight.data.to(torch.float32)\n        mean_embeddings = torch.mean(old_embeddings_weight, axis=0)\n        old_centered_embeddings = old_embeddings_weight - mean_embeddings\n        covariance = old_centered_embeddings.T @ old_centered_embeddings / old_num_tokens\n\n        # Check if the covariance is positive definite.\n        epsilon = 1e-9\n        is_covariance_psd = constraints.positive_definite.check(epsilon * covariance).all()\n        if is_covariance_psd:\n            # If covariances is positive definite, a distribution can be created. and we can sample new weights from it.\n            distribution = torch.distributions.multivariate_normal.MultivariateNormal(\n                mean_embeddings, covariance_matrix=epsilon * covariance\n            )\n            new_embeddings.weight.data[-1 * added_num_tokens :, :] = distribution.sample(\n                sample_shape=(added_num_tokens,)\n            ).to(old_embeddings.weight.dtype)\n        else:\n            # Otherwise, just initialize with the mean. because distribution will not be created.\n            new_embeddings.weight.data[-1 * added_num_tokens :, :] = (\n                mean_embeddings[None, :].repeat(added_num_tokens, 1).to(old_embeddings.weight.dtype)\n            )\n\n    def _init_added_lm_head_weights_with_mean(\n        self,\n        old_lm_head,\n        new_lm_head,\n        old_lm_head_dim,\n        old_num_tokens,\n        added_num_tokens,\n        transposed: bool = False,\n    ):\n        if transposed:\n            # Transpose to the desired shape for the function.\n            new_lm_head.weight.data = new_lm_head.weight.data.T\n            old_lm_head.weight.data = old_lm_head.weight.data.T\n\n        # The same initialization logic as Embeddings.\n        self._init_added_embeddings_weights_with_mean(old_lm_head, new_lm_head, old_num_tokens, added_num_tokens)\n\n        if transposed:\n            # Transpose again to the correct shape.\n            new_lm_head.weight.data = new_lm_head.weight.data.T\n            old_lm_head.weight.data = old_lm_head.weight.data.T\n\n    def _init_added_lm_head_bias_with_mean(self, old_lm_head, new_lm_head, added_num_tokens):\n        bias_mean = torch.mean(old_lm_head.bias.data, axis=0, dtype=torch.float32)\n        bias_std = torch.std(old_lm_head.bias.data, axis=0).to(torch.float32)\n        new_lm_head.bias.data[-1 * added_num_tokens :].normal_(mean=bias_mean, std=1e-9 * bias_std)\n\n    def _copy_lm_head_original_to_resized(\n        self, new_lm_head, old_lm_head, num_tokens_to_copy, transposed, has_new_lm_head_bias\n    ):\n        # Copy old lm head weights to new lm head\n        if not transposed:\n            new_lm_head.weight.data[:num_tokens_to_copy, :] = old_lm_head.weight.data[:num_tokens_to_copy, :]\n        else:\n            new_lm_head.weight.data[:, :num_tokens_to_copy] = old_lm_head.weight.data[:, :num_tokens_to_copy]\n\n        # Copy bias weights to new lm head\n        if has_new_lm_head_bias:\n            new_lm_head.bias.data[:num_tokens_to_copy] = old_lm_head.bias.data[:num_tokens_to_copy]\n\n    def resize_position_embeddings(self, new_num_position_embeddings: int):\n        raise NotImplementedError(\n            f\"`resize_position_embeddings` is not implemented for {self.__class__}`. To implement it, you should \"\n            f\"overwrite this method in the class {self.__class__} in `modeling_{self.__class__.__module__}.py`\"\n        )\n\n    def get_position_embeddings(self) -> Union[nn.Embedding, tuple[nn.Embedding]]:\n        raise NotImplementedError(\n            f\"`get_position_embeddings` is not implemented for {self.__class__}`. To implement it, you should \"\n            f\"overwrite this method in the class {self.__class__} in `modeling_{self.__class__.__module__}.py`\"\n        )\n\n    def init_weights(self):\n        \"\"\"\n        Maybe initializes weights. If using a custom `PreTrainedModel`, you need to implement any\n        initialization logic in `_init_weights`.\n        \"\"\"\n        if _init_weights:\n            # Initialize weights\n            self.initialize_weights()\n\n            # Tie weights should be skipped when not initializing all weights\n            # since from_pretrained(...) calls tie weights anyways\n            self.tie_weights()\n\n    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n        \"\"\"\n        Activates gradient checkpointing for the current model.\n\n        We pass the `__call__` method of the modules instead of `forward` because `__call__` attaches all the hooks of\n        the module. https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2\n\n        Args:\n            gradient_checkpointing_kwargs (dict, *optional*):\n                Additional keyword arguments passed along to the `torch.utils.checkpoint.checkpoint` function.\n        \"\"\"\n        if not self.supports_gradient_checkpointing:\n            raise ValueError(f\"{self.__class__.__name__} does not support gradient checkpointing.\")\n\n        if gradient_checkpointing_kwargs is None:\n            gradient_checkpointing_kwargs = {\"use_reentrant\": True}\n\n        gradient_checkpointing_func = functools.partial(checkpoint, **gradient_checkpointing_kwargs)\n\n        # For old GC format (transformers < 4.35.0) for models that live on the Hub\n        # we will fall back to the overwritten `_set_gradient_checkpointing` method\n        _is_using_old_format = \"value\" in inspect.signature(self._set_gradient_checkpointing).parameters\n\n        if not _is_using_old_format:\n            self._set_gradient_checkpointing(enable=True, gradient_checkpointing_func=gradient_checkpointing_func)\n        else:\n            self.apply(partial(self._set_gradient_checkpointing, value=True))\n            logger.warning(\n                \"You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).\"\n                \"Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\"\n            )\n\n        if getattr(self, \"_hf_peft_config_loaded\", False):\n            # When using PEFT + gradient checkpointing + Trainer we need to make sure the input has requires_grad=True\n            # we do it also on PEFT: https://github.com/huggingface/peft/blob/85013987aa82aa1af3da1236b6902556ce3e483e/src/peft/peft_model.py#L334\n            # When training with PEFT, only LoRA layers will have requires grad set to True, but the output of frozen layers need to propagate\n            # the gradients to make sure the gradient flows.\n            self.enable_input_require_grads()\n\n    def _set_gradient_checkpointing(self, enable: bool = True, gradient_checkpointing_func: Callable = checkpoint):\n        is_gradient_checkpointing_set = False\n\n        # Apply it on the top-level module in case the top-level modules supports it\n        # for example, LongT5Stack inherits from `PreTrainedModel`.\n        if hasattr(self, \"gradient_checkpointing\"):\n            self._gradient_checkpointing_func = gradient_checkpointing_func\n            self.gradient_checkpointing = enable\n            is_gradient_checkpointing_set = True\n\n        for module in self.modules():\n            if hasattr(module, \"gradient_checkpointing\"):\n                module._gradient_checkpointing_func = gradient_checkpointing_func\n                module.gradient_checkpointing = enable\n                is_gradient_checkpointing_set = True\n\n        if not is_gradient_checkpointing_set:\n            raise ValueError(\n                f\"{self.__class__.__name__} is not compatible with gradient checkpointing. Make sure all the architecture support it by setting a boolean attribute\"\n                \" `gradient_checkpointing` to modules of the model that uses checkpointing.\"\n            )\n\n    def gradient_checkpointing_disable(self):\n        \"\"\"\n        Deactivates gradient checkpointing for the current model.\n        \"\"\"\n        if self.supports_gradient_checkpointing:\n            # For old GC format (transformers < 4.35.0) for models that live on the Hub\n            # we will fall back to the overwritten `_set_gradient_checkpointing` method\n            _is_using_old_format = \"value\" in inspect.signature(self._set_gradient_checkpointing).parameters\n            if not _is_using_old_format:\n                self._set_gradient_checkpointing(enable=False)\n            else:\n                logger.warning(\n                    \"You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).\"\n                    \"Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\"\n                )\n                self.apply(partial(self._set_gradient_checkpointing, value=False))\n\n        if getattr(self, \"_hf_peft_config_loaded\", False):\n            self.disable_input_require_grads()\n\n    @property\n    def is_gradient_checkpointing(self) -> bool:\n        \"\"\"\n        Whether gradient checkpointing is activated for this model or not.\n        \"\"\"\n        return any(hasattr(m, \"gradient_checkpointing\") and m.gradient_checkpointing for m in self.modules())\n\n    def save_pretrained(\n        self,\n        save_directory: Union[str, os.PathLike],\n        is_main_process: bool = True,\n        state_dict: Optional[dict] = None,\n        save_function: Callable = torch.save,\n        push_to_hub: bool = False,\n        max_shard_size: Union[int, str] = \"5GB\",\n        safe_serialization: bool = True,\n        variant: Optional[str] = None,\n        token: Optional[Union[str, bool]] = None,\n        save_peft_format: bool = True,\n        **kwargs,\n    ):\n        \"\"\"\n        Save a model and its configuration file to a directory, so that it can be re-loaded using the\n        [`~PreTrainedModel.from_pretrained`] class method.\n\n        Arguments:\n            save_directory (`str` or `os.PathLike`):\n                Directory to which to save. Will be created if it doesn't exist.\n            is_main_process (`bool`, *optional*, defaults to `True`):\n                Whether the process calling this is the main process or not. Useful when in distributed training like\n                TPUs and need to call this function on all processes. In this case, set `is_main_process=True` only on\n                the main process to avoid race conditions.\n            state_dict (nested dictionary of `torch.Tensor`):\n                The state dictionary of the model to save. Will default to `self.state_dict()`, but can be used to only\n                save parts of the model or if special precautions need to be taken when recovering the state dictionary\n                of a model (like when using model parallelism).\n            save_function (`Callable`):\n                The function to use to save the state dictionary. Useful on distributed training like TPUs when one\n                need to replace `torch.save` by another method.\n            push_to_hub (`bool`, *optional*, defaults to `False`):\n                Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n                repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n                namespace).\n            max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n                The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size\n                lower than this size. If expressed as a string, needs to be digits followed by a unit (like `\"5MB\"`).\n                We default it to 5GB in order for models to be able to run easily on free-tier google colab instances\n                without CPU OOM issues.\n\n                <Tip warning={true}>\n\n                If a single weight of the model is bigger than `max_shard_size`, it will be in its own checkpoint shard\n                which will be bigger than `max_shard_size`.\n\n                </Tip>\n\n            safe_serialization (`bool`, *optional*, defaults to `True`):\n                Whether to save the model using `safetensors` or the traditional PyTorch way (that uses `pickle`).\n            variant (`str`, *optional*):\n                If specified, weights are saved in the format pytorch_model.<variant>.bin.\n            token (`str` or `bool`, *optional*):\n                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use\n                the token generated when running `hf auth login` (stored in `~/.huggingface`).\n            save_peft_format (`bool`, *optional*, defaults to `True`):\n                For backward compatibility with PEFT library, in case adapter weights are attached to the model, all\n                keys of the state dict of adapters needs to be prepended with `base_model.model`. Advanced users can\n                disable this behaviours by setting `save_peft_format` to `False`.\n            kwargs (`dict[str, Any]`, *optional*):\n                Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n        \"\"\"\n        ignore_metadata_errors = kwargs.pop(\"ignore_metadata_errors\", False)\n\n        if token is not None:\n            kwargs[\"token\"] = token\n\n        _hf_peft_config_loaded = getattr(self, \"_hf_peft_config_loaded\", False)\n\n        hf_quantizer = getattr(self, \"hf_quantizer\", None)\n        quantization_serializable = (\n            hf_quantizer is not None\n            and isinstance(hf_quantizer, HfQuantizer)\n            and hf_quantizer.is_serializable(safe_serialization=safe_serialization)\n        )\n\n        if hf_quantizer is not None and not _hf_peft_config_loaded and not quantization_serializable:\n            raise ValueError(\n                f\"The model is quantized with {hf_quantizer.quantization_config.quant_method} and is not serializable - check out the warnings from\"\n                \" the logger on the traceback to understand the reason why the quantized model is not serializable.\"\n            )\n\n        if \"save_config\" in kwargs:\n            warnings.warn(\n                \"`save_config` is deprecated and will be removed in v5 of Transformers. Use `is_main_process` instead.\"\n            )\n            is_main_process = kwargs.pop(\"save_config\")\n\n        # we need to check against tp_size, not tp_plan, as tp_plan is substituted to the class one\n        if self._tp_size is not None and not is_huggingface_hub_greater_or_equal(\"0.31.4\"):\n            raise ImportError(\n                \"Saving a model with tensor parallelism requires `huggingface_hub` version 0.31.4 or higher.\"\n            )\n\n        if os.path.isfile(save_directory):\n            logger.error(f\"Provided path ({save_directory}) should be a directory, not a file\")\n            return\n\n        os.makedirs(save_directory, exist_ok=True)\n\n        if push_to_hub:\n            commit_message = kwargs.pop(\"commit_message\", None)\n            repo_id = kwargs.pop(\"repo_id\", save_directory.split(os.path.sep)[-1])\n            create_pr = kwargs.pop(\"create_pr\", False)\n            repo_id = self._create_repo(repo_id, **kwargs)\n            files_timestamps = self._get_files_timestamps(save_directory)\n\n        metadata = {}\n        if hf_quantizer is not None:\n            state_dict, metadata = hf_quantizer.get_state_dict_and_metadata(self, safe_serialization)\n        metadata[\"format\"] = \"pt\"\n\n        # Only save the model itself if we are using distributed training\n        model_to_save = unwrap_model(self)\n        # save the string version of dtype to the config, e.g. convert torch.float32 => \"float32\"\n        # we currently don't use this setting automatically, but may start to use with v5\n        dtype = get_parameter_dtype(model_to_save)\n        model_to_save.config.dtype = str(dtype).split(\".\")[1]\n\n        # Attach architecture to the config\n        # When using FSDP2, unwrapping is a noop, so the model name doesn't change back to the original model name\n        model_to_save.config.architectures = [model_to_save.__class__.__name__.removeprefix(\"FSDP\")]\n\n        # If we have a custom model, we copy the file defining it in the folder and set the attributes so it can be\n        # loaded from the Hub.\n        if self._auto_class is not None:\n            custom_object_save(self, save_directory, config=self.config)\n\n        # Save the config\n        if is_main_process:\n            if not _hf_peft_config_loaded:\n                # If the model config has set attributes that should be in the generation config, move them there.\n                misplaced_generation_parameters = model_to_save.config._get_non_default_generation_parameters()\n                if self.can_generate() and len(misplaced_generation_parameters) > 0:\n                    warnings.warn(\n                        \"Moving the following attributes in the config to the generation config: \"\n                        f\"{misplaced_generation_parameters}. You are seeing this warning because you've set \"\n                        \"generation parameters in the model config, as opposed to in the generation config.\",\n                        UserWarning,\n                    )\n                    for param_name, param_value in misplaced_generation_parameters.items():\n                        setattr(model_to_save.generation_config, param_name, param_value)\n                        setattr(model_to_save.config, param_name, None)\n\n                model_to_save.config.save_pretrained(save_directory)\n            if self.can_generate():\n                model_to_save.generation_config.save_pretrained(save_directory)\n\n            if _hf_peft_config_loaded:\n                logger.info(\n                    \"Detected adapters on the model, saving the model in the PEFT format, only adapter weights will be saved.\"\n                )\n                state_dict = model_to_save.get_adapter_state_dict(state_dict=state_dict)\n\n                if save_peft_format:\n                    logger.info(\n                        \"To match the expected format of the PEFT library, all keys of the state dict of adapters will be prepended with `base_model.model`.\"\n                    )\n                    peft_state_dict = {}\n                    for key, value in state_dict.items():\n                        peft_state_dict[f\"base_model.model.{key}\"] = value\n                    state_dict = peft_state_dict\n\n                active_adapter = self.active_adapters()\n\n                if len(active_adapter) > 1:\n                    raise ValueError(\n                        \"Multiple active adapters detected, saving multiple active adapters is not supported yet. You can save adapters separately one by one \"\n                        \"by iteratively calling `model.set_adapter(adapter_name)` then `model.save_pretrained(...)`\"\n                    )\n                active_adapter = active_adapter[0]\n\n                current_peft_config = self.peft_config[active_adapter]\n                current_peft_config.save_pretrained(save_directory)\n\n        # for offloaded modules\n        module_map = {}\n\n        # Save the model\n        if state_dict is None:\n            # if any model parameters are offloaded, make module map\n            if (\n                hasattr(self, \"hf_device_map\")\n                and len(set(self.hf_device_map.values())) > 1\n                and (\"cpu\" in self.hf_device_map.values() or \"disk\" in self.hf_device_map.values())\n            ):\n                warnings.warn(\n                    \"Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\"\n                )\n                for name, module in model_to_save.named_modules():\n                    if name == \"\":\n                        continue\n                    module_state_dict = module.state_dict()\n\n                    for key in module_state_dict:\n                        module_map[name + f\".{key}\"] = module\n            state_dict = model_to_save.state_dict()\n\n        if any(\n            allowed_name in class_name.__name__.lower()\n            for class_name in self.__class__.__mro__[:-1]\n            for allowed_name in VLMS\n        ):\n            reverse_key_mapping = {v: k for k, v in self._checkpoint_conversion_mapping.items()}\n\n            original_state_dict = {}\n            for key, value in state_dict.items():\n                for pattern, replacement in reverse_key_mapping.items():\n                    replacement = replacement.lstrip(\"^\")  # strip off un-needed chars and patterns\n                    replacement = re.sub(r\"\\(.*\\)\", \"\", replacement)\n                    key, n_replace = re.subn(pattern, replacement, key)\n                    # Early exit of the loop\n                    if n_replace > 0:\n                        break\n                original_state_dict[key] = value\n            state_dict = original_state_dict\n\n        # Translate state_dict from smp to hf if saving with smp >= 1.10\n        if IS_SAGEMAKER_MP_POST_1_10:\n            for smp_to_hf, _ in smp.state.module_manager.translate_functions:\n                state_dict = smp_to_hf(state_dict)\n\n        # Handle the case where some state_dict keys shouldn't be saved\n        if self._keys_to_ignore_on_save is not None:\n            for ignore_key in self._keys_to_ignore_on_save:\n                if ignore_key in state_dict:\n                    del state_dict[ignore_key]\n\n        # Rename state_dict keys before saving to file. Do nothing unless overridden in a particular model.\n        # (initially introduced with TimmWrapperModel to remove prefix and make checkpoints compatible with timm)\n        state_dict = self._fix_state_dict_keys_on_save(state_dict)\n        # If model was sharded, we cannot properly determine sizes of tensors that `local_*` strategy was used,\n        # therefore we replace them with DTensors that are equivalently sharded\n        if self._tp_size is not None:\n            state_dict = replace_state_dict_local_with_dtensor(state_dict, self._tp_plan, self._device_mesh)\n\n        if safe_serialization:\n            # TODO: fix safe_serialization for tied weights\n            # Safetensors does not allow tensor aliasing.\n            # We're going to remove aliases before saving\n            ptrs = collections.defaultdict(list)\n            for name, tensor in state_dict.items():\n                if not isinstance(tensor, torch.Tensor):\n                    # Sometimes in the state_dict we have non-tensor objects.\n                    # e.g. in bitsandbytes we have some `str` objects in the state_dict\n                    # In the non-tensor case, fall back to the pointer of the object itself\n                    ptrs[id(tensor)].append(name)\n\n                elif tensor.device.type == \"meta\":\n                    # In offloaded cases, there may be meta tensors in the state_dict.\n                    # For these cases, key by the pointer of the original tensor object\n                    # (state_dict tensors are detached and therefore no longer shared)\n                    tensor = self.get_parameter(name)\n                    ptrs[id(tensor)].append(name)\n\n                else:\n                    ptrs[id_tensor_storage(tensor)].append(name)\n\n            shared_ptrs = {ptr: names for ptr, names in ptrs.items() if len(names) > 1}\n\n            # Recursively descend to find tied weight keys\n            _tied_weights_keys = _get_tied_weight_keys(self)\n            error_names = []\n            to_delete_names = set()\n            for names in shared_ptrs.values():\n                # Removing the keys which are declared as known duplicates on\n                # load. This allows to make sure the name which is kept is consistent.\n                if _tied_weights_keys is not None:\n                    found = 0\n                    for name in sorted(names):\n                        matches_pattern = any(re.search(pat, name) for pat in _tied_weights_keys)\n                        if matches_pattern and name in state_dict:\n                            found += 1\n                            if found < len(names):\n                                to_delete_names.add(name)\n            # We are entering a place where the weights and the transformers configuration do NOT match.\n            shared_names, disjoint_names = _find_disjoint(shared_ptrs.values(), state_dict)\n            # Those are actually tensor sharing but disjoint from each other, we can safely clone them\n            # Reloaded won't have the same property, but it shouldn't matter in any meaningful way.\n            for name in disjoint_names:\n                state_dict[name] = state_dict[name].clone()\n\n            # When not all duplicates have been cleaned, still remove those keys, but put a clear warning.\n            # If the link between tensors was done at runtime then `from_pretrained` will not get\n            # the key back leading to random tensor. A proper warning will be shown\n            # during reload (if applicable), but since the file is not necessarily compatible with\n            # the config, better show a proper warning.\n            shared_names, identical_names = _find_identical(shared_names, state_dict)\n            # delete tensors that have identical storage\n            for inames in identical_names:\n                known = inames.intersection(to_delete_names)\n                for name in known:\n                    del state_dict[name]\n                unknown = inames.difference(to_delete_names)\n                if len(unknown) > 1:\n                    error_names.append(unknown)\n\n            if shared_names:\n                error_names.extend(shared_names)\n\n            if len(error_names) > 0:\n                raise RuntimeError(\n                    f\"The weights trying to be saved contained shared tensors {error_names} that are mismatching \"\n                    \"the transformers base configuration. Try saving using `safe_serialization=False`, setting the \"\n                    \"`_dynamic_tied_weights_keys` attribute for affected modules, or remove this tensor sharing.\",\n                )\n\n        # Shard the model if it is too big.\n        if not _hf_peft_config_loaded:\n            weights_name = SAFE_WEIGHTS_NAME if safe_serialization else WEIGHTS_NAME\n            weights_name = _add_variant(weights_name, variant)\n        else:\n            weights_name = ADAPTER_SAFE_WEIGHTS_NAME if safe_serialization else ADAPTER_WEIGHTS_NAME\n\n        filename_pattern = weights_name.replace(\".bin\", \"{suffix}.bin\").replace(\".safetensors\", \"{suffix}.safetensors\")\n        state_dict_split = split_torch_state_dict_into_shards(\n            state_dict, filename_pattern=filename_pattern, max_shard_size=max_shard_size\n        )\n        # Save index if sharded\n        index = None\n        if state_dict_split.is_sharded:\n            index = {\n                \"metadata\": {\"total_parameters\": self.num_parameters(), **state_dict_split.metadata},\n                \"weight_map\": state_dict_split.tensor_to_filename,\n            }\n\n        # Clean the folder from a previous save\n        for filename in os.listdir(save_directory):\n            full_filename = os.path.join(save_directory, filename)\n            # If we have a shard file that is not going to be replaced, we delete it, but only from the main process\n            # in distributed settings to avoid race conditions.\n            weights_no_suffix = weights_name.replace(\".bin\", \"\").replace(\".safetensors\", \"\")\n\n            # make sure that file to be deleted matches format of sharded file, e.g. pytorch_model-00001-of-00005\n            filename_no_suffix = filename.replace(\".bin\", \"\").replace(\".safetensors\", \"\")\n            reg = re.compile(r\"(.*?)-\\d{5}-of-\\d{5}\")\n\n            if (\n                filename.startswith(weights_no_suffix)\n                and os.path.isfile(full_filename)\n                and filename not in state_dict_split.filename_to_tensors\n                and is_main_process\n                and reg.fullmatch(filename_no_suffix) is not None\n            ):\n                os.remove(full_filename)\n        # Save the model\n        filename_to_tensors = state_dict_split.filename_to_tensors.items()\n        if module_map:\n            filename_to_tensors = logging.tqdm(filename_to_tensors, desc=\"Saving checkpoint shards\")\n        for shard_file, tensors in filename_to_tensors:\n            shard = {}\n            for tensor in tensors:\n                if _is_dtensor_available and isinstance(state_dict[tensor], DTensor):\n                    full_tensor = state_dict[tensor].full_tensor()\n                    # to get the correctly ordered tensor we need to repack if packed\n                    if _get_parameter_tp_plan(tensor, self._tp_plan) == \"local_packed_rowwise\":\n                        full_tensor = repack_weights(full_tensor, -1, self._tp_size, 2)\n                    shard[tensor] = full_tensor.contiguous()  # only do contiguous after it's permuted correctly\n                else:\n                    shard[tensor] = state_dict[tensor].contiguous()\n                # delete reference, see https://github.com/huggingface/transformers/pull/34890\n                del state_dict[tensor]\n\n            # remake shard with onloaded parameters if necessary\n            if module_map:\n                # init state_dict for this shard\n                shard_state_dict = dict.fromkeys(shard, \"\")\n                for module_name in shard:\n                    # note that get_state_dict_from_offload can update with meta tensors\n                    # if both a parent module and its descendant are offloaded\n                    tensor = shard_state_dict[module_name]\n                    if tensor == \"\" or (isinstance(tensor, torch.Tensor) and tensor.device.type == \"meta\"):\n                        # update state dict with onloaded parameters\n                        module = module_map[module_name]\n                        shard_state_dict = get_state_dict_from_offload(module, module_name, shard_state_dict)\n\n                # assign shard to be the completed state dict\n                shard = shard_state_dict\n                del shard_state_dict\n                gc.collect()\n\n            if safe_serialization:\n                # At some point we will need to deal better with save_function (used for TPU and other distributed\n                # joyfulness), but for now this enough.\n                safe_save_file(shard, os.path.join(save_directory, shard_file), metadata=metadata)\n            else:\n                save_function(shard, os.path.join(save_directory, shard_file))\n\n        del state_dict\n\n        if index is None:\n            path_to_weights = os.path.join(save_directory, weights_name)\n            logger.info(f\"Model weights saved in {path_to_weights}\")\n        else:\n            save_index_file = SAFE_WEIGHTS_INDEX_NAME if safe_serialization else WEIGHTS_INDEX_NAME\n            save_index_file = os.path.join(save_directory, _add_variant(save_index_file, variant))\n            # Save the index as well\n            with open(save_index_file, \"w\", encoding=\"utf-8\") as f:\n                content = json.dumps(index, indent=2, sort_keys=True) + \"\\n\"\n                f.write(content)\n            logger.info(\n                f\"The model is bigger than the maximum size per checkpoint ({max_shard_size}) and is going to be \"\n                f\"split in {len(state_dict_split.filename_to_tensors)} checkpoint shards. You can find where each parameters has been saved in the \"\n                f\"index located at {save_index_file}.\"\n            )\n\n        if push_to_hub:\n            # Eventually create an empty model card\n            model_card = create_and_tag_model_card(\n                repo_id, self.model_tags, token=token, ignore_metadata_errors=ignore_metadata_errors\n            )\n\n            # Update model card if needed:\n            model_card.save(os.path.join(save_directory, \"README.md\"))\n\n            self._upload_modified_files(\n                save_directory,\n                repo_id,\n                files_timestamps,\n                commit_message=commit_message,\n                token=token,\n                create_pr=create_pr,\n            )\n\n    @wraps(PushToHubMixin.push_to_hub)\n    def push_to_hub(self, *args, **kwargs):\n        tags = self.model_tags if self.model_tags is not None else []\n\n        tags_kwargs = kwargs.get(\"tags\", [])\n        if isinstance(tags_kwargs, str):\n            tags_kwargs = [tags_kwargs]\n\n        for tag in tags_kwargs:\n            if tag not in tags:\n                tags.append(tag)\n\n        if tags:\n            kwargs[\"tags\"] = tags\n        return super().push_to_hub(*args, **kwargs)\n\n    def get_memory_footprint(self, return_buffers=True):\n        r\"\"\"\n        Get the memory footprint of a model. This will return the memory footprint of the current model in bytes.\n        Useful to benchmark the memory footprint of the current model and design some tests. Solution inspired from the\n        PyTorch discussions: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822/2\n\n        Arguments:\n            return_buffers (`bool`, *optional*, defaults to `True`):\n                Whether to return the size of the buffer tensors in the computation of the memory footprint. Buffers\n                are tensors that do not require gradients and not registered as parameters. E.g. mean and std in batch\n                norm layers. Please see: https://discuss.pytorch.org/t/what-pytorch-means-by-buffers/120266/2\n        \"\"\"\n        mem = sum(param.nelement() * param.element_size() for param in self.parameters())\n        if return_buffers:\n            mem_bufs = sum(buf.nelement() * buf.element_size() for buf in self.buffers())\n            mem = mem + mem_bufs\n        return mem\n\n    @wraps(torch.nn.Module.cuda)\n    def cuda(self, *args, **kwargs):\n        if getattr(self, \"quantization_method\", None) == QuantizationMethod.HQQ:\n            from hqq.core.quantize import HQQLinear\n\n            # Since HQQLinear stores some tensors in the 'meta' attribute,\n            # it's necessary to manually call the `cuda` method on HQQLinear layers.\n            super().cuda(*args, **kwargs)\n            for module in self.modules():\n                if isinstance(module, HQQLinear):\n                    if len(args) > 0:\n                        device = args[0]\n                    else:\n                        device = kwargs.get(\"device\", \"cuda\")\n                    module.cuda(device)\n            return self\n\n        # Checks if the model has been loaded in 4-bit or 8-bit with BNB\n        if getattr(self, \"quantization_method\", None) == QuantizationMethod.BITS_AND_BYTES:\n            if getattr(self, \"is_loaded_in_8bit\", False):\n                raise ValueError(\n                    \"Calling `cuda()` is not supported for `8-bit` quantized models. \"\n                    \" Please use the model as it is, since the model has already been set to the correct devices.\"\n                )\n        return super().cuda(*args, **kwargs)\n\n    @wraps(torch.nn.Module.to)\n    def to(self, *args, **kwargs):\n        # For BNB/GPTQ models, we prevent users from casting the model to another dtype to restrict unwanted behaviours.\n        # the correct API should be to load the model with the desired dtype directly through `from_pretrained`.\n        dtype_present_in_args = \"dtype\" in kwargs\n\n        if not dtype_present_in_args:\n            for arg in args:\n                if isinstance(arg, torch.dtype):\n                    dtype_present_in_args = True\n                    break\n\n        if getattr(self, \"quantization_method\", None) == QuantizationMethod.HQQ:\n            from hqq.core.quantize import HQQLinear\n\n            # Since HQQLinear stores some tensors in the 'meta' attribute, we must\n            # explicitly move the parameters to the target device for each HQQLinear layer after `to`.\n            super().to(*args, **kwargs)\n            for module in self.modules():\n                if isinstance(module, HQQLinear):\n                    if \"device\" in kwargs:\n                        device = kwargs[\"device\"]\n                    else:\n                        device = args[0]\n                    if \"dtype\" in kwargs:\n                        dtype = kwargs[\"dtype\"]\n                    elif dtype_present_in_args:\n                        dtype = arg\n                    else:\n                        dtype = None\n                    # Due to the current messy implementation of HQQLinear, updating `compute_dtype`\n                    # followed by calling the `cuda` method achieves the intended behavior of `to`,\n                    # even when the target device is CPU.\n                    if dtype is not None:\n                        module.compute_dtype = dtype\n                    module.cuda(device)\n            return self\n\n        if dtype_present_in_args and getattr(self, \"quantization_method\", None) == QuantizationMethod.QUARK:\n            raise ValueError(\"Casting a Quark quantized model to a new `dtype` is not supported.\")\n\n        # Checks if the model has been loaded in 4-bit or 8-bit with BNB\n        if getattr(self, \"quantization_method\", None) == QuantizationMethod.BITS_AND_BYTES:\n            if dtype_present_in_args:\n                raise ValueError(\n                    \"You cannot cast a bitsandbytes model in a new `dtype`. Make sure to load the model using `from_pretrained` using the\"\n                    \" desired `dtype` by passing the correct `dtype` argument.\"\n                )\n\n            if getattr(self, \"is_loaded_in_8bit\", False):\n                raise ValueError(\n                    \"`.to` is not supported for `8-bit` bitsandbytes models. Please use the model as it is, since the\"\n                    \" model has already been set to the correct devices and casted to the correct `dtype`.\"\n                )\n        elif getattr(self, \"quantization_method\", None) == QuantizationMethod.GPTQ:\n            if dtype_present_in_args:\n                raise ValueError(\n                    \"You cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\"\n                    \" `dtype` by passing the correct `dtype` argument.\"\n                )\n        return super().to(*args, **kwargs)\n\n    def half(self, *args):\n        # Checks if the model is quantized\n        if getattr(self, \"is_quantized\", False):\n            raise ValueError(\n                \"`.half()` is not supported for quantized model. Please use the model as it is, since the\"\n                \" model has already been casted to the correct `dtype`.\"\n            )\n        else:\n            return super().half(*args)\n\n    def float(self, *args):\n        # Checks if the model is quantized\n        if getattr(self, \"is_quantized\", False):\n            raise ValueError(\n                \"`.float()` is not supported for quantized model. Please use the model as it is, since the\"\n                \" model has already been casted to the correct `dtype`.\"\n            )\n        else:\n            return super().float(*args)\n\n    @classmethod\n    def get_init_context(cls, is_quantized: bool, _is_ds_init_called: bool):\n        if is_deepspeed_zero3_enabled():\n            import deepspeed\n\n            init_contexts = [no_init_weights()]\n            # We cannot initialize the model on meta device with deepspeed when not quantized\n            if not is_quantized and not _is_ds_init_called:\n                logger.info(\"Detected DeepSpeed ZeRO-3: activating zero.init() for this model\")\n                init_contexts.extend([deepspeed.zero.Init(config_dict_or_path=deepspeed_config()), set_zero3_state()])\n            elif is_quantized:\n                init_contexts.extend([init_empty_weights(), set_quantized_state()])\n        else:\n            init_contexts = [no_init_weights(), init_empty_weights()]\n\n        return init_contexts\n\n    def set_use_kernels(self, use_kernels, kernel_config):\n        if use_kernels:\n            if not is_kernels_available():\n                raise ValueError(\n                    \"Kernels are not available. To use kernels, please install kernels using `pip install kernels`\"\n                )\n            from kernels import use_kernel_mapping\n\n            if kernel_config is not None and isinstance(kernel_config, KernelConfig):\n                # This will make sure the mapping is valid, and the layers are registered in the model\n                kernel_config.sanitize_kernel_mapping(self)\n\n                # This will create a compatible mapping for the model with the kernels library\n                kernel_config.create_compatible_mapping(self)\n\n                # This is a context manager to override the default kernel mapping\n                # We are calling kernelize inside this context manager using the use_kernels setter\n                with use_kernel_mapping(kernel_config.kernel_mapping):\n                    self.use_kernels = True\n            # We use the default kernel mapping in .integrations.hub_kernels\n            else:\n                self.use_kernels = True\n        else:\n            self.use_kernels = False\n\n    @classmethod\n    @restore_default_dtype\n    def from_pretrained(\n        cls: type[SpecificPreTrainedModelType],\n        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n        *model_args,\n        config: Optional[Union[PreTrainedConfig, str, os.PathLike]] = None,\n        cache_dir: Optional[Union[str, os.PathLike]] = None,\n        ignore_mismatched_sizes: bool = False,\n        force_download: bool = False,\n        local_files_only: bool = False,\n        token: Optional[Union[str, bool]] = None,\n        revision: str = \"main\",\n        use_safetensors: Optional[bool] = None,\n        weights_only: bool = True,\n        **kwargs,\n    ) -> SpecificPreTrainedModelType:\n        r\"\"\"\n        Instantiate a pretrained pytorch model from a pre-trained model configuration.\n\n        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train\n        the model, you should first set it back in training mode with `model.train()`.\n\n        The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come\n        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\n        task.\n\n        The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those\n        weights are discarded.\n\n        Parameters:\n            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n                Can be either:\n\n                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n                    - A path to a *directory* containing model weights saved using\n                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n                    - `None` if you are both providing the configuration and state dictionary (resp. with keyword\n                      arguments `config` and `state_dict`).\n            model_args (sequence of positional arguments, *optional*):\n                All remaining positional arguments will be passed to the underlying model's `__init__` method.\n            config (`Union[PreTrainedConfig, str, os.PathLike]`, *optional*):\n                Can be either:\n\n                    - an instance of a class derived from [`PreTrainedConfig`],\n                    - a string or path valid as input to [`~PreTrainedConfig.from_pretrained`].\n\n                Configuration for the model to use instead of an automatically loaded configuration. Configuration can\n                be automatically loaded when:\n\n                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained\n                      model).\n                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\n                      save directory.\n                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\n                      configuration JSON file named *config.json* is found in the directory.\n            state_dict (`dict[str, torch.Tensor]`, *optional*):\n                A state dictionary to use instead of a state dictionary loaded from saved weights file.\n\n                This option can be used if you want to create a model from a pretrained configuration but load your own\n                weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and\n                [`~PreTrainedModel.from_pretrained`] is not a simpler option.\n            cache_dir (`Union[str, os.PathLike]`, *optional*):\n                Path to a directory in which a downloaded pretrained model configuration should be cached if the\n                standard cache should not be used.\n            ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):\n                Whether or not to raise an error if some of the weights from the checkpoint do not have the same size\n                as the weights of the model (if for instance, you are instantiating a model with 10 labels from a\n                checkpoint with 3 labels).\n            force_download (`bool`, *optional*, defaults to `False`):\n                Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n                cached versions if they exist.\n            proxies (`dict[str, str]`, *optional*):\n                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n            output_loading_info(`bool`, *optional*, defaults to `False`):\n                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\n            local_files_only(`bool`, *optional*, defaults to `False`):\n                Whether or not to only look at local files (i.e., do not try to download the model).\n            token (`str` or `bool`, *optional*):\n                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use\n                the token generated when running `hf auth login` (stored in `~/.huggingface`).\n            revision (`str`, *optional*, defaults to `\"main\"`):\n                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n                identifier allowed by git.\n\n                <Tip>\n\n                To test a pull request you made on the Hub, you can pass `revision=\"refs/pr/<pr_number>\"`.\n\n                </Tip>\n            attn_implementation (`str`, *optional*):\n                The attention implementation to use in the model (if relevant). Can be any of `\"eager\"` (manual implementation of the attention), `\"sdpa\"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), `\"flash_attention_2\"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)), or `\"flash_attention_3\"` (using [Dao-AILab/flash-attention/hopper](https://github.com/Dao-AILab/flash-attention/tree/main/hopper)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `\"eager\"` implementation.\n\n                Accept HF kernel references in the form:\n                  <namespace>/<repo_name>[@<revision>][:<kernel_name>]\n\n                - <namespace> and <repo_name> are any non-\"/\" and non-\":\" sequences.\n                - \"@<revision>\" is optional (branch, tag, or commit-ish), e.g. \"@main\", \"@v1.2.0\", \"@abc123\".\n                - \":<kernel_name>\" is optional and selects a function inside the kernel repo.\n                - Both options can appear together and in this order only: @revision first, then :kernel_name.\n                - We intentionally allow a leading \"<wrapper>|\" prefix (e.g., \"flash|...\") because the code\n                  strips it before loading; '|' is not excluded in the character classes here.\n\n                Examples that match:\n                  \"org/model\"\n                  \"org/model@main\"\n                  \"org/model:custom_kernel\"\n                  \"org/model@v1.2.3:custom_kernel\"\n\n            > Parameters for big model inference\n\n            dtype (`str` or `torch.dtype`, *optional*):\n                Override the default `torch_dtype` and load the model under a specific `dtype`. The different options\n                are:\n\n                1. `torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified\n                  `dtype`, ignoring the model's `config.dtype` if one exists. If not specified\n                  - the model will get loaded in `torch.float` (fp32).\n\n                2. `\"auto\"` - A `dtype` or `torch_dtype` entry in the `config.json` file of the model will be\n                  attempted to be used. If this entry isn't found then next check the `dtype` of the first weight in\n                  the checkpoint that's of a floating point type and use that as `dtype`. This will load the model\n                  using the `dtype` it was saved in at the end of the training. It can't be used as an indicator of how\n                  the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.\n\n                3. A string that is a valid `torch.dtype`. E.g. \"float32\" loads the model in `torch.float32`, \"float16\" loads in `torch.float16` etc.\n\n                <Tip>\n\n                For some models the `dtype` they were trained in is unknown - you may try to check the model's paper or\n                reach out to the authors and ask them to add this information to the model's card and to insert the\n                `dtype` or `torch_dtype` entry in `config.json` on the hub.\n\n                </Tip>\n\n            device_map (`str` or `dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):\n                A map that specifies where each submodule should go. It doesn't need to be refined to each\n                parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the\n                same device. If we only pass the device (*e.g.*, `\"cpu\"`, `\"cuda:1\"`, `\"mps\"`, or a GPU ordinal rank\n                like `1`) on which the model will be allocated, the device map will map the entire model to this\n                device. Passing `device_map = 0` means put the whole model on GPU 0.\n\n                To have Accelerate compute the most optimized `device_map` automatically, set `device_map=\"auto\"`. For\n                more information about each option see [designing a device\n                map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).\n            max_memory (`Dict`, *optional*):\n                A dictionary device identifier to maximum memory if using `device_map`. Will default to the maximum memory available for each\n                GPU and the available CPU RAM if unset.\n            tp_plan (`Optional[Union[dict, str]]`, *optional*):\n                A torch tensor parallel plan, see [here](https://pytorch.org/tutorials/intermediate/TP_tutorial.html). Use `tp_plan=\"auto\"` to\n                use the predefined plan based on the model. If it's a dict, then it should match between module names and desired layout.\n                Note that if you use it, you should launch your script accordingly with `torchrun [args] script.py`. This will be much\n                faster than using a `device_map`, but has limitations.\n            tp_size (`str`, *optional*):\n                A torch tensor parallel degree. If not provided would default to world size.\n            device_mesh (`torch.distributed.DeviceMesh`, *optional*):\n                A torch device mesh. If not provided would default to world size. Used only for tensor parallel for now.\n                If provided, it has to contain dimension named `\"tp\"` in case it's > 1 dimensional, this dimension will be used for tensor parallelism\n            offload_folder (`str` or `os.PathLike`, *optional*):\n                If the `device_map` contains any value `\"disk\"`, the folder where we will offload weights.\n            offload_buffers (`bool`, *optional*):\n                Whether or not to offload the buffers with the model parameters.\n            quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):\n                A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g\n                bitsandbytes, gptq).\n            subfolder (`str`, *optional*, defaults to `\"\"`):\n                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n                specify the folder name here.\n            variant (`str`, *optional*):\n                If specified load weights from `variant` filename, *e.g.* pytorch_model.<variant>.bin.\n            use_safetensors (`bool`, *optional*, defaults to `None`):\n                Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`\n                is not installed, it will be set to `False`.\n            weights_only (`bool`, *optional*, defaults to `True`):\n                Indicates whether unpickler should be restricted to loading only tensors, primitive types,\n                dictionaries and any types added via torch.serialization.add_safe_globals().\n                When set to False, we can load wrapper tensor subclass weights.\n            key_mapping (`dict[str, str], *optional*):\n                A potential mapping of the weight names if using a model on the Hub which is compatible to a Transformers\n                architecture, but was not converted accordingly.\n            kwargs (remaining dictionary of keyword arguments, *optional*):\n                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\n                automatically loaded:\n\n                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\n                      underlying model's `__init__` method (we assume all relevant updates to the configuration have\n                      already been done)\n                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class\n                      initialization function ([`~PreTrainedConfig.from_pretrained`]). Each key of `kwargs` that\n                      corresponds to a configuration attribute will be used to override said attribute with the\n                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\n                      will be passed to the underlying model's `__init__` function.\n\n        <Tip>\n\n        Activate the special [\"offline-mode\"](https://huggingface.co/transformers/installation.html#offline-mode) to\n        use this method in a firewalled environment.\n\n        </Tip>\n\n        Examples:\n\n        ```python\n        >>> from transformers import BertConfig, BertModel\n\n        >>> # Download model and configuration from huggingface.co and cache.\n        >>> model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n        >>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).\n        >>> model = BertModel.from_pretrained(\"./test/saved_model/\")\n        >>> # Update configuration during loading.\n        >>> model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\", output_attentions=True)\n        >>> assert model.config.output_attentions == True\n        ```\n        \"\"\"\n        state_dict = kwargs.pop(\"state_dict\", None)\n        proxies = kwargs.pop(\"proxies\", None)\n        output_loading_info = kwargs.pop(\"output_loading_info\", False)\n        from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n        from_auto_class = kwargs.pop(\"_from_auto\", False)\n        dtype = kwargs.pop(\"dtype\", None)\n        torch_dtype = kwargs.pop(\"torch_dtype\", None)  # kept for BC\n        device_map = kwargs.pop(\"device_map\", None)\n        max_memory = kwargs.pop(\"max_memory\", None)\n        offload_folder = kwargs.pop(\"offload_folder\", None)\n        offload_buffers = kwargs.pop(\"offload_buffers\", False)\n        quantization_config = kwargs.pop(\"quantization_config\", None)\n        subfolder = kwargs.pop(\"subfolder\", \"\")\n        commit_hash = kwargs.pop(\"_commit_hash\", None)\n        variant = kwargs.pop(\"variant\", None)\n        adapter_kwargs = kwargs.pop(\"adapter_kwargs\", {})\n        adapter_name = kwargs.pop(\"adapter_name\", \"default\")\n        generation_config = kwargs.pop(\"generation_config\", None)\n        gguf_file = kwargs.pop(\"gguf_file\", None)\n        tp_plan = kwargs.pop(\"tp_plan\", None)\n        tp_size = kwargs.pop(\"tp_size\", None)\n        distributed_config: DistributedConfig = kwargs.pop(\"distributed_config\", None)\n        device_mesh = kwargs.pop(\"device_mesh\", None)\n        trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n        use_kernels = kwargs.pop(\"use_kernels\", False)\n        kernel_config = kwargs.pop(\"kernel_config\", None)\n\n        key_mapping = kwargs.pop(\"key_mapping\", None)\n        # Load models with key mapping\n        if key_mapping is None and any(\n            allowed_name in class_name.__name__.lower() for class_name in cls.__mro__[:-1] for allowed_name in VLMS\n        ):\n            key_mapping = cls._checkpoint_conversion_mapping\n\n        if distributed_config is not None and tp_plan is None:\n            tp_plan = \"auto\"\n\n        # Not used anymore -- remove them from the kwargs\n        for name in [\"mirror\", \"_fast_init\", \"low_cpu_mem_usage\", \"from_tf\", \"from_flax\", \"offload_state_dict\"]:\n            _ = kwargs.pop(name, None)\n\n        # For BC on torch_dtype argument\n        if torch_dtype is not None:\n            dtype = dtype if dtype is not None else torch_dtype\n\n        if is_offline_mode() and not local_files_only:\n            local_files_only = True\n\n        download_kwargs = {\n            \"cache_dir\": cache_dir,\n            \"force_download\": force_download,\n            \"proxies\": proxies,\n            \"local_files_only\": local_files_only,\n            \"token\": token,\n            \"revision\": revision,\n            \"subfolder\": subfolder,\n        }\n        download_kwargs_with_commit = {**download_kwargs, \"commit_hash\": commit_hash}\n\n        if state_dict is not None and (pretrained_model_name_or_path is not None or gguf_file is not None):\n            raise ValueError(\n                \"`state_dict` cannot be passed together with a model name or a `gguf_file`. Use one of the two loading strategies.\"\n            )\n\n        if device_map == \"auto\" and int(os.environ.get(\"WORLD_SIZE\", \"0\")):\n            logger.info(\n                \"You've set device_map=`auto` while triggering a distributed run with torchrun. This might lead to unexpected behavior. \"\n                \"If your plan is to load the model on each device, you should set device_map={\"\n                \": PartialState().process_index} where PartialState comes from accelerate library\"\n            )\n\n        if tp_plan is not None or tp_size is not None:  # TP warnings, and setup\n            device_map, device_mesh, tp_size = initialize_tensor_parallelism(\n                tp_plan, tp_size=tp_size, device_mesh=device_mesh, device_map=device_map\n            )\n\n        if gguf_file is not None and not is_accelerate_available():\n            raise ValueError(\"accelerate is required when loading a GGUF file `pip install accelerate`.\")\n\n        if adapter_kwargs is None:\n            adapter_kwargs = {}\n\n        _adapter_model_path, pretrained_model_name_or_path = maybe_load_adapters(\n            pretrained_model_name_or_path,\n            download_kwargs_with_commit,\n            **adapter_kwargs,\n        )\n        device_map = check_and_set_device_map(device_map)  # warn, error and fix the device map\n\n        user_agent = {\"file_type\": \"model\", \"framework\": \"pytorch\", \"from_auto_class\": from_auto_class}\n        if from_pipeline is not None:\n            user_agent[\"using_pipeline\"] = from_pipeline\n\n        # Load config if we don't provide a configuration\n        if not isinstance(config, PreTrainedConfig):\n            config_path = config if config is not None else pretrained_model_name_or_path\n            config, model_kwargs = cls.config_class.from_pretrained(\n                config_path,\n                return_unused_kwargs=True,\n                gguf_file=gguf_file,\n                _from_auto=from_auto_class,\n                _from_pipeline=from_pipeline,\n                **download_kwargs,\n                **kwargs,\n            )\n            if \"gguf_file\" in model_kwargs:\n                model_kwargs.pop(\"gguf_file\")\n            commit_hash = model_kwargs.pop(\"_commit_hash\", commit_hash)\n        else:\n            config = copy.deepcopy(config)\n            model_kwargs = kwargs\n            commit_hash = getattr(config, \"_commit_hash\", commit_hash)\n\n        download_kwargs_with_commit[\"commit_hash\"] = commit_hash\n\n        # Because some composite configs call super().__init__ before instantiating the sub-configs, we need this call\n        # to correctly redispatch recursively if the kwarg is provided\n        if \"attn_implementation\" in kwargs:\n            config._attn_implementation = kwargs.pop(\"attn_implementation\")\n\n        hf_quantizer, config, dtype, device_map = get_hf_quantizer(\n            config, quantization_config, dtype, device_map, weights_only, user_agent\n        )\n\n        if gguf_file:\n            if hf_quantizer is not None:\n                raise ValueError(\n                    \"You cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub.\"\n                )\n            if device_map is not None and (\n                (isinstance(device_map, dict) and \"disk\" in device_map.values()) or \"disk\" in device_map\n            ):\n                raise RuntimeError(\n                    \"One or more modules is configured to be mapped to disk. Disk offload is not supported for models \"\n                    \"loaded from GGUF files.\"\n                )\n\n        if kernel_config is not None and not use_kernels:\n            logger.warning_once(\n                \"A kernel_config was provided but use_kernels is False; setting use_kernels=True automatically. To suppress this warning, explicitly set use_kernels to True.\"\n            )\n            use_kernels = True\n\n        checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n            pretrained_model_name_or_path=pretrained_model_name_or_path,\n            variant=variant,\n            gguf_file=gguf_file,\n            use_safetensors=use_safetensors,\n            download_kwargs=download_kwargs_with_commit,\n            user_agent=user_agent,\n            is_remote_code=cls._auto_class is not None,\n            transformers_explicit_filename=getattr(config, \"transformers_weights\", None),\n        )\n\n        is_quantized = hf_quantizer is not None\n\n        if gguf_file:\n            from .modeling_gguf_pytorch_utils import load_gguf_checkpoint\n\n            # we need a dummy model to get the state_dict - for this reason, we keep the state_dict as if it was\n            # passed directly as a kwarg from now on\n            with torch.device(\"meta\"):\n                dummy_model = cls(config)\n            state_dict = load_gguf_checkpoint(checkpoint_files[0], return_tensors=True, model_to_load=dummy_model)[\n                \"tensors\"\n            ]\n\n        # Find the correct dtype based on current state\n        config, dtype, dtype_orig = _get_dtype(\n            cls, dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only\n        )\n\n        config.name_or_path = pretrained_model_name_or_path\n        model_init_context = cls.get_init_context(is_quantized, _is_ds_init_called)\n        config = copy.deepcopy(config)  # We do not want to modify the config inplace in from_pretrained.\n        with ContextManagers(model_init_context):\n            # Let's make sure we don't run the init function of buffer modules\n            model = cls(config, *model_args, **model_kwargs)\n\n        # Potentially upcast some modules to avoid loosing precision\n        model.upcast_modules_in_fp32(hf_quantizer, dtype)\n        # Make sure to tie the weights correctly\n        model.tie_weights()\n\n        # make sure we use the model's config since the __init__ call might have copied it\n        config = model.config\n\n        if hf_quantizer is not None:  # replace module with quantized modules (does not touch weights)\n            hf_quantizer.preprocess_model(\n                model=model,\n                device_map=device_map,\n                keep_in_fp32_modules=model._keep_in_fp32_modules,\n                config=config,\n                checkpoint_files=checkpoint_files,\n                use_kernels=use_kernels,\n            )\n\n        if _torch_distributed_available and device_mesh is not None:  # add hooks to nn.Modules: no weights\n            model = distribute_model(model, tp_plan, distributed_config, device_mesh, tp_size)\n\n        # Prepare the full device map\n        if device_map is not None:\n            device_map = _get_device_map(model, device_map, max_memory, hf_quantizer, dtype)\n\n        # restore default dtype\n        if dtype_orig is not None:\n            torch.set_default_dtype(dtype_orig)\n\n        # Finalize model weight initialization\n        model, missing_keys, unexpected_keys, mismatched_keys, offload_index, error_msgs = cls._load_pretrained_model(\n            model,\n            state_dict,\n            checkpoint_files,\n            pretrained_model_name_or_path,\n            ignore_mismatched_sizes=ignore_mismatched_sizes,\n            sharded_metadata=sharded_metadata,\n            device_map=device_map,\n            disk_offload_folder=offload_folder,\n            dtype=dtype,\n            hf_quantizer=hf_quantizer,\n            device_mesh=device_mesh,\n            key_mapping=key_mapping,\n            weights_only=weights_only,\n        )\n\n        model.tie_weights()  # make sure token embedding weights are still tied if needed\n        model.eval()  # Set model in evaluation mode to deactivate DropOut modules by default\n        model.set_use_kernels(use_kernels, kernel_config)\n\n        # If it is a model with generation capabilities, attempt to load generation files (generation config,\n        # custom generate function)\n        if model.can_generate() and hasattr(model, \"adjust_generation_fn\"):\n            model.adjust_generation_fn(\n                generation_config,\n                from_auto_class,\n                from_pipeline,\n                pretrained_model_name_or_path,\n                **download_kwargs,\n                trust_remote_code=trust_remote_code,\n                **kwargs,\n            )\n\n        # for device_map=\"auto\" : dispatch model with hooks on all devices if necessary (not needed with a tp_plan, so we skip it as it slightly\n        # harm performances).\n        if device_map is not None and device_mesh is None:\n            accelerate_dispatch(model, hf_quantizer, device_map, offload_folder, offload_index, offload_buffers)\n\n        if hf_quantizer is not None:\n            model.hf_quantizer = hf_quantizer\n            hf_quantizer.postprocess_model(model, config=config)  # usually a no-op\n\n        if _adapter_model_path is not None:\n            adapter_kwargs[\"key_mapping\"] = key_mapping  # TODO: Dynamic weight loader for adapters\n            model.load_adapter(\n                _adapter_model_path,\n                adapter_name=adapter_name,\n                token=token,\n                adapter_kwargs=adapter_kwargs,\n            )\n\n        if output_loading_info:\n            loading_info = {\n                \"missing_keys\": missing_keys,\n                \"unexpected_keys\": unexpected_keys,\n                \"mismatched_keys\": mismatched_keys,\n                \"error_msgs\": error_msgs,\n            }\n            return model, loading_info\n        return model\n\n    @staticmethod\n    def _fix_state_dict_key_on_load(key: str) -> tuple[str, bool]:\n        \"\"\"Replace legacy parameter names with their modern equivalents. E.g. beta -> bias, gamma -> weight.\"\"\"\n        # Rename LayerNorm beta & gamma params for some early models ported from Tensorflow (e.g. Bert)\n        # This rename is logged.\n        if key.endswith(\"LayerNorm.beta\"):\n            return key.replace(\"LayerNorm.beta\", \"LayerNorm.bias\"), True\n        if key.endswith(\"LayerNorm.gamma\"):\n            return key.replace(\"LayerNorm.gamma\", \"LayerNorm.weight\"), True\n\n        # Rename weight norm parametrizations to match changes across torch versions.\n        # Impacts a number of speech/wav2vec models. e.g. Hubert, Wav2Vec2, and others.\n        # This rename is not logged.\n        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n            if key.endswith(\"weight_g\"):\n                return key.replace(\"weight_g\", \"parametrizations.weight.original0\"), True\n            if key.endswith(\"weight_v\"):\n                return key.replace(\"weight_v\", \"parametrizations.weight.original1\"), True\n        else:\n            if key.endswith(\"parametrizations.weight.original0\"):\n                return key.replace(\"parametrizations.weight.original0\", \"weight_g\"), True\n            if key.endswith(\"parametrizations.weight.original1\"):\n                return key.replace(\"parametrizations.weight.original1\", \"weight_v\"), True\n\n        return key, False\n\n    def _get_key_renaming_mapping(\n        self,\n        checkpoint_keys: list[str],\n        key_mapping: Optional[dict[str, str]] = None,\n        loading_base_model_from_task_state_dict: bool = False,\n        loading_task_model_from_base_state_dict: bool = False,\n    ):\n        \"\"\"\n        Compute a mapping between the serialized keys on disk `checkpoint_keys`, and the keys that the model\n        that we are loading expects. This is the single entry point for key renaming that will be used during\n        loading.\n        Log if any parameters have been renamed.\n        \"\"\"\n        prefix = self.base_model_prefix\n        _prefix = f\"{prefix}.\"\n\n        if loading_task_model_from_base_state_dict:\n            task_specific_expected_keys, base_model_keys = [], []\n            for key in self.state_dict():\n                if key.startswith(_prefix):\n                    base_model_keys.append(key[len(_prefix) :])\n                else:\n                    task_specific_expected_keys.append(key)\n\n        renamed_keys = {}\n        key_renaming_mapping = {}\n        for key in checkpoint_keys:\n            # Class specific rename\n            new_key, has_changed = self._fix_state_dict_key_on_load(key)\n\n            # Optionally map the key according to `key_mapping`\n            if key_mapping is not None:\n                for pattern, replacement in key_mapping.items():\n                    new_key, n_replace = re.subn(pattern, replacement, new_key)\n                    # Early exit of the loop\n                    if n_replace > 0:\n                        has_changed = True\n                        break\n\n            # In this case, we need to add the prefix to the keys, to match them to the expected keys\n            if loading_task_model_from_base_state_dict:\n                # small sanity check: if we find a key that is only part of the task-specific keys, we raise\n                # (if it's also part of the base model, we do not raise and assume it comes from there)\n                if new_key in task_specific_expected_keys and new_key not in base_model_keys:\n                    raise ValueError(\n                        \"The state dictionary of the model you are trying to load is corrupted. Are you sure it was \"\n                        \"properly saved?\"\n                    )\n                new_key = \".\".join([prefix, new_key])\n            # In this case we need to remove the prefix from the key to match them to the expected keys, and use\n            # only the keys starting with the prefix\n            elif loading_base_model_from_task_state_dict:\n                if not new_key.startswith(_prefix):\n                    continue\n                new_key = new_key[len(_prefix) :]\n\n            key_renaming_mapping[key] = new_key\n\n            # track gamma/beta rename for logging\n            if has_changed:\n                if key.endswith(\"LayerNorm.gamma\"):\n                    renamed_keys[\"LayerNorm.gamma\"] = (key, new_key)\n                elif key.endswith(\"LayerNorm.beta\"):\n                    renamed_keys[\"LayerNorm.beta\"] = (key, new_key)\n\n        if renamed_keys:\n            warning_msg = f\"A pretrained model of type `{self.__class__.__name__}` \"\n            warning_msg += \"contains parameters that have been renamed internally (a few are listed below but more are present in the model):\\n\"\n            for old_key, new_key in renamed_keys.values():\n                warning_msg += f\"* `{old_key}` -> `{new_key}`\\n\"\n            warning_msg += \"If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\"\n            logger.info_once(warning_msg)\n\n        return key_renaming_mapping\n\n    @staticmethod\n    def _fix_state_dict_key_on_save(key) -> tuple[str, bool]:\n        \"\"\"\n        Similar to `_fix_state_dict_key_on_load` allows to define hook for state dict key renaming on model save.\n        Do nothing by default, but can be overridden in particular models.\n        \"\"\"\n        return key, False\n\n    def _fix_state_dict_keys_on_save(self, state_dict):\n        \"\"\"\n        Similar to `_fix_state_dict_keys_on_load` allows to define hook for state dict key renaming on model save.\n        Apply `_fix_state_dict_key_on_save` to all keys in `state_dict`.\n        \"\"\"\n        return {self._fix_state_dict_key_on_save(key)[0]: value for key, value in state_dict.items()}\n\n    @classmethod\n    def _load_pretrained_model(\n        cls,\n        model: \"PreTrainedModel\",\n        state_dict: Optional[dict],\n        checkpoint_files: Optional[list[str]],\n        pretrained_model_name_or_path: Optional[str],\n        ignore_mismatched_sizes: bool = False,\n        sharded_metadata: Optional[dict] = None,\n        device_map: Optional[dict] = None,\n        disk_offload_folder: Optional[str] = None,\n        dtype: Optional[torch.dtype] = None,\n        hf_quantizer: Optional[HfQuantizer] = None,\n        device_mesh: Optional[\"torch.distributed.device_mesh.DeviceMesh\"] = None,\n        key_mapping: Optional[dict[str, str]] = None,\n        weights_only: bool = True,\n    ):\n        # TODO: we should only be calling hf_quantizer.skip_placement or something like that\n        is_quantized = hf_quantizer is not None\n        is_hqq_or_quark = is_quantized and hf_quantizer.quantization_config.quant_method in {\n            QuantizationMethod.HQQ,\n            QuantizationMethod.QUARK,\n        }\n\n        # Get all the keys of the state dicts that we have to initialize the model with\n        if sharded_metadata is not None:\n            original_checkpoint_keys = sharded_metadata[\"all_checkpoint_keys\"]\n        elif state_dict is not None:\n            original_checkpoint_keys = list(state_dict.keys())\n        else:\n            original_checkpoint_keys = list(\n                load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n            )\n\n        # Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\n        prefix = model.base_model_prefix\n        has_prefix_module = any(s.startswith(prefix) for s in original_checkpoint_keys) if len(prefix) > 0 else False\n        expects_prefix_module = hasattr(model, prefix) if len(prefix) > 0 else False\n        loading_task_model_from_base_state_dict = not has_prefix_module and expects_prefix_module\n        loading_base_model_from_task_state_dict = has_prefix_module and not expects_prefix_module\n\n        # Find the key names that the model expects from the serialized keys\n        key_renaming_mapping = model._get_key_renaming_mapping(\n            original_checkpoint_keys,\n            key_mapping,\n            loading_base_model_from_task_state_dict,\n            loading_task_model_from_base_state_dict,\n        )\n        checkpoint_keys = list(key_renaming_mapping.values())\n\n        # Find missing and unexpected keys from the state dict\n        missing_keys, unexpected_keys = _find_missing_and_unexpected_keys(\n            model, original_checkpoint_keys, checkpoint_keys, loading_base_model_from_task_state_dict, hf_quantizer\n        )\n        # Find all the keys with shape mismatch (if we ignore the mismatch, the weights need to be newly initialized the\n        # same way as missing keys)\n        mismatched_keys, mismatched_shapes = _find_mismatched_keys(\n            model,\n            state_dict,\n            checkpoint_files,\n            ignore_mismatched_sizes,\n            key_renaming_mapping,\n            is_quantized,\n            weights_only,\n        )\n\n        # We need to update both the mapping and the list of checkpoint keys to remove the mismatched and unexpected ones\n        key_renaming_mapping = {\n            k: v for k, v in key_renaming_mapping.items() if v not in mismatched_keys and v not in unexpected_keys\n        }\n        checkpoint_keys = list(key_renaming_mapping.values())\n\n        # Move missing (and potentially mismatched) keys back to cpu from meta device (because they won't be moved when\n        # loading the weights as they are not in the loaded state dict)\n        model._move_missing_keys_from_meta_to_cpu(missing_keys + mismatched_keys, dtype, hf_quantizer)\n\n        # correctly initialize the missing (and potentially mismatched) keys\n        model._initialize_missing_keys(missing_keys + mismatched_keys, is_quantized)\n\n        # Get reverse key mapping\n        reverse_key_renaming_mapping = {v: k for k, v in key_renaming_mapping.items()}\n\n        is_offloaded_safetensors = False\n        # This offload index if for params explicitly on the \"disk\" in the device_map\n        disk_offload_index = None\n        disk_only_shard_files = []\n        # Prepare parameters offloading if needed\n        if device_map is not None and \"disk\" in device_map.values():\n            disk_offload_index, disk_only_shard_files, is_offloaded_safetensors = accelerate_disk_offload(\n                disk_offload_folder,\n                checkpoint_files,\n                device_map,\n                checkpoint_keys,\n                key_renaming_mapping,\n                sharded_metadata,\n                dtype,\n                reverse_key_renaming_mapping,\n            )\n        # To be able to iterate, even if we don't use it if the state_dict is already provided\n        elif state_dict is not None:\n            checkpoint_files = [\"\"]\n\n        # Compute expected model keys\n        expected_keys = list(model.state_dict().keys())\n        if hf_quantizer is not None:\n            expected_keys = hf_quantizer.update_expected_keys(model, expected_keys, checkpoint_keys)\n\n        if logger.level >= logging.WARNING:\n            verify_tp_plan(expected_keys, getattr(model, \"_tp_plan\", None))\n\n        # Warmup cuda to load the weights much faster on devices\n        if device_map is not None and not is_hqq_or_quark:\n            expanded_device_map = expand_device_map(device_map, expected_keys)\n            caching_allocator_warmup(model, expanded_device_map, hf_quantizer)\n\n        # Prepare and compatabilize arguments for serial and parallel shard loading\n        args_list = [\n            (\n                shard_file,\n                state_dict,\n                disk_only_shard_files,\n                is_quantized,\n                device_map,\n                hf_quantizer,\n                key_renaming_mapping,\n                weights_only,\n                model,\n                reverse_key_renaming_mapping,\n                disk_offload_folder,\n                disk_offload_index,\n                device_mesh,\n            )\n            for shard_file in checkpoint_files\n        ]\n\n        error_msgs = []\n\n        if (\n            os.environ.get(\"HF_ENABLE_PARALLEL_LOADING\", \"\").upper() in ENV_VARS_TRUE_VALUES\n            and not is_deepspeed_zero3_enabled()\n        ):\n            _error_msgs, disk_offload_index = load_shard_files_with_threadpool(args_list)\n            error_msgs += _error_msgs\n        else:\n            if len(args_list) > 1:\n                args_list = logging.tqdm(args_list, desc=\"Loading checkpoint shards\")\n\n            for args in args_list:\n                _error_msgs, disk_offload_index = load_shard_file(args)\n                error_msgs += _error_msgs\n\n        # Save offloaded index if needed\n        if disk_offload_index is not None and len(disk_offload_index) > 0 and not is_offloaded_safetensors:\n            save_offload_index(disk_offload_index, disk_offload_folder)\n            disk_offload_index = None\n\n        # Post-processing for tensor parallelism\n        if device_mesh is not None:\n            # When using TP, the device map is a single device for all parameters\n            tp_device = list(device_map.values())[0]\n            # This is needed for the RotaryEmbedding, which was not initialized on the correct device as it is\n            # not part of the state_dict (persistent=False)\n            for buffer in model.buffers():\n                if buffer.device != tp_device:\n                    buffer.data = buffer.to(tp_device)\n\n            # In this case, the top-most task module weights were not moved to device and parallelized as they\n            # were not part of the loaded weights: do it now\n            if loading_task_model_from_base_state_dict:\n                parameters_to_initialize = {\n                    name: param for name, param in model.named_parameters() if not name.startswith(prefix)\n                }\n                for name, param in parameters_to_initialize.items():\n                    # If it is still on meta here, it means that it's a tied weight that will be tied later anyway -> skip it\n                    if param.device.type == \"meta\":\n                        continue\n                    # Shard the param\n                    to_contiguous, casting_dtype = _infer_parameter_dtype(model, name, param)\n                    shard_and_distribute_module(\n                        model,\n                        param.to(tp_device),\n                        param,\n                        name,\n                        casting_dtype,\n                        to_contiguous,\n                        device_mesh.get_local_rank(),\n                        device_mesh,\n                    )\n\n        # Remove potential model-specific exceptions from the warnings\n        missing_keys, unexpected_keys = model._adjust_missing_and_unexpected_keys(\n            missing_keys, unexpected_keys, loading_task_model_from_base_state_dict\n        )\n\n        # TODO: separate this in another function: it's not core....\n        # All potential warnings/infos\n        if len(error_msgs) > 0:\n            error_msg = \"\\n\\t\".join(error_msgs)\n            if \"size mismatch\" in error_msg:\n                error_msg += (\n                    \"\\n\\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\"\n                )\n            raise RuntimeError(f\"Error(s) in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\")\n        if len(unexpected_keys) > 0:\n            archs = [] if model.config.architectures is None else model.config.architectures\n            warner = logger.warning if model.__class__.__name__ in archs else logger.info\n            warner(\n                f\"Some weights of the model checkpoint at {pretrained_model_name_or_path} were not used when\"\n                f\" initializing {model.__class__.__name__}: {update_key_name(unexpected_keys)}\\n- This IS expected if you are\"\n                f\" initializing {model.__class__.__name__} from the checkpoint of a model trained on another task or\"\n                \" with another architecture (e.g. initializing a BertForSequenceClassification model from a\"\n                \" BertForPreTraining model).\\n- This IS NOT expected if you are initializing\"\n                f\" {model.__class__.__name__} from the checkpoint of a model that you expect to be exactly identical\"\n                \" (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\"\n            )\n        if len(missing_keys) > 0:\n            logger.warning(\n                f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n                f\" {pretrained_model_name_or_path} and are newly initialized: {update_key_name(missing_keys)}\\nYou should probably\"\n                \" TRAIN this model on a down-stream task to be able to use it for predictions and inference.\"\n            )\n        if len(mismatched_keys) > 0:\n            mismatched_warning = \"\\n\".join(\n                [\n                    f\"- {key}: found shape {shape1} in the checkpoint and {shape2} in the model instantiated\"\n                    for key, (shape1, shape2) in zip(mismatched_keys, mismatched_shapes)\n                ]\n            )\n            logger.warning(\n                f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n                f\" {pretrained_model_name_or_path} and are newly initialized because the shapes did not\"\n                f\" match:\\n{mismatched_warning}\\nYou should probably TRAIN this model on a down-stream task to be able\"\n                \" to use it for predictions and inference.\"\n            )\n\n        return model, missing_keys, unexpected_keys, mismatched_keys, disk_offload_index, error_msgs\n\n    def retrieve_modules_from_names(self, names, add_prefix=False, remove_prefix=False):\n        module_keys = {\".\".join(key.split(\".\")[:-1]) for key in names}\n\n        # torch.nn.ParameterList is a special case where two parameter keywords\n        # are appended to the module name, *e.g.* bert.special_embeddings.0\n        module_keys = module_keys.union(\n            {\".\".join(key.split(\".\")[:-2]) for key in names if len(key) > 0 and key[-1].isdigit()}\n        )\n\n        retrieved_modules = []\n        # retrieve all modules that has at least one missing weight name\n        for name, module in self.named_modules():\n            if remove_prefix:\n                _prefix = f\"{self.base_model_prefix}.\"\n                name = name.removeprefix(_prefix)\n            elif add_prefix:\n                name = \".\".join([self.base_model_prefix, name]) if len(name) > 0 else self.base_model_prefix\n\n            if name in module_keys:\n                retrieved_modules.append(module)\n\n        return retrieved_modules\n\n    @classmethod\n    def register_for_auto_class(cls, auto_class=\"AutoModel\"):\n        \"\"\"\n        Register this class with a given auto class. This should only be used for custom models as the ones in the\n        library are already mapped with an auto class.\n\n\n\n        Args:\n            auto_class (`str` or `type`, *optional*, defaults to `\"AutoModel\"`):\n                The auto class to register this new model with.\n        \"\"\"\n        if not isinstance(auto_class, str):\n            auto_class = auto_class.__name__\n\n        import transformers.models.auto as auto_module\n\n        if not hasattr(auto_module, auto_class):\n            raise ValueError(f\"{auto_class} is not a valid auto class.\")\n\n        cls._auto_class = auto_class\n\n    def warn_if_padding_and_no_attention_mask(self, input_ids, attention_mask):\n        \"\"\"\n        Shows a one-time warning if the input_ids appear to contain padding and no attention mask was given.\n        \"\"\"\n\n        # Skip the check during tracing.\n        if is_tracing(input_ids):\n            return\n\n        if (attention_mask is not None) or (self.config.pad_token_id is None):\n            return\n\n        # Check only the first and last input IDs to reduce overhead.\n        if self.config.pad_token_id in input_ids[:, [-1, 0]]:\n            warn_string = (\n                \"We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \"\n                \"https://huggingface.co/docs/transformers/troubleshooting\"\n                \"#incorrect-output-when-padding-tokens-arent-masked.\"\n            )\n\n            # If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an\n            # attention_mask or not. In this case, we should still show a warning because this is a rare case.\n            if (\n                (self.config.bos_token_id is not None and self.config.bos_token_id == self.config.pad_token_id)\n                or (self.config.eos_token_id is not None and self.config.eos_token_id == self.config.pad_token_id)\n                or (self.config.sep_token_id is not None and self.config.sep_token_id == self.config.pad_token_id)\n            ):\n                warn_string += (\n                    f\"\\nYou may ignore this warning if your `pad_token_id` ({self.config.pad_token_id}) is identical \"\n                    f\"to the `bos_token_id` ({self.config.bos_token_id}), `eos_token_id` ({self.config.eos_token_id}), \"\n                    f\"or the `sep_token_id` ({self.config.sep_token_id}), and your input is not padded.\"\n                )\n\n            logger.warning_once(warn_string)\n\n    @property\n    def supports_tp_plan(self):\n        \"\"\"\n        Returns whether the model has a tensor parallelism plan.\n        \"\"\"\n        if self._tp_plan is not None:\n            return True\n        # Check if base model has a TP plan\n        if getattr(self.base_model, \"_tp_plan\", None) is not None:\n            return True\n        if self.config.base_model_tp_plan is not None:\n            return True\n        return False\n\n    @property\n    def tp_size(self):\n        \"\"\"\n        Returns the model's tensor parallelism degree.\n        \"\"\"\n        # if None, the model didn't undergo tensor parallel sharding\n        return self._tp_size\n\n    @property\n    def supports_pp_plan(self):\n        if self._pp_plan is not None:\n            return True\n        # Check if base model has PP plan\n        if getattr(self.base_model, \"_pp_plan\", None) is not None:\n            return True\n        return False\n\n    @property\n    def loss_function(self):\n        if hasattr(self, \"_loss_function\"):\n            return self._loss_function\n\n        loss_type = getattr(self, \"loss_type\", None)\n\n        if loss_type is None or loss_type not in LOSS_MAPPING:\n            logger.warning_once(\n                f\"`loss_type={loss_type}` was set in the config but it is unrecognized. \"\n                f\"Using the default loss: `ForCausalLMLoss`.\"\n            )\n            loss_type = \"ForCausalLM\"\n        return LOSS_MAPPING[loss_type]\n\n    @loss_function.setter\n    def loss_function(self, value):\n        self._loss_function = value\n\n    def kernelize(self, mode=None):\n        if not is_kernels_available():\n            raise ValueError(\n                \"Kernels are not available. To use kernels, please install kernels using `pip install kernels`\"\n            )\n        from kernels import Device, Mode, kernelize\n\n        mode = Mode.INFERENCE if not self.training else Mode.TRAINING if mode is None else mode\n        kernelize(self, device=Device(type=self.device.type), mode=mode)\n        self._use_kernels = True\n\n    @property\n    def use_kernels(self) -> bool:\n        return getattr(self, \"_use_kernels\", False)\n\n    @use_kernels.setter\n    def use_kernels(self, value: bool) -> None:\n        # Avoid re-kernelizing if already enabled\n        if bool(value) and getattr(self, \"_use_kernels\", False):\n            return\n\n        if value:\n            self.kernelize()\n        else:\n            if getattr(self, \"_use_kernels\", False):\n                logger.warning_once(\n                    \"Disabling kernels at runtime is a no-op as there is no 'unkernelize' routine; keeping current kernels active.\"\n                )\n            self._use_kernels = False\n\n    def get_compiled_call(self, compile_config: Optional[CompileConfig]) -> Callable:\n        \"\"\"Return a `torch.compile`'d version of `self.__call__`. This is useful to dynamically choose between\n        non-compiled/compiled `forward` during inference, especially to switch between prefill (where we don't\n        want to use compiled version to avoid recomputing the graph with new shapes) and iterative decoding\n        (where we want the speed-ups of compiled version with static shapes).\"\"\"\n        # Only reset it if not present or different from previous config\n        if \"llama4\" in self.config.model_type:  # TODO try to enable for FULL COMPILE HYBRID CACHE SUPPORT\n            return self.__call__\n        compile_config = compile_config or CompileConfig()\n        default_config = getattr(self.generation_config, \"compile_config\", None) or CompileConfig()\n        if (\n            not hasattr(self, \"_compiled_call\")\n            or getattr(self, \"_last_compile_config\", default_config) != compile_config\n        ):\n            self._last_compile_config = compile_config\n            self._compiled_call = torch.compile(self.__call__, **compile_config.to_dict())\n        return self._compiled_call\n\n    @classmethod\n    def is_backend_compatible(cls):\n        return cls._supports_attention_backend\n\n    def _move_missing_keys_from_meta_to_cpu(\n        self, missing_keys: list[str], dtype: torch.dtype, hf_quantizer: Optional[HfQuantizer]\n    ) -> None:\n        \"\"\"Move the missing keys (keys that are part of the model parameters, but were NOT found in the loaded state dicts) back\n        from meta device to cpu.\n        \"\"\"\n        is_quantized = hf_quantizer is not None\n\n        # In this case we need to move everything back\n        if is_fsdp_enabled() and not is_local_dist_rank_0() and not is_quantized:\n            # We only do it for the parameters, as the buffers are not initialized on the meta device by default\n            for key, param in self.named_parameters():\n                value = torch.empty_like(param, dtype=dtype, device=\"cpu\")\n                _load_parameter_into_model(self, key, value)\n            return\n\n        model_state_dict = self.state_dict()\n        for key in missing_keys:\n            param = model_state_dict[key]\n            # Buffers are not initialized on the meta device, so we still need this check to avoid overwriting them\n            if param.device == torch.device(\"meta\"):\n                value = torch.empty_like(param, dtype=dtype, device=\"cpu\")\n                if not is_quantized or not hf_quantizer.param_needs_quantization(self, key):\n                    _load_parameter_into_model(self, key, value)\n                else:\n                    hf_quantizer.create_quantized_param(self, value, key, \"cpu\")\n\n    def _initialize_missing_keys(self, missing_keys: list[str], is_quantized: bool) -> None:\n        \"\"\"Initialize the missing keys (keys that are part of the model parameters, but were NOT found in the loaded state dicts), according to\n        `_initialize_weights`. Indeed, since the corresponding weights are missing from the state dict, they will not be replaced and need to\n        be initialized correctly (i.e. weight initialization distribution).\n        Also take care of setting the `_is_hf_initialized` flag for keys that are not missing.\n        \"\"\"\n        for key in self.state_dict():\n            # If it's part of the keys that will be loaded, mark it as already initialized\n            if key not in missing_keys:\n                param_or_buffer = self.get_parameter_or_buffer(key)\n                param_or_buffer._is_hf_initialized = True\n\n        def set_is_initialized_for_modules(module):\n            # A module is already initialized if and only if all its children are also already initialized, and all\n            # its immediate `nn.Parameter` and persistent buffers are also already initialized\n            if (\n                # All immediate children are initialized\n                all(getattr(child, \"_is_hf_initialized\", False) for child in module.children())\n                # All immediate parameters are initialized\n                and all(getattr(param, \"_is_hf_initialized\", False) for param in module.parameters(recurse=False))\n                # All immediate persistent buffers are initialized\n                and all(\n                    getattr(buffer, \"_is_hf_initialized\", False)\n                    for name, buffer in module.named_buffers(recurse=False)\n                    if name not in module._non_persistent_buffers_set\n                )\n            ):\n                module._is_hf_initialized = True\n\n        # Set the flag on the modules as well. We do it recursively (depth-first), as it's more efficient (we do not\n        # need to check the entire state dict of each module, only the immediate children, so we only iterate once over\n        # each param)\n        self.apply(set_is_initialized_for_modules)\n\n        # This will only initialize submodules that are not marked as initialized by the line above.\n        if is_deepspeed_zero3_enabled() and not is_quantized:\n            import deepspeed\n\n            # keep_vars=True as we need the original tensors, so that the \"_is_hf_initialized\" is present on them\n            not_initialized_parameters = list(\n                {v for v in self.state_dict(keep_vars=True).values() if not getattr(v, \"_is_hf_initialized\", False)}\n            )\n            with deepspeed.zero.GatheredParameters(not_initialized_parameters, modifier_rank=0):\n                self.initialize_weights()\n        else:\n            self.initialize_weights()\n\n    def _adjust_missing_and_unexpected_keys(\n        self, missing_keys: list[str], unexpected_keys: list[str], loading_task_model_from_base_state_dict: bool\n    ) -> tuple[list[str], list[str]]:\n        \"\"\"Adjust the `missing_keys` and `unexpected_keys` based on current model's exception rules, to avoid\n        raising unneeded warnings/errors.\n        \"\"\"\n        # Old checkpoints may have keys for rotary_emb.inv_freq for each layer, however we moved this buffer to the main model\n        # (so the buffer name has changed). Remove them in such a case. This is another exception that was not added to\n        # `_keys_to_ignore_on_load_unexpected` as it touches many models -> we add it manually to the existing patterns\n        has_inv_freq_buffers = any(buffer.endswith(\"rotary_emb.inv_freq\") for buffer, _ in self.named_buffers())\n        additional_unexpected_patterns = [r\"rotary_emb\\.inv_freq\"] if has_inv_freq_buffers else []\n\n        missing_patterns = self._keys_to_ignore_on_load_missing or []\n        unexpected_patterns = (self._keys_to_ignore_on_load_unexpected or []) + additional_unexpected_patterns\n        ignore_missing_regex, ignore_unexpected_regex = None, None\n        if len(missing_patterns) > 0:\n            ignore_missing_regex = re.compile(\"|\".join(rf\"({pattern})\" for pattern in missing_patterns))\n        if len(unexpected_patterns) > 0:\n            ignore_unexpected_regex = re.compile(\"|\".join(rf\"({pattern})\" for pattern in unexpected_patterns))\n\n        # Clean-up missing keys\n        if ignore_missing_regex is not None:\n            missing_keys = [key for key in missing_keys if ignore_missing_regex.search(key) is None]\n\n        # Clean-up unexpected keys\n        if ignore_unexpected_regex is not None:\n            unexpected_keys = [key for key in unexpected_keys if ignore_unexpected_regex.search(key) is None]\n\n        # Note: only the unexpected keys should remove the added prefix here, to correctly display the original name\n        # in the warnings. For missing keys, we should show the prefix in the warning as it's part of the final model\n        if loading_task_model_from_base_state_dict:\n            _prefix = f\"{self.base_model_prefix}.\"\n            unexpected_keys = [k.removeprefix(_prefix) for k in unexpected_keys]\n\n        return missing_keys, unexpected_keys\n\n    def get_parameter_or_buffer(self, target: str):\n        \"\"\"\n        Return the parameter or buffer given by `target` if it exists, otherwise throw an error. This combines\n        `get_parameter()` and `get_buffer()` in a single handy function. If the target is an `_extra_state` attribute,\n        it will return the extra state provided by the module. Note that it only work if `target` is a leaf of the model.\n        \"\"\"\n        try:\n            return self.get_parameter(target)\n        except AttributeError:\n            pass\n        try:\n            return self.get_buffer(target)\n        except AttributeError:\n            pass\n        module, param_name = get_module_from_name(self, target)\n        if (\n            param_name == \"_extra_state\"\n            and getattr(module.__class__, \"get_extra_state\", torch.nn.Module.get_extra_state)\n            is not torch.nn.Module.get_extra_state\n        ):\n            return module.get_extra_state()\n\n        raise AttributeError(f\"`{target}` is neither a parameter, buffer, nor extra state.\")\n\n    def train(self, mode: bool = True):\n        out = super().train(mode)\n        if self.use_kernels:\n            self.kernelize()\n        return out\n\n    def eval(self):\n        return self.train(False)\n\n    def upcast_modules_in_fp32(self, hf_quantizer: HfQuantizer | None, dtype: torch.dtype) -> None:\n        \"\"\"\n        Upcast modules defined in `_keep_in_fp32_modules` and `_keep_in_fp32_modules_strict` in fp32, if\n        `dtype` is different than fp32.\n        \"\"\"\n        # If the dtype is already fp32, we can skip\n        if dtype == torch.float32:\n            return\n\n        keep_in_fp32_modules = []\n        # The _keep_in_fp32_modules flag is only used to avoid bf16 -> fp16 casting precision issues. It was introduced\n        # in case of force loading a model that should stay bf16 in fp16 (which includes a few quantizers as this is a pre-processing\n        # step for e.g. bitsandbytes). See https://github.com/huggingface/transformers/issues/20287 for details.\n        if self._keep_in_fp32_modules is not None and (\n            dtype == torch.float16 or getattr(hf_quantizer, \"use_keep_in_fp32_modules\", False)\n        ):\n            keep_in_fp32_modules.extend(self._keep_in_fp32_modules)\n\n        if self._keep_in_fp32_modules_strict is not None and (dtype == torch.float16 or dtype == torch.bfloat16):\n            keep_in_fp32_modules.extend(self._keep_in_fp32_modules_strict)\n\n        if len(keep_in_fp32_modules) > 0:\n            # We need to match exact layers, so we add either `.` on each side, or start/end of string\n            keep_in_fp32_regex = re.compile(\"|\".join([rf\"((^|\\.){module}($|\\.))\" for module in keep_in_fp32_modules]))\n            for name, param in self.named_parameters():\n                if keep_in_fp32_regex.search(name):\n                    # param = param.to(torch.float32) does not work here as only in the local scope.\n                    param.data = param.data.to(torch.float32)\n\n\n\ndef caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: dict, hf_quantizer: Optional[HfQuantizer]):\n    \"\"\"This function warm-ups the caching allocator based on the size of the model tensors that will reside on each\n    device. It allows to have one large call to Malloc, instead of recursively calling it later when loading\n    the model, which is actually the loading speed bottleneck.\n    Calling this function allows to cut the model loading time by a very large margin.\n\n    A few facts related to loading speed (taking into account the use of this function):\n    - When loading a model the first time, it is usually slower than the subsequent times, because the OS is very likely\n    to cache the different state dicts (if enough resources/RAM are available)\n    - Trying to force the OS to cache the files in advance (by e.g. accessing a small portion of them) is really hard,\n    and not a good idea in general as this is low level OS optimizations that depend on resource usage anyway\n    - As of 18/03/2025, loading a Llama 70B model with TP takes ~1 min without file cache, and ~13s with full file cache.\n    The baseline, i.e. only loading the tensor shards on device and adjusting dtype (i.e. copying them) is ~5s with full cache.\n    These numbers are reported for TP on 4 H100 GPUs.\n    - It is useless to pre-allocate more than the model size in this function (i.e. using an `allocation_factor` > 1) as\n    cudaMalloc is not a bottleneck at all anymore\n    - Loading speed bottleneck is now almost only tensor copy (i.e. changing the dtype) and moving the tensors to the devices.\n    However, we cannot really improve on those aspects obviously, as the data needs to be moved/copied in the end.\n    \"\"\"\n    factor = 2 if hf_quantizer is None else hf_quantizer.get_accelerator_warm_up_factor()\n\n    # Remove disk, cpu and meta devices, and cast to proper torch.device\n    accelerator_device_map = {\n        param: torch.device(device) for param, device in expanded_device_map.items() if is_accelerator_device(device)\n    }\n    if not accelerator_device_map:\n        return\n\n    tp_plan = getattr(model, \"_tp_plan\", []) or []\n    tp_plan_regex = (\n        re.compile(\"|\".join([re.escape(plan) for plan in tp_plan]))\n        if _torch_distributed_available and torch.distributed.is_initialized()\n        else None\n    )\n    total_byte_count = defaultdict(lambda: 0)\n    tied_param_names = _get_tied_weight_keys(model)\n    for param_name, device in accelerator_device_map.items():\n        # Skip if the parameter has already been accounted for (tied weights)\n        if param_name in tied_param_names:\n            continue\n\n        # For example in the case of MXFP4 quantization, we need to update the param name to the original param name\n        # because the checkpoint contains blocks, and scales, but since we are dequantizing, we need to use the original param name\n        if hf_quantizer is not None:\n            param_name = hf_quantizer.get_param_name(param_name)\n\n        try:\n            param = model.get_parameter_or_buffer(param_name)\n        except AttributeError:\n            raise AttributeError(f\"Parameter {param_name} not found in model\")\n\n        # The dtype of different parameters may be different with composite models or `keep_in_fp32_modules`\n        param_byte_count = param.numel() * param.element_size()\n\n        if tp_plan_regex is not None:\n            generic_name = re.sub(r\"\\.\\d+\\.\", \".*.\", param_name)\n            param_byte_count //= torch.distributed.get_world_size() if tp_plan_regex.search(generic_name) else 1\n\n        total_byte_count[device] += param_byte_count\n\n    # This will kick off the caching allocator to avoid having to Malloc afterwards\n    for device, byte_count in total_byte_count.items():\n        if device.type in [\"cuda\", \"xpu\"]:\n            torch_accelerator_module = getattr(torch, device.type)\n            index = device.index if device.index is not None else torch_accelerator_module.current_device()\n            device_memory = torch_accelerator_module.mem_get_info(index)[0]\n            # Allow up to (max device memory - 1.2 GiB) in resource-constrained hardware configurations. Trying to reserve more\n            # than that amount might sometimes lead to unnecessary cuda/xpu OOM, if the last parameter to be loaded on the device is large,\n            # and the remaining reserved memory portion is smaller than the param size -> torch will then try to fully re-allocate all\n            # the param size, instead of using the remaining reserved part, and allocating only the difference, which can lead\n            # to OOM. See https://github.com/huggingface/transformers/issues/37436#issuecomment-2808982161 for more details.\n            # Note that we use an absolute value instead of device proportion here, as a 8GiB device could still allocate too much\n            # if using e.g. 90% of device size, while a 140GiB device would allocate too little\n            byte_count = min(byte_count, max(0, int(device_memory - 1.2 * 1024**3)))\n            # If there is *unused* reserved cuda/xpu memory, we can skip/reduce the allocation.\n            unused_memory = torch_accelerator_module.memory_reserved(\n                index\n            ) - torch_accelerator_module.memory_allocated(index)\n            byte_count = max(0, byte_count - unused_memory)\n        # Allocate memory\n        _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)"
                },
                "component_dependencies": {
                    "PreTrainedModel": [
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/distributed.py#DistributedConfig",
                        "transformers/dynamic_module_utils.py#custom_object_save",
                        "transformers/generation.py#CompileConfig",
                        "transformers/generation.py#GenerationConfig.from_model_config",
                        "transformers/integrations.py#PeftAdapterMixin",
                        "transformers/integrations.py#deepspeed_config",
                        "transformers/integrations.py#is_deepspeed_zero3_enabled",
                        "transformers/integrations.py#is_fsdp_enabled",
                        "transformers/integrations/accelerate.py#_get_device_map",
                        "transformers/integrations/accelerate.py#accelerate_disk_offload",
                        "transformers/integrations/accelerate.py#accelerate_dispatch",
                        "transformers/integrations/accelerate.py#check_and_set_device_map",
                        "transformers/integrations/accelerate.py#expand_device_map",
                        "transformers/integrations/accelerate.py#init_empty_weights",
                        "transformers/integrations/hub_kernels.py#is_kernel",
                        "transformers/integrations/hub_kernels.py#load_and_register_attn_kernel",
                        "transformers/integrations/peft.py#maybe_load_adapters",
                        "transformers/integrations/tensor_parallel.py#ALL_PARALLEL_STYLES",
                        "transformers/integrations/tensor_parallel.py#ALL_PARALLEL_STYLES.keys",
                        "transformers/integrations/tensor_parallel.py#_get_parameter_tp_plan",
                        "transformers/integrations/tensor_parallel.py#distribute_model",
                        "transformers/integrations/tensor_parallel.py#initialize_tensor_parallelism",
                        "transformers/integrations/tensor_parallel.py#repack_weights",
                        "transformers/integrations/tensor_parallel.py#replace_state_dict_local_with_dtensor",
                        "transformers/integrations/tensor_parallel.py#shard_and_distribute_module",
                        "transformers/integrations/tensor_parallel.py#verify_tp_plan",
                        "transformers/loss/loss_utils.py#LOSS_MAPPING",
                        "transformers/modeling_flash_attention_utils.py#lazy_import_flash_attention",
                        "transformers/modeling_utils.py#EmbeddingAccessMixin",
                        "transformers/modeling_utils.py#ModuleUtilsMixin",
                        "transformers/modeling_utils.py#SpecificPreTrainedModelType",
                        "transformers/modeling_utils.py#VLMS",
                        "transformers/modeling_utils.py#_add_variant",
                        "transformers/modeling_utils.py#_find_disjoint",
                        "transformers/modeling_utils.py#_find_identical",
                        "transformers/modeling_utils.py#_find_mismatched_keys",
                        "transformers/modeling_utils.py#_find_missing_and_unexpected_keys",
                        "transformers/modeling_utils.py#_get_dtype",
                        "transformers/modeling_utils.py#_get_resolved_checkpoint_files",
                        "transformers/modeling_utils.py#_get_tied_weight_keys",
                        "transformers/modeling_utils.py#_infer_parameter_dtype",
                        "transformers/modeling_utils.py#_init_weights",
                        "transformers/modeling_utils.py#_is_ds_init_called",
                        "transformers/modeling_utils.py#_is_dtensor_available",
                        "transformers/modeling_utils.py#_is_quantized",
                        "transformers/modeling_utils.py#_load_parameter_into_model",
                        "transformers/modeling_utils.py#_torch_distributed_available",
                        "transformers/modeling_utils.py#get_parameter_dtype",
                        "transformers/modeling_utils.py#is_accelerator_device",
                        "transformers/modeling_utils.py#is_local_dist_rank_0",
                        "transformers/modeling_utils.py#load_shard_file",
                        "transformers/modeling_utils.py#load_shard_files_with_threadpool",
                        "transformers/modeling_utils.py#load_state_dict",
                        "transformers/modeling_utils.py#logger",
                        "transformers/modeling_utils.py#no_init_weights",
                        "transformers/modeling_utils.py#restore_default_dtype",
                        "transformers/modeling_utils.py#set_quantized_state",
                        "transformers/modeling_utils.py#set_zero3_state",
                        "transformers/modeling_utils.py#unwrap_model",
                        "transformers/modeling_utils.py#update_key_name",
                        "transformers/pytorch_utils.py#id_tensor_storage",
                        "transformers/quantizers.py#HfQuantizer",
                        "transformers/quantizers/auto.py#get_hf_quantizer",
                        "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                        "transformers/utils.py#ADAPTER_SAFE_WEIGHTS_NAME",
                        "transformers/utils.py#ADAPTER_WEIGHTS_NAME",
                        "transformers/utils.py#ContextManagers",
                        "transformers/utils.py#DUMMY_INPUTS",
                        "transformers/utils.py#KernelConfig",
                        "transformers/utils.py#PushToHubMixin",
                        "transformers/utils.py#PushToHubMixin.push_to_hub",
                        "transformers/utils.py#SAFE_WEIGHTS_INDEX_NAME",
                        "transformers/utils.py#SAFE_WEIGHTS_NAME",
                        "transformers/utils.py#WEIGHTS_INDEX_NAME",
                        "transformers/utils.py#WEIGHTS_NAME",
                        "transformers/utils.py#is_accelerate_available",
                        "transformers/utils.py#is_flash_attn_2_available",
                        "transformers/utils.py#is_flash_attn_3_available",
                        "transformers/utils.py#is_kernels_available",
                        "transformers/utils.py#is_offline_mode",
                        "transformers/utils.py#is_torch_flex_attn_available",
                        "transformers/utils.py#is_torch_mlu_available",
                        "transformers/utils.py#is_torch_npu_available",
                        "transformers/utils.py#logging.WARNING",
                        "transformers/utils.py#logging.tqdm",
                        "transformers/utils/generic.py#OutputRecorder",
                        "transformers/utils/generic.py#_CAN_RECORD_REGISTRY",
                        "transformers/utils/hub.py#create_and_tag_model_card",
                        "transformers/utils/import_utils.py#ENV_VARS_TRUE_VALUES",
                        "transformers/utils/import_utils.py#is_huggingface_hub_greater_or_equal",
                        "transformers/utils/import_utils.py#is_tracing",
                        "transformers/utils/quantization_config.py#QuantizationMethod.BITS_AND_BYTES",
                        "transformers/utils/quantization_config.py#QuantizationMethod.GPTQ",
                        "transformers/utils/quantization_config.py#QuantizationMethod.HQQ",
                        "transformers/utils/quantization_config.py#QuantizationMethod.QUARK"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/models/mixtral/modeling_mixtral.py#MixtralAttention": {
                "sorted_modules": {
                    "MixtralAttention": "\n\nclass MixtralAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: MixtralConfig, layer_idx: int):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        self.head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n        self.scaling = self.head_dim**-0.5\n        self.attention_dropout = config.attention_dropout\n        self.is_causal = True\n        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n        attention_mask: Optional[torch.Tensor],\n        past_key_values: Optional[Cache] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs: Unpack[FlashAttentionKwargs],\n    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n        input_shape = hidden_states.shape[:-1]\n        hidden_shape = (*input_shape, -1, self.head_dim)\n\n        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n\n        cos, sin = position_embeddings\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_values is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        attention_interface: Callable = eager_attention_forward\n        if self.config._attn_implementation != \"eager\":\n            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n\n        attn_output, attn_weights = attention_interface(\n            self,\n            query_states,\n            key_states,\n            value_states,\n            attention_mask,\n            dropout=0.0 if not self.training else self.attention_dropout,\n            scaling=self.scaling,\n            sliding_window=getattr(self.config, \"sliding_window\", None),  # main diff with Llama\n            **kwargs,\n        )\n\n        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n        attn_output = self.o_proj(attn_output)\n        return attn_output, attn_weights"
                },
                "component_dependencies": {
                    "MixtralAttention": [
                        "transformers/cache_utils.py#Cache",
                        "transformers/modeling_flash_attention_utils.py#FlashAttentionKwargs",
                        "transformers/modeling_utils.py#ALL_ATTENTION_FUNCTIONS",
                        "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig",
                        "transformers/models/mixtral/modeling_mixtral.py#apply_rotary_pos_emb",
                        "transformers/models/mixtral/modeling_mixtral.py#eager_attention_forward",
                        "transformers/processing_utils.py#Unpack"
                    ]
                },
                "warning": null
            },
            "transformers/configuration_utils.py#PreTrainedConfig": {
                "sorted_modules": {
                    "PreTrainedConfig": "\n\nclass PreTrainedConfig(PushToHubMixin):\n    # no-format\n    r\"\"\"\n    Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as\n    methods for loading/downloading/saving configurations.\n\n    <Tip>\n\n    A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to\n    initialize a model does **not** load the model weights. It only affects the model's configuration.\n\n    </Tip>\n\n    Class attributes (overridden by derived classes):\n\n    - **model_type** (`str`) -- An identifier for the model type, serialized into the JSON file, and used to recreate\n      the correct object in [`~transformers.AutoConfig`].\n    - **has_no_defaults_at_init** (`bool`) -- Whether the config class can be initialized without providing input arguments.\n      Some configurations requires inputs to be defined at init and have no default values, usually these are composite configs,\n      (but not necessarily) such as [`~transformers.EncoderDecoderConfig`] or [`~RagConfig`]. They have to be initialized from\n      two or more configs of type [`~transformers.PreTrainedConfig`].\n    - **keys_to_ignore_at_inference** (`list[str]`) -- A list of keys to ignore by default when looking at dictionary\n      outputs of the model during inference.\n    - **attribute_map** (`dict[str, str]`) -- A dict that maps model specific attribute names to the standardized\n      naming of attributes.\n    - **base_model_tp_plan** (`dict[str, Any]`) -- A dict that maps sub-modules FQNs of a base model to a tensor\n      parallel plan applied to the sub-module when `model.tensor_parallel` is called.\n    - **base_model_pp_plan** (`dict[str, tuple[list[str]]]`) -- A dict that maps child-modules of a base model to a\n      pipeline parallel plan that enables users to place the child-module on the appropriate device.\n\n    Common attributes (present in all subclasses):\n\n    - **vocab_size** (`int`) -- The number of tokens in the vocabulary, which is also the first dimension of the\n      embeddings matrix (this attribute may be missing for models that don't have a text modality like ViT).\n    - **hidden_size** (`int`) -- The hidden size of the model.\n    - **num_attention_heads** (`int`) -- The number of attention heads used in the multi-head attention layers of the\n      model.\n    - **num_hidden_layers** (`int`) -- The number of blocks in the model.\n\n    <Tip warning={true}>\n\n    Setting parameters for sequence generation in the model config is deprecated. For backward compatibility, loading\n    some of them will still be possible, but attempting to overwrite them will throw an exception -- you should set\n    them in a [~transformers.GenerationConfig]. Check the documentation of [~transformers.GenerationConfig] for more\n    information about the individual parameters.\n\n    </Tip>\n\n    Arg:\n        name_or_path (`str`, *optional*, defaults to `\"\"`):\n            Store the string that was passed to [`PreTrainedModel.from_pretrained`] as `pretrained_model_name_or_path`\n            if the configuration was created with such a method.\n        output_hidden_states (`bool`, *optional*, defaults to `False`):\n            Whether or not the model should return all hidden-states.\n        output_attentions (`bool`, *optional*, defaults to `False`):\n            Whether or not the model should returns all attentions.\n        return_dict (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return a [`~transformers.utils.ModelOutput`] instead of a plain tuple.\n        is_encoder_decoder (`bool`, *optional*, defaults to `False`):\n            Whether the model is used as an encoder/decoder or not.\n        is_decoder (`bool`, *optional*, defaults to `False`):\n            Whether to only use the decoder in an encoder-decoder architecture, otherwise it has no effect on\n            decoder-only or encoder-only architectures.\n        cross_attention_hidden_size (`bool`, *optional*):\n            The hidden size of the cross-attention layer in case the model is used as a decoder in an encoder-decoder\n            setting and the cross-attention hidden dimension differs from `self.config.hidden_size`.\n        add_cross_attention (`bool`, *optional*, defaults to `False`):\n            Whether cross-attention layers should be added to the model. Note, this option is only relevant for models\n            that can be used as decoder models within the [`EncoderDecoderModel`] class, which consists of all models\n            in `AUTO_MODELS_FOR_CAUSAL_LM`.\n        tie_encoder_decoder (`bool`, *optional*, defaults to `False`):\n            Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder\n            and decoder model to have the exact same parameter names.\n        chunk_size_feed_forward (`int`, *optional*, defaults to `0`):\n            The chunk size of all feed forward layers in the residual attention blocks. A chunk size of `0` means that\n            the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes `n` <\n            sequence_length embeddings at a time. For more information on feed forward chunking, see [How does Feed\n            Forward Chunking work?](../glossary.html#feed-forward-chunking).\n\n        > Parameters for fine-tuning tasks\n\n        architectures (`list[str]`, *optional*):\n            Model architectures that can be used with the model pretrained weights.\n        finetuning_task (`str`, *optional*):\n            Name of the task used to fine-tune the model.\n        id2label (`dict[int, str]`, *optional*):\n            A map from index (for instance prediction index, or target index) to label.\n        label2id (`dict[str, int]`, *optional*):\n            A map from label to index for the model.\n        num_labels (`int`, *optional*):\n            Number of labels to use in the last layer added to the model, typically for a classification task.\n        task_specific_params (`dict[str, Any]`, *optional*):\n            Additional keyword arguments to store for the current task.\n        problem_type (`str`, *optional*):\n            Problem type for `XxxForSequenceClassification` models. Can be one of `\"regression\"`,\n            `\"single_label_classification\"` or `\"multi_label_classification\"`.\n\n        > Parameters linked to the tokenizer\n\n        tokenizer_class (`str`, *optional*):\n            The name of the associated tokenizer class to use (if none is set, will use the tokenizer associated to the\n            model by default).\n        prefix (`str`, *optional*):\n            A specific prompt that should be added at the beginning of each text before calling the model.\n        bos_token_id (`int`, *optional*):\n            The id of the _beginning-of-stream_ token.\n        pad_token_id (`int`, *optional*):\n            The id of the _padding_ token.\n        eos_token_id (`int`, *optional*):\n            The id of the _end-of-stream_ token.\n        decoder_start_token_id (`int`, *optional*):\n            If an encoder-decoder model starts decoding with a different token than _bos_, the id of that token.\n        sep_token_id (`int`, *optional*):\n            The id of the _separation_ token.\n\n        > PyTorch specific parameters\n\n        tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n            Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the\n            model has a output word embedding layer.\n        dtype (`str`, *optional*):\n            The `dtype` of the weights. This attribute can be used to initialize the model to a non-default `dtype`\n            (which is normally `float32`) and thus allow for optimal storage allocation. For example, if the saved\n            model is `float16`, ideally we want to load it back using the minimal amount of memory needed to load\n            `float16` weights.\n    \"\"\"\n\n    model_type: str = \"\"\n    base_config_key: str = \"\"\n    sub_configs: dict[str, type[\"PreTrainedConfig\"]] = {}\n    has_no_defaults_at_init: bool = False\n    attribute_map: dict[str, str] = {}\n    base_model_tp_plan: Optional[dict[str, Any]] = None\n    base_model_pp_plan: Optional[dict[str, tuple[list[str]]]] = None\n    base_model_ep_plan: Optional[dict[str, tuple[list[str]]]] = None\n    _auto_class: Optional[str] = None\n\n    def __setattr__(self, key, value):\n        if key in super().__getattribute__(\"attribute_map\"):\n            key = super().__getattribute__(\"attribute_map\")[key]\n        super().__setattr__(key, value)\n\n    def __getattribute__(self, key):\n        if key != \"attribute_map\" and key in super().__getattribute__(\"attribute_map\"):\n            key = super().__getattribute__(\"attribute_map\")[key]\n        return super().__getattribute__(key)\n\n    def __init__(\n        self,\n        *,\n        # All models common arguments\n        output_hidden_states: bool = False,\n        output_attentions: bool = False,\n        return_dict: bool = True,\n        dtype: Optional[Union[str, \"torch.dtype\"]] = None,\n        # Common arguments\n        tie_word_embeddings: bool = True,\n        chunk_size_feed_forward: int = 0,\n        is_encoder_decoder: bool = False,\n        is_decoder: bool = False,\n        cross_attention_hidden_size: Optional[int] = None,\n        add_cross_attention: bool = False,\n        tie_encoder_decoder: bool = False,\n        # Fine-tuning task arguments\n        architectures: Optional[list[str]] = None,\n        finetuning_task: Optional[str] = None,\n        id2label: Optional[dict[int, str]] = None,\n        label2id: Optional[dict[str, int]] = None,\n        num_labels: Optional[int] = None,\n        task_specific_params: Optional[dict[str, Any]] = None,\n        problem_type: Optional[str] = None,\n        # Tokenizer kwargs\n        tokenizer_class: Optional[str] = None,\n        prefix: Optional[str] = None,\n        bos_token_id: Optional[int] = None,\n        pad_token_id: Optional[int] = None,\n        eos_token_id: Optional[int] = None,\n        sep_token_id: Optional[int] = None,\n        decoder_start_token_id: Optional[int] = None,\n        **kwargs,\n    ):\n        # Validation for some arguments\n        if label2id is not None and not isinstance(label2id, dict):\n            raise ValueError(\"Argument label2id should be a dictionary.\")\n        if id2label is not None and not isinstance(id2label, dict):\n            raise ValueError(\"Argument id2label should be a dictionary.\")\n        if num_labels is not None and id2label is not None and len(id2label) != num_labels:\n            logger.warning(\n                f\"You passed `num_labels={num_labels}` which is incompatible to \"\n                f\"the `id2label` map of length `{len(id2label)}`.\"\n            )\n        if problem_type is not None and problem_type not in (\n            \"regression\",\n            \"single_label_classification\",\n            \"multi_label_classification\",\n        ):\n            raise ValueError(\n                f\"The config parameter `problem_type` was not understood: received {problem_type} \"\n                \"but only 'regression', 'single_label_classification' and 'multi_label_classification' are valid.\"\n            )\n        # BC for the `torch_dtype` argument instead of the simpler `dtype`\n        # Do not warn, as it would otherwise always be triggered since most configs on the hub have `torch_dtype`\n        if (torch_dtype := kwargs.pop(\"torch_dtype\", None)) is not None:\n            # If both are provided, keep `dtype`\n            dtype = dtype if dtype is not None else torch_dtype\n        if dtype is not None and isinstance(dtype, str) and is_torch_available():\n            # we will start using self.dtype in v5, but to be consistent with\n            # from_pretrained's dtype arg convert it to an actual torch.dtype object\n            import torch\n\n            dtype = getattr(torch, dtype)\n\n        # Attributes common for all models\n        self.return_dict = return_dict\n        self.output_hidden_states = output_hidden_states\n        self.dtype = dtype\n        self._output_attentions = output_attentions  # has public property\n\n        # Less common kwargs, only used by some models\n        self.tie_word_embeddings = tie_word_embeddings\n        self.chunk_size_feed_forward = chunk_size_feed_forward\n\n        # Encoder-decoder models attributes\n        self.is_encoder_decoder = is_encoder_decoder\n        self.is_decoder = is_decoder  # used in encoder-decoder models to differentiate encoder from decoder\n        self.cross_attention_hidden_size = cross_attention_hidden_size\n        self.add_cross_attention = add_cross_attention\n        self.tie_encoder_decoder = tie_encoder_decoder\n\n        # Fine-tuning task attributes\n        self.architectures = architectures\n        self.finetuning_task = finetuning_task\n        self.id2label = id2label\n        self.label2id = label2id\n        self.task_specific_params = task_specific_params\n        self.problem_type = problem_type\n\n        if self.id2label is None:\n            self._create_id_label_maps(num_labels if num_labels is not None else 2)\n        else:\n            # Keys are always strings in JSON so convert ids to int here.\n            self.id2label = {int(key): value for key, value in self.id2label.items()}\n\n        # Tokenizer attributes\n        self.tokenizer_class = tokenizer_class\n        self.prefix = prefix\n        self.bos_token_id = bos_token_id\n        self.pad_token_id = pad_token_id\n        self.eos_token_id = eos_token_id\n        self.sep_token_id = sep_token_id\n        self.decoder_start_token_id = decoder_start_token_id\n\n        # Retrocompatibility: Parameters for sequence generation. While we will keep the ability to load these\n        # parameters, saving them will be deprecated. In a distant future, we won't need to load them.\n        for parameter_name, default_value in self._get_global_generation_defaults().items():\n            setattr(self, parameter_name, kwargs.pop(parameter_name, default_value))\n\n        # Name or path to the pretrained checkpoint\n        self._name_or_path = str(kwargs.pop(\"name_or_path\", \"\"))\n        self._commit_hash = kwargs.pop(\"_commit_hash\", None)\n\n        # Attention implementation to use, if relevant (it sets it recursively on sub-configs)\n        self._attn_implementation = kwargs.pop(\"attn_implementation\", None)\n\n        # Drop the transformers version info\n        self.transformers_version = kwargs.pop(\"transformers_version\", None)\n\n        # Deal with gradient checkpointing\n        if kwargs.get(\"gradient_checkpointing\", False):\n            warnings.warn(\n                \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n                \"Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the \"\n                \"`Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\"\n            )\n\n        # Additional attributes without default values\n        for key, value in kwargs.items():\n            try:\n                setattr(self, key, value)\n            except AttributeError as err:\n                logger.error(f\"Can't set {key} with value {value} for {self}\")\n                raise err\n\n    def _create_id_label_maps(self, num_labels: int):\n        self.id2label = {i: f\"LABEL_{i}\" for i in range(num_labels)}\n        self.label2id = dict(zip(self.id2label.values(), self.id2label.keys()))\n\n    @property\n    def name_or_path(self) -> Optional[str]:\n        return getattr(self, \"_name_or_path\", None)\n\n    @name_or_path.setter\n    def name_or_path(self, value):\n        self._name_or_path = str(value)  # Make sure that name_or_path is a string (for JSON encoding)\n\n    @property\n    def output_attentions(self):\n        \"\"\"\n        `bool`: Whether or not the model should returns all attentions.\n        \"\"\"\n        return self._output_attentions\n\n    @output_attentions.setter\n    def output_attentions(self, value: bool):\n        # If we set `output_attentions` explicitly before the attn implementation, dispatch eager\n        if value and self._attn_implementation is None:\n            self._attn_implementation = \"eager\"\n        if value and self._attn_implementation != \"eager\":\n            raise ValueError(\n                \"The `output_attentions` attribute is not supported when using the `attn_implementation` set to \"\n                f\"{self._attn_implementation}. Please set it to 'eager' instead.\"\n            )\n        self._output_attentions = value\n\n    @property\n    def use_return_dict(self) -> bool:\n        \"\"\"\n        `bool`: Whether or not return [`~utils.ModelOutput`] instead of tuples.\n        \"\"\"\n        return self.return_dict\n\n    @property\n    def num_labels(self) -> int:\n        \"\"\"\n        `int`: The number of labels for classification models.\n        \"\"\"\n        return len(self.id2label)\n\n    @num_labels.setter\n    def num_labels(self, num_labels: int):\n        # we do not store `num_labels` attribute in config, but instead\n        # compute it based on the length of the `id2label` map\n        if self.id2label is None or self.num_labels != num_labels:\n            self._create_id_label_maps(num_labels)\n\n    @property\n    def _attn_implementation(self):\n        return self._attn_implementation_internal\n\n    @_attn_implementation.setter\n    def _attn_implementation(self, value: str | dict | None):\n        \"\"\"We set it recursively on the sub-configs as well\"\"\"\n        # Set if for current config\n        current_attn = getattr(self, \"_attn_implementation\", None)\n        attn_implementation = value if not isinstance(value, dict) else value.get(\"\", current_attn)\n        self._attn_implementation_internal = attn_implementation\n\n        # Set it recursively on the subconfigs\n        for subconfig_key in self.sub_configs:\n            subconfig = getattr(self, subconfig_key, None)\n            if subconfig is not None:\n                current_subconfig_attn = getattr(subconfig, \"_attn_implementation\", None)\n                sub_implementation = (\n                    value if not isinstance(value, dict) else value.get(subconfig_key, current_subconfig_attn)\n                )\n                subconfig._attn_implementation = sub_implementation\n\n    @property\n    def torch_dtype(self):\n        logger.warning_once(\"`torch_dtype` is deprecated! Use `dtype` instead!\")\n        return self.dtype\n\n    @torch_dtype.setter\n    def torch_dtype(self, value):\n        logger.warning_once(\"`torch_dtype` is deprecated! Use `dtype` instead!\")\n        self.dtype = value\n\n    @property\n    def rope_scaling(self):\n        return self.rope_parameters\n\n    @rope_scaling.setter\n    def rope_scaling(self, value):\n        self.rope_parameters = value\n\n    def save_pretrained(self, save_directory: str | os.PathLike, push_to_hub: bool = False, **kwargs):\n        \"\"\"\n        Save a configuration object to the directory `save_directory`, so that it can be re-loaded using the\n        [`~PreTrainedConfig.from_pretrained`] class method.\n\n        Args:\n            save_directory (`str` or `os.PathLike`):\n                Directory where the configuration JSON file will be saved (will be created if it does not exist).\n            push_to_hub (`bool`, *optional*, defaults to `False`):\n                Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n                repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n                namespace).\n            kwargs (`dict[str, Any]`, *optional*):\n                Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n        \"\"\"\n        if os.path.isfile(save_directory):\n            raise AssertionError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n\n        non_default_generation_parameters = self._get_non_default_generation_parameters()\n        if len(non_default_generation_parameters) > 0:\n            # TODO (joao): this should be an exception if the user has modified the loaded config. See #33886\n            warnings.warn(\n                \"Some non-default generation parameters are set in the model config. These should go into either a) \"\n                \"`model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file \"\n                \"(https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).\"\n                \"This warning will become an exception in the future.\"\n                f\"\\nNon-default generation parameters: {str(non_default_generation_parameters)}\",\n                UserWarning,\n            )\n\n        os.makedirs(save_directory, exist_ok=True)\n\n        if push_to_hub:\n            commit_message = kwargs.pop(\"commit_message\", None)\n            repo_id = kwargs.pop(\"repo_id\", save_directory.split(os.path.sep)[-1])\n            repo_id = self._create_repo(repo_id, **kwargs)\n            files_timestamps = self._get_files_timestamps(save_directory)\n\n        # This attribute is important to know on load, but should not be serialized on save.\n        if \"transformers_weights\" in self:\n            delattr(self, \"transformers_weights\")\n\n        # If we have a custom config, we copy the file defining it in the folder and set the attributes so it can be\n        # loaded from the Hub.\n        if self._auto_class is not None:\n            custom_object_save(self, save_directory, config=self)\n\n        # If we save using the predefined names, we can load using `from_pretrained`\n        output_config_file = os.path.join(save_directory, CONFIG_NAME)\n\n        self.to_json_file(output_config_file, use_diff=True)\n        logger.info(f\"Configuration saved in {output_config_file}\")\n\n        if push_to_hub:\n            self._upload_modified_files(\n                save_directory,\n                repo_id,\n                files_timestamps,\n                commit_message=commit_message,\n                token=kwargs.get(\"token\"),\n            )\n\n    @classmethod\n    def from_pretrained(\n        cls: type[SpecificPreTrainedConfigType],\n        pretrained_model_name_or_path: str | os.PathLike,\n        cache_dir: str | os.PathLike | None = None,\n        force_download: bool = False,\n        local_files_only: bool = False,\n        token: str | bool | None = None,\n        revision: str = \"main\",\n        **kwargs,\n    ) -> SpecificPreTrainedConfigType:\n        r\"\"\"\n        Instantiate a [`PreTrainedConfig`] (or a derived class) from a pretrained model configuration.\n\n        Args:\n            pretrained_model_name_or_path (`str` or `os.PathLike`):\n                This can be either:\n\n                - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n                  huggingface.co.\n                - a path to a *directory* containing a configuration file saved using the\n                  [`~PreTrainedConfig.save_pretrained`] method, e.g., `./my_model_directory/`.\n                - a path or url to a saved configuration JSON *file*, e.g., `./my_model_directory/configuration.json`.\n            cache_dir (`str` or `os.PathLike`, *optional*):\n                Path to a directory in which a downloaded pretrained model configuration should be cached if the\n                standard cache should not be used.\n            force_download (`bool`, *optional*, defaults to `False`):\n                Whether or not to force to (re-)download the configuration files and override the cached versions if\n                they exist.\n            proxies (`dict[str, str]`, *optional*):\n                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n            token (`str` or `bool`, *optional*):\n                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use\n                the token generated when running `hf auth login` (stored in `~/.huggingface`).\n            revision (`str`, *optional*, defaults to `\"main\"`):\n                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n                identifier allowed by git.\n\n                <Tip>\n\n                To test a pull request you made on the Hub, you can pass `revision=\"refs/pr/<pr_number>\"`.\n\n                </Tip>\n\n            return_unused_kwargs (`bool`, *optional*, defaults to `False`):\n                If `False`, then this function returns just the final configuration object.\n\n                If `True`, then this functions returns a `Tuple(config, unused_kwargs)` where *unused_kwargs* is a\n                dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the\n                part of `kwargs` which has not been used to update `config` and is otherwise ignored.\n            subfolder (`str`, *optional*, defaults to `\"\"`):\n                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n                specify the folder name here.\n            kwargs (`dict[str, Any]`, *optional*):\n                The values in kwargs of any keys which are configuration attributes will be used to override the loaded\n                values. Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled\n                by the `return_unused_kwargs` keyword parameter.\n\n        Returns:\n            [`PreTrainedConfig`]: The configuration object instantiated from this pretrained model.\n\n        Examples:\n\n        ```python\n        # We can't instantiate directly the base class *PreTrainedConfig* so let's show the examples on a\n        # derived class: BertConfig\n        config = BertConfig.from_pretrained(\n            \"google-bert/bert-base-uncased\"\n        )  # Download configuration from huggingface.co and cache.\n        config = BertConfig.from_pretrained(\n            \"./test/saved_model/\"\n        )  # E.g. config (or model) was saved using *save_pretrained('./test/saved_model/')*\n        config = BertConfig.from_pretrained(\"./test/saved_model/my_configuration.json\")\n        config = BertConfig.from_pretrained(\"google-bert/bert-base-uncased\", output_attentions=True, foo=False)\n        assert config.output_attentions == True\n        config, unused_kwargs = BertConfig.from_pretrained(\n            \"google-bert/bert-base-uncased\", output_attentions=True, foo=False, return_unused_kwargs=True\n        )\n        assert config.output_attentions == True\n        assert unused_kwargs == {\"foo\": False}\n        ```\"\"\"\n        kwargs[\"cache_dir\"] = cache_dir\n        kwargs[\"force_download\"] = force_download\n        kwargs[\"local_files_only\"] = local_files_only\n        kwargs[\"revision\"] = revision\n\n        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n        if cls.base_config_key and cls.base_config_key in config_dict:\n            config_dict = config_dict[cls.base_config_key]\n\n        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n            # sometimes the config has no `base_config_key` if the config is used in several composite models\n            # e.g. LlamaConfig. In that case we try to see if there is match in `model_type` before raising a warning\n            for v in config_dict.values():\n                if isinstance(v, dict) and v.get(\"model_type\") == cls.model_type:\n                    config_dict = v\n\n            # raise warning only if we still can't see a match in `model_type`\n            if config_dict[\"model_type\"] != cls.model_type:\n                logger.warning(\n                    f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n                    f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n                )\n\n        return cls.from_dict(config_dict, **kwargs)\n\n    @classmethod\n    def get_config_dict(\n        cls, pretrained_model_name_or_path: str | os.PathLike, **kwargs\n    ) -> tuple[dict[str, Any], dict[str, Any]]:\n        \"\"\"\n        From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instantiating a\n        [`PreTrainedConfig`] using `from_dict`.\n\n        Parameters:\n            pretrained_model_name_or_path (`str` or `os.PathLike`):\n                The identifier of the pre-trained checkpoint from which we want the dictionary of parameters.\n\n        Returns:\n            `tuple[Dict, Dict]`: The dictionary(ies) that will be used to instantiate the configuration object.\n\n        \"\"\"\n        original_kwargs = copy.deepcopy(kwargs)\n        # Get config dict associated with the base config file\n        config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n        if config_dict is None:\n            return {}, kwargs\n        if \"_commit_hash\" in config_dict:\n            original_kwargs[\"_commit_hash\"] = config_dict[\"_commit_hash\"]\n\n        # That config file may point us toward another config file to use.\n        if \"configuration_files\" in config_dict:\n            configuration_file = get_configuration_file(config_dict[\"configuration_files\"])\n            config_dict, kwargs = cls._get_config_dict(\n                pretrained_model_name_or_path, _configuration_file=configuration_file, **original_kwargs\n            )\n\n        return config_dict, kwargs\n\n    @classmethod\n    def _get_config_dict(\n        cls, pretrained_model_name_or_path: str | os.PathLike, **kwargs\n    ) -> tuple[dict[str, Any], dict[str, Any]]:\n        cache_dir = kwargs.pop(\"cache_dir\", None)\n        force_download = kwargs.pop(\"force_download\", False)\n        proxies = kwargs.pop(\"proxies\", None)\n        token = kwargs.pop(\"token\", None)\n        local_files_only = kwargs.pop(\"local_files_only\", False)\n        revision = kwargs.pop(\"revision\", None)\n        trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n        subfolder = kwargs.pop(\"subfolder\", \"\")\n        from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n        from_auto_class = kwargs.pop(\"_from_auto\", False)\n        commit_hash = kwargs.pop(\"_commit_hash\", None)\n\n        gguf_file = kwargs.get(\"gguf_file\")\n\n        if trust_remote_code is True:\n            logger.warning(\n                \"The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is\"\n                \" ignored.\"\n            )\n\n        user_agent = {\"file_type\": \"config\", \"from_auto_class\": from_auto_class}\n        if from_pipeline is not None:\n            user_agent[\"using_pipeline\"] = from_pipeline\n\n        pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n\n        is_local = os.path.isdir(pretrained_model_name_or_path)\n        if os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path)):\n            # Special case when pretrained_model_name_or_path is a local file\n            resolved_config_file = pretrained_model_name_or_path\n            is_local = True\n        elif is_remote_url(pretrained_model_name_or_path):\n            configuration_file = pretrained_model_name_or_path if gguf_file is None else gguf_file\n            resolved_config_file = download_url(pretrained_model_name_or_path)\n        else:\n            configuration_file = kwargs.pop(\"_configuration_file\", CONFIG_NAME) if gguf_file is None else gguf_file\n\n            try:\n                # Load from local folder or from cache or download from model Hub and cache\n                resolved_config_file = cached_file(\n                    pretrained_model_name_or_path,\n                    configuration_file,\n                    cache_dir=cache_dir,\n                    force_download=force_download,\n                    proxies=proxies,\n                    local_files_only=local_files_only,\n                    token=token,\n                    user_agent=user_agent,\n                    revision=revision,\n                    subfolder=subfolder,\n                    _commit_hash=commit_hash,\n                )\n                if resolved_config_file is None:\n                    return None, kwargs\n                commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n            except OSError:\n                # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\n                # the original exception.\n                raise\n            except Exception:\n                # For any other exception, we throw a generic error.\n                raise OSError(\n                    f\"Can't load the configuration of '{pretrained_model_name_or_path}'. If you were trying to load it\"\n                    \" from 'https://huggingface.co/models', make sure you don't have a local directory with the same\"\n                    f\" name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory\"\n                    f\" containing a {configuration_file} file\"\n                )\n\n        try:\n            if gguf_file:\n                config_dict = load_gguf_checkpoint(resolved_config_file, return_tensors=False)[\"config\"]\n            else:\n                # Load config dict\n                config_dict = cls._dict_from_json_file(resolved_config_file)\n\n            config_dict[\"_commit_hash\"] = commit_hash\n        except (json.JSONDecodeError, UnicodeDecodeError):\n            raise OSError(f\"It looks like the config file at '{resolved_config_file}' is not a valid JSON file.\")\n\n        if is_local:\n            logger.info(f\"loading configuration file {resolved_config_file}\")\n        else:\n            logger.info(f\"loading configuration file {configuration_file} from cache at {resolved_config_file}\")\n\n        # timm models are not saved with the model_type in the config file\n        if \"model_type\" not in config_dict and is_timm_config_dict(config_dict):\n            config_dict[\"model_type\"] = \"timm_wrapper\"\n\n        return config_dict, kwargs\n\n    @classmethod\n    def from_dict(\n        cls: type[SpecificPreTrainedConfigType], config_dict: dict[str, Any], **kwargs\n    ) -> SpecificPreTrainedConfigType:\n        \"\"\"\n        Instantiates a [`PreTrainedConfig`] from a Python dictionary of parameters.\n\n        Args:\n            config_dict (`dict[str, Any]`):\n                Dictionary that will be used to instantiate the configuration object. Such a dictionary can be\n                retrieved from a pretrained checkpoint by leveraging the [`~PreTrainedConfig.get_config_dict`] method.\n            kwargs (`dict[str, Any]`):\n                Additional parameters from which to initialize the configuration object.\n\n        Returns:\n            [`PreTrainedConfig`]: The configuration object instantiated from those parameters.\n        \"\"\"\n        return_unused_kwargs = kwargs.pop(\"return_unused_kwargs\", False)\n        # Those arguments may be passed along for our internal telemetry.\n        # We remove them so they don't appear in `return_unused_kwargs`.\n        kwargs.pop(\"_from_auto\", None)\n        kwargs.pop(\"_from_pipeline\", None)\n        # The commit hash might have been updated in the `config_dict`, we don't want the kwargs to erase that update.\n        if \"_commit_hash\" in kwargs and \"_commit_hash\" in config_dict:\n            kwargs[\"_commit_hash\"] = config_dict[\"_commit_hash\"]\n\n        # For BC on the old `torch_dtype`\n        if (torch_dtype := kwargs.pop(\"torch_dtype\", None)) is not None:\n            logger.warning_once(\"`torch_dtype` is deprecated! Use `dtype` instead!\")\n            # If both are present, use `dtype`\n            kwargs[\"dtype\"] = kwargs.get(\"dtype\", torch_dtype)\n\n        # We remove it from kwargs so that it does not appear in `return_unused_kwargs`.\n        config_dict[\"attn_implementation\"] = kwargs.pop(\"attn_implementation\", None)\n\n        config = cls(**config_dict)\n\n        # Update config with kwargs if needed\n        if \"num_labels\" in kwargs and \"id2label\" in kwargs:\n            num_labels = kwargs[\"num_labels\"]\n            id2label = kwargs[\"id2label\"] if kwargs[\"id2label\"] is not None else []\n            if len(id2label) != num_labels:\n                raise ValueError(\n                    f\"You passed along `num_labels={num_labels}` with an incompatible id to label map: \"\n                    f\"{kwargs['id2label']}. Since those arguments are inconsistent with each other, you should remove \"\n                    \"one of them.\"\n                )\n        to_remove = []\n        for key, value in kwargs.items():\n            if hasattr(config, key):\n                current_attr = getattr(config, key)\n                # To authorize passing a custom subconfig as kwarg in models that have nested configs.\n                # We need to update only custom kwarg values instead and keep other attributes in subconfig.\n                if isinstance(current_attr, PreTrainedConfig) and isinstance(value, dict):\n                    current_attr_updated = current_attr.to_dict()\n                    current_attr_updated.update(value)\n                    value = current_attr.__class__(**current_attr_updated)\n                setattr(config, key, value)\n                if key != \"dtype\":\n                    to_remove.append(key)\n        for key in to_remove:\n            kwargs.pop(key, None)\n\n        logger.info(f\"Model config {config}\")\n        if return_unused_kwargs:\n            return config, kwargs\n        else:\n            return config\n\n    @classmethod\n    def from_json_file(\n        cls: type[SpecificPreTrainedConfigType], json_file: str | os.PathLike\n    ) -> SpecificPreTrainedConfigType:\n        \"\"\"\n        Instantiates a [`PreTrainedConfig`] from the path to a JSON file of parameters.\n\n        Args:\n            json_file (`str` or `os.PathLike`):\n                Path to the JSON file containing the parameters.\n\n        Returns:\n            [`PreTrainedConfig`]: The configuration object instantiated from that JSON file.\n\n        \"\"\"\n        config_dict = cls._dict_from_json_file(json_file)\n        return cls(**config_dict)\n\n    @classmethod\n    def _dict_from_json_file(cls, json_file: str | os.PathLike):\n        with open(json_file, encoding=\"utf-8\") as reader:\n            text = reader.read()\n        return json.loads(text)\n\n    def __eq__(self, other):\n        return isinstance(other, PreTrainedConfig) and (self.__dict__ == other.__dict__)\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__} {self.to_json_string()}\"\n\n    def __iter__(self):\n        yield from self.__dict__\n\n    def to_diff_dict(self) -> dict[str, Any]:\n        \"\"\"\n        Removes all attributes from the configuration that correspond to the default config attributes for\n        better readability, while always retaining the `config` attribute from the class. Serializes to a\n        Python dictionary.\n\n        Returns:\n            dict[str, Any]: Dictionary of all the attributes that make up this configuration instance.\n        \"\"\"\n        config_dict = self.to_dict()\n\n        # Get the default config dict (from a fresh PreTrainedConfig instance)\n        default_config_dict = PreTrainedConfig().to_dict()\n\n        # get class specific config dict\n        class_config_dict = self.__class__().to_dict() if not self.has_no_defaults_at_init else {}\n\n        serializable_config_dict = {}\n\n        # Only serialize values that differ from the default config,\n        # except always keep the 'config' attribute.\n        for key, value in config_dict.items():\n            if (\n                isinstance(getattr(self, key, None), PreTrainedConfig)\n                and key in class_config_dict\n                and isinstance(class_config_dict[key], dict)\n            ):\n                # For nested configs we need to clean the diff recursively\n                diff = recursive_diff_dict(value, default_config_dict, config_obj=getattr(self, key, None))\n                if \"model_type\" in value:\n                    # Needs to be set even if it's not in the diff\n                    diff[\"model_type\"] = value[\"model_type\"]\n\n                serializable_config_dict[key] = diff\n            elif (\n                key not in default_config_dict\n                or key == \"transformers_version\"\n                or key == \"vocab_file\"\n                or value != default_config_dict[key]\n                or (key in default_config_dict and value != class_config_dict.get(key, value))\n            ):\n                serializable_config_dict[key] = value\n\n        self._remove_keys_not_serialized(serializable_config_dict)\n\n        # Key removed only in diff dict\n        if \"_name_or_path\" in serializable_config_dict:\n            del serializable_config_dict[\"_name_or_path\"]\n\n        if hasattr(self, \"quantization_config\"):\n            serializable_config_dict[\"quantization_config\"] = (\n                self.quantization_config.to_dict()\n                if not isinstance(self.quantization_config, dict)\n                else self.quantization_config\n            )\n        self.dict_dtype_to_str(serializable_config_dict)\n\n        return serializable_config_dict\n\n    def to_dict(self) -> dict[str, Any]:\n        \"\"\"\n        Serializes this instance to a Python dictionary.\n\n        Returns:\n            `dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n        \"\"\"\n        output = copy.deepcopy(self.__dict__)\n        if hasattr(self.__class__, \"model_type\"):\n            output[\"model_type\"] = self.__class__.model_type\n\n        # Transformers version when serializing the model\n        output[\"transformers_version\"] = __version__\n\n        for key, value in output.items():\n            # Deal with nested configs like CLIP\n            if isinstance(value, PreTrainedConfig):\n                value = value.to_dict()\n                del value[\"transformers_version\"]\n\n            output[key] = value\n\n        self._remove_keys_not_serialized(output)\n\n        if hasattr(self, \"quantization_config\"):\n            output[\"quantization_config\"] = (\n                self.quantization_config.to_dict()\n                if not isinstance(self.quantization_config, dict)\n                else self.quantization_config\n            )\n        self.dict_dtype_to_str(output)\n\n        return output\n\n    def to_json_string(self, use_diff: bool = True) -> str:\n        \"\"\"\n        Serializes this instance to a JSON string.\n\n        Args:\n            use_diff (`bool`, *optional*, defaults to `True`):\n                If set to `True`, only the difference between the config instance and the default `PreTrainedConfig()`\n                is serialized to JSON string.\n\n        Returns:\n            `str`: String containing all the attributes that make up this configuration instance in JSON format.\n        \"\"\"\n        if use_diff is True:\n            config_dict = self.to_diff_dict()\n        else:\n            config_dict = self.to_dict()\n        return json.dumps(config_dict, indent=2, sort_keys=True) + \"\\n\"\n\n    def to_json_file(self, json_file_path: str | os.PathLike, use_diff: bool = True):\n        \"\"\"\n        Save this instance to a JSON file.\n\n        Args:\n            json_file_path (`str` or `os.PathLike`):\n                Path to the JSON file in which this configuration instance's parameters will be saved.\n            use_diff (`bool`, *optional*, defaults to `True`):\n                If set to `True`, only the difference between the config instance and the default `PreTrainedConfig()`\n                is serialized to JSON file.\n        \"\"\"\n        with open(json_file_path, \"w\", encoding=\"utf-8\") as writer:\n            writer.write(self.to_json_string(use_diff=use_diff))\n\n    def update(self, config_dict: dict[str, Any]):\n        \"\"\"\n        Updates attributes of this class with attributes from `config_dict`.\n\n        Args:\n            config_dict (`dict[str, Any]`): Dictionary of attributes that should be updated for this class.\n        \"\"\"\n        for key, value in config_dict.items():\n            setattr(self, key, value)\n\n    def update_from_string(self, update_str: str):\n        \"\"\"\n        Updates attributes of this class with attributes from `update_str`.\n\n        The expected format is ints, floats and strings as is, and for booleans use `true` or `false`. For example:\n        \"n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index\"\n\n        The keys to change have to already exist in the config object.\n\n        Args:\n            update_str (`str`): String with attributes that should be updated for this class.\n\n        \"\"\"\n\n        d = dict(x.split(\"=\") for x in update_str.split(\",\"))\n        for k, v in d.items():\n            if not hasattr(self, k):\n                raise ValueError(f\"key {k} isn't in the original config dict\")\n\n            old_v = getattr(self, k)\n            if isinstance(old_v, bool):\n                if v.lower() in [\"true\", \"1\", \"y\", \"yes\"]:\n                    v = True\n                elif v.lower() in [\"false\", \"0\", \"n\", \"no\"]:\n                    v = False\n                else:\n                    raise ValueError(f\"can't derive true or false from {v} (key {k})\")\n            elif isinstance(old_v, int):\n                v = int(v)\n            elif isinstance(old_v, float):\n                v = float(v)\n            elif not isinstance(old_v, str):\n                raise TypeError(\n                    f\"You can only update int, float, bool or string values in the config, got {v} for key {k}\"\n                )\n\n            setattr(self, k, v)\n\n    def dict_dtype_to_str(self, d: dict[str, Any]) -> None:\n        \"\"\"\n        Checks whether the passed dictionary and its nested dicts have a *dtype* key and if it's not None,\n        converts torch.dtype to a string of just the type. For example, `torch.float32` get converted into *\"float32\"*\n        string, which can then be stored in the json format.\n        \"\"\"\n        if d.get(\"dtype\") is not None:\n            if isinstance(d[\"dtype\"], dict):\n                d[\"dtype\"] = {k: str(v).split(\".\")[-1] for k, v in d[\"dtype\"].items()}\n            # models like Emu3 can have \"dtype\" as token in config's vocabulary map,\n            # so we also exclude int type here to avoid error in this special case.\n            elif not isinstance(d[\"dtype\"], (str, int)):\n                d[\"dtype\"] = str(d[\"dtype\"]).split(\".\")[1]\n        for value in d.values():\n            if isinstance(value, dict):\n                self.dict_dtype_to_str(value)\n\n    def _remove_keys_not_serialized(self, d: dict[str, Any]) -> None:\n        \"\"\"\n        Checks and removes if there are any keys in the dict that should not be serialized when saving the config.\n        Runs recursive check on the dict, to remove from all sub configs.\n        \"\"\"\n        if hasattr(self, \"quantization_config\"):\n            # Pop the `_pre_quantization_dtype` as torch.dtypes are not serializable.\n            _ = d.pop(\"_pre_quantization_dtype\", None)\n\n        if \"_auto_class\" in d:\n            del d[\"_auto_class\"]\n        if \"_output_attentions\" in d:\n            d[\"output_attentions\"] = d.pop(\"_output_attentions\")\n        if \"_commit_hash\" in d:\n            del d[\"_commit_hash\"]\n        if \"_attn_implementation_internal\" in d:\n            del d[\"_attn_implementation_internal\"]\n        # Do not serialize `base_model_tp_plan` for now\n        if \"base_model_tp_plan\" in d:\n            del d[\"base_model_tp_plan\"]\n        # Do not serialize `base_model_pp_plan` for now\n        if \"base_model_pp_plan\" in d:\n            del d[\"base_model_pp_plan\"]\n        for value in d.values():\n            if isinstance(value, dict):\n                self._remove_keys_not_serialized(value)\n\n    @classmethod\n    def register_for_auto_class(cls, auto_class=\"AutoConfig\"):\n        \"\"\"\n        Register this class with a given auto class. This should only be used for custom configurations as the ones in\n        the library are already mapped with `AutoConfig`.\n\n\n\n        Args:\n            auto_class (`str` or `type`, *optional*, defaults to `\"AutoConfig\"`):\n                The auto class to register this new configuration with.\n        \"\"\"\n        if not isinstance(auto_class, str):\n            auto_class = auto_class.__name__\n\n        import transformers.models.auto as auto_module\n\n        if not hasattr(auto_module, auto_class):\n            raise ValueError(f\"{auto_class} is not a valid auto class.\")\n\n        cls._auto_class = auto_class\n\n    @staticmethod\n    def _get_global_generation_defaults() -> dict[str, Any]:\n        return {\n            \"max_length\": 20,\n            \"min_length\": 0,\n            \"do_sample\": False,\n            \"early_stopping\": False,\n            \"num_beams\": 1,\n            \"temperature\": 1.0,\n            \"top_k\": 50,\n            \"top_p\": 1.0,\n            \"typical_p\": 1.0,\n            \"repetition_penalty\": 1.0,\n            \"length_penalty\": 1.0,\n            \"no_repeat_ngram_size\": 0,\n            \"encoder_no_repeat_ngram_size\": 0,\n            \"bad_words_ids\": None,\n            \"num_return_sequences\": 1,\n            \"output_scores\": False,\n            \"return_dict_in_generate\": False,\n            \"forced_bos_token_id\": None,\n            \"forced_eos_token_id\": None,\n            \"remove_invalid_values\": False,\n            \"exponential_decay_length_penalty\": None,\n            \"suppress_tokens\": None,\n            \"begin_suppress_tokens\": None,\n            # Deprecated arguments (moved to the Hub). TODO joao, manuel: remove in v4.62.0\n            \"num_beam_groups\": 1,\n            \"diversity_penalty\": 0.0,\n        }\n\n    def _get_non_default_generation_parameters(self) -> dict[str, Any]:\n        \"\"\"\n        Gets the non-default generation parameters on the PreTrainedConfig instance\n        \"\"\"\n        non_default_generation_parameters = {}\n        decoder_attribute_name = None\n\n        # Some composite models don't have a default config, use their decoder config as a fallback for default values\n        # If no known pattern is matched, then `default_config = None` -> check against the global generation defaults\n        if not self.has_no_defaults_at_init:\n            default_config = self.__class__()\n        else:\n            decoder_config = self.get_text_config(decoder=True)\n            if decoder_config is not self:\n                default_config = decoder_config.__class__()\n            else:\n                default_config = None\n\n        # If it is a composite model, we want to check the subconfig that will be used for generation\n        self_decoder_config = self if decoder_attribute_name is None else getattr(self, decoder_attribute_name)\n\n        for parameter_name, default_global_value in self._get_global_generation_defaults().items():\n            if hasattr(self_decoder_config, parameter_name):\n                is_default_in_config = is_default_generation_value = None\n                parameter_value = getattr(self_decoder_config, parameter_name)\n                # Three cases in which is okay for the model config to hold generation config parameters:\n                # 1. The parameter is set to `None`, effectively delegating its value to the generation config\n                if parameter_value is None:\n                    continue\n                # 2. If we have a default config, then the instance should hold the same generation defaults\n                if default_config is not None:\n                    is_default_in_config = parameter_value == getattr(default_config, parameter_name)\n                # 3. if we don't have a default config, then the instance should hold the global generation defaults\n                else:\n                    is_default_generation_value = parameter_value == default_global_value\n\n                is_non_default = (is_default_in_config is False) or (\n                    is_default_in_config is None and is_default_generation_value is False\n                )\n                if is_non_default:\n                    non_default_generation_parameters[parameter_name] = getattr(self_decoder_config, parameter_name)\n\n        return non_default_generation_parameters\n\n    def get_text_config(self, decoder=None, encoder=None) -> \"PreTrainedConfig\":\n        \"\"\"\n        Returns the text config related to the text input (encoder) or text output (decoder) of the model. The\n        `decoder` and `encoder` input arguments can be used to specify which end of the model we are interested in,\n        which is useful on models that have both text input and output modalities.\n\n        There are three possible outcomes of using this method:\n        1. On most models, it returns the original config instance itself.\n        2. On newer (2024+) composite models, it returns the text section of the config, which is nested under a set\n            of valid names.\n        3. On older (2023-) composite models, it discards decoder-only parameters when `encoder=True` and vice-versa.\n\n        Args:\n            decoder (`Optional[bool]`, *optional*):\n                If set to `True`, then only search for decoder config names.\n            encoder (`Optional[bool]`, *optional*):\n                If set to `True`, then only search for encoder config names.\n        \"\"\"\n        return_both = decoder == encoder  # both unset or both set -> search all possible names\n\n        decoder_possible_text_config_names = (\"decoder\", \"generator\", \"text_config\")\n        encoder_possible_text_config_names = (\"text_encoder\",)\n        if return_both:\n            possible_text_config_names = encoder_possible_text_config_names + decoder_possible_text_config_names\n        elif decoder:\n            possible_text_config_names = decoder_possible_text_config_names\n        else:\n            possible_text_config_names = encoder_possible_text_config_names\n\n        valid_text_config_names = []\n        for text_config_name in possible_text_config_names:\n            if hasattr(self, text_config_name):\n                text_config = getattr(self, text_config_name, None)\n                if text_config is not None:\n                    valid_text_config_names += [text_config_name]\n\n        if len(valid_text_config_names) > 1:\n            raise ValueError(\n                f\"Multiple valid text configs were found in the model config: {valid_text_config_names}. In this \"\n                \"case, using `get_text_config()` would be ambiguous. Please specify the desired text config directly, \"\n                \"e.g. `text_config = config.sub_config_name`\"\n            )\n        elif len(valid_text_config_names) == 1:\n            config_to_return = getattr(self, valid_text_config_names[0])\n        else:\n            config_to_return = self\n\n        # handle legacy models with flat config structure, when we only want one of the configs\n        if not return_both and len(valid_text_config_names) == 0 and config_to_return.is_encoder_decoder:\n            config_to_return = copy.deepcopy(config_to_return)\n            prefix_to_discard = \"encoder\" if decoder else \"decoder\"\n            prefix_to_keep = \"decoder\" if decoder else \"encoder\"\n            for key in config_to_return.to_dict():\n                # NOTE: We don't want to discard the key if it is mapped from a different attribute name at read time\n                if key.startswith(prefix_to_discard) and key not in config_to_return.attribute_map.values():\n                    delattr(config_to_return, key)\n                if key.startswith(prefix_to_keep):\n                    # [encoder/decoder]_layers -> num_hidden_layers\n                    if key == prefix_to_keep + \"_layers\":\n                        new_key = \"num_hidden_layers\"\n                    # [encoder/decoder]_attention_heads -> num_attention_heads\n                    elif key == prefix_to_keep + \"_attention_heads\":\n                        new_key = \"num_attention_heads\"\n                    # e.g. encoder_hidden_act -> hidden_act\n                    else:\n                        new_key = key[len(prefix_to_keep) + 1 :]\n\n                    # Does the class map the new key into a different attribute name at read time? if so, let's write\n                    # into that attribute instead\n                    if new_key in config_to_return.attribute_map:\n                        new_key = config_to_return.attribute_map[new_key]\n\n                    value = getattr(config_to_return, key)\n                    delattr(config_to_return, key)\n                    setattr(config_to_return, new_key, value)\n\n        return config_to_return\n\n\n\ndef recursive_diff_dict(dict_a, dict_b, config_obj=None):\n    \"\"\"\n    Helper function to recursively take the diff between two nested dictionaries. The resulting diff only contains the\n    values from `dict_a` that are different from values in `dict_b`.\n\n    dict_b : the default config dictionary. We want to remove values that are in this one\n    \"\"\"\n    diff = {}\n    default = config_obj.__class__().to_dict() if config_obj is not None else {}\n    for key, value in dict_a.items():\n        obj_value = getattr(config_obj, str(key), None)\n        if isinstance(obj_value, PreTrainedConfig) and key in dict_b and isinstance(dict_b[key], dict):\n            diff_value = recursive_diff_dict(value, dict_b[key], config_obj=obj_value)\n            diff[key] = diff_value\n        elif key not in dict_b or (value != default[key]):\n            diff[key] = value\n    return diff"
                },
                "component_dependencies": {
                    "PreTrainedConfig": [
                        "transformers/__init__.py#__version__",
                        "transformers/configuration_utils.py#SpecificPreTrainedConfigType",
                        "transformers/configuration_utils.py#get_configuration_file",
                        "transformers/configuration_utils.py#logger",
                        "transformers/dynamic_module_utils.py#custom_object_save",
                        "transformers/modeling_gguf_pytorch_utils.py#load_gguf_checkpoint",
                        "transformers/utils.py#CONFIG_NAME",
                        "transformers/utils.py#PushToHubMixin",
                        "transformers/utils.py#cached_file",
                        "transformers/utils.py#download_url",
                        "transformers/utils.py#extract_commit_hash",
                        "transformers/utils.py#is_remote_url",
                        "transformers/utils.py#is_torch_available",
                        "transformers/utils/generic.py#is_timm_config_dict"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/masking_utils.py#_preprocess_mask_arguments": {
                "sorted_modules": {
                    "_preprocess_mask_arguments": "\n\ndef _preprocess_mask_arguments(\n    config: PreTrainedConfig,\n    input_embeds: torch.Tensor,\n    attention_mask: Optional[Union[torch.Tensor, BlockMask]],\n    cache_position: torch.Tensor,\n    past_key_values: Optional[Cache],\n    position_ids: Optional[torch.Tensor],\n    layer_idx: Optional[int],\n) -> tuple[bool, Optional[Union[torch.Tensor, BlockMask]], int, int]:\n    \"\"\"\n    Perform some common pre-processing of the mask arguments we get from the modeling code. Mostly determine the\n    key-value length and offsets, and if we should early exit or not.\n\n    Args:\n        config (`PreTrainedConfig`):\n            The model config.\n        input_embeds (`torch.Tensor`):\n            The input embeddings of shape (batch_size, query_length, hidden_dim). This is used only to infer the\n            batch size, query length and dtype.\n        attention_mask (`torch.Tensor`, optional):\n            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length).\n            It can also be an already prepared 4D mask, in which case it is returned as-is.\n        cache_position (`torch.Tensor`):\n            A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n        past_key_values (`Cache`, optional):\n            The past key values, if we use a cache.\n        position_ids (`torch.Tensor`, optional)\n            A 2D tensor of shape (batch_size, query_length) indicating the positions of each token in the sequences.\n        layer_idx (`int`, optional):\n            If `past_key_values` is not None, this is the layer index of the cache from which to get the key-value\n            length and offset. Indeed, for hybrid caches, different layers may return different lengths.\n\n    Returns:\n        early_exit (`bool`):\n            Whether we should early exit mask creation, and return the mask as-is.\n        attention_mask (`torch.Tensor` or `BlockMask` or `None`):\n            The attention mask to either return immediately, or to use in downstream mask creation.\n        packed_sequence_mask (`torch.Tensor`, optional):\n            In case we detected packed sequence format, this is a tensor where each similar integer indicates that\n            the tokens belong to the same sequence.\n        kv_length (`int`):\n            The size that the key and value states will have during the attention computation.\n        kv_offset (`int`):\n            An offset to indicate at which first position the key and values states will refer to.\n    \"\"\"\n    # If the mask is already 4D, simply return as-is (it was already prepared, or it is custom)\n    if isinstance(attention_mask, (torch.Tensor, BlockMask)) and len(attention_mask.shape) == 4:\n        return True, attention_mask, None, None, None\n\n    # For TGI/vLLM backends, or other custom attention without equivalent mask creation: we don't need a mask!\n    # Note: it's not ideal to check the `_global_mapping` attribute instead of the object itself, however otherwise\n    # full graph dynamo tracing (i.e. torch.export or compile with `fullgraph=True`) will fail on Python<3.11\n    # with `torch._dynamo.exc.Unsupported: 'inline in skipfiles:Mapping.__contains__ | __contains__, skipped\n    # according trace_rules.lookup SKIP_DIRS'` -- can be removed when we require Python>=3.11\n    if config._attn_implementation not in ALL_MASK_ATTENTION_FUNCTIONS._global_mapping:\n        return True, None, None, None, None\n\n    # Move the mask to correct device, and potentially switch dtype for efficiency\n    if attention_mask is not None and attention_mask.ndim == 2:\n        attention_mask = attention_mask.to(device=cache_position.device, dtype=torch.bool)\n\n    # If using a cache, it can give all information about mask sizes based on seen tokens\n    if past_key_values is not None:\n        kv_length, kv_offset = past_key_values.get_mask_sizes(cache_position, layer_idx)\n    # Otherwise, the sizes are simply the input sizes\n    else:\n        kv_length, kv_offset = input_embeds.shape[1], 0\n\n    # We check the position_ids for potential packed sequence format (only if the 2D attention mask is explicitly None,\n    # and we don't have past_key_values, i.e. generally a training setup)\n    packed_sequence_mask = None\n    if position_ids is not None and attention_mask is None and past_key_values is None:\n        batch_size = input_embeds.shape[0]\n        # The position ids are sometimes just unsqueezed, without being expanded\n        if batch_size != position_ids.shape[0]:\n            position_ids = position_ids.expand(batch_size, -1)\n        packed_sequence_mask = find_packed_sequence_indices(position_ids)\n\n    return False, attention_mask, packed_sequence_mask, kv_length, kv_offset"
                },
                "component_dependencies": {
                    "_preprocess_mask_arguments": [
                        "transformers/cache_utils.py#Cache",
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/masking_utils.py#find_packed_sequence_indices"
                    ]
                },
                "warning": null
            },
            "transformers/masking_utils.py#causal_mask_function": {
                "sorted_modules": {
                    "causal_mask_function": "\n\ndef causal_mask_function(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n    \"\"\"\n    This creates a basic lower-diagonal causal mask.\n    \"\"\"\n    return kv_idx <= q_idx"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/masking_utils.py#sliding_window_causal_mask_function": {
                "sorted_modules": {
                    "sliding_window_causal_mask_function": "\n\ndef sliding_window_causal_mask_function(sliding_window: int) -> Callable:\n    \"\"\"\n    This return the mask_function function to create a sliding window mask.\n    \"\"\"\n    return and_masks(sliding_window_overlay(sliding_window), causal_mask_function)"
                },
                "component_dependencies": {
                    "sliding_window_causal_mask_function": [
                        "transformers/masking_utils.py#and_masks",
                        "transformers/masking_utils.py#causal_mask_function",
                        "transformers/masking_utils.py#sliding_window_overlay"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_rope_utils.py#RopeParameters": {
                "sorted_modules": {
                    "RopeParameters": "\n\nclass RopeParameters(TypedDict):\n    \"\"\"\n    Args:\n        rope_theta (`float`):\n            The base period of the RoPE embeddings.\n        rope_type (`str`, *optional*, defaults to \"default\"):\n            The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n            'llama3'], with 'default' being the original RoPE implementation.\n        factor (`float`, *optional*):\n            Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n            most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n            original maximum pre-trained length.\n        original_max_position_embeddings (`int`, *optional*):\n            Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n            pretraining.\n        attention_factor (`float`, *optional*):\n            Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n            computation. If unspecified, it defaults to value recommended by the implementation, using the\n            `factor` field to infer the suggested value.\n        beta_fast (`float`, *optional*):\n            Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n            ramp function. If unspecified, it defaults to 32.\n        beta_slow (`float`, *optional*):\n            Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n            ramp function. If unspecified, it defaults to 1.\n        short_factor (`list[float]`, *optional*):\n            Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n            `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n            size divided by the number of attention heads divided by 2\n        long_factor (`list[float]`, *optional*):\n            Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n            `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n            size divided by the number of attention heads divided by 2\n        low_freq_factor (`float`, *optional*):\n            Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n        high_freq_factor (`float`, *optional*):\n            Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n    \"\"\"\n\n    rope_theta: float\n    rope_type: Optional[str]\n    factor: Optional[float]\n    original_max_position_embeddings: Optional[int]\n    attention_factor: Optional[float]\n    beta_fast: Optional[float]\n    beta_slow: Optional[float]\n    short_factor: Optional[list[float]]\n    long_factor: Optional[list[float]]\n    low_freq_factor: Optional[float]\n    high_freq_factor: Optional[float]"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/modeling_rope_utils.py#rope_config_validation": {
                "sorted_modules": {
                    "rope_config_validation": "\n\ndef rope_config_validation(config: PreTrainedConfig, ignore_keys: Optional[set] = None):\n    \"\"\"\n    Validate the RoPE config arguments, given a `PreTrainedConfig` object\n    \"\"\"\n    rope_parameters_dict = getattr(config, \"rope_parameters\", None)  # not a default parameter in `PreTrainedConfig`\n    if rope_parameters_dict is None:\n        return\n\n    if getattr(config, \"layer_types\", None) is not None and all(\n        key in config.layer_types for key in rope_parameters_dict.keys()\n    ):\n        pass\n    else:\n        rope_parameters_dict = {\"full_attention\": rope_parameters_dict}\n\n    for rope_parameters in rope_parameters_dict.values():\n        rope_type = rope_parameters.get(\"rope_type\", rope_parameters.get(\"type\", \"default\"))\n        validation_fn = ROPE_VALIDATION_FUNCTIONS.get(rope_type)\n\n        rope_parameters[\"rope_type\"] = rope_type\n        # BC: \"rope_theta\" was originally saved in config\n        rope_parameters[\"rope_theta\"] = rope_parameters.get(\"rope_theta\", getattr(config, \"rope_theta\", None))\n\n        if validation_fn is not None:\n            validation_fn(rope_parameters, config=config, ignore_keys=ignore_keys)\n        else:\n            logger.warning(\n                f\"Missing validation function mapping in `ROPE_VALIDATION_FUNCTIONS` for 'rope_type'='{rope_type}'\"\n            )"
                },
                "component_dependencies": {
                    "rope_config_validation": [
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/modeling_rope_utils.py#ROPE_VALIDATION_FUNCTIONS",
                        "transformers/modeling_rope_utils.py#logger"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_rope_utils.py#standardize_rope_params": {
                "sorted_modules": {
                    "standardize_rope_params": "\n\ndef standardize_rope_params(config, rope_theta: float | dict[str, float] | None = None):\n    \"\"\"\n    Helper to standardize the config's rope params field by ensuring the params are defined for each\n    later type. For old model the fn will duplicate a single rope param in each layer type (backward compatibility)\n    \"\"\"\n    rope_parameters = getattr(config, \"rope_parameters\", None)\n    layer_types = getattr(config, \"layer_types\", None)\n    if rope_theta is None:\n        rope_theta = getattr(config, \"rope_theta\", None)\n\n    # Case 1: one RoPE theat = one RoPE param per model without nesting\n    if not isinstance(rope_theta, dict):\n        if rope_parameters is None:\n            rope_parameters = {\"rope_type\": \"default\", \"rope_theta\": rope_theta}\n        else:\n            # BC: if there is a 'type' field, copy it it to 'rope_type'.\n            rope_type = rope_parameters.get(\"rope_type\", rope_parameters.get(\"type\", \"default\"))\n            rope_theta = rope_parameters.get(\"rope_theta\") or rope_theta\n            rope_parameters.update({\"rope_theta\": rope_theta, \"rope_type\": rope_type})\n        config.rope_parameters = rope_parameters\n\n    # Case 2: different RoPE for each layer as nested dict\n    else:\n        rope_parameters_per_layer_type = {}\n        for layer_type in layer_types:\n            if rope_parameters is None:\n                rope_parameters_per_layer_type[layer_type] = {\n                    \"rope_type\": \"default\",\n                    \"rope_theta\": rope_theta[layer_type],\n                }\n            else:\n                is_field_in_new_format = any(layer_type in rope_parameters for layer_type in layer_types)\n                if not is_field_in_new_format:\n                    curr_rope_type = rope_parameters.get(\"rope_type\", rope_parameters.get(\"type\"))\n                    rope_parameters_per_layer_type[layer_type] = {\n                        **rope_parameters,\n                        \"rope_type\": curr_rope_type,\n                        \"rope_theta\": rope_theta[layer_type],\n                    }\n                else:\n                    curr_rope_type = rope_parameters[layer_type].get(\n                        \"rope_type\", rope_parameters[layer_type].get(\"type\")\n                    )\n                    rope_parameters_per_layer_type[layer_type] = {\n                        **rope_parameters[layer_type],\n                        \"rope_type\": curr_rope_type,\n                        \"rope_theta\": rope_theta[layer_type],\n                    }\n            config.rope_parameters = rope_parameters_per_layer_type"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/models/mixtral/modeling_mixtral.py#MixtralSparseMoeBlock": {
                "sorted_modules": {
                    "MixtralSparseMoeBlock": "\n\nclass MixtralSparseMoeBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.top_k = config.num_experts_per_tok\n        self.jitter_noise = config.router_jitter_noise\n        self.gate = nn.Linear(config.hidden_size, config.num_experts, bias=False)\n        self.experts = MixtralExperts(config)\n\n    def route_tokens_to_experts(self, router_logits):\n        routing_weights = torch.nn.functional.softmax(router_logits.float(), dim=-1)\n        top_k_weights, top_k_index = torch.topk(routing_weights, self.top_k, dim=-1)\n        top_k_weights /= top_k_weights.sum(dim=-1, keepdim=True)\n        return top_k_index, top_k_weights.to(router_logits.dtype)\n\n    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n        batch_size, sequence_length, hidden_dim = hidden_states.shape\n        if self.training and self.jitter_noise > 0:\n            hidden_states *= torch.empty_like(hidden_states).uniform_(1.0 - self.jitter_noise, 1.0 + self.jitter_noise)\n        hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n        router_logits = self.gate(hidden_states)\n        top_k_index, top_k_weights = self.route_tokens_to_experts(router_logits)\n        hidden_states = self.experts(hidden_states, top_k_index, top_k_weights.to(hidden_states.dtype))\n        hidden_states = hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n        return hidden_states"
                },
                "component_dependencies": {
                    "MixtralSparseMoeBlock": [
                        "transformers/models/mixtral/modeling_mixtral.py#MixtralExperts"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_rope_utils.py#ROPE_INIT_FUNCTIONS": {
                "sorted_modules": {
                    "ROPE_INIT_FUNCTIONS": "\n\n# This maps the \"rope_type\" string field in rope config to the corresponding function to compute the RoPE parameters\n# from the model config. You can append new {'rope_type': callable} pairs to this rope_parameters to enable custom RoPE\n# parameterizations, as long as the callable has the same signature.\nROPE_INIT_FUNCTIONS = {\n    \"linear\": _compute_linear_scaling_rope_parameters,\n    \"dynamic\": _compute_dynamic_ntk_parameters,\n    \"yarn\": _compute_yarn_parameters,\n    \"longrope\": _compute_longrope_parameters,\n    \"llama3\": _compute_llama3_parameters,\n}"
                },
                "component_dependencies": {
                    "ROPE_INIT_FUNCTIONS": [
                        "transformers/modeling_rope_utils.py#_compute_dynamic_ntk_parameters",
                        "transformers/modeling_rope_utils.py#_compute_linear_scaling_rope_parameters",
                        "transformers/modeling_rope_utils.py#_compute_llama3_parameters",
                        "transformers/modeling_rope_utils.py#_compute_longrope_parameters",
                        "transformers/modeling_rope_utils.py#_compute_yarn_parameters"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_rope_utils.py#dynamic_rope_update": {
                "sorted_modules": {
                    "dynamic_rope_update": "\n\ndef dynamic_rope_update(rope_forward):\n    \"\"\"\n    Decorator function to update the RoPE parameters in the forward pass, if the model is using a dynamic RoPE\n    (i.e. a RoPE implementation that may recompute its frequencies in the forward pass).\n\n    Args:\n        rope_forward (Callable):\n            The forward pass of the RoPE implementation.\n\n    Returns:\n        The decorated forward pass.\n    \"\"\"\n\n    def longrope_frequency_update(self, position_ids, device, layer_type=None):\n        \"\"\"Longrope uses long factor if sequence is larger than original pretraining length, short otherwise.\"\"\"\n        seq_len = torch.max(position_ids) + 1\n        original_max_position_embeddings = getattr(\n            self.config, \"original_max_position_embeddings\", self.config.max_position_embeddings\n        )\n        if layer_type is None:\n            rope_type = self.rope_type\n            original_inv_freq = self.original_inv_freq\n            prefix = \"\"\n        else:\n            rope_type = self.rope_type[layer_type]\n            original_inv_freq = getattr(self, f\"{layer_type}_original_inv_freq\")\n            prefix = f\"{layer_type}_\"\n\n        if seq_len > original_max_position_embeddings:\n            if not hasattr(self, f\"{layer_type}_long_inv_freq\"):\n                rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]\n                long_inv_freq, _ = rope_init_fn(\n                    self.config,\n                    device,\n                    seq_len=original_max_position_embeddings + 1,\n                    layer_type=layer_type,\n                )\n            self.register_buffer(f\"{prefix}inv_freq\", long_inv_freq, persistent=False)\n            setattr(self, f\"{prefix}long_inv_freq\", long_inv_freq)\n        else:\n            # This .to() is needed if the model has been moved to a device after being initialized (because\n            # the buffer is automatically moved, but not the original copy)\n            original_inv_freq = original_inv_freq.to(device)\n            self.register_buffer(f\"{prefix}inv_freq\", original_inv_freq, persistent=False)\n            setattr(self, f\"{prefix}original_inv_freq\", original_inv_freq)\n\n    def dynamic_frequency_update(self, position_ids, device, layer_type=None):\n        \"\"\"\n        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n        1 - growing beyond the cached sequence length (allow scaling)\n        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n        \"\"\"\n        seq_len = torch.max(position_ids) + 1\n        if layer_type is None:\n            rope_type = self.rope_type\n            max_seq_len_cached = self.max_seq_len_cached\n            original_inv_freq = self.original_inv_freq\n            prefix = \"\"\n        else:\n            rope_type = self.rope_type[layer_type]\n            max_seq_len_cached = getattr(self, f\"{layer_type}_max_seq_len_cached\", self.max_seq_len_cached)\n            original_inv_freq = getattr(self, f\"{layer_type}_original_inv_freq\")\n            prefix = f\"{layer_type}_\"\n\n        if seq_len > max_seq_len_cached:  # growth\n            rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]\n            inv_freq, self.attention_scaling = rope_init_fn(\n                self.config,\n                device,\n                seq_len=seq_len,\n                layer_type=layer_type,\n            )\n            # TODO joao: may break with compilation\n            self.register_buffer(f\"{prefix}inv_freq\", inv_freq, persistent=False)\n            setattr(self, f\"{layer_type}_max_seq_len_cached\", seq_len)\n\n        if seq_len < self.original_max_seq_len and max_seq_len_cached > self.original_max_seq_len:  # reset\n            # This .to() is needed if the model has been moved to a device after being initialized (because\n            # the buffer is automatically moved, but not the original copy)\n            original_inv_freq = original_inv_freq.to(device)\n            self.register_buffer(f\"{prefix}inv_freq\", original_inv_freq, persistent=False)\n            setattr(self, f\"{prefix}original_inv_freq\", original_inv_freq)\n            setattr(self, f\"{layer_type}_max_seq_len_cached\", self.original_max_seq_len)\n\n    @wraps(rope_forward)\n    def wrapper(self, x, position_ids, layer_type=None):\n        rope_type = self.rope_type if layer_type is None else self.rope_type[layer_type]\n        kwargs = {\"layer_type\": layer_type} if layer_type is not None else {}\n        if \"dynamic\" in rope_type:\n            dynamic_frequency_update(self, position_ids, device=x.device, **kwargs)\n        elif rope_type == \"longrope\":\n            longrope_frequency_update(self, position_ids, device=x.device, **kwargs)\n        return rope_forward(self, x, position_ids, **kwargs)\n\n    return wrapper"
                },
                "component_dependencies": {
                    "dynamic_rope_update": [
                        "transformers/modeling_rope_utils.py#ROPE_INIT_FUNCTIONS"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_utils.py#EmbeddingAccessMixin": {
                "sorted_modules": {
                    "EmbeddingAccessMixin": "\n\nclass EmbeddingAccessMixin:\n    \"\"\"\n    Base utilities to regroup getters and setters for embeddings.\n    Introduces the `input_layer_embed` attribute, which indicates\n    where the input embeddings come from and where they\n    should be set.\n    \"\"\"\n\n    _input_embed_layer = \"embed_tokens\"  # default layer that holds input embeddings.\n\n    def get_input_embeddings(self) -> nn.Module:\n        \"\"\"\n        Returns the model's input embeddings.\n\n        Returns:\n            `nn.Module`: A torch module mapping vocabulary to hidden states.\n        \"\"\"\n\n        # 1) Check if the model has an attribute named 'embed_tokens' (the standard input embedding layer\n        #  for most NLP models), and if so, return it.\n\n        name = getattr(self, \"_input_embed_layer\", \"embed_tokens\")\n\n        if (default_embedding := getattr(self, name, None)) is not None:\n            return default_embedding\n        # 2) encoder/decoder and VLMs like `Gemma3nForConditionalGeneration`\n\n        if hasattr(self, \"model\") and hasattr(self.model, \"embed_tokens\"):\n            return self.model.embed_tokens\n\n        # 3) vanilla decoder\u2011only architectures\n        elif hasattr(self, \"embed_tokens\"):\n            return self.embed_tokens\n        else:\n            base_model = getattr(self, \"base_model_prefix\", None)\n            if base_model is not None:\n                base_model = getattr(self, base_model, None)\n                if base_model is not None and base_model is not self:\n                    return base_model.get_input_embeddings()\n            raise NotImplementedError(\n                f\"`get_input_embeddings` not auto\u2011handled for {self.__class__.__name__}; \"\n                \"please override in the subclass.\"\n            )\n\n    def set_input_embeddings(self, value: nn.Module):\n        \"\"\"Fallback setter that handles **~70%** of models in the code-base.\n\n        Order of attempts:\n        1. `self.model.embed_tokens`\n        2. `self.embed_tokens`\n        3. delegate to the *base model* if one exists\n        4. otherwise raise `NotImplementedError` so subclasses still can (and\n            should) override for exotic layouts.\n        \"\"\"\n\n        # 1) encoder/decoder and VLMs like `Gemma3nForConditionalGeneration`\n        name = getattr(self, \"_input_embed_layer\", \"embed_tokens\")\n        if hasattr(self, \"model\") and hasattr(self.model, name):\n            setattr(self.model, name, value)\n        # 2) as well as vanilla decoder\u2011only architectures\n        elif hasattr(self, name):\n            setattr(self, name, value)\n        # 3) recurse once into the registered *base* model (e.g. for encoder/decoder)\n        elif getattr(self, self.base_model_prefix, self) is not self:\n            base_model = getattr(self, self.base_model_prefix, self)\n            base_model.set_input_embeddings(value)\n        else:\n            raise NotImplementedError(\n                f\"`set_input_embeddings` not auto\u2011handled for {self.__class__.__name__}; please override in the subclass.\"\n            )\n\n    def get_output_embeddings(self):\n        if not hasattr(self, \"lm_head\"):\n            return None\n        try:\n            # Speech / vision backbones raise here, so we return None.\n            # Legit use of get_input_embs?\n            self.get_input_embeddings()\n        except NotImplementedError:\n            return None\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        \"\"\"\n        Sets the model's output embedding, defaulting to setting new_embeddings to lm_head.\n        \"\"\"\n        if getattr(self, \"lm_head\"):\n            self.lm_head = new_embeddings"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#ModuleUtilsMixin": {
                "sorted_modules": {
                    "ModuleUtilsMixin": "\n\nclass ModuleUtilsMixin:\n    \"\"\"\n    A few utilities for `torch.nn.Modules`, to be used as a mixin.\n    \"\"\"\n\n    @staticmethod\n    def _hook_rss_memory_pre_forward(module, *args, **kwargs):\n        try:\n            import psutil\n        except ImportError:\n            raise ImportError(\"You need to install psutil (pip install psutil) to use memory tracing.\")\n\n        process = psutil.Process(os.getpid())\n        mem = process.memory_info()\n        module.mem_rss_pre_forward = mem.rss\n        return None\n\n    @staticmethod\n    def _hook_rss_memory_post_forward(module, *args, **kwargs):\n        try:\n            import psutil\n        except ImportError:\n            raise ImportError(\"You need to install psutil (pip install psutil) to use memory tracing.\")\n\n        process = psutil.Process(os.getpid())\n        mem = process.memory_info()\n        module.mem_rss_post_forward = mem.rss\n        mem_rss_diff = module.mem_rss_post_forward - module.mem_rss_pre_forward\n        module.mem_rss_diff = mem_rss_diff + (module.mem_rss_diff if hasattr(module, \"mem_rss_diff\") else 0)\n        return None\n\n    def add_memory_hooks(self):\n        \"\"\"\n        Add a memory hook before and after each sub-module forward pass to record increase in memory consumption.\n\n        Increase in memory consumption is stored in a `mem_rss_diff` attribute for each module and can be reset to zero\n        with `model.reset_memory_hooks_state()`.\n        \"\"\"\n        for module in self.modules():\n            module.register_forward_pre_hook(self._hook_rss_memory_pre_forward)\n            module.register_forward_hook(self._hook_rss_memory_post_forward)\n        self.reset_memory_hooks_state()\n\n    def reset_memory_hooks_state(self):\n        \"\"\"\n        Reset the `mem_rss_diff` attribute of each module (see [`~modeling_utils.ModuleUtilsMixin.add_memory_hooks`]).\n        \"\"\"\n        for module in self.modules():\n            module.mem_rss_diff = 0\n            module.mem_rss_post_forward = 0\n            module.mem_rss_pre_forward = 0\n\n    @property\n    def device(self) -> torch.device:\n        \"\"\"\n        `torch.device`: The device on which the module is (assuming that all the module parameters are on the same\n        device).\n        \"\"\"\n        return get_parameter_device(self)\n\n    @property\n    def dtype(self) -> torch.dtype:\n        \"\"\"\n        `torch.dtype`: The dtype of the module (assuming that all the module parameters have the same dtype).\n        \"\"\"\n        return get_parameter_dtype(self)\n\n    def invert_attention_mask(self, encoder_attention_mask: Tensor) -> Tensor:\n        \"\"\"\n        Invert an attention mask (e.g., switches 0. and 1.).\n\n        Args:\n            encoder_attention_mask (`torch.Tensor`): An attention mask.\n\n        Returns:\n            `torch.Tensor`: The inverted attention mask.\n        \"\"\"\n        if encoder_attention_mask.dim() == 3:\n            encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n        if encoder_attention_mask.dim() == 2:\n            encoder_extended_attention_mask = encoder_attention_mask[:, None, None, :]\n        # T5 has a mask that can compare sequence ids, we can simulate this here with this transposition\n        # encoder_extended_attention_mask = (encoder_extended_attention_mask ==\n        # encoder_extended_attention_mask.transpose(-1, -2))\n        encoder_extended_attention_mask = encoder_extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n        encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * torch.finfo(self.dtype).min\n\n        return encoder_extended_attention_mask\n\n    @staticmethod\n    def create_extended_attention_mask_for_decoder(input_shape, attention_mask, device=None):\n        if device is not None:\n            warnings.warn(\n                \"The `device` argument is deprecated and will be removed in v5 of Transformers.\", FutureWarning\n            )\n        else:\n            device = attention_mask.device\n        batch_size, seq_length = input_shape\n        seq_ids = torch.arange(seq_length, device=device)\n        causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n        # in case past_key_values are used we need to add a prefix ones mask to the causal mask\n        causal_mask = causal_mask.to(attention_mask.dtype)\n\n        if causal_mask.shape[1] < attention_mask.shape[1]:\n            prefix_seq_len = attention_mask.shape[1] - causal_mask.shape[1]\n            causal_mask = torch.cat(\n                [\n                    torch.ones((batch_size, seq_length, prefix_seq_len), device=device, dtype=causal_mask.dtype),\n                    causal_mask,\n                ],\n                axis=-1,\n            )\n\n        extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n        return extended_attention_mask\n\n    def get_extended_attention_mask(\n        self,\n        attention_mask: Tensor,\n        input_shape: tuple[int, ...],\n        device: Optional[torch.device] = None,\n        dtype: Optional[torch.dtype] = None,\n    ) -> Tensor:\n        \"\"\"\n        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n\n        Arguments:\n            attention_mask (`torch.Tensor`):\n                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n            input_shape (`tuple[int]`):\n                The shape of the input to the model.\n\n        Returns:\n            `torch.Tensor` The extended attention mask, with a the same dtype as `attention_mask.dtype`.\n        \"\"\"\n        if dtype is None:\n            dtype = self.dtype\n\n        if not (attention_mask.dim() == 2 and self.config.is_decoder):\n            # show warning only if it won't be shown in `create_extended_attention_mask_for_decoder`\n            if device is not None:\n                warnings.warn(\n                    \"The `device` argument is deprecated and will be removed in v5 of Transformers.\", FutureWarning\n                )\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        if attention_mask.dim() == 3:\n            extended_attention_mask = attention_mask[:, None, :, :]\n        elif attention_mask.dim() == 2:\n            # Provided a padding mask of dimensions [batch_size, seq_length]\n            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n            if self.config.is_decoder:\n                extended_attention_mask = ModuleUtilsMixin.create_extended_attention_mask_for_decoder(\n                    input_shape, attention_mask, device\n                )\n            else:\n                extended_attention_mask = attention_mask[:, None, None, :]\n        else:\n            raise ValueError(\n                f\"Wrong shape for input_ids (shape {input_shape}) or attention_mask (shape {attention_mask.shape})\"\n            )\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and the dtype's smallest value for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=dtype)  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(dtype).min\n        return extended_attention_mask\n\n    def num_parameters(self, only_trainable: bool = False, exclude_embeddings: bool = False) -> int:\n        \"\"\"\n        Get number of (optionally, trainable or non-embeddings) parameters in the module.\n\n        Args:\n            only_trainable (`bool`, *optional*, defaults to `False`):\n                Whether or not to return only the number of trainable parameters\n\n            exclude_embeddings (`bool`, *optional*, defaults to `False`):\n                Whether or not to return only the number of non-embeddings parameters\n\n        Returns:\n            `int`: The number of parameters.\n        \"\"\"\n\n        if exclude_embeddings:\n            embedding_param_names = [\n                f\"{name}.weight\" for name, module_type in self.named_modules() if isinstance(module_type, nn.Embedding)\n            ]\n            total_parameters = [\n                parameter for name, parameter in self.named_parameters() if name not in embedding_param_names\n            ]\n        else:\n            total_parameters = list(self.parameters())\n\n        total_numel = []\n        is_loaded_in_4bit = getattr(self, \"is_loaded_in_4bit\", False)\n\n        if is_loaded_in_4bit:\n            if is_bitsandbytes_available():\n                import bitsandbytes as bnb\n            else:\n                raise ValueError(\n                    \"bitsandbytes is not installed but it seems that the model has been loaded in 4bit precision, something went wrong\"\n                    \" make sure to install bitsandbytes with `pip install bitsandbytes`. You also need a GPU. \"\n                )\n\n        for param in total_parameters:\n            if param.requires_grad or not only_trainable:\n                # For 4bit models, we need to multiply the number of parameters by 2 as half of the parameters are\n                # used for the 4bit quantization (uint8 tensors are stored)\n                if is_loaded_in_4bit and isinstance(param, bnb.nn.Params4bit):\n                    if hasattr(param, \"element_size\"):\n                        num_bytes = param.element_size()\n                    elif hasattr(param, \"quant_storage\"):\n                        num_bytes = param.quant_storage.itemsize\n                    else:\n                        num_bytes = 1\n                    total_numel.append(param.numel() * 2 * num_bytes)\n                else:\n                    total_numel.append(param.numel())\n\n        return sum(total_numel)\n\n    def estimate_tokens(self, input_dict: dict[str, Union[torch.Tensor, Any]]) -> int:\n        \"\"\"\n        Helper function to estimate the total number of tokens from the model inputs.\n\n        Args:\n            inputs (`dict`): The model inputs.\n\n        Returns:\n            `int`: The total number of tokens.\n        \"\"\"\n        if not hasattr(self, \"warnings_issued\"):\n            self.warnings_issued = {}\n        if self.main_input_name in input_dict:\n            return input_dict[self.main_input_name].numel()\n        elif \"estimate_tokens\" not in self.warnings_issued:\n            logger.warning(\n                \"Could not estimate the number of tokens of the input, floating-point operations will not be computed\"\n            )\n            self.warnings_issued[\"estimate_tokens\"] = True\n        return 0\n\n    def floating_point_ops(\n        self, input_dict: dict[str, Union[torch.Tensor, Any]], exclude_embeddings: bool = True\n    ) -> int:\n        \"\"\"\n        Get number of (optionally, non-embeddings) floating-point operations for the forward and backward passes of a\n        batch with this transformer model. Default approximation neglects the quadratic dependency on the number of\n        tokens (valid if `12 * d_model << sequence_length`) as laid out in [this\n        paper](https://huggingface.co/papers/2001.08361) section 2.1. Should be overridden for transformers with parameter\n        re-use e.g. Albert or Universal Transformers, or if doing long-range modeling with very high sequence lengths.\n\n        Args:\n            batch_size (`int`):\n                The batch size for the forward pass.\n\n            sequence_length (`int`):\n                The number of tokens in each line of the batch.\n\n            exclude_embeddings (`bool`, *optional*, defaults to `True`):\n                Whether or not to count embedding and softmax operations.\n\n        Returns:\n            `int`: The number of floating-point operations.\n        \"\"\"\n\n        return 6 * self.estimate_tokens(input_dict) * self.num_parameters(exclude_embeddings=exclude_embeddings)"
                },
                "component_dependencies": {
                    "ModuleUtilsMixin": [
                        "transformers/modeling_utils.py#get_parameter_device",
                        "transformers/modeling_utils.py#get_parameter_dtype",
                        "transformers/modeling_utils.py#logger",
                        "transformers/utils.py#is_bitsandbytes_available"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#_init_weights": {
                "sorted_modules": {
                    "_init_weights": "_init_weights = True"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/quantizers/quantizers_utils.py#get_module_from_name": {
                "sorted_modules": {
                    "get_module_from_name": "\n\ndef get_module_from_name(module, tensor_name: str) -> tuple[Any, str]:\n    if \".\" in tensor_name:\n        module_name, tensor_name = tensor_name.rsplit(\".\", 1)\n        module = module.get_submodule(module_name)\n    return module, tensor_name"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/models/mixtral/modeling_mixtral.py#apply_rotary_pos_emb": {
                "sorted_modules": {
                    "apply_rotary_pos_emb": "\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n\n    Args:\n        q (`torch.Tensor`): The query tensor.\n        k (`torch.Tensor`): The key tensor.\n        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n        sin (`torch.Tensor`): The sine part of the rotary embedding.\n        position_ids (`torch.Tensor`, *optional*):\n            Deprecated and unused.\n        unsqueeze_dim (`int`, *optional*, defaults to 1):\n            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n    Returns:\n        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n    \"\"\"\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed"
                },
                "component_dependencies": {
                    "apply_rotary_pos_emb": [
                        "transformers/models/mixtral/modeling_mixtral.py#rotate_half"
                    ]
                },
                "warning": null
            },
            "transformers/models/mixtral/modeling_mixtral.py#eager_attention_forward": {
                "sorted_modules": {
                    "eager_attention_forward": "\n\ndef eager_attention_forward(\n    module: nn.Module,\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    attention_mask: Optional[torch.Tensor],\n    scaling: float,\n    dropout: float = 0.0,\n    **kwargs: Unpack[TransformersKwargs],\n):\n    key_states = repeat_kv(key, module.num_key_value_groups)\n    value_states = repeat_kv(value, module.num_key_value_groups)\n\n    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n    if attention_mask is not None:\n        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n        attn_weights = attn_weights + causal_mask\n\n    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n    attn_output = torch.matmul(attn_weights, value_states)\n    attn_output = attn_output.transpose(1, 2).contiguous()\n\n    return attn_output, attn_weights"
                },
                "component_dependencies": {
                    "eager_attention_forward": [
                        "transformers/models/mixtral/modeling_mixtral.py#repeat_kv",
                        "transformers/processing_utils.py#Unpack",
                        "transformers/utils.py#TransformersKwargs"
                    ]
                },
                "warning": null
            },
            "transformers/masking_utils.py#find_packed_sequence_indices": {
                "sorted_modules": {
                    "find_packed_sequence_indices": "\n\ndef find_packed_sequence_indices(position_ids: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Find the indices of the sequence to which each new query token in the sequence belongs when using packed\n    tensor format (i.e. several sequences packed in the same batch dimension).\n\n    Args:\n        position_ids (`torch.Tensor`)\n            A 2D tensor of shape (batch_size, query_length) indicating the positions of each token in the sequences.\n\n    Returns:\n        A 2D tensor where each similar integer indicates that the tokens belong to the same sequence. For example, if we\n        pack 3 sequences of 2, 3 and 1 tokens respectively along a single batch dim, this will return [[0, 0, 1, 1, 1, 2]].\n    \"\"\"\n    # What separate different sequences is when 2 consecutive positions_ids are separated by more than 1. So\n    # taking the diff (by prepending the first value - 1 to keep correct indexing) and applying cumsum to the result\n    # gives exactly the sequence indices\n    # Note that we assume that a single sequence cannot span several batch dimensions, i.e. 1 single sequence\n    # cannot be part of the end of the first batch dim and the start of the 2nd one for example\n    first_dummy_value = position_ids[:, :1] - 1  # We just need the diff on this first value to be 1\n    position_diff = torch.diff(position_ids, prepend=first_dummy_value, dim=-1)\n    packed_sequence_mask = (position_diff != 1).cumsum(-1)\n\n    # Here it would be nice to return None if we did not detect packed sequence format, i.e. if `packed_sequence_mask[:, -1] == 0`\n    # but it causes issues with export\n    return packed_sequence_mask"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/masking_utils.py#and_masks": {
                "sorted_modules": {
                    "and_masks": "\n\ndef and_masks(*mask_functions: Callable) -> Callable:\n    \"\"\"Returns a mask function that is the intersection of provided mask functions\"\"\"\n    if not all(callable(arg) for arg in mask_functions):\n        raise RuntimeError(f\"All inputs should be callable mask_functions: {mask_functions}\")\n\n    def and_mask(batch_idx, head_idx, q_idx, kv_idx):\n        result = q_idx.new_ones((), dtype=torch.bool)\n        for mask in mask_functions:\n            result = result & mask(batch_idx, head_idx, q_idx, kv_idx).to(result.device)\n        return result\n\n    return and_mask"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/masking_utils.py#sliding_window_overlay": {
                "sorted_modules": {
                    "sliding_window_overlay": "\n\ndef sliding_window_overlay(sliding_window: int) -> Callable:\n    \"\"\"\n    This is an overlay depicting a sliding window pattern. Add it on top of a causal mask for a proper sliding\n    window mask.\n    \"\"\"\n\n    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n        return kv_idx > q_idx - sliding_window\n\n    return inner_mask"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/modeling_rope_utils.py#ROPE_VALIDATION_FUNCTIONS": {
                "sorted_modules": {
                    "ROPE_VALIDATION_FUNCTIONS": "\n\n# Like `ROPE_INIT_FUNCTIONS`, this validation function mapping can be dynamically updated for custom RoPE types.\nROPE_VALIDATION_FUNCTIONS = {\n    \"default\": _validate_default_rope_parameters,\n    \"linear\": _validate_linear_scaling_rope_parameters,\n    \"dynamic\": _validate_dynamic_scaling_rope_parameters,\n    \"yarn\": _validate_yarn_parameters,\n    \"longrope\": _validate_longrope_parameters,\n    \"llama3\": _validate_llama3_parameters,\n}"
                },
                "component_dependencies": {
                    "ROPE_VALIDATION_FUNCTIONS": [
                        "transformers/modeling_rope_utils.py#_validate_default_rope_parameters",
                        "transformers/modeling_rope_utils.py#_validate_dynamic_scaling_rope_parameters",
                        "transformers/modeling_rope_utils.py#_validate_linear_scaling_rope_parameters",
                        "transformers/modeling_rope_utils.py#_validate_llama3_parameters",
                        "transformers/modeling_rope_utils.py#_validate_longrope_parameters",
                        "transformers/modeling_rope_utils.py#_validate_yarn_parameters"
                    ]
                },
                "warning": null
            },
            "transformers/models/mixtral/modeling_mixtral.py#MixtralExperts": {
                "sorted_modules": {
                    "MixtralExperts": "\n\nclass MixtralExperts(nn.ModuleList):\n    \"\"\"\n    ModuleList of experts.\n    \"\"\"\n\n    def __init__(self, config: MixtralConfig):\n        super().__init__()\n        self.top_k = config.num_experts_per_tok\n        self.num_experts = config.num_local_experts\n        for _ in range(self.num_experts):\n            self.append(MixtralMLP(config))\n\n    def forward(\n        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            hidden_states: (batch_size * sequence_length, hidden_dim)\n            selected_experts: (batch_size * sequence_length, top_k)\n            routing_weights: (batch_size * sequence_length, top_k)\n        Returns:\n            (batch_size * sequence_length, hidden_dim)\n        \"\"\"\n        final_hidden_states = torch.zeros_like(hidden_states)\n        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n\n        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n        for expert_idx in expert_hit:\n            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n        return final_hidden_states"
                },
                "component_dependencies": {
                    "MixtralExperts": [
                        "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig",
                        "transformers/models/mixtral/modeling_mixtral.py#MixtralMLP"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_rope_utils.py#_compute_dynamic_ntk_parameters": {
                "sorted_modules": {
                    "_compute_dynamic_ntk_parameters": "\n\ndef _compute_dynamic_ntk_parameters(\n    config: Optional[PreTrainedConfig] = None,\n    device: Optional[\"torch.device\"] = None,\n    seq_len: Optional[int] = None,\n    layer_type: Optional[str] = None,\n) -> tuple[\"torch.Tensor\", float]:\n    \"\"\"\n    Computes the inverse frequencies with NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\n\n    Args:\n        config ([`~transformers.PreTrainedConfig`]):\n            The model configuration. This function assumes that the config will provide at least the following\n            properties:\n\n            *   rope_theta (`float`): The base wavelength from which the inverse frequencies will be derived.\n            *   hidden_size (`int`): The numerator when deriving a head_dim, if not provided directly.\n            *   num_attention_heads (`int`): The denominator when deriving a head_dim, if not provided directly.\n            *   max_position_embeddings (`int`): The default sequence length used to update the dynamic RoPE at\n                inference time\n            *   rope_parameters (`dict[str, float]`): The standard RoPE scaling parameters, from which `factor`\n                will be accessed. The value of `factor` is used to determine the new base frequency, along with the\n                current sequence length (seq_len), the maximum positional embeddings (max_position_embeddings), and the\n                computed dimensionality (dim) of the rotary embeddings. If seq_len <= max_position_embeddings, this\n                factor has no effect. If seq_len <= max_position_embeddings, this factor effectively stretches the\n                context window using an exponent derived from `dim`.\n\n            Additionally, this function will make use of the following properties if they are found in the config:\n\n            *   head_dim (`int`, *optional*): The size of the key-value heads in the model. If None, this value will be\n                derived as hidden_size // num_attention_heads.\n            *   partial_rotary_factor (`float`, *optional*): If less than 1.0, inverse frequencies will be returned for\n                the first fraction of the head_dim. Defaults to 1.0.\n        device (`torch.device`):\n            The device to use for initialization of the inverse frequencies.\n        seq_len (`int`, *optional*):\n            The current sequence length, used to update the dynamic RoPE at inference time. If `None` or shorter than\n            max_position_embeddings, this value will be overridden by max_position_embeddings.\n\n    Returns:\n        Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n        post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n    \"\"\"\n    # TODO (joao): use the new `original_max_position_embeddings` from rope_parameters\n    # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n    standardize_rope_params(config)\n    rope_parameters_dict = config.rope_parameters[layer_type] if layer_type is not None else config.rope_parameters\n\n    base = rope_parameters_dict[\"rope_theta\"]\n    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n    head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n    dim = int(head_dim * partial_rotary_factor)\n    max_position_embeddings = config.max_position_embeddings\n    factor = rope_parameters_dict[\"factor\"]\n    attention_factor = 1.0  # Unused in this type of RoPE\n\n    # seq_len: default to max_position_embeddings, e.g. at init time\n    if seq_len is None:\n        seq_len = max_position_embeddings\n    elif isinstance(seq_len, torch.Tensor):\n        seq_len = torch.maximum(\n            seq_len,\n            torch.tensor(max_position_embeddings, dtype=seq_len.dtype, device=seq_len.device),\n        )\n    else:\n        seq_len = max(seq_len, max_position_embeddings)\n\n    # Compute the inverse frequencies\n    base = base * ((factor * seq_len / max_position_embeddings) - (factor - 1)) ** (dim / (dim - 2))\n    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim))\n    return inv_freq, attention_factor"
                },
                "component_dependencies": {
                    "_compute_dynamic_ntk_parameters": [
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/modeling_rope_utils.py#standardize_rope_params"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_rope_utils.py#_compute_linear_scaling_rope_parameters": {
                "sorted_modules": {
                    "_compute_linear_scaling_rope_parameters": "\n\ndef _compute_linear_scaling_rope_parameters(\n    config: Optional[PreTrainedConfig] = None,\n    device: Optional[\"torch.device\"] = None,\n    seq_len: Optional[int] = None,\n    layer_type: Optional[str] = None,\n) -> tuple[\"torch.Tensor\", float]:\n    \"\"\"\n    Computes the inverse frequencies with linear scaling. Credits to the Reddit user /u/kaiokendev\n    Args:\n        config ([`~transformers.PreTrainedConfig`]):\n            The model configuration. This function assumes that the config will provide at least the following\n            properties:\n\n            *   rope_theta (`float`): The base wavelength from which the inverse frequencies will be derived.\n            *   hidden_size (`int`): The numerator when deriving a head_dim, if not provided directly.\n            *   num_attention_heads (`int`): The denominator when deriving a head_dim, if not provided directly.\n\n            Additionally, this function will make use of the following properties if they are found in the config:\n\n            *   head_dim (`int`, *optional*): The size of the key-value heads in the model. If None, this value will be\n                derived as hidden_size // num_attention_heads.\n            *   partial_rotary_factor (`float`, *optional*): If less than 1.0, inverse frequencies will be returned for\n                the first fraction of the head_dim. Defaults to 1.0.\n        device (`torch.device`):\n            The device to use for initialization of the inverse frequencies.\n        seq_len (`int`, *optional*):\n            The current sequence length. Unused for this type of RoPE.\n\n    Returns:\n        Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n        post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n    \"\"\"\n    # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n    standardize_rope_params(config)\n    rope_parameters_dict = config.rope_parameters[layer_type] if layer_type is not None else config.rope_parameters\n    factor = rope_parameters_dict[\"factor\"]\n\n    # Gets the default RoPE parameters\n    base = rope_parameters_dict[\"rope_theta\"]\n    partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n    head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n    dim = int(head_dim * partial_rotary_factor)\n    attention_factor = 1.0  # Unused in this type of RoPE\n\n    # Compute the inverse frequencies\n    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim))\n\n    # Then applies linear scaling to the frequencies.\n    # NOTE: originally, scaling was applied to the position_ids. However, we get `embs = inv_freq @ position_ids`, so\n    # applying scaling to the inverse frequencies is equivalent.\n    inv_freq /= factor\n    return inv_freq, attention_factor"
                },
                "component_dependencies": {
                    "_compute_linear_scaling_rope_parameters": [
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/modeling_rope_utils.py#standardize_rope_params"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_rope_utils.py#_compute_llama3_parameters": {
                "sorted_modules": {
                    "_compute_llama3_parameters": "\n\ndef _compute_llama3_parameters(\n    config: PreTrainedConfig,\n    device: \"torch.device\",\n    seq_len: Optional[int] = None,\n    layer_type: Optional[str] = None,\n) -> tuple[\"torch.Tensor\", float]:\n    \"\"\"\n    Computes the inverse frequencies for llama 3.1.\n\n    Args:\n        config ([`~transformers.PreTrainedConfig`]):\n            The model configuration. This function assumes that the config will provide at least the following\n            properties:\n\n            *   rope_theta (`float`): The base wavelength from which the inverse frequencies will be derived.\n            *   hidden_size (`int`): The numerator when deriving a head_dim, if not provided directly.\n            *   num_attention_heads (`int`): The denominator when deriving a head_dim, if not provided directly.\n            *   rope_parameters (`dict[str, float | int]`): The standard RoPE scaling parameters, from which the following\n                keys will be accessed:\n                *   `factor` (`float`, *optional*): The scaling factor applied to the inverse frequencies when 1) the\n                    wavelength is greater than `low_freq_wavelen` prior to smoothing, and 2) to all inverse frequencies\n                    during smoothing.\n                *   `high_freq_factor` (`float`): The scale factor used to compute `high_freq_wavelen` and\n                    the value for the denominator of the smoothing factor prior to the `low_freq_factor` shift.\n                *   `low_freq_factor` (`float`): The scale factor used to compute `low_freq_wavelen` and\n                    the shift applied to the numerator and denominator of the smoothing factor.\n                    frequencies if `seq_len` is None or less-than-or-equal-to `original_max_position_embeddings`.\n                *   `original_max_position_embeddings` (`int`): The original max position embeddings used\n                    during pretraining. If not provided, the function falls back to `max_position_embeddings`.\n\n            Additionally, this function will make use of the following properties if they are found in the config:\n\n            *   head_dim (`int`, *optional*): The size of the key-value heads in the model. If None, this value will be\n                derived as hidden_size // num_attention_heads.\n            *   partial_rotary_factor (`float`, *optional*): If less than 1.0, inverse frequencies will be returned for\n                the first fraction of the head_dim. Defaults to 1.0.\n        device (`torch.device`):\n            The device to use for initialization of the inverse frequencies.\n        seq_len (`int`, *optional*):\n            The current sequence length. Unused for this type of RoPE.\n    Returns:\n        Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n        post-processing scaling factor applied to the computed cos/sin.\n    \"\"\"\n    # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n    standardize_rope_params(config)\n    rope_parameters_dict = config.rope_parameters[layer_type] if layer_type is not None else config.rope_parameters\n\n    # Gets the default RoPE parameters\n    base = rope_parameters_dict[\"rope_theta\"]\n    partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n    head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n    dim = int(head_dim * partial_rotary_factor)\n    attention_factor = 1.0  # Unused in this type of RoPE\n\n    # Compute the inverse frequencies\n    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim))\n\n    factor = rope_parameters_dict[\"factor\"]  # `8` in the original implementation\n    low_freq_factor = rope_parameters_dict[\"low_freq_factor\"]  # `1` in the original implementation\n    high_freq_factor = rope_parameters_dict[\"high_freq_factor\"]  # `4` in the original implementation\n    old_context_len = rope_parameters_dict[\"original_max_position_embeddings\"]  # `8192` in the original implementation\n\n    low_freq_wavelen = old_context_len / low_freq_factor\n    high_freq_wavelen = old_context_len / high_freq_factor\n\n    wavelen = 2 * math.pi / inv_freq\n    # wavelen < high_freq_wavelen: do nothing\n    # wavelen > low_freq_wavelen: divide by factor\n    inv_freq_llama = torch.where(wavelen > low_freq_wavelen, inv_freq / factor, inv_freq)\n    # otherwise: interpolate between the two, using a smooth factor\n    smooth_factor = (old_context_len / wavelen - low_freq_factor) / (high_freq_factor - low_freq_factor)\n    smoothed_inv_freq = (1 - smooth_factor) * inv_freq_llama / factor + smooth_factor * inv_freq_llama\n    is_medium_freq = ~(wavelen < high_freq_wavelen) * ~(wavelen > low_freq_wavelen)\n    inv_freq_llama = torch.where(is_medium_freq, smoothed_inv_freq, inv_freq_llama)\n\n    return inv_freq_llama, attention_factor"
                },
                "component_dependencies": {
                    "_compute_llama3_parameters": [
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/modeling_rope_utils.py#standardize_rope_params"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_rope_utils.py#_compute_longrope_parameters": {
                "sorted_modules": {
                    "_compute_longrope_parameters": "\n\ndef _compute_longrope_parameters(\n    config: PreTrainedConfig,\n    device: \"torch.device\",\n    seq_len: Optional[int] = None,\n    layer_type: Optional[str] = None,\n) -> tuple[\"torch.Tensor\", float]:\n    \"\"\"\n    Computes the inverse frequencies with LongRoPE scaling. Please refer to the\n    [original implementation](https://github.com/microsoft/LongRoPE)\n\n    Args:\n        config ([`~transformers.PreTrainedConfig`]):\n            The model configuration. This function assumes that the config will provide at least the following\n            properties:\n\n            *   rope_theta (`float`): The base wavelength from which the inverse frequencies will be derived.\n            *   hidden_size (`int`): The numerator when deriving a head_dim, if not provided directly.\n            *   num_attention_heads (`int`): The denominator when deriving a head_dim, if not provided directly.\n            *   max_position_embeddings (`int`): The maximum length of the positional embeddings.\n            *   original_max_position_embeddings (`int`, *optional*): The original max position embeddings used during\n                pretraining. If not provided, defaults to `max_position_embeddings`.\n            *   rope_parameters (`dict[str, float]`): The standard RoPE scaling parameters, from which the following keys\n                will be accessed:\n                *   `attention_factor` (`float`, *optional*): The scaling factor to be applied on the attention\n                    computation. If unspecified, it defaults to value recommended by the implementation, inferred from\n                    the value of `factor`.\n                *   `factor` (`float`, *optional*): The scaling factor to apply to the RoPE embeddings. If both\n                    `max_position_embeddings` and `original_max_position_embeddings` are provided, this value will be\n                    overridden s the ratio between those values.\n                *   `long_factor` (`float`, *optional*): The scale factor applied when computing the inverse\n                    frequencies if `seq_len` is provided and greater than `original_max_position_embeddings`.\n                *   `short_factor` (`float`, *optional*): The scale factor applied when computing the inverse\n                    frequencies if `seq_len` is None or less-than-or-equal-to `original_max_position_embeddings`.\n\n            Additionally, this function will make use of the following properties if they are found in the config:\n\n            *   head_dim (`int`, *optional*): The size of the key-value heads in the model. If None, this value will be\n                derived as hidden_size // num_attention_heads.\n            *   partial_rotary_factor (`float`, *optional*, defaults to 1.0): If less than 1.0, inverse frequencies\n                will be returned for the first fraction of the head_dim.\n        device (`torch.device`):\n            The device to use for initialization of the inverse frequencies.\n        seq_len (`int`, *optional*):\n            The current sequence length.\n\n    Returns:\n        Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n        post-processing scaling factor applied to the computed cos/sin.\n    \"\"\"\n    # TODO (joao): use the new `original_max_position_embeddings` from rope_parameters\n    # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n    standardize_rope_params(config)\n    rope_parameters_dict = config.rope_parameters[layer_type] if layer_type is not None else config.rope_parameters\n\n    base = rope_parameters_dict[\"rope_theta\"]\n    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n    head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n    dim = int(head_dim * partial_rotary_factor)\n\n    long_factor = rope_parameters_dict[\"long_factor\"]\n    short_factor = rope_parameters_dict[\"short_factor\"]\n    factor = rope_parameters_dict.get(\"factor\")\n    attention_factor = rope_parameters_dict.get(\"attention_factor\")\n\n    # NOTE: Phi3 (and potentially other models) modify `max_position_embeddings` and have a\n    # `original_max_position_embeddings` field containing the pretrained value. They use the ratio between these two\n    # values to compute the default attention scaling factor, instead of using `factor`.\n    if original_max_position_embeddings := getattr(config, \"original_max_position_embeddings\", None):\n        factor = config.max_position_embeddings / original_max_position_embeddings\n    else:\n        original_max_position_embeddings = config.max_position_embeddings\n\n    # Sets the attention factor as suggested in the paper\n    if attention_factor is None:\n        if factor <= 1.0:\n            attention_factor = 1.0\n        else:\n            attention_factor = math.sqrt(1 + math.log(factor) / math.log(original_max_position_embeddings))\n\n    # Compute the inverse frequencies -- scaled based on the target sequence length\n    if seq_len and seq_len > original_max_position_embeddings:\n        ext_factors = torch.tensor(long_factor, dtype=torch.float32, device=device)\n    else:\n        ext_factors = torch.tensor(short_factor, dtype=torch.float32, device=device)\n    inv_freq_shape = torch.arange(0, dim, 2, dtype=torch.int64, device=device).float() / dim\n    inv_freq = 1.0 / (ext_factors * base**inv_freq_shape)\n\n    return inv_freq, attention_factor"
                },
                "component_dependencies": {
                    "_compute_longrope_parameters": [
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/modeling_rope_utils.py#standardize_rope_params"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_rope_utils.py#_compute_yarn_parameters": {
                "sorted_modules": {
                    "_compute_yarn_parameters": "\n\ndef _compute_yarn_parameters(\n    config: PreTrainedConfig,\n    device: \"torch.device\",\n    seq_len: Optional[int] = None,\n    layer_type: Optional[str] = None,\n) -> tuple[\"torch.Tensor\", float]:\n    \"\"\"\n    Computes the inverse frequencies with NTK scaling. Please refer to the\n    [original paper](https://huggingface.co/papers/2309.00071)\n\n    Args:\n        config ([`~transformers.PreTrainedConfig`]):\n            The model configuration. This function assumes that the config will provide at least the following\n            properties:\n\n            *   rope_theta (`float`): The base wavelength from which the inverse frequencies will be derived.\n            *   hidden_size (`int`): The numerator when deriving a head_dim, if not provided directly.\n            *   num_attention_heads (`int`): The denominator when deriving a head_dim, if not provided directly.\n            *   max_position_embeddings (`int`): The maximum length of the positional embeddings.\n            *   rope_parameters (`dict[str, float | int]`): The standard RoPE scaling parameters, from which the following\n                keys will be accessed:\n                *   `attention_factor` (`float`, *optional*): The scaling factor to be applied to the computed cos/sin.\n                    If None, the value is inferred from `factor`, `mscale`, and `mscale_all_dim` as avaialble.\n                *   `beta_fast` (`float`, *optional*, defaults to 32): Parameter to set the boundary for extrapolation\n                    (only) in the linear ramp function.\n                *   `beta_slow` (`float`, *optional*, defaults to 1): Parameter to set the boundary for interpolation\n                    (only) in the linear ramp function.\n                *   `factor` (`float`, *optional*): The scaling factor applied when interpolating the position IDs to\n                    extend the possible context length. Additionally, if `attention_factor` is None, the log of this\n                    value is used to compute a value for `attention_factor`, possibly in conjunciton with `mscale` and\n                    `mscale_all_dim`, if provided.\n                *   `mscale` (`float`, *optional*): If `attention_factor` is None and both `mscale` and\n                    `mscale_all_dim` are provided, `mscale` acts scalar augmenting `log(factor)` when computing the\n                    numerator for the inferred value of `attention_factor`. If not provided, `attention_factor` will be\n                    calculated based on `factor` only.\n                *   `mscale_all_dim` (`float`, *optional*): If `attention_factor` is None and both `mscale` and\n                    `mscale_all_dim` are provided, `mscale_all_dim` acts scalar augmenting `log(factor)` when computing\n                    the denominator for the inferred value of `attention_factor`. If not provided, `attention_factor`\n                    will be calculated based on `factor` only.\n                *   `original_max_position_embeddings` (`int`, *optional*): The original max position embeddings used\n                    during pretraining. If not provided, the function falls back to `max_position_embeddings`.\n                *   `truncate` (`bool`, *optional*): Whether to truncate the correction range.\n\n            Additionally, this function will make use of the following properties if they are found in the config:\n\n            *   head_dim (`int`, *optional*): The size of the key-value heads in the model. If None, this value will be\n                derived as hidden_size // num_attention_heads.\n            *   partial_rotary_factor (`float`, *optional*, defaults to 1.0): If less than 1.0, inverse frequencies\n                will be returned for the first fraction of the head_dim.\n        device (`torch.device`):\n            The device to use for initialization of the inverse frequencies.\n        seq_len (`int`, *optional*):\n            The current sequence length. Unused for this type of RoPE.\n\n    Returns:\n        Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n        post-processing scaling factor applied to the computed cos/sin.\n    \"\"\"\n    # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n    standardize_rope_params(config)\n    rope_parameters_dict = config.rope_parameters[layer_type] if layer_type is not None else config.rope_parameters\n\n    base = rope_parameters_dict[\"rope_theta\"]\n    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n    head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n    dim = int(head_dim * partial_rotary_factor)\n\n    factor = rope_parameters_dict[\"factor\"]\n    attention_factor = rope_parameters_dict.get(\"attention_factor\")\n    mscale = rope_parameters_dict.get(\"mscale\")\n    mscale_all_dim = rope_parameters_dict.get(\"mscale_all_dim\")\n\n    # NOTE: DeekSeek-V3 (and potentially other models) modify `max_position_embeddings` and have a\n    # `original_max_position_embeddings` field containing the pretrained value. They use the ratio between these two\n    # values to compute the default attention scaling factor, instead of using `factor`.\n    if \"original_max_position_embeddings\" in rope_parameters_dict:\n        original_max_position_embeddings = rope_parameters_dict[\"original_max_position_embeddings\"]\n        factor = config.max_position_embeddings / original_max_position_embeddings\n    else:\n        original_max_position_embeddings = config.max_position_embeddings\n\n    def get_mscale(scale, mscale=1):\n        if scale <= 1:\n            return 1.0\n        return 0.1 * mscale * math.log(scale) + 1.0\n\n    # Sets the attention factor as suggested in the paper\n    if attention_factor is None:\n        if mscale and mscale_all_dim:\n            attention_factor = float(get_mscale(factor, mscale) / get_mscale(factor, mscale_all_dim))\n        else:\n            attention_factor = get_mscale(factor)\n\n    # Optional config options\n    # beta_fast/beta_slow: as suggested in the paper, default to 32/1 (correspondingly)\n    beta_fast = rope_parameters_dict.get(\"beta_fast\") or 32\n    beta_slow = rope_parameters_dict.get(\"beta_slow\") or 1\n\n    # Compute the inverse frequencies\n    def find_correction_dim(num_rotations, dim, base, max_position_embeddings):\n        \"\"\"Inverse dimension formula to find the dimension based on the number of rotations\"\"\"\n        return (dim * math.log(max_position_embeddings / (num_rotations * 2 * math.pi))) / (2 * math.log(base))\n\n    def find_correction_range(low_rot, high_rot, dim, base, max_position_embeddings, truncate):\n        \"\"\"Find dimension range bounds based on rotations\"\"\"\n        low = find_correction_dim(low_rot, dim, base, max_position_embeddings)\n        high = find_correction_dim(high_rot, dim, base, max_position_embeddings)\n        if truncate:\n            low = math.floor(low)\n            high = math.ceil(high)\n        return max(low, 0), min(high, dim - 1)\n\n    def linear_ramp_factor(min, max, dim):\n        if min == max:\n            max += 0.001  # Prevent singularity\n\n        linear_func = (torch.arange(dim, dtype=torch.float32) - min) / (max - min)\n        ramp_func = torch.clamp(linear_func, 0, 1)\n        return ramp_func\n\n    # Note on variable naming: \"interpolation\" comes from the original technique, where we interpolate the position IDs\n    # to expand the possible context length. In other words, interpolation = apply scaling factor.\n    pos_freqs = base ** (torch.arange(0, dim, 2).to(device=device, dtype=torch.float) / dim)\n    inv_freq_extrapolation = 1.0 / pos_freqs\n    inv_freq_interpolation = 1.0 / (factor * pos_freqs)\n\n    truncate = config.rope_parameters.get(\"truncate\", True)\n    low, high = find_correction_range(beta_fast, beta_slow, dim, base, original_max_position_embeddings, truncate)\n\n    # Get n-dimensional rotational scaling corrected for extrapolation\n    inv_freq_extrapolation_factor = 1 - linear_ramp_factor(low, high, dim // 2).to(device=device, dtype=torch.float)\n    inv_freq = (\n        inv_freq_interpolation * (1 - inv_freq_extrapolation_factor)\n        + inv_freq_extrapolation * inv_freq_extrapolation_factor\n    )\n    return inv_freq, attention_factor"
                },
                "component_dependencies": {
                    "_compute_yarn_parameters": [
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/modeling_rope_utils.py#standardize_rope_params"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_utils.py#get_parameter_device": {
                "sorted_modules": {
                    "get_parameter_device": "\n\ndef get_parameter_device(parameter: Union[nn.Module, \"ModuleUtilsMixin\"]):\n    try:\n        return next(parameter.parameters()).device\n    except StopIteration:\n        # For nn.DataParallel compatibility in PyTorch 1.5\n\n        def find_tensor_attributes(module: nn.Module) -> list[tuple[str, Tensor]]:\n            tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n            return tuples\n\n        gen = parameter._named_members(get_members_fn=find_tensor_attributes)\n        first_tuple = next(gen)\n        return first_tuple[1].device"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#get_parameter_dtype": {
                "sorted_modules": {
                    "get_parameter_dtype": "\n\ndef get_parameter_dtype(parameter: Union[nn.Module, \"ModuleUtilsMixin\"]):\n    \"\"\"\n    Returns the first found floating dtype in parameters if there is one, otherwise returns the last dtype it found.\n    \"\"\"\n    last_dtype = None\n    for t in parameter.parameters():\n        last_dtype = t.dtype\n        if t.is_floating_point():\n            # Adding fix for https://github.com/pytorch/xla/issues/4152\n            # Fixes issue where the model code passes a value that is out of range for XLA_USE_BF16=1\n            # and XLA_DOWNCAST_BF16=1 so the conversion would cast it to -inf\n            # NOTE: `is_torch_xla_available()` is checked last as it induces a graph break in torch dynamo\n            if XLA_USE_BF16 in ENV_VARS_TRUE_VALUES and is_torch_xla_available():\n                return torch.bfloat16\n            if XLA_DOWNCAST_BF16 in ENV_VARS_TRUE_VALUES and is_torch_xla_available():\n                if t.dtype == torch.float:\n                    return torch.bfloat16\n                if t.dtype == torch.double:\n                    return torch.float32\n            return t.dtype\n\n    if last_dtype is not None:\n        # if no floating dtype was found return whatever the first dtype is\n        return last_dtype\n\n    # For nn.DataParallel compatibility in PyTorch > 1.5\n    def find_tensor_attributes(module: nn.Module) -> list[tuple[str, Tensor]]:\n        tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n        return tuples\n\n    gen = parameter._named_members(get_members_fn=find_tensor_attributes)\n    last_tuple = None\n    for gen_tuple in gen:\n        last_tuple = gen_tuple\n        if gen_tuple[1].is_floating_point():\n            return gen_tuple[1].dtype\n\n    if last_tuple is not None:\n        # fallback to the last dtype\n        return last_tuple[1].dtype\n\n    # fallback to buffer dtype\n    for t in parameter.buffers():\n        last_dtype = t.dtype\n        if t.is_floating_point():\n            return t.dtype\n    return last_dtype"
                },
                "component_dependencies": {
                    "get_parameter_dtype": [
                        "transformers/modeling_utils.py#XLA_DOWNCAST_BF16",
                        "transformers/modeling_utils.py#XLA_USE_BF16",
                        "transformers/utils.py#is_torch_xla_available",
                        "transformers/utils/import_utils.py#ENV_VARS_TRUE_VALUES"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/models/mixtral/modeling_mixtral.py#rotate_half": {
                "sorted_modules": {
                    "rotate_half": "\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/models/mixtral/modeling_mixtral.py#repeat_kv": {
                "sorted_modules": {
                    "repeat_kv": "\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/models/mixtral/modeling_mixtral.py#MixtralMLP": {
                "sorted_modules": {
                    "MixtralMLP": "\n\nclass MixtralMLP(nn.Module):\n    def __init__(self, config: MixtralConfig):\n        super().__init__()\n        self.ffn_dim = config.intermediate_size\n        self.hidden_dim = config.hidden_size\n\n        self.w1 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n        self.w2 = nn.Linear(self.ffn_dim, self.hidden_dim, bias=False)\n        self.w3 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n\n        self.act_fn = ACT2FN[config.hidden_act]\n\n    def forward(self, hidden_states):\n        current_hidden_states = self.act_fn(self.w1(hidden_states)) * self.w3(hidden_states)\n        current_hidden_states = self.w2(current_hidden_states)\n        return current_hidden_states"
                },
                "component_dependencies": {
                    "MixtralMLP": [
                        "transformers/activations.py#ACT2FN",
                        "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig"
                    ]
                },
                "warning": null
            },
            "transformers/activations.py#ACT2FN": {
                "sorted_modules": {
                    "ACT2FN": "\n\nACT2CLS = {\n    \"gelu\": GELUActivation,\n    \"gelu_10\": (ClippedGELUActivation, {\"min\": -10, \"max\": 10}),\n    \"gelu_fast\": FastGELUActivation,\n    \"gelu_new\": NewGELUActivation,\n    \"gelu_python\": (GELUActivation, {\"use_gelu_python\": True}),\n    \"gelu_pytorch_tanh\": GELUTanh,\n    \"gelu_python_tanh\": (GELUTanh, {\"use_gelu_tanh_python\": True}),\n    \"gelu_accurate\": AccurateGELUActivation,\n    \"laplace\": LaplaceActivation,\n    \"leaky_relu\": nn.LeakyReLU,\n    \"linear\": LinearActivation,\n    \"mish\": MishActivation,\n    \"quick_gelu\": QuickGELUActivation,\n    \"relu\": nn.ReLU,\n    \"relu2\": ReLUSquaredActivation,\n    \"relu6\": nn.ReLU6,\n    \"sigmoid\": nn.Sigmoid,\n    \"silu\": SiLUActivation,\n    \"swish\": nn.SiLU,\n    \"tanh\": nn.Tanh,\n    \"prelu\": nn.PReLU,\n    \"xielu\": XIELUActivation,\n}\n\nACT2FN = ClassInstantier(ACT2CLS)\n\n\n\nclass ClippedGELUActivation(nn.Module):\n    \"\"\"\n    Clip the range of possible GeLU outputs between [min, max]. This is especially useful for quantization purpose, as\n    it allows mapping negatives values in the GeLU spectrum. For more information on this trick, please refer to\n    https://huggingface.co/papers/2004.09602.\n\n    Gaussian Error Linear Unit. Original Implementation of the gelu activation function in Google Bert repo when\n    initially created.\n\n    For information: OpenAI GPT's gelu is slightly different (and gives slightly different results): 0.5 * x * (1 +\n    torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))). See https://huggingface.co/papers/1606.08415\n    \"\"\"\n\n    def __init__(self, min: float, max: float):\n        if min > max:\n            raise ValueError(f\"min should be < max (got min: {min}, max: {max})\")\n\n        super().__init__()\n        self.min = min\n        self.max = max\n\n    def forward(self, x: Tensor) -> Tensor:\n        return torch.clip(gelu(x), self.min, self.max)\n\ngelu = get_activation(\"gelu\")\n\n\n\ndef get_activation(activation_string):\n    if activation_string in ACT2FN:\n        return ACT2FN[activation_string]\n    else:\n        raise KeyError(f\"function {activation_string} not found in ACT2FN mapping {list(ACT2FN.keys())}\")"
                },
                "component_dependencies": {
                    "ACT2FN": [
                        "transformers/activations.py#AccurateGELUActivation",
                        "transformers/activations.py#ClassInstantier",
                        "transformers/activations.py#FastGELUActivation",
                        "transformers/activations.py#GELUActivation",
                        "transformers/activations.py#GELUTanh",
                        "transformers/activations.py#LaplaceActivation",
                        "transformers/activations.py#LinearActivation",
                        "transformers/activations.py#MishActivation",
                        "transformers/activations.py#NewGELUActivation",
                        "transformers/activations.py#QuickGELUActivation",
                        "transformers/activations.py#ReLUSquaredActivation",
                        "transformers/activations.py#SiLUActivation",
                        "transformers/activations.py#XIELUActivation"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/activations.py#AccurateGELUActivation": {
                "sorted_modules": {
                    "AccurateGELUActivation": "\n\nclass AccurateGELUActivation(nn.Module):\n    \"\"\"\n    Applies GELU approximation that is faster than default and more accurate than QuickGELU. See:\n    https://github.com/hendrycks/GELUs\n\n    Implemented along with MEGA (Moving Average Equipped Gated Attention)\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.precomputed_constant = math.sqrt(2 / math.pi)\n\n    def forward(self, input: Tensor) -> Tensor:\n        return 0.5 * input * (1 + torch.tanh(self.precomputed_constant * (input + 0.044715 * torch.pow(input, 3))))"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/activations.py#ClassInstantier": {
                "sorted_modules": {
                    "ClassInstantier": "\n\nclass ClassInstantier(OrderedDict):\n    def __getitem__(self, key):\n        content = super().__getitem__(key)\n        cls, kwargs = content if isinstance(content, tuple) else (content, {})\n        return cls(**kwargs)"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/activations.py#FastGELUActivation": {
                "sorted_modules": {
                    "FastGELUActivation": "\n\n@use_kernel_forward_from_hub(\"FastGELU\")\nclass FastGELUActivation(nn.Module):\n    \"\"\"\n    Applies GELU approximation that is slower than QuickGELU but more accurate. See: https://github.com/hendrycks/GELUs\n    \"\"\"\n\n    def forward(self, input: Tensor) -> Tensor:\n        return 0.5 * input * (1.0 + torch.tanh(input * 0.7978845608 * (1.0 + 0.044715 * input * input)))"
                },
                "component_dependencies": {
                    "FastGELUActivation": [
                        "transformers/integrations/hub_kernels.py#use_kernel_forward_from_hub"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/activations.py#GELUActivation": {
                "sorted_modules": {
                    "GELUActivation": "\n\n@use_kernel_forward_from_hub(\"GeLU\")\nclass GELUActivation(nn.Module):\n    \"\"\"\n    Original Implementation of the GELU activation function in Google BERT repo when initially created. For\n    information: OpenAI GPT's GELU is slightly different (and gives slightly different results): 0.5 * x * (1 +\n    torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) This is now written in C in nn.functional\n    Also see the Gaussian Error Linear Units paper: https://huggingface.co/papers/1606.08415\n    \"\"\"\n\n    def __init__(self, use_gelu_python: bool = False):\n        super().__init__()\n        if use_gelu_python:\n            self.act = self._gelu_python\n        else:\n            self.act = nn.functional.gelu\n\n    def _gelu_python(self, input: Tensor) -> Tensor:\n        return input * 0.5 * (1.0 + torch.erf(input / math.sqrt(2.0)))\n\n    def forward(self, input: Tensor) -> Tensor:\n        return self.act(input)"
                },
                "component_dependencies": {
                    "GELUActivation": [
                        "transformers/integrations/hub_kernels.py#use_kernel_forward_from_hub"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/activations.py#GELUTanh": {
                "sorted_modules": {
                    "GELUTanh": "\n\n@use_kernel_forward_from_hub(\"GeluTanh\")\nclass GELUTanh(nn.Module):\n    \"\"\"\n    A fast C implementation of the tanh approximation of the GeLU activation function. See\n    https://huggingface.co/papers/1606.08415.\n\n    This implementation is equivalent to NewGELU and FastGELU but much faster. However, it is not an exact numerical\n    match due to rounding errors.\n    \"\"\"\n\n    def __init__(self, use_gelu_tanh_python: bool = False):\n        super().__init__()\n        if use_gelu_tanh_python:\n            self.act = self._gelu_tanh_python\n        else:\n            self.act = functools.partial(nn.functional.gelu, approximate=\"tanh\")\n\n    def _gelu_tanh_python(self, input: Tensor) -> Tensor:\n        return input * 0.5 * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n\n    def forward(self, input: Tensor) -> Tensor:\n        return self.act(input)"
                },
                "component_dependencies": {
                    "GELUTanh": [
                        "transformers/integrations/hub_kernels.py#use_kernel_forward_from_hub"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/activations.py#LaplaceActivation": {
                "sorted_modules": {
                    "LaplaceActivation": "\n\nclass LaplaceActivation(nn.Module):\n    \"\"\"\n    Applies elementwise activation based on Laplace function, introduced in MEGA as an attention activation. See\n    https://huggingface.co/papers/2209.10655\n\n    Inspired by squared relu, but with bounded range and gradient for better stability\n    \"\"\"\n\n    def forward(self, input, mu=0.707107, sigma=0.282095):\n        input = (input - mu).div(sigma * math.sqrt(2.0))\n        return 0.5 * (1.0 + torch.erf(input))"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/activations.py#LinearActivation": {
                "sorted_modules": {
                    "LinearActivation": "\n\nclass LinearActivation(nn.Module):\n    \"\"\"\n    Applies the linear activation function, i.e. forwarding input directly to output.\n    \"\"\"\n\n    def forward(self, input: Tensor) -> Tensor:\n        return input"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/activations.py#MishActivation": {
                "sorted_modules": {
                    "MishActivation": "\n\nclass MishActivation(nn.Module):\n    \"\"\"\n    See Mish: A Self-Regularized Non-Monotonic Activation Function (Misra., https://huggingface.co/papers/1908.08681). Also\n    visit the official repository for the paper: https://github.com/digantamisra98/Mish\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.act = nn.functional.mish\n\n    def _mish_python(self, input: Tensor) -> Tensor:\n        return input * torch.tanh(nn.functional.softplus(input))\n\n    def forward(self, input: Tensor) -> Tensor:\n        return self.act(input)"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/activations.py#NewGELUActivation": {
                "sorted_modules": {
                    "NewGELUActivation": "\n\n@use_kernel_forward_from_hub(\"NewGELU\")\nclass NewGELUActivation(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n    the Gaussian Error Linear Units paper: https://huggingface.co/papers/1606.08415\n    \"\"\"\n\n    def forward(self, input: Tensor) -> Tensor:\n        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))"
                },
                "component_dependencies": {
                    "NewGELUActivation": [
                        "transformers/integrations/hub_kernels.py#use_kernel_forward_from_hub"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/activations.py#QuickGELUActivation": {
                "sorted_modules": {
                    "QuickGELUActivation": "\n\n@use_kernel_forward_from_hub(\"QuickGELU\")\nclass QuickGELUActivation(nn.Module):\n    \"\"\"\n    Applies GELU approximation that is fast but somewhat inaccurate. See: https://github.com/hendrycks/GELUs\n    \"\"\"\n\n    def forward(self, input: Tensor) -> Tensor:\n        return input * torch.sigmoid(1.702 * input)"
                },
                "component_dependencies": {
                    "QuickGELUActivation": [
                        "transformers/integrations/hub_kernels.py#use_kernel_forward_from_hub"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/activations.py#ReLUSquaredActivation": {
                "sorted_modules": {
                    "ReLUSquaredActivation": "\n\nclass ReLUSquaredActivation(nn.Module):\n    \"\"\"\n    Applies the relu^2 activation introduced in https://huggingface.co/papers/2109.08668v2\n    \"\"\"\n\n    def forward(self, input):\n        relu_applied = nn.functional.relu(input)\n        squared = torch.square(relu_applied)\n        return squared"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/activations.py#SiLUActivation": {
                "sorted_modules": {
                    "SiLUActivation": "\n\n@use_kernel_forward_from_hub(\"SiLU\")\nclass SiLUActivation(nn.Module):\n    \"\"\"\n    See Gaussian Error Linear Units (Hendrycks et al., https://arxiv.org/abs/1606.08415) where the SiLU (Sigmoid Linear\n    Unit) was originally introduced and coined, and see Sigmoid-Weighted Linear Units for Neural Network Function\n    Approximation in Reinforcement Learning (Elfwing et al., https://arxiv.org/abs/1702.03118) and Swish: a Self-Gated\n    Activation Function (Ramachandran et al., https://arxiv.org/abs/1710.05941v1) where the SiLU was experimented with\n    later.\n    \"\"\"\n\n    def forward(self, input: Tensor) -> Tensor:\n        return nn.functional.silu(input)"
                },
                "component_dependencies": {
                    "SiLUActivation": [
                        "transformers/integrations/hub_kernels.py#use_kernel_forward_from_hub"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/activations.py#XIELUActivation": {
                "sorted_modules": {
                    "XIELUActivation": "\n\nclass XIELUActivation(nn.Module):\n    \"\"\"\n    Applies the xIELU activation function introduced in https://arxiv.org/abs/2411.13010\n\n    If the user has installed the nickjbrowning/XIELU wheel, we import xIELU CUDA\n    Otherwise, we emit a single warning and use xIELU Python\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha_p_init=0.8,\n        alpha_n_init=0.8,\n        beta=0.5,\n        eps=-1e-6,\n        dtype=torch.bfloat16,\n        with_vector_loads=False,\n    ):\n        super().__init__()\n        self.alpha_p = nn.Parameter(torch.log(torch.expm1(torch.tensor(alpha_p_init, dtype=dtype))).unsqueeze(0))\n        self.alpha_n = nn.Parameter(\n            torch.log(torch.expm1(torch.tensor(alpha_n_init - beta, dtype=dtype))).unsqueeze(0)\n        )\n        self.register_buffer(\"beta\", torch.tensor(beta, dtype=dtype))\n        self.register_buffer(\"eps\", torch.tensor(eps, dtype=dtype))\n        self.with_vector_loads = with_vector_loads\n        # Temporary until xIELU CUDA fully implemented\n        self._beta_scalar = float(self.beta.detach().cpu().float().item())\n        self._eps_scalar = float(self.eps.detach().cpu().float().item())\n\n        self._xielu_cuda_obj = None\n        try:\n            import xielu.ops  # noqa: F401\n\n            self._xielu_cuda_obj = torch.classes.xielu.XIELU()\n            msg = \"Using experimental xIELU CUDA.\"\n            try:\n                from torch._dynamo import allow_in_graph\n\n                self._xielu_cuda_fn = allow_in_graph(self._xielu_cuda)\n                msg += \" Enabled torch._dynamo for xIELU CUDA.\"\n            except Exception as err:\n                msg += f\" Could not enable torch._dynamo for xIELU ({err}) - this may result in slower performance.\"\n                self._xielu_cuda_fn = self._xielu_cuda\n            logger.warning_once(msg)\n        except Exception as err:\n            logger.warning_once(\n                \"CUDA-fused xIELU not available (%s) \u2013 falling back to a Python version.\\n\"\n                \"For CUDA xIELU (experimental), `pip install git+https://github.com/nickjbrowning/XIELU`\",\n                str(err),\n            )\n\n    def _xielu_python(self, x: Tensor) -> Tensor:\n        alpha_p = nn.functional.softplus(self.alpha_p)\n        alpha_n = self.beta + nn.functional.softplus(self.alpha_n)\n        return torch.where(\n            x > 0,\n            alpha_p * x * x + self.beta * x,\n            (torch.expm1(torch.min(x, self.eps)) - x) * alpha_n + self.beta * x,\n        )\n\n    def _xielu_cuda(self, x: Tensor) -> Tensor:\n        \"\"\"Firewall function to prevent torch.compile from seeing .item() calls\"\"\"\n        original_shape = x.shape\n        # CUDA kernel expects 3D tensors, reshape if needed\n        while x.dim() < 3:\n            x = x.unsqueeze(0)\n        if x.dim() > 3:\n            x = x.view(-1, 1, x.size(-1))\n        if original_shape != x.shape:\n            logger.warning_once(\n                \"Warning: xIELU input tensor expects 3 dimensions but got (shape: %s). Reshaping to (shape: %s).\",\n                original_shape,\n                x.shape,\n            )\n        result = self._xielu_cuda_obj.forward(\n            x,\n            self.alpha_p.to(x.dtype),\n            self.alpha_n.to(x.dtype),\n            # Temporary until xIELU CUDA fully implemented -> self.{beta,eps}.item()\n            self._beta_scalar,\n            self._eps_scalar,\n            self.with_vector_loads,\n        )\n        return result.view(original_shape)\n\n    def forward(self, input: Tensor) -> Tensor:\n        if self._xielu_cuda_obj is not None and input.is_cuda:\n            if not is_torchdynamo_compiling():\n                return self._xielu_cuda_fn(input)\n            else:\n                logger.warning_once(\"torch._dynamo is compiling, using Python version of xIELU.\")\n        return self._xielu_python(input)"
                },
                "component_dependencies": {
                    "XIELUActivation": [
                        "transformers/activations.py#logger",
                        "transformers/utils/import_utils.py#is_torchdynamo_compiling"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            }
        },
        "files_to_convert": [
            {
                "comp_id": "transformers/modeling_outputs.py#MoeModelOutputWithPast",
                "filepath": "transformers/modeling_outputs.py",
                "comp_name": "MoeModelOutputWithPast",
                "Dependencies": [
                    "transformers/utils.py#ModelOutput"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/mixtral/modeling_mixtral.py#load_balancing_loss_func",
                "filepath": "transformers/models/mixtral/modeling_mixtral.py",
                "comp_name": "load_balancing_loss_func",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/cache_utils.py#CacheLayerMixin",
                "filepath": "transformers/cache_utils.py",
                "comp_name": "CacheLayerMixin",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/cache_utils.py#Cache",
                "filepath": "transformers/cache_utils.py",
                "comp_name": "Cache",
                "Dependencies": [
                    "transformers/cache_utils.py#CacheLayerMixin"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_outputs.py#MoeCausalLMOutputWithPast",
                "filepath": "transformers/modeling_outputs.py",
                "comp_name": "MoeCausalLMOutputWithPast",
                "Dependencies": [
                    "transformers/utils.py#ModelOutput"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/mixtral/modeling_mixtral.py#MixtralRMSNorm",
                "filepath": "transformers/models/mixtral/modeling_mixtral.py",
                "comp_name": "MixtralRMSNorm",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/configuration_utils.py#PreTrainedConfig",
                "filepath": "transformers/configuration_utils.py",
                "comp_name": "PreTrainedConfig",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/masking_utils.py#causal_mask_function",
                "filepath": "transformers/masking_utils.py",
                "comp_name": "causal_mask_function",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#RopeParameters",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "RopeParameters",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#standardize_rope_params",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "standardize_rope_params",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#EmbeddingAccessMixin",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "EmbeddingAccessMixin",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#_init_weights",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "_init_weights",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                "filepath": "transformers/quantizers/quantizers_utils.py",
                "comp_name": "get_module_from_name",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/masking_utils.py#find_packed_sequence_indices",
                "filepath": "transformers/masking_utils.py",
                "comp_name": "find_packed_sequence_indices",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/masking_utils.py#_preprocess_mask_arguments",
                "filepath": "transformers/masking_utils.py",
                "comp_name": "_preprocess_mask_arguments",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/masking_utils.py#find_packed_sequence_indices"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/masking_utils.py#create_causal_mask",
                "filepath": "transformers/masking_utils.py",
                "comp_name": "create_causal_mask",
                "Dependencies": [
                    "transformers/cache_utils.py#Cache",
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/masking_utils.py#_preprocess_mask_arguments",
                    "transformers/masking_utils.py#causal_mask_function"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/masking_utils.py#and_masks",
                "filepath": "transformers/masking_utils.py",
                "comp_name": "and_masks",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/masking_utils.py#sliding_window_overlay",
                "filepath": "transformers/masking_utils.py",
                "comp_name": "sliding_window_overlay",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/masking_utils.py#sliding_window_causal_mask_function",
                "filepath": "transformers/masking_utils.py",
                "comp_name": "sliding_window_causal_mask_function",
                "Dependencies": [
                    "transformers/masking_utils.py#and_masks",
                    "transformers/masking_utils.py#causal_mask_function",
                    "transformers/masking_utils.py#sliding_window_overlay"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/masking_utils.py#create_sliding_window_causal_mask",
                "filepath": "transformers/masking_utils.py",
                "comp_name": "create_sliding_window_causal_mask",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/masking_utils.py#_preprocess_mask_arguments",
                    "transformers/masking_utils.py#sliding_window_causal_mask_function"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#ROPE_VALIDATION_FUNCTIONS",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "ROPE_VALIDATION_FUNCTIONS",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#rope_config_validation",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "rope_config_validation",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/modeling_rope_utils.py#ROPE_VALIDATION_FUNCTIONS"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig",
                "filepath": "transformers/models/mixtral/configuration_mixtral.py",
                "comp_name": "MixtralConfig",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/modeling_rope_utils.py#RopeParameters",
                    "transformers/modeling_rope_utils.py#rope_config_validation",
                    "transformers/modeling_rope_utils.py#standardize_rope_params"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#_compute_dynamic_ntk_parameters",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "_compute_dynamic_ntk_parameters",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/modeling_rope_utils.py#standardize_rope_params"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#_compute_linear_scaling_rope_parameters",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "_compute_linear_scaling_rope_parameters",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/modeling_rope_utils.py#standardize_rope_params"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#_compute_llama3_parameters",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "_compute_llama3_parameters",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/modeling_rope_utils.py#standardize_rope_params"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#_compute_longrope_parameters",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "_compute_longrope_parameters",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/modeling_rope_utils.py#standardize_rope_params"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#_compute_yarn_parameters",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "_compute_yarn_parameters",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/modeling_rope_utils.py#standardize_rope_params"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#ROPE_INIT_FUNCTIONS",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "ROPE_INIT_FUNCTIONS",
                "Dependencies": [
                    "transformers/modeling_rope_utils.py#_compute_dynamic_ntk_parameters",
                    "transformers/modeling_rope_utils.py#_compute_linear_scaling_rope_parameters",
                    "transformers/modeling_rope_utils.py#_compute_llama3_parameters",
                    "transformers/modeling_rope_utils.py#_compute_longrope_parameters",
                    "transformers/modeling_rope_utils.py#_compute_yarn_parameters"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#dynamic_rope_update",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "dynamic_rope_update",
                "Dependencies": [
                    "transformers/modeling_rope_utils.py#ROPE_INIT_FUNCTIONS"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/mixtral/modeling_mixtral.py#MixtralRotaryEmbedding",
                "filepath": "transformers/models/mixtral/modeling_mixtral.py",
                "comp_name": "MixtralRotaryEmbedding",
                "Dependencies": [
                    "transformers/modeling_rope_utils.py#ROPE_INIT_FUNCTIONS",
                    "transformers/modeling_rope_utils.py#dynamic_rope_update",
                    "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#get_parameter_device",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "get_parameter_device",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#get_parameter_dtype",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "get_parameter_dtype",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#ModuleUtilsMixin",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "ModuleUtilsMixin",
                "Dependencies": [
                    "transformers/modeling_utils.py#get_parameter_device",
                    "transformers/modeling_utils.py#get_parameter_dtype"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#PreTrainedModel",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "PreTrainedModel",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/modeling_utils.py#EmbeddingAccessMixin",
                    "transformers/modeling_utils.py#ModuleUtilsMixin",
                    "transformers/modeling_utils.py#_init_weights",
                    "transformers/quantizers/quantizers_utils.py#get_module_from_name"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/mixtral/modeling_mixtral.py#rotate_half",
                "filepath": "transformers/models/mixtral/modeling_mixtral.py",
                "comp_name": "rotate_half",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/mixtral/modeling_mixtral.py#apply_rotary_pos_emb",
                "filepath": "transformers/models/mixtral/modeling_mixtral.py",
                "comp_name": "apply_rotary_pos_emb",
                "Dependencies": [
                    "transformers/models/mixtral/modeling_mixtral.py#rotate_half"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/mixtral/modeling_mixtral.py#repeat_kv",
                "filepath": "transformers/models/mixtral/modeling_mixtral.py",
                "comp_name": "repeat_kv",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/mixtral/modeling_mixtral.py#eager_attention_forward",
                "filepath": "transformers/models/mixtral/modeling_mixtral.py",
                "comp_name": "eager_attention_forward",
                "Dependencies": [
                    "transformers/models/mixtral/modeling_mixtral.py#repeat_kv"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/mixtral/modeling_mixtral.py#MixtralAttention",
                "filepath": "transformers/models/mixtral/modeling_mixtral.py",
                "comp_name": "MixtralAttention",
                "Dependencies": [
                    "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig",
                    "transformers/models/mixtral/modeling_mixtral.py#apply_rotary_pos_emb",
                    "transformers/models/mixtral/modeling_mixtral.py#eager_attention_forward"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/activations.py#AccurateGELUActivation",
                "filepath": "transformers/activations.py",
                "comp_name": "AccurateGELUActivation",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/activations.py#ClassInstantier",
                "filepath": "transformers/activations.py",
                "comp_name": "ClassInstantier",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/activations.py#FastGELUActivation",
                "filepath": "transformers/activations.py",
                "comp_name": "FastGELUActivation",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/activations.py#GELUActivation",
                "filepath": "transformers/activations.py",
                "comp_name": "GELUActivation",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/activations.py#GELUTanh",
                "filepath": "transformers/activations.py",
                "comp_name": "GELUTanh",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/activations.py#LaplaceActivation",
                "filepath": "transformers/activations.py",
                "comp_name": "LaplaceActivation",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/activations.py#LinearActivation",
                "filepath": "transformers/activations.py",
                "comp_name": "LinearActivation",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/activations.py#MishActivation",
                "filepath": "transformers/activations.py",
                "comp_name": "MishActivation",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/activations.py#NewGELUActivation",
                "filepath": "transformers/activations.py",
                "comp_name": "NewGELUActivation",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/activations.py#QuickGELUActivation",
                "filepath": "transformers/activations.py",
                "comp_name": "QuickGELUActivation",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/activations.py#ReLUSquaredActivation",
                "filepath": "transformers/activations.py",
                "comp_name": "ReLUSquaredActivation",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/activations.py#SiLUActivation",
                "filepath": "transformers/activations.py",
                "comp_name": "SiLUActivation",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/activations.py#XIELUActivation",
                "filepath": "transformers/activations.py",
                "comp_name": "XIELUActivation",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/activations.py#ACT2FN",
                "filepath": "transformers/activations.py",
                "comp_name": "ACT2FN",
                "Dependencies": [
                    "transformers/activations.py#AccurateGELUActivation",
                    "transformers/activations.py#ClassInstantier",
                    "transformers/activations.py#FastGELUActivation",
                    "transformers/activations.py#GELUActivation",
                    "transformers/activations.py#GELUTanh",
                    "transformers/activations.py#LaplaceActivation",
                    "transformers/activations.py#LinearActivation",
                    "transformers/activations.py#MishActivation",
                    "transformers/activations.py#NewGELUActivation",
                    "transformers/activations.py#QuickGELUActivation",
                    "transformers/activations.py#ReLUSquaredActivation",
                    "transformers/activations.py#SiLUActivation",
                    "transformers/activations.py#XIELUActivation"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/mixtral/modeling_mixtral.py#MixtralMLP",
                "filepath": "transformers/models/mixtral/modeling_mixtral.py",
                "comp_name": "MixtralMLP",
                "Dependencies": [
                    "transformers/activations.py#ACT2FN",
                    "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/mixtral/modeling_mixtral.py#MixtralExperts",
                "filepath": "transformers/models/mixtral/modeling_mixtral.py",
                "comp_name": "MixtralExperts",
                "Dependencies": [
                    "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig",
                    "transformers/models/mixtral/modeling_mixtral.py#MixtralMLP"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/mixtral/modeling_mixtral.py#MixtralSparseMoeBlock",
                "filepath": "transformers/models/mixtral/modeling_mixtral.py",
                "comp_name": "MixtralSparseMoeBlock",
                "Dependencies": [
                    "transformers/models/mixtral/modeling_mixtral.py#MixtralExperts"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/mixtral/modeling_mixtral.py#MixtralDecoderLayer",
                "filepath": "transformers/models/mixtral/modeling_mixtral.py",
                "comp_name": "MixtralDecoderLayer",
                "Dependencies": [
                    "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig",
                    "transformers/models/mixtral/modeling_mixtral.py#MixtralAttention",
                    "transformers/models/mixtral/modeling_mixtral.py#MixtralRMSNorm",
                    "transformers/models/mixtral/modeling_mixtral.py#MixtralSparseMoeBlock"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/mixtral/modeling_mixtral.py#MixtralPreTrainedModel",
                "filepath": "transformers/models/mixtral/modeling_mixtral.py",
                "comp_name": "MixtralPreTrainedModel",
                "Dependencies": [
                    "transformers/modeling_utils.py#PreTrainedModel",
                    "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig",
                    "transformers/models/mixtral/modeling_mixtral.py#MixtralAttention",
                    "transformers/models/mixtral/modeling_mixtral.py#MixtralDecoderLayer"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/mixtral/modeling_mixtral.py#MixtralModel",
                "filepath": "transformers/models/mixtral/modeling_mixtral.py",
                "comp_name": "MixtralModel",
                "Dependencies": [
                    "transformers/masking_utils.py#create_causal_mask",
                    "transformers/masking_utils.py#create_sliding_window_causal_mask",
                    "transformers/modeling_outputs.py#MoeModelOutputWithPast",
                    "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig",
                    "transformers/models/mixtral/modeling_mixtral.py#MixtralDecoderLayer",
                    "transformers/models/mixtral/modeling_mixtral.py#MixtralPreTrainedModel",
                    "transformers/models/mixtral/modeling_mixtral.py#MixtralRMSNorm",
                    "transformers/models/mixtral/modeling_mixtral.py#MixtralRotaryEmbedding"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/mixtral/modeling_mixtral.py#MixtralForCausalLM",
                "filepath": "transformers/models/mixtral/modeling_mixtral.py",
                "comp_name": "MixtralForCausalLM",
                "Dependencies": [
                    "transformers/cache_utils.py#Cache",
                    "transformers/modeling_outputs.py#MoeCausalLMOutputWithPast",
                    "transformers/modeling_outputs.py#MoeModelOutputWithPast",
                    "transformers/models/mixtral/modeling_mixtral.py#MixtralModel",
                    "transformers/models/mixtral/modeling_mixtral.py#MixtralPreTrainedModel",
                    "transformers/models/mixtral/modeling_mixtral.py#load_balancing_loss_func"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/qwen3_moe/modular_qwen3_moe.py#Qwen3MoeModel",
                "filepath": "transformers/models/qwen3_moe/modular_qwen3_moe.py",
                "comp_name": "Qwen3MoeModel",
                "Dependencies": [
                    "transformers/models/mixtral/modeling_mixtral.py#MixtralModel"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/qwen3_moe/modular_qwen3_moe.py#Qwen3MoeForCausalLM",
                "filepath": "transformers/models/qwen3_moe/modular_qwen3_moe.py",
                "comp_name": "Qwen3MoeForCausalLM",
                "Dependencies": [
                    "transformers/cache_utils.py#Cache",
                    "transformers/modeling_outputs.py#MoeCausalLMOutputWithPast",
                    "transformers/modeling_outputs.py#MoeModelOutputWithPast",
                    "transformers/models/mixtral/modeling_mixtral.py#MixtralForCausalLM",
                    "transformers/models/mixtral/modeling_mixtral.py#load_balancing_loss_func",
                    "transformers/models/qwen3_moe/modular_qwen3_moe.py#Qwen3MoeModel"
                ],
                "JaxDependencies": {}
            }
        ],
        "original_dependencies": {
            "Qwen3MoeForCausalLM": [
                "transformers/cache_utils.py#Cache",
                "transformers/modeling_outputs.py#MoeCausalLMOutputWithPast",
                "transformers/modeling_outputs.py#MoeModelOutputWithPast",
                "transformers/models/mixtral/modeling_mixtral.py#MixtralForCausalLM",
                "transformers/models/mixtral/modeling_mixtral.py#load_balancing_loss_func",
                "transformers/models/qwen3_moe/modular_qwen3_moe.py#Qwen3MoeModel"
            ],
            "Cache": [
                "transformers/cache_utils.py#CacheLayerMixin"
            ],
            "MoeCausalLMOutputWithPast": [
                "transformers/utils.py#ModelOutput"
            ],
            "MoeModelOutputWithPast": [
                "transformers/utils.py#ModelOutput"
            ],
            "MixtralForCausalLM": [
                "transformers/cache_utils.py#Cache",
                "transformers/modeling_outputs.py#MoeCausalLMOutputWithPast",
                "transformers/modeling_outputs.py#MoeModelOutputWithPast",
                "transformers/models/mixtral/modeling_mixtral.py#MixtralModel",
                "transformers/models/mixtral/modeling_mixtral.py#MixtralPreTrainedModel",
                "transformers/models/mixtral/modeling_mixtral.py#load_balancing_loss_func"
            ],
            "Qwen3MoeModel": [
                "transformers/models/mixtral/modeling_mixtral.py#MixtralModel"
            ],
            "MixtralModel": [
                "transformers/masking_utils.py#create_causal_mask",
                "transformers/masking_utils.py#create_sliding_window_causal_mask",
                "transformers/modeling_outputs.py#MoeModelOutputWithPast",
                "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig",
                "transformers/models/mixtral/modeling_mixtral.py#MixtralDecoderLayer",
                "transformers/models/mixtral/modeling_mixtral.py#MixtralPreTrainedModel",
                "transformers/models/mixtral/modeling_mixtral.py#MixtralRMSNorm",
                "transformers/models/mixtral/modeling_mixtral.py#MixtralRotaryEmbedding"
            ],
            "MixtralPreTrainedModel": [
                "transformers/modeling_utils.py#PreTrainedModel",
                "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig",
                "transformers/models/mixtral/modeling_mixtral.py#MixtralAttention",
                "transformers/models/mixtral/modeling_mixtral.py#MixtralDecoderLayer"
            ],
            "create_causal_mask": [
                "transformers/cache_utils.py#Cache",
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/masking_utils.py#_preprocess_mask_arguments",
                "transformers/masking_utils.py#causal_mask_function"
            ],
            "create_sliding_window_causal_mask": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/masking_utils.py#_preprocess_mask_arguments",
                "transformers/masking_utils.py#sliding_window_causal_mask_function"
            ],
            "MixtralConfig": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/modeling_rope_utils.py#RopeParameters",
                "transformers/modeling_rope_utils.py#rope_config_validation",
                "transformers/modeling_rope_utils.py#standardize_rope_params"
            ],
            "MixtralDecoderLayer": [
                "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig",
                "transformers/models/mixtral/modeling_mixtral.py#MixtralAttention",
                "transformers/models/mixtral/modeling_mixtral.py#MixtralRMSNorm",
                "transformers/models/mixtral/modeling_mixtral.py#MixtralSparseMoeBlock"
            ],
            "MixtralRMSNorm": [],
            "MixtralRotaryEmbedding": [
                "transformers/modeling_rope_utils.py#ROPE_INIT_FUNCTIONS",
                "transformers/modeling_rope_utils.py#dynamic_rope_update",
                "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig"
            ],
            "PreTrainedModel": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/modeling_utils.py#EmbeddingAccessMixin",
                "transformers/modeling_utils.py#ModuleUtilsMixin",
                "transformers/modeling_utils.py#_init_weights",
                "transformers/quantizers/quantizers_utils.py#get_module_from_name"
            ],
            "MixtralAttention": [
                "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig",
                "transformers/models/mixtral/modeling_mixtral.py#apply_rotary_pos_emb",
                "transformers/models/mixtral/modeling_mixtral.py#eager_attention_forward"
            ],
            "PreTrainedConfig": [],
            "_preprocess_mask_arguments": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/masking_utils.py#find_packed_sequence_indices"
            ],
            "sliding_window_causal_mask_function": [
                "transformers/masking_utils.py#and_masks",
                "transformers/masking_utils.py#causal_mask_function",
                "transformers/masking_utils.py#sliding_window_overlay"
            ],
            "rope_config_validation": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/modeling_rope_utils.py#ROPE_VALIDATION_FUNCTIONS"
            ],
            "MixtralSparseMoeBlock": [
                "transformers/models/mixtral/modeling_mixtral.py#MixtralExperts"
            ],
            "ROPE_INIT_FUNCTIONS": [
                "transformers/modeling_rope_utils.py#_compute_dynamic_ntk_parameters",
                "transformers/modeling_rope_utils.py#_compute_linear_scaling_rope_parameters",
                "transformers/modeling_rope_utils.py#_compute_llama3_parameters",
                "transformers/modeling_rope_utils.py#_compute_longrope_parameters",
                "transformers/modeling_rope_utils.py#_compute_yarn_parameters"
            ],
            "dynamic_rope_update": [
                "transformers/modeling_rope_utils.py#ROPE_INIT_FUNCTIONS"
            ],
            "ModuleUtilsMixin": [
                "transformers/modeling_utils.py#get_parameter_device",
                "transformers/modeling_utils.py#get_parameter_dtype"
            ],
            "apply_rotary_pos_emb": [
                "transformers/models/mixtral/modeling_mixtral.py#rotate_half"
            ],
            "eager_attention_forward": [
                "transformers/models/mixtral/modeling_mixtral.py#repeat_kv"
            ],
            "ROPE_VALIDATION_FUNCTIONS": [],
            "MixtralExperts": [
                "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig",
                "transformers/models/mixtral/modeling_mixtral.py#MixtralMLP"
            ],
            "_compute_dynamic_ntk_parameters": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/modeling_rope_utils.py#standardize_rope_params"
            ],
            "_compute_linear_scaling_rope_parameters": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/modeling_rope_utils.py#standardize_rope_params"
            ],
            "_compute_llama3_parameters": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/modeling_rope_utils.py#standardize_rope_params"
            ],
            "_compute_longrope_parameters": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/modeling_rope_utils.py#standardize_rope_params"
            ],
            "_compute_yarn_parameters": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/modeling_rope_utils.py#standardize_rope_params"
            ],
            "get_parameter_dtype": [],
            "MixtralMLP": [
                "transformers/activations.py#ACT2FN",
                "transformers/models/mixtral/configuration_mixtral.py#MixtralConfig"
            ],
            "ACT2FN": [
                "transformers/activations.py#AccurateGELUActivation",
                "transformers/activations.py#ClassInstantier",
                "transformers/activations.py#FastGELUActivation",
                "transformers/activations.py#GELUActivation",
                "transformers/activations.py#GELUTanh",
                "transformers/activations.py#LaplaceActivation",
                "transformers/activations.py#LinearActivation",
                "transformers/activations.py#MishActivation",
                "transformers/activations.py#NewGELUActivation",
                "transformers/activations.py#QuickGELUActivation",
                "transformers/activations.py#ReLUSquaredActivation",
                "transformers/activations.py#SiLUActivation",
                "transformers/activations.py#XIELUActivation"
            ],
            "FastGELUActivation": [],
            "GELUActivation": [],
            "GELUTanh": [],
            "NewGELUActivation": [],
            "QuickGELUActivation": [],
            "SiLUActivation": [],
            "XIELUActivation": []
        },
        "jax_found_dependencies": {},
        "jax_dependencies_list": []
    },
    "metadata": {
        "q": "<class 'collections.deque'>",
        "processed_components": "<class 'set'>",
        "processed_count": "<class 'int'>",
        "file_analysis_cache": "<class 'dict'>",
        "files_to_convert": "<class 'list'>",
        "original_dependencies": "<class 'dict'>",
        "jax_found_dependencies": "<class 'collections.defaultdict'>",
        "jax_dependencies_list": "<class 'list'>"
    }
}