# Copyright 2023â€“2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# GRPO Configuration
# This config consolidates common parameters for GRPO training across different model sizes

base_config: "base.yml"

use_grpo: True
train_data_columns: 'prompt'

# Dataset Configuration
dataset_type: hf  # Huggingface input pipeline
hf_path: 'gsm8k'
hf_data_split: 'main'
hf_data_files: 'train'

# Model and Tokenizer Configuration
# Override these via CLI:
# model_name, tokenizer_path, load_parameters_path

# Sequence Lengths
max_prefill_predict_length: 256
max_target_length: 768

# Training Hyperparameters
learning_rate: 3.e-6
adam_b1: 0.9
adam_b2: 0.99
weight_decay: 0.1
max_grad_norm: 0.1

# Group Relative Policy Optimization (GRPO) Parameters
num_generations: 2
grpo_beta: 0.08  # KL divergence penalty coefficient
grpo_epsilon: 0.2  # Clipping value for stable updates
inference_rollouts: 1

# Generation Configuration During Training
decode_sampling_strategy: "weighted"
decode_sampling_temperature: 0.9
decode_sampling_top_p: 1.0
decode_sampling_top_k: 50

# Training Loop Configuration  
steps: 100
per_device_batch_size: 1
eval_interval: 10
eval_steps: 5

# Checkpoint Configuration
enable_checkpointing: True
async_checkpointing: True
checkpoint_period: 50

# Pathways Inference Configuration
# For multi-host/multi-slice setups
use_pathways_reshard: False
inference_devices_per_replica: 4
inference_replicas: 1

# Tokenizer Settings
add_bos: False
add_eos: False
return_log_prob: True

# Performance and Memory
weight_dtype: bfloat16
dtype: bfloat16

# Profiling
profiler: xplane
skip_first_n_steps_for_profiler: 5
profiler_steps: 3

# Splash Attention Block Sizes
# Tuned for GRPO workloads
sa_block_q: 128
sa_block_kv: 128
sa_block_kv_compute: 128
sa_block_q_dkv: 128
sa_block_kv_dkv: 128
sa_block_kv_dkv_compute: 128
sa_block_q_dq: 128
sa_block_kv_dq: 128
sa_use_fused_bwd_kernel: False
sa_q_layout: "HEAD_DIM_MINOR"
sa_k_layout: "HEAD_DIM_MINOR"
sa_v_layout: "HEAD_DIM_MINOR"

# Model-Specific Overrides (examples)
# For Llama3.1-8B:
#   model_name: llama3.1-8b
#   tokenizer_path: meta-llama/Llama-3.1-8B-Instruct
#   ici_fsdp_parallelism: 8
#
# For Llama3.1-70B with Pathways:
#   model_name: llama3.1-70b
#   tokenizer_path: meta-llama/Llama-3.1-70B-Instruct
#   use_pathways_reshard: True
#   ici_fsdp_parallelism: 16

