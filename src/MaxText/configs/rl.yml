# Copyright 2023‚Äì2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# RL Configuration
# This config consolidates common parameters for RL training across different model sizes

base_config: "base.yml"

# ====== Hardware =====
trainer_devices_fraction: 0.5
sampler_devices_fraction: 0.5
chips_per_vm: 4 # depends on hardware, for v5p this is 4
num_trainer_slices: -1
num_samplers_slices: -1

# ====== Reproducibility ======
data_shuffle_seed: 42

# ====== GRPO ======

# The number of times the policy generates multiple responses for a given prompt
# within a single training step. This corresponds to `G` in Algorithm 1 in the
# paper. The "group" in GRPO comes from here.
num_generations: 2

# === other GRPO configs ===
# The number of iterations per batch (ùúá in GRPO algo 1).
num_iterations: 1

# The coefficient for the KL divergence penalty (ùõΩ) in the GRPO loss function.
# Important to keep a high enough value for this, otherwise, the KL divergence
# can increase unchecked.
grpo_beta: 0.08
# Epsilon value for clipping (ùúÄ in GRPO loss in paper). Similar to PPO, for
# stable updates.
grpo_epsilon: 0.2
loss_algo: "grpo" # grpo or gspo-token

# ====== Models ======
# for MaxText
# Model and Tokenizer Configuration
# Override these via CLI:
# model_name, tokenizer_path, load_parameters_path
# Model-Specific Overrides (examples)
# For Llama3.1-8B:
#   model_name: llama3.1-8b
#   HF tokenizer_path: meta-llama/Llama-3.1-8B-Instruct
#
# For Llama3.1-70B with Pathways:
#   model_name: llama3.1-70b
#   HF tokenizer_path: meta-llama/Llama-3.1-70B-Instruct

# ====== MaxText configs ======
base_output_directory: "/tmp/grpo_output"
weight_dtype: "bfloat16"
attention: "dot_product"
remat_policy: "custom"
decoder_layer_input: "offload"
query_proj: "offload"
key_proj: "offload"
value_proj: "offload"
checkpoint_storage_use_ocdbt: False # For Pathways
checkpoint_storage_use_zarr3: False # For Pathways
use_pathways: True
log_period: 20

# ====== Debugging ======
debug:
  rl: True
# If True, Tunix-managed metrics measurement will be enabled. The metrics will be
# uploaded to tensorboard.
enable_tunix_perf_metrics: False

# ====== Training ======
batch_size: 1
# Increase `batch_size` and `MAX_STEPS` for better results.
# num_batches: 3738
num_batches: 4 # 200
# A batch can be split into multiple micro batches for memory management
# and/or async sampling and training.
micro_batch_size: -1
# Keep `num_test_batches` low so that evaluation runs quickly. It can be
# increased to a max. of 330 (if batch size is 4).
num_test_batches: 5 # 200
train_fraction: 1.0

eval_interval: 10 # this doesn't matter if `TRAIN_FRACTION = 1.0`.

num_epoch: 1 # can potentially train for more epochs

learning_rate: 3e-6
adam_b1: 0.9 # Exponential decay rate to track the first moment of past gradients.
adam_b2: 0.99 # Exponential decay rate to track the second moment of past gradients.
gradient_clipping_threshold: 0.1

# ====== Evaluation ======
eval_sampling_strategy: "greedy" # can be "greedy", "standard", or "liberal"
generation_configs:
  greedy:
    eval_temperature: 0.01
    eval_top_k: 1
    eval_top_p: 1.0
  standard:
    eval_temperature: 0.7
    eval_top_k: 50
    eval_top_p: 0.95
  liberal:
    eval_temperature: 0.85
    eval_top_k: 2000
    eval_top_p: 1.0

num_eval_passes: 1 # Number of generation passes during evaluation
eval_corr_lst: False # If True, only include correct responses in the list during evaluation
eval_make_lst: False # If True, return a list of (question, answer, responses) during evaluation

# ====== Inference ======
# === Generation during GRPO training ===
# max Lengths for prompt and completion
max_prefill_predict_length: 256
max_target_length: 1024
kv_cache_buffer: 256
hbm_utilization_vllm: 0.4
swap_space_vllm_gb: 2
# Generation Configuration During Training
# Important to keep a high-ish temperature for varied, diverse responses during
# training.
decode_sampling_temperature: 0.9
decode_sampling_top_k: 50
decode_sampling_nucleus_p: 1.0

# ====== Checkpoint Configuration ======
enable_checkpointing: True
async_checkpointing: False
checkpoint_period: 50
max_num_checkpoints_to_keep: 10

# ====== Reward ======

reward_exact_format_match: 3.0
reward_white_space_format_match: 1.5
reward_partial_format_match: 0.5
reward_ratio_guess_to_answer_high: 0.5
reward_ratio_guess_to_answer_low: 0.25
penalty_incorrect_format: -0.5
penalty_incorrect_answer: -1.0

# ====== Special tokens/templates for GSM8K reasoning ======
reasoning_start_token: "<reasoning>"
reasoning_end_token: "</reasoning>"
solution_start_token: "<answer>"
solution_end_token: "</answer>"
chat_template_path: "src/MaxText/examples/chat_templates/gsm8k_rl.json"
skip_jax_distributed_system: True

# # TODO(@mazumdera): fix this
# Dataset Configuration
dataset_name: "gsm8k"
train_split: "train"
eval_split: "test"
tokenizer_type: "huggingface"
