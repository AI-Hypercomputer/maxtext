# Copyright 2023‚Äì2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# RL Configuration
# This config consolidates common parameters for RL training across different model sizes

base_config: "base.yml"

# ====== Hardware =====
trainer_devices_fraction: 0.5
sampler_devices_fraction: 0.5
chips_per_vm: 4  # depends on hardware, for v5p this is 4

# ====== Debug ======
debug: True

# ====== Reproducibility ======
data_shuffle_seed: 42

# ====== Checkpoint saving ======
save_interval_steps: 500
max_to_keep: 4

# ====== GRPO ======

# The number of times the policy generates multiple responses for a given prompt
# within a single training step. This corresponds to `G` in Algorithm 1 in the
# paper. The "group" in GRPO comes from here.
num_generations: 2

# === other GRPO configs ===
# The number of iterations per batch (ùúá in GRPO algo 1).
num_iterations: 1

# The coefficient for the KL divergence penalty (ùõΩ) in the GRPO loss function.
# Important to keep a high enough value for this, otherwise, the KL divergence
# can increase unchecked.
grpo_beta: 0.08
# Epsilon value for clipping (ùúÄ in GRPO loss in paper). Similar to PPO, for
# stable updates.
grpo_epsilon: 0.2
loss_algo: 'grpo' # grpo or gspo-token


# ====== Models ======
# for MaxText
# Model and Tokenizer Configuration
# Override these via CLI:
# model_name, tokenizer_path, load_parameters_path
# Model-Specific Overrides (examples)
# For Llama3.1-8B:
#   model_name: llama3.1-8b
#   tokenizer_path: meta-llama/Llama-3.1-8B-Instruct
#
# For Llama3.1-70B with Pathways:
#   model_name: llama3.1-70b
#   tokenizer_path: meta-llama/Llama-3.1-70B-Instruct

async_checkpointing: 'false'
checkpoint_period: 5
skip_jax_distributed_system: True
weight_dtype: 'bfloat16'
attention: 'dot_product'
remat_policy: 'custom'
decoder_layer_input: 'offload'
query_proj: 'offload'
key_proj: 'offload'
value_proj: 'offload'
# for vLLM
hf_model_name: 'meta-llama/Llama-3.1-70B-Instruct'

# ====== Training ======
batch_size: 1
# Increase `batch_size` and `MAX_STEPS` for better results.
# num_batches = 3738
num_batches = 4  # 200
# Keep `num_test_batches` low so that evaluation runs quickly. It can be
# increased to a max. of 330 (if batch size is 4).
num_test_batches = 5  # 200
train_fraction: 1.0

eval_interval: 10  # this doesn't matter if `TRAIN_FRACTION = 1.0`.

num_epochs: 1  # can potentially train for more epochs

gradient_clipping_threshold: 0.1


# greedy
eval_temperature: 0.01
eval_top_k: 1
eval_top_p: 1.0
# # some randomness
# 'standard': {'eval_temperature': 0.7, 'eval_top_k': 50, 'eval_top_p': 0.95},
# # liberal
# # 'liberal': {'eval_temperature': 0.85, 'eval_top_k': 2000, 'eval_top_p': 1.0},


# ====== Inference ======
# Important to keep a high-ish temperature for varied, diverse responses during
# training.
decode_sampling_temperature: 0.9
decode_sampling_top_k: 50
decode_sampling_nucleus_p: 1.0

# for vLLM
# === Generation during GRPO training ===
# max Lengths for prompt and completion
max_prefill_predict_length: 256
max_target_length: 768
kv_cache_buffer: 256
hbm_utilization_vllm: 0.72
swap_space_vllm_gb: 2
# Generation Configuration During Training
decode_sampling_temperature: 0.9
decode_sampling_top_p: 1.0
decode_sampling_top_k: 50

# ====== Checkpoint Configuration ======
enable_checkpointing: True
async_checkpointing: True
checkpoint_period: 50
max_num_checkpoints_to_keep: 10

# ====== Reward ======

reward_exact_format_match: 3.0
reward_white_space_format_match: 1.5
reward_partial_format_match: 0.5
reward_ratio_guess_to_answer_high:  0.5
reward_ratio_guess_to_answer_low: 0.25
penalty_incorrect_format: -0.5
penalty_incorrect_answer: -1.0

# ====== Special tokens for GSM8K reasoning ======
reasoning_start_token: '<reasoning>'
reasoning_end_token: '</reasoning>'
solution_start_token: '<answer>'
solution_end_token: '</answer>'

# ====== System prompt and Templates ======

system_prompt: |
  You are given a problem. Think about the problem and provide your reasoning. Place it between {reasoning_start_token} and {reasoning_end_token}. Then, provide the final answer (i.e., just one numerical value) between {solution_start_token} and {solution_end_token}.

template: |
  <start_of_turn>user
  {system_prompt}
  
  {question}<end_of_turn>
  <start_of_turn>model


# TODO: fix this
# Dataset Configuration
dataset_type: hf  # Huggingface input pipeline
hf_path: 'gsm8k'
hf_data_split: 'main'
hf_data_files: 'train'


# Pathways Inference Configuration
# For multi-host/multi-slice setups
use_pathways_reshard: False
inference_devices_per_replica: 4
inference_replicas: 1

# Tokenizer Settings
add_bos: False
add_eos: False
return_log_prob: True

# Performance and Memory
weight_dtype: bfloat16
dtype: bfloat16

# Splash Attention Block Sizes
# Tuned for GRPO workloads
sa_block_q: 128
sa_block_kv: 128
sa_block_kv_compute: 128
sa_block_q_dkv: 128
sa_block_kv_dkv: 128
sa_block_kv_dkv_compute: 128
sa_block_q_dq: 128
sa_block_kv_dq: 128
sa_use_fused_bwd_kernel: False
sa_q_layout: "HEAD_DIM_MINOR"
sa_k_layout: "HEAD_DIM_MINOR"
sa_v_layout: "HEAD_DIM_MINOR"