{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO Llama3.1-8B Demo\n",
    "\n",
    "This notebook demonstrates GRPO (Group Relative Policy Optimization) training using the unified `rl_train` function.\n",
    "\n",
    "## What is GRPO?\n",
    "\n",
    "GRPO is an RL algorithm that enhances reasoning abilities of LLMs by:\n",
    "1. Generating multiple responses for each prompt\n",
    "2. Evaluating responses using reward models  \n",
    "3. Calculating relative advantages to update the policy\n",
    "\n",
    "## Hardware Requirements\n",
    "\n",
    "- Single host TPUVM (v6e-8/v5p-8) or multi-host with Pathways\n",
    "- Sufficient memory for Llama3.1-8B model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install dependencies and set up the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone MaxText repository\n",
    "!git clone https://github.com/AI-Hypercomputer/maxtext\n",
    "%cd maxtext/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash tools/setup/setup.sh\n",
    "%pip uninstall -y jax jaxlib libtpu\n",
    "\n",
    "%pip install aiohttp==3.12.15\n",
    "\n",
    "# Install Python packages that enable pip to authenticate with Google Artifact Registry automatically.\n",
    "%pip install keyring keyrings.google-artifactregistry-auth\n",
    "\n",
    "# Install vLLM for Jax and TPUs from the artifact registry\n",
    "!VLLM_TARGET_DEVICE=\"tpu\" pip install --no-cache-dir --pre \\\n",
    "    --index-url https://us-python.pkg.dev/cloud-tpu-images/maxtext-rl/simple/ \\\n",
    "    --extra-index-url https://pypi.org/simple/ \\\n",
    "    --extra-index-url https://us-python.pkg.dev/ml-oss-artifacts-published/jax/simple/ \\\n",
    "    --extra-index-url https://download.pytorch.org/whl/nightly/cpu \\\n",
    "    --find-links https://storage.googleapis.com/jax-releases/libtpu_releases.html \\\n",
    "    --find-links https://storage.googleapis.com/libtpu-wheels/index.html \\\n",
    "    --find-links https://storage.googleapis.com/libtpu-releases/index.html \\\n",
    "    --find-links https://storage.googleapis.com/jax-releases/jax_nightly_releases.html \\\n",
    "    --find-links https://storage.googleapis.com/jax-releases/jaxlib_nightly_releases.html \\\n",
    "    vllm==0.11.1rc1.dev292+g1b86bd8e1.tpu\n",
    "\n",
    "# Install tpu-commons from the artifact registry\n",
    "%pip install --no-cache-dir --pre \\\n",
    "    --index-url https://us-python.pkg.dev/cloud-tpu-images/maxtext-rl/simple/ \\\n",
    "    --extra-index-url https://pypi.org/simple/ \\\n",
    "    --extra-index-url https://us-python.pkg.dev/ml-oss-artifacts-published/jax/simple/ \\\n",
    "    --find-links https://storage.googleapis.com/jax-releases/libtpu_releases.html \\\n",
    "    tpu-commons==0.1.2\n",
    "\n",
    "%pip install numba==0.61.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install nest_asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Fix for Colab event loop\n",
    "\n",
    "%cd maxtext/src/\n",
    "\n",
    "#Fix nnx problems\n",
    "!pip uninstall flax \n",
    "!pip uninstall qwix\n",
    "!pip install flax \n",
    "!pip install qwix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up the training parameters. Defaults are hardcoded for Llama3.1-8B:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-host Pathways\n",
    "\n",
    "To run this demo on a multi-host Pathways setup:\n",
    "- Set `use_pathways=True` in `rl.yml` (enabled by default).\n",
    "- Override `trainer_devices_fraction` and `sampler_devices_fraction` in `config_argv` to split the mesh across hosts.\n",
    "- Launch the Colab kernel on the controller host and export Pathways runtime variables (for example `JAX_PLATFORMS=proxy` and `ENABLE_PATHWAYS_PERSISTENCE=1`) before running training.\n",
    "- Update `chips_per_vm` to match your slice topology; Pathways will shard trainer and rollout workers automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for GRPO training\n",
    "import os\n",
    "import MaxText\n",
    "\n",
    "# Set up paths (adjust if needed)\n",
    "MAXTEXT_REPO_ROOT = os.path.dirname(MaxText.__file__)\n",
    "RUN_NAME=\"grpo_test\"\n",
    "# Hardcoded defaults for Llama3.1-8B\n",
    "MODEL_NAME = \"llama3.1-8b\"\n",
    "HF_REPO_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "CHAT_TEMPLATE_PATH = f\"{MAXTEXT_REPO_ROOT}/examples/chat_templates/gsm8k_rl.json\"\n",
    "LOSS_ALGO=\"gspo-token\"\n",
    "\n",
    "# Required: Set these before running\n",
    "MODEL_CHECKPOINT_PATH = \"\"  # Update this!\n",
    "OUTPUT_DIRECTORY = \"/tmp/gpo_output\"  # Update this!\n",
    "HF_TOKEN = \"\" # Set HF_TOKEN environment variable\n",
    "\n",
    "# Optional: Override training parameters\n",
    "STEPS = 10  # Reduced for demo purposes\n",
    "PER_DEVICE_BATCH_SIZE = 1\n",
    "LEARNING_RATE = 3e-6\n",
    "NUM_GENERATIONS = 2\n",
    "GRPO_BETA = 0.08\n",
    "GRPO_EPSILON = 0.2\n",
    "CHIPS_PER_VM = 1\n",
    "\n",
    "print(f\"üìÅ MaxText Home: {MAXTEXT_REPO_ROOT}\")\n",
    "print(f\"ü§ñ Model: {MODEL_NAME}\")\n",
    "print(f\"üì¶ Checkpoint: {MODEL_CHECKPOINT_PATH}\")\n",
    "print(f\"üíæ Output: {OUTPUT_DIRECTORY}\")\n",
    "print(f\"üîë HF Token: {'‚úÖ Set' if HF_TOKEN else '‚ùå Missing - set HF_TOKEN env var'}\")\n",
    "print(f\"üìä Steps: {STEPS}\")\n",
    "print(f\"Loss Algorithm : {LOSS_ALGO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add MaxText to Python path\n",
    "maxtext_path = Path(MAXTEXT_REPO_ROOT) \n",
    "sys.path.insert(0, str(maxtext_path))\n",
    "\n",
    "from MaxText import pyconfig, max_utils\n",
    "from MaxText.rl.train_rl import rl_train\n",
    "import jax\n",
    "\n",
    "# Initialize JAX and Pathways\n",
    "import pathwaysutils\n",
    "pathwaysutils.initialize()\n",
    "jax.config.update(\"jax_default_prng_impl\", \"unsafe_rbg\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"0\"\n",
    "os.environ[\"SKIP_JAX_PRECOMPILE\"] = \"1\"  # Faster startup for vLLM\n",
    "\n",
    "if \"xla_tpu_spmd_rng_bit_generator_unsafe\" not in os.environ.get(\"LIBTPU_INIT_ARGS\", \"\"):\n",
    "    os.environ[\"LIBTPU_INIT_ARGS\"] = (\n",
    "        os.environ.get(\"LIBTPU_INIT_ARGS\", \"\") + \" --xla_tpu_spmd_rng_bit_generator_unsafe=true\"\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Successfully imported modules\")\n",
    "print(f\"üìÅ MaxText path: {maxtext_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build configuration for GRPO training\n",
    "config_file = os.path.join(MAXTEXT_REPO_ROOT, \"configs/rl.yml\")\n",
    "\n",
    "# Verify chat template exists\n",
    "if not os.path.exists(os.path.join(MAXTEXT_REPO_ROOT, CHAT_TEMPLATE_PATH)):\n",
    "    raise FileNotFoundError(f\"Chat template not found: {CHAT_TEMPLATE_PATH}\")\n",
    "\n",
    "# Build argv list for pyconfig.initialize()\n",
    "config_argv = [\n",
    "    \"\",  # argv[0] placeholder\n",
    "    config_file,\n",
    "    f\"model_name={MODEL_NAME}\",\n",
    "    f\"tokenizer_path={HF_REPO_ID}\",\n",
    "    f\"run_name={RUN_NAME}\",\n",
    "    f\"chat_template_path={CHAT_TEMPLATE_PATH}\",\n",
    "    f\"load_parameters_path={MODEL_CHECKPOINT_PATH}\",\n",
    "    f\"base_output_directory={OUTPUT_DIRECTORY}\",\n",
    "    f\"hf_access_token={HF_TOKEN}\",\n",
    "    f\"steps={STEPS}\",\n",
    "    f\"per_device_batch_size={PER_DEVICE_BATCH_SIZE}\",\n",
    "    f\"learning_rate={LEARNING_RATE}\",\n",
    "    f\"num_generations={NUM_GENERATIONS}\",\n",
    "    f\"grpo_beta={GRPO_BETA}\",\n",
    "    f\"grpo_epsilon={GRPO_EPSILON}\",\n",
    "    f\"chips_per_vm={CHIPS_PER_VM}\",\n",
    "    f\"loss_algo={LOSS_ALGO}\"\n",
    "]\n",
    "\n",
    "# Initialize configuration\n",
    "print(f\"üîß Initializing configuration from: {config_file}\")\n",
    "config = pyconfig.initialize(config_argv)\n",
    "max_utils.print_system_information()\n",
    "\n",
    "print(\"\\n‚úÖ Configuration initialized successfully\")\n",
    "print(f\"üìä Training steps: {config.steps}\")\n",
    "print(f\"üìÅ Output directory: {config.base_output_directory}\")\n",
    "print(f\"ü§ñ Model: {config.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build configuration for GRPO training\n",
    "# Using rl.yml as the base config (not grpo.yml)\n",
    "config_file = os.path.join(MAXTEXT_REPO_ROOT, \"src/MaxText/configs/rl.yml\")\n",
    "\n",
    "# Verify chat template exists\n",
    "if not os.path.exists(os.path.join(MAXTEXT_REPO_ROOT, CHAT_TEMPLATE_PATH)):\n",
    "    raise FileNotFoundError(f\"Chat template not found: {CHAT_TEMPLATE_PATH}\")\n",
    "\n",
    "# Build argv list for pyconfig.initialize()\n",
    "config_argv = [\n",
    "    \"\",  # argv[0] placeholder\n",
    "    config_file,\n",
    "    f\"model_name={MODEL_NAME}\",\n",
    "    f\"tokenizer_path={HF_REPO_ID}\",\n",
    "    f\"hf_model_name={HF_REPO_ID}\",\n",
    "    f\"chat_template_path={CHAT_TEMPLATE_PATH}\",\n",
    "    f\"load_parameters_path={MODEL_CHECKPOINT_PATH}\",\n",
    "    f\"base_output_directory={OUTPUT_DIRECTORY}\",\n",
    "    f\"hf_access_token={HF_TOKEN}\",\n",
    "    f\"steps={STEPS}\",\n",
    "    f\"per_device_batch_size={PER_DEVICE_BATCH_SIZE}\",\n",
    "    f\"learning_rate={LEARNING_RATE}\",\n",
    "    f\"num_generations={NUM_GENERATIONS}\",\n",
    "    f\"grpo_beta={GRPO_BETA}\",\n",
    "    f\"grpo_epsilon={GRPO_EPSILON}\",\n",
    "    f\"chips_per_vm={CHIPS_PER_VM}\",\n",
    "]\n",
    "\n",
    "# Initialize configuration\n",
    "print(f\"üîß Initializing configuration from: {config_file}\")\n",
    "config = pyconfig.initialize(config_argv)\n",
    "max_utils.print_system_information()\n",
    "\n",
    "print(\"\\n‚úÖ Configuration initialized successfully\")\n",
    "print(f\"üìä Training steps: {config.steps}\")\n",
    "print(f\"üìÅ Output directory: {config.base_output_directory}\")\n",
    "print(f\"ü§ñ Model: {config.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute GRPO/GSPO training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ Starting Training...\")\n",
    "print(\"=\"*80)\n",
    "print(1)\n",
    "try:\n",
    "    # Call the rl_train function (it handles everything internally)\n",
    "    rl_train(config)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ Training Completed Successfully!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìÅ Checkpoints saved to: {config.checkpoint_dir}\")\n",
    "    print(f\"üìä TensorBoard logs: {config.tensorboard_dir}\")\n",
    "    print(f\"üéØ Model ready for inference!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚ùåTraining Failed!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nüí° Common issues:\")\n",
    "    print(\"  - Check that MODEL_CHECKPOINT_PATH points to a valid checkpoint\")\n",
    "    print(\"  - Ensure HF_TOKEN environment variable is set\")\n",
    "    print(\"  - Verify OUTPUT_DIRECTORY is writable\")\n",
    "    print(\"  - Check hardware requirements (TPU/GPU availability)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Learn More\n",
    "\n",
    "- **CLI Usage**: Run `python3 -m src.MaxText.rl.train_rl src/MaxText/configs/rl.yml --model_name=llama3.1-8b ...`\n",
    "- **Configuration**: See `src/MaxText/configs/rl.yml` for all available options\n",
    "- **Documentation**: Check `src/MaxText/rl/train_rl.py` for the `rl_train` function implementation\n",
    "- **Examples**: See other examples in `src/MaxText/examples/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
