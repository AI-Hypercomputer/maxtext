{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO Llama3.1-8B Demo: Direct Function Call\n",
    "\n",
    "This notebook demonstrates GRPO training by directly calling the `grpo_train` function from `grpo_tunix_trainer.py`.\n",
    "\n",
    "## What is GRPO?\n",
    "\n",
    "GRPO (Group Relative Policy Optimization) is an RL algorithm that enhances reasoning abilities of LLMs by:\n",
    "1. Generating multiple responses for each prompt\n",
    "2. Evaluating responses using reward models  \n",
    "3. Calculating relative advantages to update the policy\n",
    "\n",
    "\n",
    "This notebook imports and calls the `grpo_train` function \n",
    "\n",
    "## Hardware Requirements\n",
    "\n",
    "- Single host TPUVM (v6e-8/v5p-8) or multi-host with Pathways\n",
    "- Sufficient memory for Llama3.1-8B model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install dependencies and set up the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone MaxText repository\n",
    "!git clone https://github.com/AI-Hypercomputer/maxtext.git\n",
    "%cd maxtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!chmod +x setup.sh\n",
    "!./setup.sh\n",
    "\n",
    "# Install GRPO-specific dependencies\n",
    "!./src/MaxText/examples/install_tunix_vllm_requirement.sh\n",
    "\n",
    "# Install additional requirements\n",
    "%pip install --force-reinstall numpy==2.1.2\n",
    "%pip install nest_asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Fix for Colab event loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up the training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for GRPO training\n",
    "import os\n",
    "\n",
    "# Set up paths\n",
    "MAXTEXT_REPO_ROOT = os.path.expanduser(\"~\") + \"/maxtext\"\n",
    "print(f\"MaxText Home directory: {MAXTEXT_REPO_ROOT}\")\n",
    "\n",
    "# Training configuration\n",
    "MODEL_CHECKPOINT_PATH = \"gs://maxtext-model-checkpoints/llama3.1-8b/2025-01-23-19-04/scanned/0/items\"\n",
    "OUTPUT_DIRECTORY = \"/tmp/grpo_output\"\n",
    "STEPS = 10  # Reduced for demo purposes\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"your_hf_token_here\")\n",
    "\n",
    "print(f\"Model checkpoint: {MODEL_CHECKPOINT_PATH}\")\n",
    "print(f\"Output directory: {OUTPUT_DIRECTORY}\")\n",
    "print(f\"Training steps: {STEPS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GRPO training function directly\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add MaxText to Python path\n",
    "maxtext_path = Path(MAXTEXT_REPO_ROOT) / \"src\" / \"MaxText\"\n",
    "sys.path.insert(0, str(maxtext_path))\n",
    "\n",
    "# Import required modules\n",
    "from MaxText import pyconfig\n",
    "from MaxText.experimental.rl.grpo_tunix_trainer import grpo_train\n",
    "\n",
    "print(\"‚úÖ Successfully imported GRPO training function\")\n",
    "print(f\"üìÅ MaxText path: {maxtext_path}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Starting GRPO Training...\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build configuration for GRPO training\n",
    "config_argv = [\n",
    "    \"\",  # Placeholder for argv[0]\n",
    "    \"src/MaxText/configs/grpo.yml\",  # Base config\n",
    "    f\"model_name=llama3.1-8b\",\n",
    "    f\"tokenizer_path=meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    f\"load_parameters_path={MODEL_CHECKPOINT_PATH}\",\n",
    "    f\"base_output_directory={OUTPUT_DIRECTORY}\",\n",
    "    f\"hf_access_token={HF_TOKEN}\",\n",
    "    f\"steps={STEPS}\",\n",
    "    \"per_device_batch_size=1\",\n",
    "    \"learning_rate=3e-6\",\n",
    "    \"num_generations=2\",\n",
    "    \"grpo_beta=0.08\",\n",
    "    \"grpo_epsilon=0.2\",\n",
    "    \"trainer_devices_fraction=0.5\",\n",
    "    \"sampler_devices_fraction=0.5\",\n",
    "    \"chips_per_vm=4\"\n",
    "]\n",
    "\n",
    "# Create configuration object\n",
    "config = pyconfig.Config()\n",
    "config.parse_flags(config_argv)\n",
    "\n",
    "print(\"‚úÖ Configuration created successfully\")\n",
    "print(f\"üìä Training steps: {config.steps}\")\n",
    "print(f\"üìÅ Output directory: {config.base_output_directory}\")\n",
    "print(f\"ü§ñ Model: {config.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute GRPO training directly\n",
    "try:\n",
    "    # Call the grpo_train function\n",
    "    grpo_trainer, rl_cluster = grpo_train(config)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ GRPO Training Completed Successfully!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìÅ Checkpoints saved to: {config.base_output_directory}/checkpoints\")\n",
    "    print(f\"üìä Logs available in: {config.base_output_directory}/logs\")\n",
    "    print(f\"üéØ Final model ready for inference!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚ùå GRPO Training Failed!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    print(\"\\nPlease check the error message and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö **Learn More**\n",
    "- See `src/MaxText/examples/grpo_runner.py` for CLI usage\n",
    "- Check `src/MaxText/configs/grpo.yml` for configuration options\n",
    "- Read `src/MaxText/examples/README.md` for more examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
