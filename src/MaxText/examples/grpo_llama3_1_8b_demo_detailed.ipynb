{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO Llama3.1-8B-Instruct Demo: Group Relative Policy Optimization\n",
    "\n",
    "This tutorial demonstrates training the Llama3.1 8B-Instruct model on the GSM8K math reasoning benchmark using Group Relative Policy Optimization (GRPO). GRPO can enhance your model's problem-solving skills on mathematical word problems, coding problems, etc.\n",
    "\n",
    "## What is GRPO?\n",
    "\n",
    "GRPO is an RL algorithm designed to enhance the reasoning abilities of LLMs. It is a variant of Proximal Policy Optimization (PPO) that reduces memory usage by eliminating the need for a separate value function model. GRPO works by:\n",
    "\n",
    "1. Generating multiple responses for a given prompt\n",
    "2. Evaluating these responses using a reward model\n",
    "3. Calculating a relative advantage based on the group's performance to update the policy\n",
    "\n",
    "## Libraries Used\n",
    "\n",
    "- **Tunix**: Library for GRPO implementation\n",
    "- **vLLM**: Library for efficient model inference and generation\n",
    "- **MaxText**: For model creation and training infrastructure\n",
    "\n",
    "## Hardware Requirements\n",
    "\n",
    "This tutorial uses a single host TPUVM such as `v6e-8/v5p-8`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Necessary Libraries\n",
    "\n",
    "First, let's install the required dependencies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### (Optional) Run this if you just have this file and nothing else\n",
    "\n",
    "# 1. Clone the MaxText repository (from AI‚ÄëHypercomputer)\n",
    "!git clone https://github.com/AI-Hypercomputer/maxtext.git\n",
    "\n",
    "# 2. Navigate into the cloned directory\n",
    "%cd maxtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### (Optional) Do not run this if you already installed the dependencies\n",
    "\n",
    "# 3. Ensure setup.sh is executable\n",
    "!chmod +x setup.sh\n",
    "\n",
    "# 4. Execute the setup script\n",
    "!./setup.sh\n",
    "\n",
    "# Install vllm requirements\n",
    "!./src/MaxText/examples/install_tunix_vllm_requirement.sh\n",
    "\n",
    "# force numpy version\n",
    "!pip install --force-reinstall numpy==2.1.2\n",
    "# install nest_asyncio\n",
    "!pip install nest_asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "# To fix \"This event loop is already running\" error in Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import all necessary libraries for GRPO training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the variables for the script\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Set the MaxText home directory (where you cloned the maxtext repo)\n",
    "MAXTEXT_REPO_ROOT = os.path.expanduser(\"~\") + \"/maxtext\"\n",
    "print(f\"MaxText Home directory (from Python): {MAXTEXT_REPO_ROOT}\")\n",
    "\n",
    "DEBUG = False  # set to True to run in debug mode, for more print statements\n",
    "# set this to the path of the checkpoint you want to load, gs://<bucket> supported\n",
    "MODEL_CHECKPOINT_PATH = \"/path/to/scanned/model/ckpt_load_dir/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GRPO training using the unified script\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Build the command\n",
    "cmd = [\n",
    "    \"python3\", \"src/MaxText/examples/grpo_runner.py\",\n",
    "    \"--model_name=llama3.1-8b\",\n",
    "    \"--tokenizer_path=meta-llama/Llama-3.1-8B-Instruct\", \n",
    "    f\"--load_parameters_path={MODEL_CHECKPOINT_PATH}\",\n",
    "    f\"--base_output_directory={OUTPUT_DIRECTORY}\",\n",
    "    f\"--hf_access_token={HF_TOKEN}\",\n",
    "    f\"--steps={STEPS}\",\n",
    "    \"--per_device_batch_size=1\",\n",
    "    \"--learning_rate=3e-6\",\n",
    "    \"--num_generations=2\",\n",
    "    \"--grpo_beta=0.08\",\n",
    "    \"--grpo_epsilon=0.2\"\n",
    "]\n",
    "\n",
    "print(\"Running GRPO training with the following command:\")\n",
    "print(\" \".join(cmd))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Starting GRPO Training...\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the GRPO training\n",
    "result = subprocess.run(cmd, capture_output=False, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ GRPO Training Completed Successfully!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìÅ Checkpoints saved to: {OUTPUT_DIRECTORY}\")\n",
    "    print(f\"üìä Logs available in: {OUTPUT_DIRECTORY}/logs\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚ùå GRPO Training Failed!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Exit code: {result.returncode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summary\n",
    "\n",
    "This simplified notebook demonstrates GRPO training using the unified `grpo_runner.py` script. The key benefits are:\n",
    "\n",
    "### ‚úÖ **Simplified Approach**\n",
    "- **Single command** - All GRPO logic is consolidated\n",
    "- **Easy configuration** - Just set parameters and run\n",
    "- **Consistent interface** - Same as CLI usage\n",
    "\n",
    "### ‚úÖ **What Happened**\n",
    "1. **Model Loading** - Llama3.1-8B with Tunix adapter\n",
    "2. **Dataset Processing** - GSM8K math reasoning dataset\n",
    "3. **GRPO Training** - Multiple reward functions for math problems\n",
    "4. **Checkpointing** - Model weights saved for inference\n",
    "\n",
    "### ‚úÖ **Next Steps**\n",
    "- **Inference** - Use the trained model for math problem solving\n",
    "- **Evaluation** - Test on GSM8K test set\n",
    "- **Customization** - Modify parameters for different models/datasets\n",
    "\n",
    "### üìö **Learn More**\n",
    "- See `src/MaxText/examples/grpo_runner.py` for CLI usage\n",
    "- Check `src/MaxText/configs/grpo.yml` for configuration options\n",
    "- Read `src/MaxText/examples/README.md` for more examples\n",
    "\n",
    "# Use your Hugging Face token (recommended)\n",
    "# Get your token from: https://huggingface.co/settings/tokens\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"hf_your_token_here\"\n",
    "login(token=os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Let's define the configuration we are going to use. Note that this is by no means a \"perfect\" set of hyperparameters. To get good results, you might have to train the model for longer.\n",
    "\n",
    "### Data Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Data ======\n",
    "TRAIN_DATA_DIR = f\"{MAXTEXT_REPO_ROOT}/data/train\"\n",
    "TEST_DATA_DIR = f\"{MAXTEXT_REPO_ROOT}/data/test\"\n",
    "if not os.path.exists(TRAIN_DATA_DIR):\n",
    "  os.makedirs(TRAIN_DATA_DIR)\n",
    "if not os.path.exists(TEST_DATA_DIR):\n",
    "  os.makedirs(TEST_DATA_DIR)\n",
    "TRAIN_FRACTION = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint and Logging Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Checkpoint directory =====\n",
    "LOG_DIR = f\"{MAXTEXT_REPO_ROOT}/content/tensorboard/grpo/logs_llama3/\"\n",
    "if not os.path.exists(LOG_DIR):\n",
    "  os.makedirs(LOG_DIR)\n",
    "\n",
    "# ===== Profiling =====\n",
    "PROFILE_DIR = f\"{MAXTEXT_REPO_ROOT}/content/jax_traces/grpo/profiles_llama3/\"\n",
    "if not os.path.exists(PROFILE_DIR):\n",
    "  os.makedirs(PROFILE_DIR)\n",
    "\n",
    "# ====== Checkpoint saving ======\n",
    "CKPT_DIR = f\"{MAXTEXT_REPO_ROOT}/content/ckpts_llama3/\"\n",
    "\n",
    "if not os.path.exists(CKPT_DIR):\n",
    "  os.makedirs(CKPT_DIR)\n",
    "\n",
    "SAVE_INTERVAL_STEPS = 500\n",
    "MAX_TO_KEEP = 4\n",
    "\n",
    "# ====== Reproducibility ======\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRPO Configuration\n",
    "\n",
    "GRPO-specific hyperparameters for generation and training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== GRPO ======\n",
    "# === Generation during GRPO training ===\n",
    "MAX_PROMPT_LENGTH = 256\n",
    "TOTAL_GENERATION_STEPS = 768\n",
    "# Important to keep a high-ish temperature for varied, diverse responses during\n",
    "# training.\n",
    "TEMPERATURE = 0.9\n",
    "TOP_P = 1.0\n",
    "TOP_K = 50\n",
    "# The number of times the policy generates multiple responses for a given prompt\n",
    "# within a single training step. This corresponds to `G` in Algorithm 1 in the\n",
    "# paper. The \"group\" in GRPO comes from here.\n",
    "NUM_GENERATIONS = 2\n",
    "\n",
    "# === other GRPO configs ===\n",
    "# The number of iterations per batch (ùúá in GRPO algo 1).\n",
    "NUM_ITERATIONS = 1\n",
    "# The coefficient for the KL divergence penalty (ùõΩ) in the GRPO loss function.\n",
    "# Important to keep a high enough value for this, otherwise, the KL divergence\n",
    "# can increase unchecked.\n",
    "BETA = 0.08\n",
    "# Epsilon value for clipping (ùúÄ in GRPO loss in paper). Similar to PPO, for\n",
    "# stable updates.\n",
    "EPSILON = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configuration\n",
    "\n",
    "Training hyperparameters including batch size, learning rate, and optimization settings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Training ======\n",
    "BATCH_SIZE = 1\n",
    "# Increase `NUM_BATCHES` and `MAX_STEPS` for better results.\n",
    "NUM_BATCHES = 4  # 200\n",
    "# Keep `NUM_TEST_BATCHES` low so that evaluation runs quickly. It can be\n",
    "# increased to a max. of 330 (if batch size is 4).\n",
    "NUM_TEST_BATCHES = 5  # 200\n",
    "\n",
    "SEQUENCE_LENGTH = 1024\n",
    "\n",
    "EVAL_EVERY_N_STEPS = 10  # this doesn't matter if `TRAIN_FRACTION = 1.0`.\n",
    "NUM_EPOCHS = 1  # can potentially train for more epochs\n",
    "\n",
    "# Number of training steps.\n",
    "MAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\n",
    "\n",
    "# === AdamW, warmup, cosine scheduler ===\n",
    "LEARNING_RATE = 3e-6\n",
    "B1 = 0.9\n",
    "B2 = 0.99\n",
    "WEIGHT_DECAY = 0.1\n",
    "# == Cosine decay with warmup scheduler ==\n",
    "# Linearly increase learning rate from 0. to 5e-6 in the first 10% training\n",
    "# steps, and then gradually decrease the learning rate to 0 using cosine\n",
    "# scheduler.\n",
    "WARMUP_STEPS = int(0.1 * MAX_STEPS)\n",
    "# == Grad clipping ==\n",
    "# Grad clipping to prevent large gradients. Found this\n",
    "# important to keep KL divergence in check.\n",
    "MAX_GRAD_NORM = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference and Reward Configuration\n",
    "\n",
    "Configuration for model inference and reward function parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Inference ======\n",
    "GENERATION_CONFIGS = {\n",
    "    # greedy search\n",
    "    \"greedy\": {\"temperature\": 0.01, \"top_k\": 1, \"top_p\": 1.0},\n",
    "    # some randomness\n",
    "    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "    # liberal\n",
    "    \"liberal\": {\"temperature\": 0.85, \"top_k\": 2000, \"top_p\": 1.0},\n",
    "}\n",
    "\n",
    "# ====== Reward ======\n",
    "REWARD_EXACT_FORMAT_MATCH = 3.0\n",
    "REWARD_WHITE_SPACE_FORMAT_MATCH = 1.5\n",
    "REWARD_PARTIAL_FORMAT_MATCH = 0.5\n",
    "REWARD_RATIO_GUESS_TO_ANSWER_HIGH = 0.5\n",
    "REWARD_RATIO_GUESS_TO_ANSWER_LOW = 0.25\n",
    "PENALTY_INCORRECT_FORMAT = -0.5\n",
    "PENALTY_INCORRECT_ANSWER = -1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Helper functions for monitoring memory usage and other utilities:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_hbm_usage():\n",
    "  \"\"\"Displays memory usage per device.\"\"\"\n",
    "  fmt_size = functools.partial(humanize.naturalsize, binary=True)\n",
    "\n",
    "  for d in jax.local_devices():\n",
    "    stats = d.memory_stats()\n",
    "    used = stats[\"bytes_in_use\"]\n",
    "    limit = stats[\"bytes_limit\"]\n",
    "    print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:%}) on {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "First, let's define some special tokens. We instruct the model to first reason between the `<reasoning>` and `</reasoning>` tokens. After reasoning, we expect it to provide the answer between the `<answer>` and `</answer>` tokens.\n",
    "\n",
    "We use OpenAI's GSM8K dataset. GSM8K comprises grade school math word problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "\n",
    "reasoning_start = \"<reasoning>\"\n",
    "reasoning_end = \"</reasoning>\"\n",
    "solution_start = \"<answer>\"\n",
    "solution_end = \"</answer>\"\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"You are given a problem. Think about the problem and \\\n",
    "provide your reasoning. Place it between {reasoning_start} and \\\n",
    "{reasoning_end}. Then, provide the final answer (i.e., just one numerical \\\n",
    "value) between {solution_start} and {solution_end}.\"\"\"\n",
    "\n",
    "TEMPLATE = \"\"\"<start_of_turn>user\n",
    "{system_prompt}\n",
    "\n",
    "{question}<end_of_turn>\n",
    "<start_of_turn>model\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hash_answer(text: str) -> str | None:\n",
    "  if DEBUG:\n",
    "    print(f\"Extracting answer from: {text}\")\n",
    "  if \"####\" not in text:\n",
    "    return None\n",
    "  return text.split(\"####\")[1].strip()\n",
    "\n",
    "\n",
    "def get_dataset(data_dir, split=\"train\") -> grain.MapDataset:\n",
    "  # Download data\n",
    "  if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "  data = tfds.data_source(\n",
    "      \"gsm8k\",\n",
    "      split=split,\n",
    "      data_dir=data_dir,\n",
    "      builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n",
    "      download=True,\n",
    "  )\n",
    "\n",
    "  loaded_dataset = (\n",
    "      grain.MapDataset.source(data)\n",
    "      .shuffle(seed=SEED)\n",
    "      .map(\n",
    "          lambda x: {\n",
    "              # passed to model forward pass\n",
    "              \"prompts\": model_tokenizer.apply_chat_template(\n",
    "                  [\n",
    "                      {\n",
    "                          \"role\": \"user\",\n",
    "                          \"content\": TEMPLATE.format(\n",
    "                              system_prompt=SYSTEM_PROMPT,\n",
    "                              question=x[\"question\"].decode(\"utf-8\"),\n",
    "                          ),\n",
    "                      },\n",
    "                  ],\n",
    "                  tokenize=False,\n",
    "                  add_generation_prompt=True,\n",
    "              ),\n",
    "              # passed to reward functions\n",
    "              \"question\": x[\"question\"].decode(\"utf-8\"),\n",
    "              # passed to reward functions\n",
    "              \"answer\": extract_hash_answer(x[\"answer\"].decode(\"utf-8\")),\n",
    "          }\n",
    "      )\n",
    "  )\n",
    "  return loaded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(TRAIN_DATA_DIR, \"train\").batch(BATCH_SIZE)[:NUM_BATCHES]\n",
    "\n",
    "if TRAIN_FRACTION == 1.0:\n",
    "  train_dataset = dataset.repeat(NUM_EPOCHS)\n",
    "  val_dataset = None\n",
    "else:\n",
    "  train_dataset = dataset[: int(len(dataset) * TRAIN_FRACTION)]\n",
    "  train_dataset = train_dataset.repeat(NUM_EPOCHS)\n",
    "\n",
    "  val_dataset = dataset[int(len(dataset) * TRAIN_FRACTION) :].repeat(NUM_EPOCHS)\n",
    "\n",
    "test_dataset = get_dataset(TEST_DATA_DIR, \"test\").batch(BATCH_SIZE)[:NUM_TEST_BATCHES]\n",
    "\n",
    "\n",
    "# Let's see how one batch of the dataset looks like!\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "  for ele in train_dataset[:1]:\n",
    "    pprint(ele)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Policy Model and the Reference Model\n",
    "\n",
    "The policy model is the model which is actually trained and whose weights are updated. The reference model is the model with which we compute KL divergence. This is to ensure that the policy updates are not huge and that it does not deviate too much from the reference model.\n",
    "\n",
    "Typically, the reference model is the base model, and the policy model is the same base model, but with potentially LoRA parameters where only the LoRA parameters are updated. This script is not using LoRA, so both the reference and policy models are the same.\n",
    "\n",
    "Note: We perform full precision (fp32) training. You can, however, leverage Qwix for QAT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HBM usage before loading model:\")\n",
    "show_hbm_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MaxText Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref_maxtext_model(config):\n",
    "\n",
    "  model, mesh = model_creation_utils.create_nnx_model(config)\n",
    "  with mesh:\n",
    "    tunix_model = TunixMaxTextAdapter(\n",
    "        base_model=model,\n",
    "    )\n",
    "\n",
    "    model_config = llama3_lib.ModelConfig.llama3_1_8b()\n",
    "    tunix_model.config = model_config\n",
    "\n",
    "  return tunix_model, mesh\n",
    "\n",
    "\n",
    "model_config = llama3_lib.ModelConfig.llama3_1_8b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Reference Model\n",
    "\n",
    "Note: pass the path to your scanned checkpoint for \"load_parameters_path\". To create a scanned checkpoint, you can use `/maxtext/src/MaxText/utils/ckpt_conversion/to_maxtext.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reference model\n",
    "config_ref = pyconfig.initialize(\n",
    "    [\n",
    "        \"\",\n",
    "        f\"{MAXTEXT_REPO_ROOT}/src/MaxText/configs/base.yml\",\n",
    "    ],\n",
    "    base_output_directory=\"dummy\",  # This is not used in Tunix.\n",
    "    run_name=\"test-tunix-maxtext-llama3.1-8b\",\n",
    "    tokenizer_type=\"tiktoken\",\n",
    "    tokenizer_path=\"assets/tokenizer_llama3.tiktoken\",\n",
    "    load_parameters_path=f\"{MODEL_CHECKPOINT_PATH}\",\n",
    "    max_target_length=SEQUENCE_LENGTH,\n",
    "    async_checkpointing=\"false\",\n",
    "    model_name=\"llama3.1-8b\",\n",
    "    skip_jax_distributed_system=\"true\",\n",
    "    weight_dtype=\"bfloat16\",\n",
    "    attention=\"dot_product\",\n",
    "    remat_policy=\"custom\",\n",
    "    decoder_layer_input=\"offload\",\n",
    "    query_proj=\"offload\",\n",
    "    key_proj=\"offload\",\n",
    "    value_proj=\"offload\",\n",
    ")\n",
    "\n",
    "llama3_1_8b, mesh = get_ref_maxtext_model(config_ref)\n",
    "\n",
    "llama3_1_8b.config = model_config\n",
    "\n",
    "nnx.display(llama3_1_8b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "  print(\"Model initialized successfully\")\n",
    "  print(f\"Model mesh shape: {mesh.shape}\")\n",
    "  print(f\"Model config: {model_config}\")\n",
    "\n",
    "  # Sanity check that weights are loaded correctly\n",
    "  _maxtext_state_flatten = nnx.state(llama3_1_8b).flat_state()\n",
    "  maxtext_state_flatten = {\".\".join(str(key) for key in keys): v for keys, v in _maxtext_state_flatten}\n",
    "  print(\n",
    "      f\"maxtext_state_flatten[base.token_embedder.embedding].value={maxtext_state_flatten['base.token_embedder.embedding'].value}\"\n",
    "  )\n",
    "\n",
    "\n",
    "# See the memory use after loading the reference model:\n",
    "print(\"HBM usage after loading ref model:\")\n",
    "show_hbm_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Policy Model\n",
    "\n",
    "Note: pass the path to your scanned checkpoint for \"load_parameters_path\". To create a scanned checkpoint, you can use `/maxtext/src/MaxText/utils/ckpt_conversion/to_maxtext.py`\n",
    "\n",
    "TODO: @mazumdera: change this to use lora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_policy = pyconfig.initialize(\n",
    "    [\n",
    "        \"\",\n",
    "        f\"{MAXTEXT_REPO_ROOT}/src/MaxText/configs/base.yml\",\n",
    "    ],\n",
    "    base_output_directory=\"dummy\",  # This is not used in Tunix.\n",
    "    run_name=\"test-tunix-maxtext-llama3.1-8b\",  # This is not used in Tunix.\n",
    "    tokenizer_type=\"tiktoken\",\n",
    "    tokenizer_path=\"assets/tokenizer_llama3.tiktoken\",\n",
    "    load_parameters_path=f\"{MODEL_CHECKPOINT_PATH}\",\n",
    "    max_target_length=SEQUENCE_LENGTH,\n",
    "    async_checkpointing=\"false\",\n",
    "    model_name=\"llama3.1-8b\",\n",
    "    skip_jax_distributed_system=\"true\",\n",
    "    weight_dtype=\"bfloat16\",\n",
    "    attention=\"dot_product\",\n",
    "    remat_policy=\"custom\",\n",
    "    decoder_layer_input=\"offload\",\n",
    "    query_proj=\"offload\",\n",
    "    key_proj=\"offload\",\n",
    "    value_proj=\"offload\",\n",
    ")\n",
    "llama3_1_8b_policy, mesh_policy = get_ref_maxtext_model(config_policy)\n",
    "\n",
    "llama3_1_8b_policy.config = model_config\n",
    "\n",
    "nnx.display(llama3_1_8b_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "  print(\"Model initialized successfully\")\n",
    "  print(f\"Model mesh shape: {mesh_policy.shape}\")\n",
    "  print(f\"Model config: {model_config}\")\n",
    "\n",
    "  # Sanity check that weights are loaded correctly\n",
    "  _maxtext_state_flatten = nnx.state(llama3_1_8b_policy).flat_state()\n",
    "  maxtext_state_flatten = {\".\".join(str(key) for key in keys): v for keys, v in _maxtext_state_flatten}\n",
    "  print(\n",
    "      f\"maxtext_state_flatten[base.token_embedder.embedding].value={maxtext_state_flatten['base.token_embedder.embedding'].value}\"\n",
    "  )\n",
    "\n",
    "# See memory usage after loading the policy model:\n",
    "print(\"HBM usage after loading policy model:\")\n",
    "show_hbm_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Reward Functions\n",
    "\n",
    "We define four reward functions:\n",
    "\n",
    "1. **Format Matching**: Reward if the format of the output exactly matches the instruction given in `TEMPLATE`\n",
    "2. **Approximate Format Matching**: Reward if the format of the output approximately matches the instruction given in `TEMPLATE`\n",
    "3. **Answer Correctness**: Reward if the answer is correct/partially correct\n",
    "4. **Number Extraction**: Sometimes, the text between `<answer>`, `</answer>` might not be one number. So, extract the number, and reward the model if the answer is correct.\n",
    "\n",
    "The reward functions are inspired from [here](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb).\n",
    "\n",
    "First off, let's define a RegEx for checking whether the format matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_format = re.compile(\n",
    "    rf\"^[\\s]{{0,}}\" rf\"{reasoning_start}.+?{reasoning_end}.*?\" rf\"{solution_start}(.+?){solution_end}\" rf\"[\\s]{{0,}}$\",\n",
    "    flags=re.MULTILINE | re.DOTALL,\n",
    ")\n",
    "\n",
    "match_format.search(\n",
    "    f\"{reasoning_start}Let me\" f\" think!{reasoning_end}{solution_start}2{solution_end}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Function 1: Exact Format Matching\n",
    "\n",
    "Give the model a reward of 3 points if the format matches exactly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_format_exactly(prompts, completions, **kargs):\n",
    "  scores = []\n",
    "  for completion in completions:\n",
    "    score = 0\n",
    "    response = completion\n",
    "    # Match if format is seen exactly!\n",
    "    if match_format.search(response) is not None:\n",
    "      score += REWARD_EXACT_FORMAT_MATCH\n",
    "    scores.append(score)\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Function 2: Approximate Format Matching\n",
    "\n",
    "We also reward the model if the format of the output matches partially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_format_approximately(prompts, completions, **kargs):\n",
    "  scores = []\n",
    "\n",
    "  for completion in completions:\n",
    "    score = 0\n",
    "    response = completion\n",
    "    # Count how many keywords are seen - we penalize if too many!\n",
    "    # If we see 1, then plus some points!\n",
    "    score += REWARD_PARTIAL_FORMAT_MATCH if response.count(reasoning_start) == 1 else PENALTY_INCORRECT_FORMAT\n",
    "    score += REWARD_PARTIAL_FORMAT_MATCH if response.count(reasoning_end) == 1 else PENALTY_INCORRECT_FORMAT\n",
    "    score += REWARD_PARTIAL_FORMAT_MATCH if response.count(solution_start) == 1 else PENALTY_INCORRECT_FORMAT\n",
    "    score += REWARD_PARTIAL_FORMAT_MATCH if response.count(solution_end) == 1 else PENALTY_INCORRECT_FORMAT\n",
    "    scores.append(score)\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Function 3: Answer Correctness\n",
    "\n",
    "Reward the model if the answer is correct. A reward is also given if the answer does not match exactly, i.e., based on how close the answer is to the correct value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(prompts, completions, answer, **kargs):\n",
    "  responses = completions\n",
    "\n",
    "  extracted_responses = [guess.group(1) if (guess := match_format.search(r)) is not None else None for r in responses]\n",
    "\n",
    "  scores = []\n",
    "  for guess, true_answer in zip(extracted_responses, answer):\n",
    "    score = 0\n",
    "    if guess is None:\n",
    "      scores.append(0)\n",
    "      continue\n",
    "    # Correct answer gets 3 points!\n",
    "    if guess == true_answer:\n",
    "      score += REWARD_EXACT_FORMAT_MATCH\n",
    "    # Match if spaces are seen\n",
    "    elif guess.strip() == true_answer.strip():\n",
    "      score += REWARD_WHITE_SPACE_FORMAT_MATCH\n",
    "    else:\n",
    "      # We also reward it if the answer is close via ratios!\n",
    "      # Ie if the answer is within some range, reward it!\n",
    "      try:\n",
    "        ratio = float(guess) / float(true_answer)\n",
    "        if ratio >= 0.9 and ratio <= 1.1:\n",
    "          score += REWARD_RATIO_GUESS_TO_ANSWER_HIGH\n",
    "        elif ratio >= 0.8 and ratio <= 1.2:\n",
    "          score += REWARD_RATIO_GUESS_TO_ANSWER_LOW\n",
    "        else:\n",
    "          score += PENALTY_INCORRECT_ANSWER  # Penalize wrong answers\n",
    "      except:\n",
    "        score += PENALTY_INCORRECT_FORMAT  # Penalize\n",
    "    scores.append(score)\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Function 4: Number Extraction\n",
    "\n",
    "Sometimes, the text between `<answer>` and `</answer>` might not be one number; it can be a sentence. So, we extract the number and compare the answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_numbers = re.compile(rf\"{solution_start}.*?([\\d\\.]{{1,}})\", flags=re.MULTILINE | re.DOTALL)\n",
    "match_numbers.findall(f\"{solution_start}  0.34  {solution_end}\")\n",
    "\n",
    "\n",
    "def check_numbers(prompts, completions, answer, **kargs):\n",
    "  question = kargs[\"question\"]\n",
    "  responses = completions\n",
    "\n",
    "  extracted_responses = [guess.group(1) if (guess := match_numbers.search(r)) is not None else None for r in responses]\n",
    "\n",
    "  scores = []\n",
    "  if DEBUG:\n",
    "    print(\"START ============================\")\n",
    "    print(f\"Question: {question[0]}\")\n",
    "    print(f\"Answer: {answer[0]}\")\n",
    "    print(f\"Response: {responses[0]}\")\n",
    "    print(f\"Extracted: {extracted_responses[0]}\")\n",
    "    print(\"END ==============================\")\n",
    "  for guess, true_answer in zip(extracted_responses, answer):\n",
    "    if guess is None:\n",
    "      scores.append(0)\n",
    "      continue\n",
    "    # Convert to numbers\n",
    "    try:\n",
    "      true_answer = float(true_answer.strip())\n",
    "      guess = float(guess.strip())\n",
    "      scores.append(1.5 if guess == true_answer else 0.0)\n",
    "    except:\n",
    "      scores.append(0)\n",
    "      continue\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions\n",
    "\n",
    "Before we train the model, let's evaluate the model on the test set so we can see the improvement post training.\n",
    "\n",
    "We evaluate it in two ways:\n",
    "\n",
    "**Quantitative**\n",
    "- **Answer Accuracy**: percentage of samples for which the model predicts the correct final numerical answer\n",
    "- **Answer (Partial) Accuracy**: percentage of samples for which the model predicts a final numerical answer such that the `model answer / answer` ratio lies between 0.9 and 1.1.\n",
    "- **Format Accuracy**: percentage of samples for which the model outputs the correct format, i.e., reasoning between the reasoning special tokens, and the final answer between the `<start_answer>`, `<end_answer>` tokens.\n",
    "\n",
    "**Qualitative**\n",
    "\n",
    "We'll also print outputs for a few given questions so that we can compare the generated output later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(\n",
    "    prompts,\n",
    "    rl_cluster,\n",
    "    num_passes=1,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "):\n",
    "  \"\"\"\n",
    "  Generate responses for a batch of prompts across multiple passes.\n",
    "\n",
    "  Args:\n",
    "      prompts: List of prompts to generate responses for\n",
    "      rl_cluster: Model cluster for generation\n",
    "      num_passes: Number of generation passes\n",
    "      temperature: Sampling temperature\n",
    "      top_k: Top-k sampling parameter\n",
    "      top_p: Top-p sampling parameter\n",
    "\n",
    "  Returns:\n",
    "      List of lists containing responses for each prompt across passes\n",
    "  \"\"\"\n",
    "  multiple_call_responses = [[] for _ in range(len(prompts))]\n",
    "\n",
    "  for p in range(num_passes):\n",
    "    responses = rl_cluster.rollout.generate(\n",
    "        prompts,\n",
    "        rollout_config=RolloutConfig(\n",
    "            max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "        ),\n",
    "    )\n",
    "    responses = responses.text\n",
    "\n",
    "    if DEBUG:\n",
    "      print(f\"Pass {p+1}/{num_passes}, responses: {responses}\")\n",
    "\n",
    "    for idx, response in enumerate(responses):\n",
    "      multiple_call_responses[idx].append(response)\n",
    "\n",
    "  return multiple_call_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_responses(question, responses, answer):\n",
    "  \"\"\"\n",
    "  Score a set of responses for a single question.\n",
    "\n",
    "  Args:\n",
    "      question: The evaluation question\n",
    "      responses: List of generated responses for this question\n",
    "      answer: The correct answer\n",
    "\n",
    "  Returns:\n",
    "      Tuple of (is_correct, is_partially_correct, has_correct_format)\n",
    "  \"\"\"\n",
    "  if DEBUG:\n",
    "    print(\"========================================\")\n",
    "    print(f\"Evaluation Question: {question}\")\n",
    "    print(f\"Evaluation Answer: {answer}\")\n",
    "    print(f\"Evaluation Responses: {responses}\")\n",
    "    print(\"========================================\")\n",
    "\n",
    "  is_correct = False\n",
    "  is_partially_correct = False\n",
    "  has_correct_format = False\n",
    "\n",
    "  for response in responses:\n",
    "    # Extract numerical response\n",
    "    extracted_response = guess.group(1) if (guess := match_numbers.search(response)) is not None else \"-1000000\"\n",
    "\n",
    "    if DEBUG:\n",
    "      print(f\"Evaluation extracted_response: {extracted_response}\")\n",
    "\n",
    "    # Check exact correctness\n",
    "    try:\n",
    "      if float(extracted_response.strip()) == float(answer.strip()):\n",
    "        is_correct = True\n",
    "\n",
    "      # Check partial correctness (within 10%)\n",
    "      ratio = float(extracted_response.strip()) / float(answer.strip())\n",
    "      if 0.9 <= ratio <= 1.1:\n",
    "        is_partially_correct = True\n",
    "    except Exception as e:\n",
    "      if DEBUG:\n",
    "        print(f\"Evaluation Exception: {e}\")\n",
    "        print(\"SKIPPED\")\n",
    "\n",
    "    # Check format correctness\n",
    "    if match_format.search(response) is not None:\n",
    "      has_correct_format = True\n",
    "\n",
    "    # Early exit if all criteria are met\n",
    "    if is_correct and is_partially_correct and has_correct_format:\n",
    "      break\n",
    "\n",
    "  return is_correct, is_partially_correct, has_correct_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    dataset,\n",
    "    rl_cluster,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_passes=1,\n",
    "    corr_lst=False,\n",
    "    make_lst=False,\n",
    "):\n",
    "  \"\"\"\n",
    "  Computes accuracy and percentage of outputs matching the format.\n",
    "\n",
    "  Args:\n",
    "      dataset: The evaluation dataset\n",
    "      rl_cluster: Model cluster for generation\n",
    "      temperature: Sampling temperature\n",
    "      top_k: Top-k sampling parameter\n",
    "      top_p: Top-p sampling parameter\n",
    "      num_passes: Number of generation passes\n",
    "      corr_lst: If True, only include correct responses in the list\n",
    "      make_lst: If True, return a list of (question, answer, responses)\n",
    "\n",
    "  Returns:\n",
    "      Tuple of statistics and optionally the response list\n",
    "  \"\"\"\n",
    "  response_lst = []\n",
    "  corr = 0\n",
    "  partially_corr = 0\n",
    "  corr_format = 0\n",
    "  total = 0\n",
    "\n",
    "  for batch in tqdm(dataset):\n",
    "    answers = batch[\"answer\"]\n",
    "    questions = batch[\"question\"]\n",
    "    prompts = batch[\"prompts\"]\n",
    "\n",
    "    # Generate responses for all prompts in the batch\n",
    "    multiple_call_responses = generate_responses(\n",
    "        prompts=prompts,\n",
    "        rl_cluster=rl_cluster,\n",
    "        num_passes=num_passes,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "    # Score each question-answer pair\n",
    "    for question, responses, answer in zip(questions, multiple_call_responses, answers):\n",
    "      is_correct, is_partially_correct, has_correct_format = score_responses(\n",
    "          question=question,\n",
    "          responses=responses,\n",
    "          answer=answer,\n",
    "      )\n",
    "\n",
    "      # Update counters\n",
    "      if is_correct:\n",
    "        corr += 1\n",
    "        if corr_lst and make_lst:\n",
    "          response_lst.append((question, answer, responses))\n",
    "      else:\n",
    "        if not corr_lst and make_lst:\n",
    "          response_lst.append((question, answer, responses))\n",
    "\n",
    "      if is_partially_correct:\n",
    "        partially_corr += 1\n",
    "\n",
    "      if has_correct_format:\n",
    "        corr_format += 1\n",
    "\n",
    "      total += 1\n",
    "\n",
    "      # Print progress every 10 items\n",
    "      if total % 10 == 0:\n",
    "        print(\n",
    "            f\"===> {corr=}, {total=}, {corr / total * 100=}, \"\n",
    "            f\"{partially_corr / total * 100=}, {corr_format / total * 100=}\"\n",
    "        )\n",
    "\n",
    "  # Prepare return values\n",
    "  to_return = (\n",
    "      corr,\n",
    "      total,\n",
    "      corr / total * 100,\n",
    "      partially_corr / total * 100,\n",
    "      corr_format / total * 100,\n",
    "  )\n",
    "\n",
    "  if make_lst:\n",
    "    return to_return, response_lst\n",
    "  return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "Let's set up all the configs first - checkpointing, metric logging and training. We then train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ckpt saving\n",
    "checkpointing_options = ocp.CheckpointManagerOptions(save_interval_steps=SAVE_INTERVAL_STEPS, max_to_keep=MAX_TO_KEEP)\n",
    "\n",
    "# Metrics logger\n",
    "metrics_logging_options = metrics_logger.MetricsLoggerOptions(log_dir=LOG_DIR, flush_every_n_steps=20)\n",
    "\n",
    "\n",
    "# Logs\n",
    "print(f\"TensorBoard logs directory: {LOG_DIR}\")\n",
    "print(f\"tensorboard --logdir {LOG_DIR} --port=8086\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer, learning rate scheduler, gradient clipping\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
    "        init_value=0.0,\n",
    "        peak_value=LEARNING_RATE,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        decay_steps=MAX_STEPS,\n",
    "        end_value=0.0,\n",
    "    ),\n",
    "    b1=B1,\n",
    "    b2=B2,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "if MAX_GRAD_NORM is not None:\n",
    "  optimizer = optax.chain(\n",
    "      optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n",
    "      optimizer,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL Cluster config\n",
    "# Note that we use vLLM as the rollout engine.\n",
    "# and we are using Tensor Parallelism for rollout\n",
    "\n",
    "cluster_config = rl_cluster_lib.ClusterConfig(\n",
    "    role_to_mesh={\n",
    "        rl_cluster_lib.Role.ACTOR: mesh,\n",
    "        rl_cluster_lib.Role.REFERENCE: mesh,\n",
    "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
    "    },\n",
    "    rollout_engine=\"vllm\",\n",
    "    offload_to_cpu=False,\n",
    "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
    "        actor_optimizer=optimizer,\n",
    "        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
    "        max_steps=MAX_STEPS,\n",
    "        gradient_accumulation_steps=1,\n",
    "        # metrics logging\n",
    "        metrics_logging_options=metrics_logging_options,\n",
    "        # checkpoint saving\n",
    "        checkpoint_root_directory=CKPT_DIR,\n",
    "        checkpointing_options=checkpointing_options,\n",
    "    ),\n",
    "    rollout_config=base_rollout.RolloutConfig(\n",
    "        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n",
    "        max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
    "        temperature=TEMPERATURE,\n",
    "        top_p=TOP_P,\n",
    "        top_k=TOP_K,\n",
    "    ),\n",
    "    rollout_vllm_model_version=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    rollout_vllm_hbm_utilization=0.2,\n",
    "    rollout_vllm_tpu_backend_type=\"jax\",\n",
    ")\n",
    "\n",
    "grpo_config = GrpoConfig(\n",
    "    num_generations=NUM_GENERATIONS,\n",
    "    num_iterations=NUM_ITERATIONS,\n",
    "    beta=BETA,\n",
    "    epsilon=EPSILON,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL cluster\n",
    "\n",
    "rl_cluster = rl_cluster_lib.RLCluster(\n",
    "    actor=llama3_1_8b_policy,\n",
    "    reference=llama3_1_8b,\n",
    "    tokenizer=model_tokenizer,\n",
    "    cluster_config=cluster_config,\n",
    ")\n",
    "\n",
    "# GRPO Trainer\n",
    "grpo_trainer = GrpoLearner(\n",
    "    rl_cluster=rl_cluster,\n",
    "    reward_fns=[\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "    ],\n",
    "    grpo_config=grpo_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "  # verify if vllm sampler works\n",
    "  output = rl_cluster.rollout.generate(\n",
    "      [\"The capital of France is\"],\n",
    "      rollout_config=RolloutConfig(max_tokens_to_generate=64, temperature=0.1),\n",
    "  )\n",
    "\n",
    "  print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Before Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n",
    "    test_dataset,\n",
    "    rl_cluster,\n",
    "    **GENERATION_CONFIGS[\"greedy\"],\n",
    ")\n",
    "print(f\"Pre GRPO Training: {corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\" f\" {format_accuracy=}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.profiler.start_trace(PROFILE_DIR)\n",
    "with mesh, nn_partitioning.axis_rules(config_policy.logical_axis_rules):\n",
    "  grpo_trainer.train(dataset)\n",
    "jax.profiler.stop_trace()\n",
    "\n",
    "print(\"HBM usage after training:\")\n",
    "show_hbm_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation\n",
    "\n",
    "Let's evaluate our model after training!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n",
    "    test_dataset,\n",
    "    rl_cluster,\n",
    "    **GENERATION_CONFIGS[\"greedy\"],\n",
    ")\n",
    "print(f\"Post GRPO Training: {corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\" f\" {format_accuracy=}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
