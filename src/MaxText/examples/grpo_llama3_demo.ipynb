{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO/GSPO Llama3.1-8B Demo\n",
    "\n",
    "This notebook demonstrates GRPO (Group Relative Policy Optimization) training using the unified `rl_train` function or GSPO (Group Sequence Policy Optimization) - the change is in loss function which is a parameter\n",
    "\n",
    "## What is GRPO/GSPO?\n",
    "\n",
    "GRPO/GSPO is an RL algorithm that enhances reasoning abilities of LLMs by:\n",
    "1. Generating multiple responses for each prompt\n",
    "2. Evaluating responses using reward models  \n",
    "3. Calculating relative advantages to update the policy\n",
    "\n",
    "The difference is in the loss function - either it's optimizing each token (GRPO) or the whole sequence(GSPO).\n",
    "\n",
    "## Hardware Requirements\n",
    "\n",
    "- Single host TPUVM (v6e-8/v5p-8) or multi-host with Pathways\n",
    "- Sufficient memory for Llama3.1-8B model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Your Hugging Face Token\n",
    "\n",
    "To access model checkpoint from the Hugging Face Hub, you need to authenticate with a personal access token.\n",
    "\n",
    "**Follow these steps to get your token:**\n",
    "\n",
    "1.  **Navigate to the Access Tokens page** in your Hugging Face account settings. You can go there directly by visiting this URL:\n",
    "    *   [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "\n",
    "2.  **Create a new token** by clicking the **\"+ Create new token\"** button.\n",
    "\n",
    "3.  **Give your token a name** and assign it a **`read` role**. The `read` role is sufficient for downloading models.\n",
    "\n",
    "4.  **Copy the generated token**. You will need to paste it in the next step.\n",
    "\n",
    "**Follow these steps to store your token:**\n",
    "\n",
    "Just put your token in the line below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"\" # Set HF_TOKEN environment variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install dependencies and set up the environment:\n",
    "https://maxtext.readthedocs.io/latest/tutorials/grpo.html#from-github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up the training parameters. We do not use Pathways and do use a single host. Defaults are hardcoded for Llama3.1-8B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ~/maxtext/src/ #  make sure we are in the right directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for GRPO training\n",
    "import os\n",
    "import MaxText\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Set up paths (adjust if needed)\n",
    "MAXTEXT_REPO_ROOT = os.path.dirname(MaxText.__file__)\n",
    "RUN_NAME=\"grpo_test\"\n",
    "# Hardcoded defaults for Llama3.1-8B\n",
    "MODEL_NAME = \"llama3.1-8b\"\n",
    "HF_REPO_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "CHAT_TEMPLATE_PATH = f\"{MAXTEXT_REPO_ROOT}/examples/chat_templates/gsm8k_rl.json\"\n",
    "LOSS_ALGO=\"grpo\" #  or \"gspo-token\" if you want to use GSPO\n",
    "\n",
    "# Required: Set these before running\n",
    "MODEL_CHECKPOINT_PATH = \"\"  # Update this!\n",
    "if not MODEL_CHECKPOINT_PATH:\n",
    "    raise RuntimeError(\"MODEL_CHECKPOINT_PATH is not set\")\n",
    "    \n",
    "OUTPUT_DIRECTORY = \"/tmp/gpo_output\"  # Update this!\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"Authenticated with Hugging Face\")\n",
    "else:\n",
    "    print(\"Authentication failed: Hugging Face token not set\")\n",
    "\n",
    "# Optional: Override training parameters\n",
    "STEPS = 10  # Reduced for demo purposes\n",
    "PER_DEVICE_BATCH_SIZE = 1\n",
    "LEARNING_RATE = 3e-6\n",
    "NUM_GENERATIONS = 2\n",
    "GRPO_BETA = 0.08\n",
    "GRPO_EPSILON = 0.2\n",
    "CHIPS_PER_VM = 1\n",
    "\n",
    "print(f\"üìÅ MaxText Home: {MAXTEXT_REPO_ROOT}\")\n",
    "print(f\"ü§ñ Model: {MODEL_NAME}\")\n",
    "print(f\"üì¶ Checkpoint: {MODEL_CHECKPOINT_PATH}\")\n",
    "print(f\"üíæ Output: {OUTPUT_DIRECTORY}\")\n",
    "print(f\"üîë HF Token: {'‚úÖ Set' if HF_TOKEN else '‚ùå Missing - set HF_TOKEN env var'}\")\n",
    "print(f\"üìä Steps: {STEPS}\")\n",
    "print(f\"Loss Algorithm : {LOSS_ALGO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add MaxText to Python path\n",
    "maxtext_path = Path(MAXTEXT_REPO_ROOT) \n",
    "sys.path.insert(0, str(maxtext_path))\n",
    "\n",
    "from MaxText import pyconfig, max_utils\n",
    "from MaxText.rl.train_rl import rl_train, setup_configs_and_devices\n",
    "import jax\n",
    "\n",
    "# Initialize JAX and Pathways\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"0\"\n",
    "os.environ[\"SKIP_JAX_PRECOMPILE\"] = \"1\"  # Faster startup for vLLM\n",
    "\n",
    "print(\"‚úÖ Successfully imported modules\")\n",
    "print(f\"üìÅ MaxText path: {maxtext_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build configuration for GRPO training\n",
    "config_file = os.path.join(MAXTEXT_REPO_ROOT, \"configs/rl.yml\")\n",
    "\n",
    "# Verify chat template exists\n",
    "if not os.path.exists(CHAT_TEMPLATE_PATH)):\n",
    "    raise FileNotFoundError(f\"Chat template not found: {CHAT_TEMPLATE_PATH}\")\n",
    "\n",
    "# Build argv list for pyconfig.initialize()\n",
    "config_argv = [\n",
    "    \"\",  # argv[0] placeholder\n",
    "    config_file,\n",
    "    f\"model_name={MODEL_NAME}\",\n",
    "    f\"tokenizer_path={HF_REPO_ID}\",\n",
    "    f\"run_name={RUN_NAME}\",\n",
    "    f\"chat_template_path={CHAT_TEMPLATE_PATH}\",\n",
    "    f\"load_parameters_path={MODEL_CHECKPOINT_PATH}\",\n",
    "    f\"base_output_directory={OUTPUT_DIRECTORY}\",\n",
    "    f\"hf_access_token={HF_TOKEN}\",\n",
    "    f\"steps={STEPS}\",\n",
    "    f\"per_device_batch_size={PER_DEVICE_BATCH_SIZE}\",\n",
    "    f\"learning_rate={LEARNING_RATE}\",\n",
    "    f\"num_generations={NUM_GENERATIONS}\",\n",
    "    f\"grpo_beta={GRPO_BETA}\",\n",
    "    f\"grpo_epsilon={GRPO_EPSILON}\",\n",
    "    f\"chips_per_vm={CHIPS_PER_VM}\",\n",
    "    f\"loss_algo={LOSS_ALGO}\",\n",
    "    \"use_pathways=False\"\n",
    "]\n",
    "\n",
    "# Initialize configuration\n",
    "print(f\"üîß Initializing configuration from: {config_file}\")\n",
    "config = pyconfig.initialize(config_argv)\n",
    "max_utils.print_system_information()\n",
    "\n",
    "print(\"\\n‚úÖ Configuration initialized successfully\")\n",
    "print(f\"üìä Training steps: {config.steps}\")\n",
    "print(f\"üìÅ Output directory: {config.base_output_directory}\")\n",
    "print(f\"ü§ñ Model: {config.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute GRPO/GSPO training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ Starting Training...\")\n",
    "print(\"=\"*80)\n",
    "print(1)\n",
    "try:\n",
    "    # Call the rl_train function (it handles everything internally)\n",
    "    rl_train(config)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ Training Completed Successfully!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìÅ Checkpoints saved to: {config.checkpoint_dir}\")\n",
    "    print(f\"üìä TensorBoard logs: {config.tensorboard_dir}\")\n",
    "    print(f\"üéØ Model ready for inference!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚ùåTraining Failed!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nüí° Common issues:\")\n",
    "    print(\"  - Check that MODEL_CHECKPOINT_PATH points to a valid checkpoint\")\n",
    "    print(\"  - Ensure HF_TOKEN environment variable is set\")\n",
    "    print(\"  - Verify OUTPUT_DIRECTORY is writable\")\n",
    "    print(\"  - Check hardware requirements (TPU/GPU availability)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Learn More\n",
    "\n",
    "- **CLI Usage**: Run `python3 -m src.MaxText.rl.train_rl src/MaxText/configs/rl.yml --model_name=llama3.1-8b ...`\n",
    "- **Configuration**: See `src/MaxText/configs/rl.yml` for all available options\n",
    "- **Documentation**: Check `src/MaxText/rl/train_rl.py` for the `rl_train` function implementation\n",
    "- **Examples**: See other examples in `src/MaxText/examples/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
