{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nb_Ppf2ZUQL"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AI-Hypercomputer/maxtext/blob/main/src/MaxText/examples/sft_qwen3_demo.ipynb)\n",
        "\n",
        "# Qwen3-0.6B Supervised Fine-Tuning (SFT) Demo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGbe4_YQZUQL"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook performs SFT training and evaluation workflow on [OpenAI's GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k).\n",
        "The primary goal is to demonstrate the end-to-end process of:\n",
        "1. Pre-SFT Evaluation: Calcuating baseline accuracy for the model before training.\n",
        "2. SFT Training: Fine-tune the model using MaxText & Tunix SFT trainer.\n",
        "3. Post-SFT Evaluation: Re-running the evaluation loop after training to measure the performance gain achieved by SFT.\n",
        "\n",
        "This notebook can run on the **public TPU v5e-1**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zolxPWhQZUQL"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "### Change Runtime Type\n",
        "\n",
        "**Instructions:**\n",
        "1.  Navigate to the menu at the top of the screen.\n",
        "2.  Click on **Runtime**.\n",
        "3.  Select **Change runtime type** from the dropdown menu.\n",
        "4.  Select **v5e-1 TPU** as the **Hardware accelerator**.\n",
        "5. Click on **Save**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk_QpVVuZUQL"
      },
      "source": [
        "### Get Your Hugging Face Token\n",
        "\n",
        "To access model checkpoint from the Hugging Face Hub, you need to authenticate with a personal access token.\n",
        "\n",
        "**Follow these steps to get your token:**\n",
        "\n",
        "1.  **Navigate to the Access Tokens page** in your Hugging Face account settings. You can go there directly by visiting this URL:\n",
        "    *   [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "\n",
        "2.  **Create a new token** by clicking the **\"+ Create new token\"** button.\n",
        "\n",
        "3.  **Give your token a name** and assign it a **`read` role**. The `read` role is sufficient for downloading models.\n",
        "\n",
        "4.  **Copy the generated token**. You will need to paste it in the next step.\n",
        "\n",
        "**Follow these steps to store your token:**\n",
        "\n",
        "1. On the left sidebar of your Colab window, click the key icon (the Secrets tab).\n",
        "\n",
        "2. Click **\"+ Add new secret\"**.\n",
        "\n",
        "3. Set the Name as **HF_TOKEN**.\n",
        "\n",
        "4. Paste your token into the Value field.\n",
        "\n",
        "5. Ensure the Notebook access toggle is turned On."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9ms-jTSZUQL"
      },
      "source": [
        "## Installation: MaxText & Other Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AI-Hypercomputer/maxtext.git\n",
        "%cd /content/maxtext\n",
        "\n",
        "# Install uv, a fast Python package installer\n",
        "!pip install uv\n",
        "\n",
        "# Install MaxText and its dependencies\n",
        "!uv pip install -e .[tpu] --resolution=lowest\n",
        "\n",
        "# Install vLLM\n",
        "!VLLM_TARGET_DEVICE=\"tpu\" pip install --no-cache-dir --pre \\\n",
        "    --index-url https://us-python.pkg.dev/cloud-tpu-images/maxtext-rl/simple/ \\\n",
        "    --extra-index-url https://pypi.org/simple/ \\\n",
        "    --extra-index-url https://us-python.pkg.dev/ml-oss-artifacts-published/jax/simple/ \\\n",
        "    --extra-index-url https://download.pytorch.org/whl/nightly/cpu \\\n",
        "    --find-links https://storage.googleapis.com/jax-releases/libtpu_releases.html \\\n",
        "    --find-links https://storage.googleapis.com/libtpu-wheels/index.html \\\n",
        "    --find-links https://storage.googleapis.com/libtpu-releases/index.html \\\n",
        "    --find-links https://storage.googleapis.com/jax-releases/jax_nightly_releases.html \\\n",
        "    --find-links https://storage.googleapis.com/jax-releases/jaxlib_nightly_releases.html \\\n",
        "    vllm==0.11.1rc1.dev292+g1b86bd8e1.tpu\n",
        "!pip install --no-cache-dir --pre \\\n",
        "    --index-url https://us-python.pkg.dev/cloud-tpu-images/maxtext-rl/simple/ \\\n",
        "    --extra-index-url https://pypi.org/simple/ \\\n",
        "    --extra-index-url https://us-python.pkg.dev/ml-oss-artifacts-published/jax/simple/ \\\n",
        "    --find-links https://storage.googleapis.com/jax-releases/libtpu_releases.html \\\n",
        "    tpu-commons==0.1.2"
      ],
      "metadata": {
        "id": "OSPRVbi7n6tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywtealAxZUQM"
      },
      "source": [
        "### Restart Session\n",
        "To apply certain changes, you need to restart the session.\n",
        "\n",
        "**Instructions:**\n",
        "1.  Navigate to the menu at the top of the screen.\n",
        "2.  Click on **Runtime**.\n",
        "3.  Select **Restart session** from the dropdown menu.\n",
        "\n",
        "You will be asked to confirm the action in a pop-up dialog. Click on **Yes**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Clexf-j7ZUQM"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkBI9A3JZUQM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import transformers\n",
        "\n",
        "import MaxText\n",
        "from MaxText import pyconfig\n",
        "from MaxText.examples.sft_train_and_evaluate import evaluate_model, get_test_dataset\n",
        "from MaxText.integration.tunix.tunix_adapter import TunixMaxTextAdapter\n",
        "from MaxText.sft import sft_trainer\n",
        "\n",
        "from tunix.rl.rollout.vllm_rollout import VllmRollout\n",
        "\n",
        "from datetime import datetime\n",
        "from flax import nnx\n",
        "from huggingface_hub import login\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "MAXTEXT_REPO_ROOT=os.path.dirname(MaxText.__file__)\n",
        "print(f\"MaxText installation path: {MAXTEXT_REPO_ROOT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBbPN-uVZUQM"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "except Exception:\n",
        "    HF_TOKEN=os.environ.get(\"HF_TOKEN\")\n",
        "if HF_TOKEN:\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"Authenticated with Hugging Face successfully!\")\n",
        "else:\n",
        "    print(\"Authentication failed: HF_TOKEN is not set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aENuzm9iZUQM"
      },
      "source": [
        "## Model Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjPYYl3zZUQM"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"qwen3-0.6b\"\n",
        "TOKENIZER_PATH = \"Qwen/Qwen3-0.6B\"\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "  TOKENIZER_PATH,\n",
        "  token=HF_TOKEN,\n",
        ")\n",
        "\n",
        "# This colab will download the checkpoint from HF and store at `MODEL_CHECKPOINT_PATH`\n",
        "MODEL_CHECKPOINT_PATH = f\"{MAXTEXT_REPO_ROOT}/qwen_checkpoint\"\n",
        "\n",
        "RUN_NAME = datetime.now().strftime(\"%Y-%m-%d-%H-%m-%S\")\n",
        "\n",
        "# This is the directory where the fine-tuned model checkpoint will be saved\n",
        "BASE_OUTPUT_DIRECTORY = f\"/tmp/maxtext_qwen06\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L37Ij4NZUQM"
      },
      "source": [
        "## Download Qwen3-0.6B Model Checkpoint from Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJanDAc0ZUQM"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!python3 -m MaxText.utils.ckpt_conversion.to_maxtext \\\n",
        "    $MAXTEXT_REPO_ROOT/configs/base.yml \\\n",
        "    model_name=$MODEL_NAME \\\n",
        "    base_output_directory=$MODEL_CHECKPOINT_PATH \\\n",
        "    hf_access_token=$HF_TOKEN \\\n",
        "    use_multimodal=false \\\n",
        "    scan_layers=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1T0bm82lZUQM"
      },
      "outputs": [],
      "source": [
        "print(f\"Model checkpoint can be found at: {MODEL_CHECKPOINT_PATH}/0/items\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC-hILG0ZUQM"
      },
      "source": [
        "## Dataset Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3MLdr9kZUQM"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = \"openai/gsm8k\"\n",
        "TRAIN_DATA_SPLIT = \"train\"\n",
        "TEST_DATA_SPLIT = \"test\"\n",
        "HF_DATA_DIR = \"main\"\n",
        "TRAIN_DATA_COLUMNS = [\"question\", \"answer\"]\n",
        "CHAT_TEMPLATE_PATH = f\"{MAXTEXT_REPO_ROOT}/examples/chat_templates/math_qa.json\"\n",
        "NUM_TEST_SAMPLES = 20 # Total number of samples to test\n",
        "BATCH_SIZE = 1 # Number of test samples to process in a batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeAHmxSYZUQM"
      },
      "source": [
        "## MaxText Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "In-jdp1AAwrL"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "config = pyconfig.initialize([\n",
        "    \"\",\n",
        "    f\"{MAXTEXT_REPO_ROOT}/configs/sft.yml\",\n",
        "    f\"load_parameters_path={MODEL_CHECKPOINT_PATH}/0/items\",\n",
        "    f\"model_name={MODEL_NAME}\",\n",
        "    f\"hf_access_token={HF_TOKEN}\",\n",
        "    f\"base_output_directory={BASE_OUTPUT_DIRECTORY}\",\n",
        "    f\"run_name={RUN_NAME}\",\n",
        "    f\"tokenizer_path={TOKENIZER_PATH}\",\n",
        "    f\"hf_path={DATASET_NAME}\",\n",
        "    f\"train_split={TRAIN_DATA_SPLIT}\",\n",
        "    f\"hf_data_dir={HF_DATA_DIR}\",\n",
        "    f\"train_data_columns={TRAIN_DATA_COLUMNS}\",\n",
        "    \"steps=500\",\n",
        "    \"per_device_batch_size=1\",\n",
        "    \"max_target_length=1024\",\n",
        "    \"learning_rate=3e-6\",\n",
        "    \"weight_dtype=bfloat16\",\n",
        "    \"dtype=bfloat16\",\n",
        "    f\"chat_template_path={CHAT_TEMPLATE_PATH}\",\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9b0GWo-ZUQM"
      },
      "source": [
        "## Initial Setup & Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDqFmvUCZUQM"
      },
      "source": [
        "### Create Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wscWYxrtZUQM"
      },
      "outputs": [],
      "source": [
        "test_dataset = get_test_dataset(config, tokenizer)\n",
        "test_dataset = test_dataset[:NUM_TEST_SAMPLES]\n",
        "test_dataset = test_dataset.to_iter_dataset().batch(BATCH_SIZE, drop_remainder=True)\n",
        "TOTAL_BATCHES = NUM_TEST_SAMPLES // BATCH_SIZE\n",
        "print(f\"Processing {NUM_TEST_SAMPLES} examples with a batch size of {BATCH_SIZE}. This will result in {TOTAL_BATCHES} total batches for the test run.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLSvOOEUZUQM"
      },
      "source": [
        "### Create SFT Trainer State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IHsC0m6ZUQM"
      },
      "outputs": [],
      "source": [
        "trainer, mesh = sft_trainer.setup_trainer_state(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpKtEqzFZUQM"
      },
      "source": [
        "### Create vLLM Rollout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-pf_rbqZUQM"
      },
      "outputs": [],
      "source": [
        "tunix_model = TunixMaxTextAdapter(trainer.model)\n",
        "vllm_rollout = VllmRollout(\n",
        "    model=tunix_model,\n",
        "    tokenizer=tokenizer,\n",
        "    cache_config_or_size=1280,\n",
        "    mesh=mesh,\n",
        "    model_version=TOKENIZER_PATH,\n",
        "    hbm_utilization=0.8,\n",
        "    init_with_random_weights=True,\n",
        "    tpu_backend_type=\"jax\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "567gTxsEZUQM"
      },
      "source": [
        "## Evaluation before SFT Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnACa3zCZUQM"
      },
      "outputs": [],
      "source": [
        "print(\"Running Pre-SFT Evaluation...\")\n",
        "score = evaluate_model(test_dataset, vllm_rollout, debug=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5-M4iYkZUQN"
      },
      "outputs": [],
      "source": [
        "print(\"========================= Score for PRE-SFT Evaluation =========================\")\n",
        "print(f\"Percentage of test samples where the model produced the correct numerical answer: {score['correct']}%\")\n",
        "print(f\"Percentage of test samples where the model produced the numerical answer within 10%: {score['partially_correct']}%\")\n",
        "print(f\"Percentage of test samples where the model's output adheres to the expected structure: {score['correct_format']}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJE1ookSAzz-"
      },
      "source": [
        "## SFT Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "mgwpNgQYCJEd",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(\"Starting SFT Training...\")\n",
        "trainer = sft_trainer.train_model(config, trainer, mesh)\n",
        "print(\"SFT Training Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEdNYRhwZUQN"
      },
      "source": [
        "## Evaluation after SFT Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcsZacZdZUQN"
      },
      "outputs": [],
      "source": [
        "print(\"Running Post-SFT Evaluation...\")\n",
        "model = TunixMaxTextAdapter(trainer.model)\n",
        "state = nnx.state(model)\n",
        "vllm_rollout.update_params(state)\n",
        "score = evaluate_model(test_dataset, vllm_rollout, debug=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "-JtYTPvJZUQN"
      },
      "outputs": [],
      "source": [
        "print(\"========================= Score for POST-SFT Evaluation =========================\")\n",
        "print(f\"Percentage of test samples where the model produced the correct numerical answer: {score['correct']}%\")\n",
        "print(f\"Percentage of test samples where the model produced the numerical answer within 10%: {score['partially_correct']}%\")\n",
        "print(f\"Percentage of test samples where the model's output adheres to the expected structure: {score['correct_format']}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}