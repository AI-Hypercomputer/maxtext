{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AI-Hypercomputer/maxtext/blob/main/src/MaxText/examples/sft_llama3_demo.ipynb)\n",
        "\n",
        "# Llama3.1-8B-Instruct Supervised Fine-Tuning (SFT) Demo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "This notebook can run on **TPU v5e-8** or **v5p-8**\n",
        "\n",
        "This notebook demonstrates how to perform Supervised Fine-Tuning (SFT) on Llama3.1-8B-Instruct using the Hugging Face ultrachat_200k dataset with MaxText and Tunix integration for efficient training.\n",
        "\n",
        "## Dataset Overview\n",
        "\n",
        "**Dataset Link:** https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k\n",
        "\n",
        "### Dataset Information:\n",
        "- **Name:** HuggingFaceH4/ultrachat_200k\n",
        "- **Type:** Supervised Fine-Tuning dataset\n",
        "- **Size:** ~200k conversations\n",
        "- **Format:** Chat conversations with human-AI pairs\n",
        "- **Splits:** train_sft, test_sft\n",
        "- **Data columns:** ['messages']\n",
        "\n",
        "### Dataset Structure:\n",
        "Each example contains a 'messages' field with:\n",
        "- **role:** 'user' or 'assistant'\n",
        "- **content:** The actual message text\n",
        "\n",
        "### Example data format:\n",
        "```json\n",
        "{\n",
        "  \"messages\": [\n",
        "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"}\n",
        "  ]\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "### Change Runtime Type\n",
        "\n",
        "**Instructions:**\n",
        "1.  Navigate to the menu at the top of the screen.\n",
        "2.  Click on **Runtime**.\n",
        "3.  Select **Change runtime type** from the dropdown menu.\n",
        "4.  Select **v5e-8** or **v5p-8 TPU** as the **Hardware accelerator**.\n",
        "5. Click on **Save**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get Your Hugging Face Token\n",
        "\n",
        "To access model checkpoint from the Hugging Face Hub, you need to authenticate with a personal access token.\n",
        "\n",
        "**Follow these steps to get your token:**\n",
        "\n",
        "1.  **Navigate to the Access Tokens page** in your Hugging Face account settings. You can go there directly by visiting this URL:\n",
        "    *   [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "\n",
        "2.  **Create a new token** by clicking the **\"+ Create new token\"** button.\n",
        "\n",
        "3.  **Give your token a name** and assign it a **`read` role**. The `read` role is sufficient for downloading models.\n",
        "\n",
        "4.  **Copy the generated token**. You will need to paste it in the next step.\n",
        "\n",
        "**Follow these steps to store your token:**\n",
        "\n",
        "1. On the left sidebar of your Colab window, click the key icon (the Secrets tab).\n",
        "\n",
        "2. Click **\"+ Add new secret\"**.\n",
        "\n",
        "3. Set the Name as **HF_TOKEN**.\n",
        "\n",
        "4. Paste your token into the Value field.\n",
        "\n",
        "5. Ensure the Notebook access toggle is turned On."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KPyOE8e9WbO"
      },
      "outputs": [],
      "source": [
        "#Install maxtext and dependencies\n",
        "# 1. Install uv, a fast Python package installer\n",
        "!pip install uv\n",
        "\n",
        "# 2. Install MaxText and its dependencies\n",
        "!uv pip install maxtext --resolution=lowest\n",
        "!python3 -m MaxText.install_maxtext_extra_deps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Restart Session\n",
        "To apply certain changes, you need to restart the session.\n",
        "\n",
        "**Instructions:**\n",
        "1.  Navigate to the menu at the top of the screen.\n",
        "2.  Click on **Runtime**.\n",
        "3.  Select **Restart session** from the dropdown menu.\n",
        "\n",
        "You will be asked to confirm the action in a pop-up dialog. Click on **Yes**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up the maxtext environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import MaxText\n",
        "from MaxText import pyconfig\n",
        "from MaxText.sft.sft_trainer import train as sft_train\n",
        "import jax\n",
        "import os\n",
        "# Hugging Face Authentication Setup\n",
        "from huggingface_hub import login\n",
        "\n",
        "# use google colab userdata to get the HF token\n",
        "from google.colab import userdata\n",
        "\n",
        "MAXTEXT_REPO_ROOT=os.path.dirname(MaxText.__file__)\n",
        "print(f\"MaxText installation path: {MAXTEXT_REPO_ROOT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Set the model, checkpoint path and output directory\n",
        "MODEL_NAME = \"llama3.1-8b\"\n",
        "# Case 1: Set `MODEL_CHECKPOINT_PATH` to the path (local or gs://) that already has `Llama3.1-8B-Instruct` model checkpoint\n",
        "# Case 2: If you do not have the checkpoint, then do not update `MODEL_CHECKPOINT_PATH`\n",
        "# and this colab will download the checkpoint from HF and store at `\"{MAXTEXT_REPO_ROOT}/llama_checkpoint\"`\n",
        "MODEL_CHECKPOINT_PATH = f\"{MAXTEXT_REPO_ROOT}/llama_checkpoint\"\n",
        "\n",
        "# This is the directory where the fine-tuned model will be saved\n",
        "# You can change it to any path you want (local or gs://) \n",
        "BASE_OUTPUT_DIRECTORY = \"/tmp/out/maxtext_llama3_8b\"\n",
        "\n",
        "# Set your Hugging Face token as a secret in the Google Colab   \n",
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "# HF_TOKEN = \"your_actual_token_here\" - use this for a private jupyter lab\n",
        "login(token=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is the command to convert the HF model to the MaxText format \n",
        "if not os.path.exists(MODEL_CHECKPOINT_PATH):\n",
        "    !python3 -m MaxText.utils.ckpt_conversion.to_maxtext \\\n",
        "        $MAXTEXT_REPO_ROOT/configs/base.yml \\\n",
        "        model_name=$MODEL_NAME \\\n",
        "        base_output_directory=$MODEL_CHECKPOINT_PATH \\\n",
        "        hf_access_token=$HF_TOKEN \\\n",
        "        use_multimodal=false \\\n",
        "        scan_layers=false"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxzKMBQd_U5-"
      },
      "outputs": [],
      "source": [
        "# this is the code to initialize jax if it's not initialized in the cell above\n",
        "if not jax.distributed.is_initialized():\n",
        "    jax.distributed.initialize()\n",
        "print(f\"JAX version: {jax.__version__}\")\n",
        "print(f\"JAX devices: {jax.devices()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MaxText imports\n",
        "try:\n",
        "    MAXTEXT_AVAILABLE = True\n",
        "    print(\"✓ MaxText imports successful\")\n",
        "except ImportError as e:\n",
        "    print(f\"⚠️ MaxText not available: {e}\")\n",
        "    MAXTEXT_AVAILABLE = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "In-jdp1AAwrL"
      },
      "outputs": [],
      "source": [
        "# Fixed configuration setup for Llama3.1-8B on TPU\n",
        "if MAXTEXT_AVAILABLE:\n",
        "    config_argv = [\n",
        "        \"\",\n",
        "        f\"{MAXTEXT_REPO_ROOT}/configs/sft.yml\",   # base SFT config\n",
        "        f\"load_parameters_path={MODEL_CHECKPOINT_PATH}/0/items/\",  # Load pre-trained weights!, replace with your checkpoint path\n",
        "        f\"model_name={MODEL_NAME}\",\n",
        "        \"steps=100\",                                     # adjust for your training needs\n",
        "        \"per_device_batch_size=1\",                      # minimal to avoid OOM\n",
        "        \"max_target_length=1024\",                        \n",
        "        \"learning_rate=2.0e-5\",                         # safe small LR\n",
        "        \"eval_steps=5\",\n",
        "        \"weight_dtype=bfloat16\",\n",
        "        \"dtype=bfloat16\",\n",
        "        \"hf_path=HuggingFaceH4/ultrachat_200k\",                       # HuggingFace dataset\n",
        "        f\"hf_access_token={HF_TOKEN}\",\n",
        "        f\"base_output_directory={BASE_OUTPUT_DIRECTORY}\",\n",
        "        \"run_name=sft_llama3_8b_test\",\n",
        "        \"tokenizer_path=meta-llama/Llama-3.1-8B-Instruct\",                # Llama tokenizer\n",
        "        \"eval_interval=10\",\n",
        "        \"profiler=xplane\",\n",
        "    ]\n",
        "\n",
        "    # Initialize configuration using MaxText's pyconfig\n",
        "    config = pyconfig.initialize(config_argv)\n",
        "\n",
        "    print(\"✓ Fixed configuration loaded:\")\n",
        "    print(f\"  - Model: {config.model_name}\")\n",
        "    print(f\"  - Dataset: {config.hf_path}\")\n",
        "    print(f\"  - Steps: {config.steps}\")\n",
        "    print(f\"  - Use SFT: {config.use_sft}\")\n",
        "    print(f\"  - Learning Rate: {config.learning_rate}\")\n",
        "else:\n",
        "    print(\"MaxText not available - cannot load configuration\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgwpNgQYCJEd"
      },
      "outputs": [],
      "source": [
        "#  Execute the training using MaxText SFT trainer's train() function\n",
        "if MAXTEXT_AVAILABLE:\n",
        "    print(\"=\"*60)\n",
        "    print(\"EXECUTING ACTUAL TRAINING\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    trainer, mesh = sft_train(config)\n",
        "\n",
        "print(\"Training complete!\")\n",
        "print(\"Model saved at: \", BASE_OUTPUT_DIRECTORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Decoding\n",
        "\n",
        "We could reuse the trainer.model to try a decode command after the SFT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decode by using the trained model\n",
        "if MAXTEXT_AVAILABLE:\n",
        "    from MaxText.vllm_decode import decode\n",
        "\n",
        "    decode_config_argv = [\n",
        "        \"\",  \n",
        "        f\"{MAXTEXT_REPO_ROOT}/configs/sft.yml\",   # Decode config\n",
        "        \"model_name=llama3.1-8b\",\n",
        "        \"per_device_batch_size=1\",\n",
        "        \"max_target_length=128\",\n",
        "        \"max_prefill_predict_length=64\",\n",
        "        \"weight_dtype=bfloat16\",\n",
        "        \"dtype=bfloat16\",\n",
        "        f\"hf_access_token={HF_TOKEN}\",\n",
        "        \"base_output_directory=/tmp/maxtext_output\",\n",
        "        \"run_name=sft_llama3_decode\",\n",
        "        \"tokenizer_path=meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        \"prompt=Suggest some famous landmarks in London.\",\n",
        "        \"use_chat_template=true\",\n",
        "        \"decode_sampling_temperature=0.0\",\n",
        "        \"decode_sampling_nucleus_p=1.0\",\n",
        "        \"decode_sampling_top_k=0.0\",\n",
        "    ]\n",
        "    \n",
        "    # Initialize configuration using MaxText's pyconfig\n",
        "    decode_config = pyconfig.initialize(decode_config_argv)\n",
        "    os.environ[\"SKIP_JAX_PRECOMPILE\"] = \"1\"\n",
        "    decode(decode_config, trainer.model, mesh)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
