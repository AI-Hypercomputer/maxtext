{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run SFT on Llama3.1-8B-Instruct model\n",
    "\n",
    "This notebook demonstrates how to perform Supervised Fine-Tuning (SFT) on Llama3.1-8B-Instruct using the Hugging Face ultrachat_200k dataset with Tunix integration for efficient training.\n",
    "\n",
    "## Dataset Overview\n",
    "https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k\n",
    "\n",
    "**Dataset Information:**\n",
    "- **Name**: HuggingFaceH4/ultrachat_200k\n",
    "- **Type**: Supervised Fine-Tuning dataset\n",
    "- **Size**: ~200k conversations\n",
    "- **Format**: Chat conversations with human-AI pairs\n",
    "- **Splits**: train_sft, test_sft\n",
    "- **Data columns**: ['messages']\n",
    "\n",
    "**Dataset Structure:**\n",
    "Each example contains a 'messages' field with:\n",
    "- role: 'user' or 'assistant'\n",
    "- content: The actual message text\n",
    "\n",
    "**Example data format:**\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "## Key Features\n",
    "- **MaxText Llama3.1-8B-Instruct model** \n",
    "- **Tunix integration** for optimized training\n",
    "- **UltraChat-200k dataset** from HuggingFace\n",
    "- Tokenizes with meta-llama/Llama-3.1-8B-Instruct\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "- MaxText environment with all dependencies\n",
    "- Tunix installation\n",
    "- HuggingFace access token for dataset download\n",
    "- Sufficient compute resources (TPU/GPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### (Optional) Run this if you just have this file and nothing else\n",
    "\n",
    "# 1. Clone the MaxText repository (from AI‑Hypercomputer)\n",
    "!git clone https://github.com/AI-Hypercomputer/maxtext.git\n",
    "\n",
    "# 2. Navigate into the cloned directory\n",
    "%cd maxtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### (Optional) Do not run this if you already installed the dependencies\n",
    "\n",
    "# 3. Ensure setup.sh is executable\n",
    "!chmod +x setup.sh\n",
    "\n",
    "# 4. Execute the setup script\n",
    "!./setup.sh\n",
    "\n",
    "# force numpy version\n",
    "!pip install --force-reinstall numpy==2.1.2\n",
    "# install nest_asyncio\n",
    "!pip install nest_asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "# To fix \"This event loop is already running\" error in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "#  Set  home directory. Change this to your home directory where maxtext is cloned\n",
    "MAXTEXT_HOME = os.path.expanduser(\"~\") + \"/maxtext\"\n",
    "print(f\"Home directory (from Python): {MAXTEXT_HOME}\")\n",
    "# set the path to the Llama3.1-8B-Instruct checkpoint you want to load, gs://<bucket> supported\n",
    "MODEL_CHECKPOINT_PATH = \"path/to/scanned/checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "# Find MaxText directory and change working directory to it\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == \"examples\":\n",
    "  # We're in the examples folder, go up one level\n",
    "  maxtext_path = current_dir.parent.parent\n",
    "else:\n",
    "  # We're in the root, MaxText is a subfolder\n",
    "  maxtext_path = Path(f\"{MAXTEXT_HOME}\") / \"src\" / \"MaxText\"\n",
    "\n",
    "# Change working directory to MaxText project root\n",
    "os.chdir(maxtext_path)\n",
    "sys.path.insert(0, str(maxtext_path))\n",
    "\n",
    "print(f\"✓ Changed working directory to: {os.getcwd()}\")\n",
    "print(f\"✓ MaxText project root: {maxtext_path}\")\n",
    "print(f\"✓ Added to Python path: {maxtext_path}\")\n",
    "import jax\n",
    "\n",
    "if not jax.distributed.is_initialized():\n",
    "  jax.distributed.initialize()\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Authentication Setup\n",
    "\n",
    "If you encounter 401 unauthorized errors when loading datasets, you need to authenticate with Hugging Face. Set your token below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Authentication Setup\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Set your Hugging Face token here\n",
    "HF_TOKEN = \"hf_your_token_here\"  # Replace with your actual token\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaxText imports\n",
    "try:\n",
    "  from MaxText import pyconfig\n",
    "  from MaxText.sft.sft_trainer import train as sft_train\n",
    "\n",
    "  MAXTEXT_AVAILABLE = True\n",
    "  print(\"✓ MaxText imports successful\")\n",
    "except ImportError as e:\n",
    "  print(f\"⚠️ MaxText not available: {e}\")\n",
    "  MAXTEXT_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Configuration Setup\n",
    "\n",
    "## Notes\n",
    "- Trains on completion only (sft_train_on_completion_only=True)\n",
    "- Please set sft_train_on_completion_only=False to train both on prompts and completions. By default SFT will train only on completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed configuration setup\n",
    "if MAXTEXT_AVAILABLE:\n",
    "  # Proper config setup using MaxText's config system\n",
    "  config_argv = [\n",
    "      \"\",\n",
    "      f\"{MAXTEXT_HOME}/src/MaxText/configs/sft.yml\",  # SFT config\n",
    "      f\"load_parameters_path={MODEL_CHECKPOINT_PATH}\",\n",
    "      \"model_name=llama3.1-8b\",\n",
    "      \"steps=100\",\n",
    "      \"per_device_batch_size=1\",\n",
    "      \"max_target_length=1024\",\n",
    "      \"learning_rate=2.0e-5\",\n",
    "      \"eval_steps=5\",\n",
    "      \"weight_dtype=bfloat16\",\n",
    "      \"dtype=bfloat16\",\n",
    "      \"hf_path=HuggingFaceH4/ultrachat_200k\",\n",
    "      f\"hf_access_token={HF_TOKEN}\",\n",
    "      \"base_output_directory=/tmp/maxtext_output\",\n",
    "      \"run_name=sft_llama3_demo\",\n",
    "      \"tokenizer_path=meta-llama/Llama-3.1-8B-Instruct\",\n",
    "      \"eval_interval=10\",\n",
    "      \"profiler=xplane\",\n",
    "  ]\n",
    "\n",
    "  # Initialize configuration using MaxText's pyconfig\n",
    "  config = pyconfig.initialize(config_argv)\n",
    "\n",
    "  print(\"✓ Fixed configuration loaded:\")\n",
    "  print(f\"  - Model: {config.model_name}\")\n",
    "  print(f\"  - Dataset: {config.hf_path}\")\n",
    "  print(f\"  - Steps: {config.steps}\")\n",
    "  print(f\"  - Use SFT: {config.use_sft}\")\n",
    "  print(f\"  - Learning Rate: {config.learning_rate}\")\n",
    "else:\n",
    "  print(\"MaxText not available - cannot load configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Execute Actual Training\n",
    "\n",
    "Let's actually run the training using the MaxText SFT trainer's `train()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Execute the training using MaxText SFT trainer's train() function\n",
    "if MAXTEXT_AVAILABLE:\n",
    "  print(\"=\" * 60)\n",
    "  print(\"EXECUTING ACTUAL TRAINING\")\n",
    "  print(\"=\" * 60)\n",
    "\n",
    "  sft_train(config)\n",
    "\n",
    "  print(\"\\n✅ Training completed successfully!\")\n",
    "\n",
    "else:\n",
    "  print(\"MaxText not available - cannot execute training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Summary\n",
    "\n",
    "This notebook demonstrated the complete MaxText & Tunix integration for SFT training.\n",
    "\n",
    "\n",
    "The integration provides the best of both worlds: MaxText's high-performance LLM training and Tunix's optimized training infrastructure, making it ideal for production SFT training on large datasets like UltraChat-200k.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
