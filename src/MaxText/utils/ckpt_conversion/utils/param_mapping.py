# Copyright 2023â€“2026 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Parameter mappings and transformation hooks for checkpoint conversion.

This module defines the necessary components to convert model checkpoints between
MaxText and Hugging Face formats for various architectures (e.g., Gemma, Qwen).
It provides two key types of mappings for each model:

1.  **Parameter Name Mappings (`PARAM_MAPPING`)**: Dictionaries that map a MaxText
    parameter key to its corresponding Hugging Face parameter(s). These mappings are
    generated by functions like `GEMMA2_MAXTEXT_TO_HF_PARAM_MAPPING`.

    **Key: MaxText parameters, with following forms:**
    - `atomic_mt_key`: A single string representing one MaxText parameter.
    - `composite_mt_key`: A tuple of strings representing multiple MaxText parameters. (e.g., GPT-OSS)

    **Value: corresponding Hugging Face parameters, with following forms:**
    - `unscanned`: A single string.
    - `scanned`: A list of strings, to be stacked along the layer axis.
    - `unscanned with expert stacking`: A list of strings, to be stacked along the expert axis.
    - `scanned with expert stacking`: A nested list of strings, to be stacked along both layer and expert axes.
    Note: Expert stacking only applies a subset of MoE models (e.g., Qwen MoE, DeepSeek, Mixtral),
      but not others (e.g., GPT-OSS).

2.  **Hook Functions (`HOOK_FNS`)**: Dictionaries that map a MaxText parameter
    name to a specific transformation function (a "hook"). These hooks handle
    the actual value conversion, which can include operations like reshaping,
    transposing, scaling, or padding tensors to match the target format's
    requirements. These are generated by functions like
    `GEMMA2_MAXTEXT_TO_HF_PARAM_HOOK_FN`.

The main conversion script uses these mappings to systematically transform each
parameter from the source checkpoint and build the target checkpoint.
"""

from MaxText.utils.ckpt_conversion.strategies.factory import ModelMapperFactory


class _DynamicDict(dict):
  """Dynamic dictionary to resolve strategies on lookup to maintain backward compatibility."""

  def __init__(self, key_type="mapping"):
    super().__init__()
    self.key_type = key_type

  def __getitem__(self, key):
    try:
      strategy = ModelMapperFactory.get_strategy(key)
      if self.key_type == "mapping":
        return strategy.get_mapping
      elif self.key_type == "hooks":
        return strategy.get_hooks
      else:
        raise ValueError(f"Unknown strategy {key}")
    except ValueError as e:
      raise KeyError(key) from e

  def __contains__(self, key):
    try:
      ModelMapperFactory.get_strategy(key)
      return True
    except ValueError:
      return False

  def get(self, key, default=None):
    try:
      return self[key]
    except KeyError:
      return default

  def keys(self):
    # Returning empty list as iteration over all supported models is not required by consumers
    # of this legacy interface, who perform direct lookups.
    return []


# {maxtext model name: {maxtext weight name: hf weight name}}
PARAM_MAPPING = _DynamicDict("mapping")

# {maxtext model name: {maxtext weight name: bi-directional transform}}
HOOK_FNS = _DynamicDict("hooks")

VLLM_HOOK_FNS = {
    # Using specific model names to resolve the correct strategy for the family
    "qwen3": lambda: ModelMapperFactory.get_strategy("qwen3-4b").get_vllm_hooks(),
    "llama3.1": lambda: ModelMapperFactory.get_strategy("llama3.1-8b").get_vllm_hooks(),
    "deepseek3": lambda: ModelMapperFactory.get_strategy("deepseek3-671b").get_vllm_hooks(),
}
