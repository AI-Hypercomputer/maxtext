{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-gq1gcpyCpT"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/tunix/blob/main/examples/qlora_demo.ipynb\" ><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "## Install necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RTlz7JP7yCpT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kagglehub\n",
        "\n",
        "!pip install -q tensorflow\n",
        "!pip install -q tensorboardX\n",
        "!pip install -q grain\n",
        "!pip install -q jax==0.6.2 jaxlib==0.6.2\n",
        "# !pip install -q git+https://github.com/google/tunix\n",
        "! pip install -q ~/tunix/\n",
        "!pip install -q git+https://github.com/google/qwix\n",
        "\n",
        "!pip uninstall -q -y flax\n",
        "!pip install -q git+https://github.com/google/flax.git@5d5c907d1b5f45c97138289d5aa2e2e8452bf52e\n",
        "\n",
        "\n",
        "!pip install -q tensorflow-datasets\n",
        "\n",
        "!pip install -q git+https://github.com/AI-Hypercomputer/pathways-utils.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NYtRLk-ojcid"
      },
      "outputs": [],
      "source": [
        "# # If you want to upload your metrics to Weights & Biases, please install the package and login. Make sure to install `wandb` before importing `tunix`.\n",
        "# !pip install wandb\n",
        "\n",
        "# import wandb\n",
        "# wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HOQuCB0o7kV"
      },
      "source": [
        "If `wandb` is installed, you'll see a message like the one below when you start the experiment:\n",
        "\n",
        "```\n",
        "Tracking run with wandb version 0.21.0\n",
        "Run data is saved locally in /content/wandb/run-20250717_224322-kmvoi0ho\n",
        "Syncing run 2025-07-17_22-43-22 to Weights & Biases (docs)\n",
        "View project at https://wandb.ai/<wandb_username>/tunix?apiKey=<api_key>\n",
        "View run at https://wandb.ai/<wandb_username>/tunix/runs/kmvoi0ho?apiKey=<api_key>\n",
        "Do NOT share these links with anyone. They can be used to claim your runs.\n",
        "```\n",
        "\n",
        "After clicking the link, you will be directed to the following Weights & Biases metrics page which contain train metrics, eval metrics, system metrics, and various custom metric you wish to report:\n",
        "\n",
        "![wandb_train.png](data:image/png;base64,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)\n",
        "\n",
        "![wandb_eval.png](data:image/png;base64,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)\n",
        "\n",
        "![wandb_system.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-datasets in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (4.9.9)\n",
            "Requirement already satisfied: absl-py in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (2.3.1)\n",
            "Requirement already satisfied: array_record>=0.5.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (0.7.2)\n",
            "Requirement already satisfied: dm-tree in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (0.1.9)\n",
            "Requirement already satisfied: etils>=1.9.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (1.13.0)\n",
            "Requirement already satisfied: immutabledict in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (4.2.1)\n",
            "Requirement already satisfied: numpy in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (2.1.3)\n",
            "Requirement already satisfied: promise in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (5.29.5)\n",
            "Requirement already satisfied: psutil in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (7.0.0)\n",
            "Requirement already satisfied: pyarrow in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (20.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (2.32.4)\n",
            "Requirement already satisfied: simple_parsing in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (1.17.2)\n",
            "Requirement already satisfied: termcolor in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (3.1.0)\n",
            "Requirement already satisfied: toml in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (4.67.1)\n",
            "Requirement already satisfied: wrapt in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (1.17.2)\n",
            "Requirement already satisfied: einops in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (0.8.1)\n",
            "Requirement already satisfied: fsspec in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (2025.7.0)\n",
            "Requirement already satisfied: importlib_resources in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (6.5.2)\n",
            "Requirement already satisfied: typing_extensions in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (4.14.1)\n",
            "Requirement already satisfied: zipp in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (3.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow-datasets) (2025.7.9)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from dm-tree->tensorflow-datasets) (25.3.0)\n",
            "Requirement already satisfied: six in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from promise->tensorflow-datasets) (1.17.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from simple_parsing->tensorflow-datasets) (0.16)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-metadata->tensorflow-datasets) (1.70.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: aqtp in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (0.8.4)\n",
            "Requirement already satisfied: absl-py in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from aqtp) (2.3.1)\n",
            "Requirement already satisfied: jax in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from aqtp) (0.6.2)\n",
            "Requirement already satisfied: jaxlib in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from aqtp) (0.6.2)\n",
            "Requirement already satisfied: flax in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from aqtp) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->aqtp) (2.1.3)\n",
            "Requirement already satisfied: msgpack in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->aqtp) (1.1.1)\n",
            "Requirement already satisfied: optax in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->aqtp) (0.2.5)\n",
            "Requirement already satisfied: orbax-checkpoint in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->aqtp) (0.11.20)\n",
            "Requirement already satisfied: tensorstore in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->aqtp) (0.1.76)\n",
            "Requirement already satisfied: rich>=11.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->aqtp) (14.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->aqtp) (4.14.1)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->aqtp) (6.0.2)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->aqtp) (0.1.9)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from jax->aqtp) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from jax->aqtp) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.12 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from jax->aqtp) (1.16.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from rich>=11.1->flax->aqtp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from rich>=11.1->flax->aqtp) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax->aqtp) (0.1.2)\n",
            "Requirement already satisfied: chex>=0.1.87 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from optax->flax->aqtp) (0.1.90)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from chex>=0.1.87->optax->flax->aqtp) (1.0.0)\n",
            "Requirement already satisfied: etils[epath,epy] in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint->flax->aqtp) (1.13.0)\n",
            "Requirement already satisfied: nest_asyncio in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint->flax->aqtp) (1.6.0)\n",
            "Requirement already satisfied: aiofiles in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint->flax->aqtp) (24.1.0)\n",
            "Requirement already satisfied: protobuf in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint->flax->aqtp) (5.29.5)\n",
            "Requirement already satisfied: humanize in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint->flax->aqtp) (4.12.3)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint->flax->aqtp) (3.20.1)\n",
            "Requirement already satisfied: fsspec in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from etils[epath,epy]->orbax-checkpoint->flax->aqtp) (2025.7.0)\n",
            "Requirement already satisfied: importlib_resources in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from etils[epath,epy]->orbax-checkpoint->flax->aqtp) (6.5.2)\n",
            "Requirement already satisfied: zipp in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from etils[epath,epy]->orbax-checkpoint->flax->aqtp) (3.23.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: pillow in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (11.3.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: omegaconf in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from omegaconf) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from omegaconf) (6.0.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: google-cloud-storage in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (3.2.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.26.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-cloud-storage) (2.40.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=2.15.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-cloud-storage) (2.25.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-cloud-storage) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.7.2 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.22.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-cloud-storage) (2.32.4)\n",
            "Requirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-cloud-storage) (1.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-api-core<3.0.0,>=2.15.0->google-cloud-storage) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-api-core<3.0.0,>=2.15.0->google-cloud-storage) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-api-core<3.0.0,>=2.15.0->google-cloud-storage) (1.26.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (4.9.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (2025.7.9)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.26.1->google-cloud-storage) (0.6.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-datasets\n",
        "!pip install aqtp\n",
        "!pip install pillow>=11.1.0\n",
        "!pip install pillow\n",
        "!pip install omegaconf\n",
        "!pip install google-cloud-storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_text in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (2.19.0)\n",
            "Requirement already satisfied: transformers in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (4.53.1)\n",
            "Requirement already satisfied: tiktoken in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (0.9.0)\n",
            "Requirement already satisfied: tensorflow<2.20,>=2.19.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow_text) (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (2.3.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (3.4.0)\n",
            "Requirement already satisfied: packaging in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (65.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (1.73.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (2.1.3)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (0.37.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (2025.7.9)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (3.1.3)\n",
            "Requirement already satisfied: filelock in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from transformers) (0.33.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.7.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (0.45.1)\n",
            "Requirement already satisfied: rich in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (14.1.0)\n",
            "Requirement already satisfied: namex in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (0.1.0)\n",
            "Requirement already satisfied: optree in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (0.1.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_text transformers tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEjoS0-JyCpT"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LshEMmSzx6W6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages/jax/__init__.py:31: UserWarning: cloud_tpu_init failed: AttributeError(\"module 'libtpu' has no attribute 'get_library_path'\")\n",
            " This a JAX bug; please report an issue at https://github.com/jax-ml/jax/issues\n",
            "  _warn(f\"cloud_tpu_init failed: {exc!r}\\n This a JAX bug; please report \"\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "import os\n",
        "import time\n",
        "\n",
        "from flax import nnx\n",
        "import os\n",
        "\n",
        "# Set the TPU library path in the environment\n",
        "os.environ['TPU_LIBRARY_PATH'] = '/home/mazumdera_google_com/.local/lib/python3.10/site-packages/libtpu/libtpu.so'\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import kagglehub\n",
        "import optax\n",
        "from orbax import checkpoint as ocp\n",
        "from qwix import lora\n",
        "from tunix.generate import sampler as sampler_lib\n",
        "from tunix.models.gemma import data as data_lib\n",
        "from tunix.models.gemma import gemma as gemma_lib\n",
        "from tunix.models.gemma import params as params_lib\n",
        "from tunix.sft import metrics_logger\n",
        "from tunix.sft import peft_trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnEZ_jXwypn-"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QBcMaL22T3Uu"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Model\n",
        "MESH = [(1, 8), (\"fsdp\", \"tp\")]\n",
        "# LoRA\n",
        "RANK = 16\n",
        "ALPHA = 2.0\n",
        "\n",
        "# Train\n",
        "MAX_STEPS = 100\n",
        "EVAL_EVERY_N_STEPS = 20\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "\n",
        "# Checkpoint saving\n",
        "INTERMEDIATE_CKPT_DIR = \"~/qlora_expt/content/intermediate_ckpt/\"\n",
        "CKPT_DIR = \"~/qlora_expt/content/ckpts/\"\n",
        "PROFILING_DIR = \"~/qlora_expt/content/profiling/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3s5Qg6xT3Uu"
      },
      "source": [
        "## Load Gemma 2B\n",
        "\n",
        "To load the model, you need to be on [Kaggle](https://www.kaggle.com/) and need\n",
        "to have agreed to the Gemma license\n",
        "[here](https://www.kaggle.com/models/google/gemma/flax/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "o_7Sk8d7T3Uu"
      },
      "outputs": [],
      "source": [
        "# Log in\n",
        "# if \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n",
        "  # kagglehub.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oJC3Hfh9T3Uv"
      },
      "outputs": [],
      "source": [
        "kaggle_ckpt_path = kagglehub.model_download(\"google/gemma/flax/2b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "r_5FWrq-T3Uv"
      },
      "outputs": [],
      "source": [
        "# # This is a workaround. The checkpoints on Kaggle don't work with NNX. So, we\n",
        "# # load the model, save the checkpoint locally, and then reload the model\n",
        "# # (sharded).\n",
        "# params = params_lib.load_and_format_params(os.path.join(kaggle_ckpt_path, \"2b\"))\n",
        "# gemma = gemma_lib.Transformer.from_params(params, version=\"2b\")\n",
        "# checkpointer = ocp.StandardCheckpointer()\n",
        "# _, state = nnx.split(gemma)\n",
        "# checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_ertAr6FT3Uv"
      },
      "outputs": [],
      "source": [
        "# # # Wait for the ckpt to save successfully.\n",
        "# time.sleep(60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nGYKuLyFT3Uv"
      },
      "outputs": [],
      "source": [
        "# # Delete the intermediate model to save memory.\n",
        "# del params\n",
        "# del gemma\n",
        "# del state\n",
        "# gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-07 05:02:57.089721: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754542977.102620 2310310 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754542977.106387 2310310 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754542977.117537 2310310 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754542977.117551 2310310 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754542977.117552 2310310 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754542977.117554 2310310 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'Pytree' from 'flax.nnx' (/home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages/flax/nnx/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, os.path.abspath(os.path.join(os.getcwd(), \u001b[33m'\u001b[39m\u001b[33m../../maxtext\u001b[39m\u001b[33m'\u001b[39m)))\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# ! pip install -r ../../maxtext/requirements.txt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMaxText\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmt\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMaxText\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pyconfig\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/maxtext/MaxText/__init__.py:32\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mflax\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nnx\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMaxText\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m maxtext_utils\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMaxText\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_utils\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMaxText\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pyconfig\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMaxText\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m models\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/maxtext/MaxText/train_utils.py:21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mflax\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nnx\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMaxText\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quantizations\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMaxText\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMaxText\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optimizers\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/maxtext/MaxText/layers/quantizations.py:38\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mflax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinen\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMaxText\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon_types\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DType, Config\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMaxText\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkvcache\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KVQuant\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Params used to define mixed precision quantization configs\u001b[39;00m\n\u001b[32m     41\u001b[39m DEFAULT = \u001b[33m\"\u001b[39m\u001b[33m__default__\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# default config\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/maxtext/MaxText/inference/kvcache.py:30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maqt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maqt_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QTensor \u001b[38;5;28;01mas\u001b[39;00m KVTensor\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maqt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mflax\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m aqt_flax\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMaxText\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nnx_wrappers\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMaxText\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minitializers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m variable_to_logically_partitioned\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMaxText\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon_types\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Array, AxisNames, AxisIdxes, Config, CACHE_BATCH_PREFILL, DType, MODEL_MODE_PREFILL, MODEL_MODE_TRAIN, MODEL_MODE_AUTOREGRESSIVE, CACHE_HEADS_NONE, DECODING_ACTIVE_SEQUENCE_INDICATOR\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/maxtext/MaxText/layers/nnx_wrappers.py:31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mflax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnnx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbridge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module \u001b[38;5;28;01mas\u001b[39;00m bdg_module\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mflax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnnx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodule\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mflax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnnx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pytree\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mflax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnnx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrnglib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Rngs\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax\u001b[39;00m\n",
            "\u001b[31mImportError\u001b[39m: cannot import name 'Pytree' from 'flax.nnx' (/home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages/flax/nnx/__init__.py)"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# add the parent directory (one level up) to sys.path\n",
        "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../../maxtext')))\n",
        "\n",
        "# ! pip install -r ../../maxtext/requirements.txt\n",
        "\n",
        "import MaxText as mt\n",
        "from MaxText import pyconfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updating keys from env and command line: ['run_name', 'model_name', 'async_checkpointing', 'checkpoint_period', 'weight_dtype', 'attention', 'base_output_directory', 'tokenizer_path', 'per_device_batch_size', 'steps', 'skip_jax_distributed_system', 'max_target_length']\n",
            "Running Model: gemma-2b\n",
            "Updating following parameters in config\n",
            "\n",
            "base_emb_dim: 2048\n",
            "base_num_query_heads: 8\n",
            "base_num_kv_heads: 1\n",
            "base_mlp_dim: 16384\n",
            "base_num_decoder_layers: 18\n",
            "head_dim: 256\n",
            "mlp_activations: ['gelu', 'linear']\n",
            "vocab_size: 256128\n",
            "decoder_block: gemma\n",
            "normalization_layer_epsilon: 1e-06\n",
            "logits_via_embedding: True\n",
            "Updating keys from model: ['base_emb_dim', 'base_num_query_heads', 'base_num_kv_heads', 'base_mlp_dim', 'base_num_decoder_layers', 'head_dim', 'mlp_activations', 'vocab_size', 'decoder_block', 'normalization_layer_epsilon', 'logits_via_embedding']\n",
            "Skipping jax distributed system due to skip_jax_distributed_system=True flag.\n",
            "Not using emergency checkpoint, ignoring local_checkpoint_directory, local_checkpoint_period, use_replicator_service and replicator_backup_interval_minutes\n",
            "dataset_type set to tfds, will use keys['dataset_path']='' and keys['dataset_name']='c4/en:3.0.1'\n",
            "Config param activations_in_float32: False\n",
            "Config param adam_b1: 0.9\n",
            "Config param adam_b2: 0.95\n",
            "Config param adam_eps: 1e-08\n",
            "Config param adam_eps_root: 0.0\n",
            "Config param adam_weight_decay: 0.1\n",
            "Config param add_bos: True\n",
            "Config param add_eos: True\n",
            "Config param allow_split_physical_axes: False\n",
            "Config param ar_cache_axis_order: 1,2,0,3\n",
            "Config param async_checkpointing: False\n",
            "Config param attention: dot_product\n",
            "Config param attention_type: global\n",
            "Config param attn_logits_soft_cap: None\n",
            "Config param autoregressive_decode_assert: \n",
            "Config param base_emb_dim: 2048\n",
            "Config param base_mlp_dim: 16384\n",
            "Config param base_moe_mlp_dim: 7168\n",
            "Config param base_num_decoder_layers: 18\n",
            "Config param base_num_kv_heads: 1\n",
            "Config param base_num_query_heads: 8\n",
            "Config param base_output_directory: gs://dummy_output_dir\n",
            "Config param beta_fast: 32\n",
            "Config param beta_slow: 1\n",
            "Config param capacity_factor: -1.0\n",
            "Config param cast_logits_to_fp32: True\n",
            "Config param checkpoint_conversion_fn: None\n",
            "Config param checkpoint_dir: gs://dummy_output_dir/test-tunix-maxtext-llama3-8b/checkpoints/\n",
            "Config param checkpoint_is_quantized: False\n",
            "Config param checkpoint_period: 5\n",
            "Config param checkpoint_storage_concurrent_gb: 96\n",
            "Config param checkpoint_storage_target_data_file_size_bytes: 2147483648\n",
            "Config param checkpoint_storage_use_ocdbt: True\n",
            "Config param checkpoint_storage_use_zarr3: True\n",
            "Config param chunk_attn_window_size: 0\n",
            "Config param collect_stack_trace: False\n",
            "Config param colocated_python_data_input: False\n",
            "Config param compile_topology: \n",
            "Config param compile_topology_num_slices: -1\n",
            "Config param compiled_trainstep_file: \n",
            "Config param compute_axis_order: 0,1,2,3\n",
            "Config param constant_bound_config: []\n",
            "Config param context: remat\n",
            "Config param context_parallel_load_balance: True\n",
            "Config param cosine_learning_rate_final_fraction: 0.1\n",
            "Config param custom_mesh: \n",
            "Config param data_sharding: (('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'),)\n",
            "Config param data_shuffle_seed: 0\n",
            "Config param dataset_name: c4/en:3.0.1\n",
            "Config param dataset_path: \n",
            "Config param dataset_type: tfds\n",
            "Config param dcn_autoregressive_parallelism: 1\n",
            "Config param dcn_context_autoregressive_parallelism: 1\n",
            "Config param dcn_context_parallelism: 1\n",
            "Config param dcn_data_parallelism: -1\n",
            "Config param dcn_expert_parallelism: 1\n",
            "Config param dcn_fsdp_parallelism: 1\n",
            "Config param dcn_fsdp_transpose_parallelism: 1\n",
            "Config param dcn_parallelism: [-1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Config param dcn_pipeline_parallelism: 1\n",
            "Config param dcn_sequence_parallelism: 1\n",
            "Config param dcn_tensor_parallelism: 1\n",
            "Config param dcn_tensor_sequence_parallelism: 1\n",
            "Config param dcn_tensor_transpose_parallelism: 1\n",
            "Config param decode_sampling_nucleus_p: -1\n",
            "Config param decode_sampling_strategy: greedy\n",
            "Config param decode_sampling_temperature: 1.0\n",
            "Config param decode_sampling_top_k: 0\n",
            "Config param decoder_block: DecoderBlockType.GEMMA\n",
            "Config param decoder_layer_input: device\n",
            "Config param dpo_beta: 0.1\n",
            "Config param dpo_label_smoothing: 0.0\n",
            "Config param dropout_rate: 0.0\n",
            "Config param dtype: bfloat16\n",
            "Config param dtype_mm: float32\n",
            "Config param dump_hlo: False\n",
            "Config param dump_hlo_delete_local_after: True\n",
            "Config param dump_hlo_gcs_dir: \n",
            "Config param dump_hlo_local_dir: /tmp/xla_dump/\n",
            "Config param dump_hlo_module_name: jit_train_step\n",
            "Config param dump_hlo_upload_all: False\n",
            "Config param dump_hlo_xla_flags: \n",
            "Config param dump_step: -1\n",
            "Config param emb_dim: 2048\n",
            "Config param enable_checkpoint_cloud_logger: False\n",
            "Config param enable_checkpointing: True\n",
            "Config param enable_data_shuffling: True\n",
            "Config param enable_dropout: True\n",
            "Config param enable_emergency_checkpoint: False\n",
            "Config param enable_gcp_goodput_metrics: True\n",
            "Config param enable_gcp_step_deviation_metrics: True\n",
            "Config param enable_goodput_recording: False\n",
            "Config param enable_jax_profiler: False\n",
            "Config param enable_llm_inference_pool: False\n",
            "Config param enable_model_warmup: False\n",
            "Config param enable_orbax_v1: False\n",
            "Config param enable_padding_causal_mask: True\n",
            "Config param enable_pathways_goodput: False\n",
            "Config param enable_prefix_caching: False\n",
            "Config param enable_single_controller: False\n",
            "Config param enable_single_replica_ckpt_restoring: False\n",
            "Config param enable_tensorboard: True\n",
            "Config param eval_data_columns: ['text']\n",
            "Config param eval_dataset_name: c4/en:3.0.1\n",
            "Config param eval_image_column: image\n",
            "Config param eval_interval: -1\n",
            "Config param eval_per_device_batch_size: 8.0\n",
            "Config param eval_split: validation\n",
            "Config param eval_steps: -1\n",
            "Config param expansion_factor_real_data: -1\n",
            "Config param final_logits_soft_cap: None\n",
            "Config param first_num_dense_layers: 0\n",
            "Config param float32_logits: False\n",
            "Config param float32_qk_product: False\n",
            "Config param force_unroll: False\n",
            "Config param freeze_vision_encoder_params: True\n",
            "Config param fused_mlp: False\n",
            "Config param fused_qkv: False\n",
            "Config param gcs_metrics: False\n",
            "Config param generate_slice: v5e-16\n",
            "Config param global_batch_size_to_eval_on: 64\n",
            "Config param global_batch_size_to_load: 64\n",
            "Config param global_batch_size_to_load_eval: 64\n",
            "Config param global_batch_size_to_train_on: 64\n",
            "Config param global_parameter_scale: 1\n",
            "Config param goodput_upload_interval_seconds: 30\n",
            "Config param gradient_accumulation_steps: 1\n",
            "Config param gradient_clipping_threshold: 1.0\n",
            "Config param grain_eval_files: \n",
            "Config param grain_file_type: arrayrecord\n",
            "Config param grain_train_files: \n",
            "Config param grain_worker_count: 1\n",
            "Config param grain_worker_count_eval: 1\n",
            "Config param hardware: tpu\n",
            "Config param head_dim: 256\n",
            "Config param heartbeat_reporting_interval_in_seconds: 5\n",
            "Config param hf_data_dir: \n",
            "Config param hf_eval_files: \n",
            "Config param hf_eval_split: \n",
            "Config param hf_path: \n",
            "Config param hf_train_files: \n",
            "Config param hidden_size_for_vit: 1408\n",
            "Config param ici_autoregressive_parallelism: 1\n",
            "Config param ici_context_autoregressive_parallelism: 1\n",
            "Config param ici_context_parallelism: 1\n",
            "Config param ici_data_parallelism: 1\n",
            "Config param ici_expert_parallelism: 1\n",
            "Config param ici_fsdp_parallelism: -1\n",
            "Config param ici_fsdp_transpose_parallelism: 1\n",
            "Config param ici_parallelism: [1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Config param ici_pipeline_parallelism: 1\n",
            "Config param ici_sequence_parallelism: 1\n",
            "Config param ici_tensor_parallelism: 1\n",
            "Config param ici_tensor_sequence_parallelism: 1\n",
            "Config param ici_tensor_transpose_parallelism: 1\n",
            "Config param image_path: \n",
            "Config param image_placeholder: <|image|>\n",
            "Config param image_size_for_vit: 896\n",
            "Config param inference_benchmark_test: False\n",
            "Config param inference_metadata_file: \n",
            "Config param inference_microbenchmark_log_file_path: \n",
            "Config param inference_microbenchmark_loop_iters: 10\n",
            "Config param inference_microbenchmark_num_samples: [1, 2, 3, 4, 5]\n",
            "Config param inference_microbenchmark_prefill_lengths: 64,128,256,512,1024\n",
            "Config param inference_microbenchmark_stages: prefill,generate\n",
            "Config param inference_server: MaxtextInterleavedServer\n",
            "Config param inhomogeneous_layer_cycle_interval: 1\n",
            "Config param init_weights_seed: 0\n",
            "Config param input_data_sharding_logical_axes: ['activation_embed_and_logits_batch', 'activation_norm_length']\n",
            "Config param interleave_moe_layer_step: 1\n",
            "Config param intermediate_size_for_vit: 5632\n",
            "Config param jax_cache_dir: ~/jax_cache\n",
            "Config param jax_debug_log_modules: \n",
            "Config param jax_distributed_initialization_timeout: 300\n",
            "Config param jax_profiler_port: 9999\n",
            "Config param key_proj: remat\n",
            "Config param kv_lora_rank: 512\n",
            "Config param kv_quant_axis: heads_and_dkv\n",
            "Config param kv_quant_dtype: int8\n",
            "Config param learning_rate: 3e-05\n",
            "Config param learning_rate_schedule_steps: 10\n",
            "Config param load_balance_loss_weight: 0.01\n",
            "Config param load_from_prefill_dir: False\n",
            "Config param load_full_state_path: \n",
            "Config param load_parameters_path: \n",
            "Config param local_checkpoint_directory: \n",
            "Config param local_checkpoint_period: 0\n",
            "Config param local_rope_max_timescale: -1\n",
            "Config param log_config: True\n",
            "Config param log_period: 100\n",
            "Config param logical_axis_rules: (('activation_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_batch_no_exp', ('data', 'fsdp', 'fsdp_transpose')), ('activation_embed_and_logits_batch', ('data', 'stage', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence', 'autoregressive')), ('activation_kv_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_length', ('sequence', 'context')), ('prefill_activation_length', ('sequence', 'context')), ('activation_length', ('context',)), ('activation_norm_length', ('tensor_sequence', 'context', 'sequence')), ('prefill_activation_norm_length', ('tensor_sequence', 'context', 'sequence')), ('activation_q_length', ('context',)), ('activation_kv_length', ()), ('activation_embed', ('tensor', 'tensor_transpose')), ('activation_mlp', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_kv', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_prefill_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_head_dim', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose')), ('activation_vocab', 'tensor_sequence'), ('activation_vocab', ('sequence', 'context')), ('activation_stage', 'stage'), ('activation_exp', ('expert',)), ('decode_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('decode_length', ('sequence',)), ('mlp', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), ('vocab', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('q_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('kv_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'context', 'expert')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'context')), ('embed_tensor_transpose', ('tensor_transpose',)), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'expert')), ('norm', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('layers', 'stage'), ('kv', ()), ('kv_head_dim', ()), ('cache_batch_prefill', ()), ('cache_batch', ()), ('cache_heads_none', ()), ('cache_heads', ('autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence')), ('cache_heads', ('autoregressive', 'tensor', 'tensor_sequence')), ('cache_kv', ()), ('cache_sequence', ()), ('exp', 'expert'), ('paged_kv_heads', ('tensor',)), ('num_pages', ()), ('tokens_per_page', ()), ('paged_kv_head_dim_size', ()))\n",
            "Config param logits_dot_in_fp32: False\n",
            "Config param logits_via_embedding: True\n",
            "Config param lora_input_adapters_path: \n",
            "Config param matmul_precision: default\n",
            "Config param max_checkify: False\n",
            "Config param max_corpus_chars: 10000000\n",
            "Config param max_position_embeddings: 163840\n",
            "Config param max_prefill_predict_length: 64\n",
            "Config param max_target_length: 8192\n",
            "Config param megablox: True\n",
            "Config param mesh_axes: ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive']\n",
            "Config param metrics_dir: gs://dummy_output_dir/test-tunix-maxtext-llama3-8b/metrics/\n",
            "Config param metrics_file: \n",
            "Config param micro_batch_size_to_eval_on: 64\n",
            "Config param micro_batch_size_to_train_on: 64\n",
            "Config param mla_naive_kvcache: True\n",
            "Config param mlp_activations: ['gelu', 'linear']\n",
            "Config param mlp_dim: 16384\n",
            "Config param mlpwi: remat\n",
            "Config param mlpwi_0: remat\n",
            "Config param mlpwi_1: remat\n",
            "Config param mlpwo: remat\n",
            "Config param model_call_mode: \n",
            "Config param model_fsdp_ag_once: False\n",
            "Config param model_name: gemma-2b\n",
            "Config param moe_mlp_dim: 7168\n",
            "Config param monitor_goodput: False\n",
            "Config param monitor_step_time_deviation: True\n",
            "Config param mscale: 1.0\n",
            "Config param mtp_eval_target_module: 0\n",
            "Config param mtp_loss_scaling_factor: 0.1\n",
            "Config param mtp_num_layers: 0\n",
            "Config param mu_dtype: bfloat16\n",
            "Config param multi_sampling: False\n",
            "Config param n_routing_groups: -1\n",
            "Config param nope_layer_interval: -1\n",
            "Config param normalization_layer_epsilon: 1e-06\n",
            "Config param normalize_embedding_logits: True\n",
            "Config param num_attention_heads_for_vit: 16\n",
            "Config param num_channels_for_vit: 3\n",
            "Config param num_decoder_layers: 18\n",
            "Config param num_epoch: 1\n",
            "Config param num_experts: 1\n",
            "Config param num_experts_per_tok: 1\n",
            "Config param num_hidden_layers_for_vit: 34\n",
            "Config param num_kv_heads: 1\n",
            "Config param num_layers_per_pipeline_stage: 1\n",
            "Config param num_pipeline_microbatches: -1\n",
            "Config param num_pipeline_repeats: -1\n",
            "Config param num_query_heads: 8\n",
            "Config param num_slices: 1\n",
            "Config param opt_type: adamw\n",
            "Config param optimize_mesh_for_tpu_v6e: False\n",
            "Config param optimizer_memory_host_offload: False\n",
            "Config param original_max_position_embeddings: 4096\n",
            "Config param out_proj: remat\n",
            "Config param override_model_config: False\n",
            "Config param packing: True\n",
            "Config param pagedattn_head_dim_alignment: 128\n",
            "Config param pagedattn_max_pages_per_group: 256\n",
            "Config param pagedattn_num_pages: 64\n",
            "Config param pagedattn_pages_per_compute_block: 4\n",
            "Config param pagedattn_tokens_per_page: 32\n",
            "Config param param_scan_axis: 1\n",
            "Config param parameter_memory_host_offload: False\n",
            "Config param patch_size_for_vit: 14\n",
            "Config param per_device_batch_size: 8.0\n",
            "Config param pipeline_delay_activation_forwarding: False\n",
            "Config param pipeline_fsdp_ag_once: False\n",
            "Config param pipeline_parallel_layers: -1\n",
            "Config param pixel_shuffle_ratio_for_vit: 0.5\n",
            "Config param prefill_cache_axis_order: 1,2,0,3\n",
            "Config param prefill_cache_dir: \n",
            "Config param prefill_chunk_size: 256\n",
            "Config param prefill_slice: v5e-16\n",
            "Config param prefix_caching_dram_byte: 100000000000\n",
            "Config param prefix_caching_hbm_byte: 10000000000\n",
            "Config param profile_cleanly: True\n",
            "Config param profile_periodically_period: -1\n",
            "Config param profiler: \n",
            "Config param profiler_steps: 5\n",
            "Config param projector_dropout_for_vit: 0.0\n",
            "Config param projector_input_dim_for_vit: 4096\n",
            "Config param projector_output_dim_for_vit: 4096\n",
            "Config param prometheus_port: 0\n",
            "Config param prompt: I love to\n",
            "Config param q_lora_rank: 0\n",
            "Config param qk_nope_head_dim: 128\n",
            "Config param qk_rope_head_dim: 64\n",
            "Config param qkv_proj: remat\n",
            "Config param quant_cfg_path: \n",
            "Config param quantization: \n",
            "Config param quantization_local_shard_count: 1\n",
            "Config param quantize_kvcache: False\n",
            "Config param query_proj: remat\n",
            "Config param ragged_block_size: 256\n",
            "Config param record_internal_nn_metrics: 0\n",
            "Config param remat_policy: full\n",
            "Config param remat_policy_for_vit: minimal\n",
            "Config param replicate_quant_scale: False\n",
            "Config param replicator_backup_interval_minutes: 0\n",
            "Config param report_heartbeat_metric_for_gcp_monitoring: False\n",
            "Config param report_performance_metric_for_gcp_monitoring: False\n",
            "Config param reshape_q: False\n",
            "Config param return_log_prob: False\n",
            "Config param reuse_example_batch: 0\n",
            "Config param rope_factor: 40\n",
            "Config param rope_max_timescale: 10000\n",
            "Config param rope_min_timescale: 1\n",
            "Config param rope_theta_for_vit: 10000\n",
            "Config param rope_type: default\n",
            "Config param rope_use_scale: True\n",
            "Config param routed_bias: False\n",
            "Config param routed_scaling_factor: 1.0\n",
            "Config param routed_score_func: \n",
            "Config param run_name: test-tunix-maxtext-llama3-8b\n",
            "Config param sa_block_kv: 512\n",
            "Config param sa_block_kv_compute: 512\n",
            "Config param sa_block_kv_dkv: 512\n",
            "Config param sa_block_kv_dkv_compute: 512\n",
            "Config param sa_block_kv_dq: 512\n",
            "Config param sa_block_q: 512\n",
            "Config param sa_block_q_dkv: 512\n",
            "Config param sa_block_q_dq: 512\n",
            "Config param sa_k_layout: HEAD_DIM_MINOR\n",
            "Config param sa_q_layout: HEAD_DIM_MINOR\n",
            "Config param sa_use_fused_bwd_kernel: False\n",
            "Config param sa_v_layout: HEAD_DIM_MINOR\n",
            "Config param save_config_to_gcs: False\n",
            "Config param save_quantized_params_path: \n",
            "Config param scan_layers: True\n",
            "Config param scan_layers_per_stage: False\n",
            "Config param scan_pipeline_iterations: True\n",
            "Config param set_remat_policy_on_layers_per_stage: False\n",
            "Config param set_remat_policy_on_pipeline_iterations: True\n",
            "Config param sft_train_on_completion_only: False\n",
            "Config param sharding_tolerance: 0.02\n",
            "Config param shardy: True\n",
            "Config param shared_experts: 1\n",
            "Config param skip_first_n_steps_for_profiler: 1\n",
            "Config param skip_jax_distributed_system: True\n",
            "Config param sliding_window_size: 0\n",
            "Config param source_checkpoint_layout: orbax\n",
            "Config param sparse_matmul: True\n",
            "Config param stack_prefill_result_cache: False\n",
            "Config param stack_trace_interval_seconds: 600\n",
            "Config param stack_trace_to_cloud: False\n",
            "Config param step_deviation_interval_seconds: 30\n",
            "Config param steps: 10\n",
            "Config param subslice_shape: \n",
            "Config param target_eval_loss: 0.0\n",
            "Config param temperature_tuning: False\n",
            "Config param tensorboard_dir: gs://dummy_output_dir/test-tunix-maxtext-llama3-8b/tensorboard/\n",
            "Config param tile_activation_dim: 1024\n",
            "Config param tile_batch_seq: 512\n",
            "Config param tile_size_for_vit: 336\n",
            "Config param tile_weight_dim: 1024\n",
            "Config param tokenize_eval_data: True\n",
            "Config param tokenize_train_data: True\n",
            "Config param tokenizer_path: ../../maxtext/assets/tokenizer.gemma\n",
            "Config param tokenizer_type: sentencepiece\n",
            "Config param topk_routing_group: -1\n",
            "Config param train_data_columns: ['text']\n",
            "Config param train_image_column: image\n",
            "Config param train_split: train\n",
            "Config param trainable_position_size: -1\n",
            "Config param upload_all_profiler_results: False\n",
            "Config param use_chat_template: False\n",
            "Config param use_chunked_prefill: False\n",
            "Config param use_dpo: False\n",
            "Config param use_iota_embed: False\n",
            "Config param use_multimodal: False\n",
            "Config param use_post_attn_norm: False\n",
            "Config param use_post_ffw_norm: False\n",
            "Config param use_qk_norm: False\n",
            "Config param use_ragged_attention: False\n",
            "Config param use_random_routing: False\n",
            "Config param use_replicator_service: False\n",
            "Config param use_sft: False\n",
            "Config param use_untrainable_positional_embedding: False\n",
            "Config param use_vertex_tensorboard: False\n",
            "Config param using_pipeline_parallelism: False\n",
            "Config param v_head_dim: 128\n",
            "Config param value_proj: remat\n",
            "Config param vertex_tensorboard_project: \n",
            "Config param vertex_tensorboard_region: \n",
            "Config param vision_output_dim_for_vit: 4096\n",
            "Config param vocab_size: 256128\n",
            "Config param warmup_steps_fraction: 0.1\n",
            "Config param weight_dtype: bfloat16\n"
          ]
        }
      ],
      "source": [
        "# from MaxText.integration.tunix.tunix_adaptor import TunixMaxTextLlama\n",
        "from flax import linen as nn\n",
        "import orbax.checkpoint as ocp\n",
        "\n",
        "  #python3 -m MaxText.train MaxText/configs/base.yml base_output_directory=${BASE_OUTPUT_DIRECTORY} dataset_path=${DATASET_PATH} tokenizer_path=assets/tokenizer.gemma load_parameters_path=${CONVERTED_CHECKPOINT} per_device_batch_size=1 run_name=${FINETUNE_RUN_NAME} max_target_length=8192 steps=10 async_checkpointing=false model_name=gemma-2b checkpoint_period=5\n",
        "\n",
        "#TODO: @mazumdera: change this to use Gemma2-2b-it\n",
        "config = pyconfig.initialize(\n",
        "    [\"\", \"../../maxtext/MaxText/configs/base.yml\"], #TODO: @mazumdera: why decode.py?\n",
        "    base_output_directory=\"gs://dummy_output_dir\",  # This is not used in Tunix.\n",
        "    run_name=\"test-tunix-maxtext-llama3-8b\",\n",
        "    # dataset_path=we use Tunix's dataset\n",
        "    # load_parameters_path=\"gs://maxtext-gemma/2b/\", #TODO: @mazumdera: change this to use checkpoint\n",
        "    # tokenizer_type=\"tiktoken\",\n",
        "    # tokenizer_path=\"assets/tokenizer_llama3.tiktoken\",\n",
        "    tokenizer_path=\"../../maxtext/assets/tokenizer.gemma\",\n",
        "    per_device_batch_size=8,\n",
        "    max_target_length=8192,\n",
        "    steps=10,\n",
        "    async_checkpointing=\"false\",\n",
        "    # model_name=\"llama3.1-8b\",\n",
        "    model_name=\"gemma-2b\",\n",
        "    checkpoint_period=5,\n",
        "    skip_jax_distributed_system=\"true\",\n",
        "    weight_dtype=\"bfloat16\",\n",
        "    attention=\"dot_product\"\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_params_from_path(\n",
        "    load_parameters_from_path, abstract_unboxed_params, checkpoint_storage_concurrent_gb, use_ocdbt=True, use_zarr3=True\n",
        "):\n",
        "  \"\"\"Load decode params from checkpoint at specified path.\"\"\"\n",
        "  assert load_parameters_from_path, \"load_parameters_from_path is not defined.\"\n",
        "  max_logging.log(f\"restoring params from {load_parameters_from_path}\")\n",
        "\n",
        "  # *_concurrent_gb should be set for large models, the default is 96.\n",
        "  max_logging.log(f\"Creating checkpoint manager with ocdbt={use_ocdbt} and zarr3={use_zarr3}\")\n",
        "  ckptr = ocp.Checkpointer(\n",
        "      ocp.PyTreeCheckpointHandler(\n",
        "          restore_concurrent_gb=checkpoint_storage_concurrent_gb,\n",
        "          save_concurrent_gb=checkpoint_storage_concurrent_gb,\n",
        "          use_ocdbt=use_ocdbt,\n",
        "          use_zarr3=use_zarr3,\n",
        "      )\n",
        "  )\n",
        "  max_logging.log(\"Checkpoint manager created!\")\n",
        "  # This is a memory optimization. We don't want to restore the entire checkpoint - only the params.\n",
        "  # Rather than pass the entire abstract state, which could unnecessarily restore opt_state and such and waste\n",
        "  # memory, we instead specify here that we are just restoring the params field of the checkpoint\n",
        "  # (which itself may be a dictionary containing a key named 'params').\n",
        "  restore_args = ocp.checkpoint_utils.construct_restore_args(abstract_unboxed_params)\n",
        "  restored = ckptr.restore(\n",
        "      epath.Path(load_parameters_from_path),\n",
        "      item={\"params\": abstract_unboxed_params},\n",
        "      transforms={},\n",
        "      restore_args={\"params\": restore_args},\n",
        "  )\n",
        "  print(f\"{restored=}\")\n",
        "  return restored[\"params\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num_devices: 8, shape (1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
            "The abstract NNX state (all leaves are abstract arrays):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<script> (()=>{ if (customElements.get('treescope-container') === undefined) { class TreescopeContainer extends HTMLElement { constructor() { super(); this.attachShadow({mode: \"open\"}); this.defns = {}; this.state = {}; } } customElements.define(\"treescope-container\", TreescopeContainer); } if (customElements.get('treescope-run-here') === undefined) { class RunHere extends HTMLElement { constructor() { super() } connectedCallback() { const run = child => { const fn = new Function(child.textContent); child.textContent = \"\"; fn.call(this); this.remove(); }; const child = this.querySelector(\"script\"); if (child) { run(child); } else { new MutationObserver(()=>{ run(this.querySelector(\"script\")); }).observe(this, {childList: true}); } } } customElements.define(\"treescope-run-here\", RunHere); } })(); </script> <treescope-container class=\"treescope_out_b0b2cef57f70468c97f57049ec684726\" style=\"display:block\"></treescope-container> <treescope-run-here><script type=\"application/octet-stream\"> const root = ( Array.from(document.getElementsByClassName( \"treescope_out_b0b2cef57f70468c97f57049ec684726\")) .filter((elt) => !elt.dataset.setup) )[0]; root.dataset.setup = 1; const msg = document.createElement(\"span\"); msg.style = \"color: #cccccc; font-family: monospace;\"; msg.textContent = \"(Loading...)\"; root.state.loadingMsg = msg; root.shadowRoot.appendChild(msg); root.state.chain = new Promise((resolve, reject) => { const observer = new IntersectionObserver((entries) => { for (const entry of entries) { if (entry.isIntersecting) { resolve(); observer.disconnect(); return; } } }, {rootMargin: \"1000px\"}); window.setTimeout(() => { observer.observe(root); }, 0); }); root.state.deferring = false; const _insertNode = (node) => { for (let oldScript of node.querySelectorAll(\"script\")) { let newScript = document.createElement(\"script\"); newScript.type = oldScript.type; newScript.textContent = oldScript.textContent; oldScript.parentNode.replaceChild(newScript, oldScript); } if (root.state.loadingMsg) { root.state.loadingMsg.remove(); root.state.loadingMsg = null; } root.shadowRoot.appendChild(node); }; root.defns.insertContent = ((contentNode, compressed) => { if (compressed) { root.state.deferring = true; } if (root.state.deferring) { root.state.chain = (async () => { await root.state.chain; if (compressed) { const encoded = contentNode.textContent; const blob = new Blob([ Uint8Array.from(atob(encoded), (m) => m.codePointAt(0)) ]); const reader = blob.stream().pipeThrough( new DecompressionStream(\"deflate\") ).pipeThrough( new TextDecoderStream(\"utf-8\") ).getReader(); const parts = []; while (true) { const step = await reader.read(); if (step.done) { break; } parts.push(step.value); } const tpl = document.createElement('template'); tpl.innerHTML = parts.join(\"\"); _insertNode(tpl.content); } else { _insertNode(contentNode.content); } })(); } else { _insertNode(contentNode.content); } }); </script></treescope-run-here><div style=\"display:none\"> <script type=\"application/octet-stream\" >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</script> <treescope-run-here><script type=\"application/octet-stream\"> const root = ( Array.from(document.getElementsByClassName( \"treescope_out_b0b2cef57f70468c97f57049ec684726\")) .filter((elt) => !elt.dataset['step0']) )[0]; root.dataset['step0'] = 1; root.defns.insertContent( this.parentNode.querySelector('script[type=\"application/octet-stream\"]'), true ); this.parentNode.remove(); </script></treescope-run-here> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "restoring params from gs://maxtext-gemma/2b/2025-08-05-04-37/0/items\n",
            "Creating checkpoint manager with ocdbt=True and zarr3=True\n",
            "Checkpoint manager created!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:The transformations API will eventually be replaced by an upgraded design. The current API will not be removed until this point, but it will no longer be actively worked on.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "restored={'params': State({\n",
            "  'decoder': {\n",
            "    'decoder_norm': {\n",
            "      'scale': Param( # 2,048 (4.1 KB)\n",
            "        value=ShapeDtypeStruct(shape=(2048,), dtype=bfloat16),\n",
            "        mesh=None,\n",
            "        sharding=('norm',),\n",
            "        sharding_rules=None,\n",
            "        linen_meta_type=LogicallyPartitioned\n",
            "      )\n",
            "    },\n",
            "    'layers': {\n",
            "      'mlp': {\n",
            "        'wi_0': {\n",
            "          'kernel': Param( # 603,979,776 (1.2 GB)\n",
            "            value=ShapeDtypeStruct(shape=(2048, 18, 16384), dtype=bfloat16),\n",
            "            mesh=None,\n",
            "            sharding=('embed', 'layers', 'mlp'),\n",
            "            sharding_rules=None,\n",
            "            linen_meta_type=LogicallyPartitioned\n",
            "          )\n",
            "        },\n",
            "        'wi_1': {\n",
            "          'kernel': Param( # 603,979,776 (1.2 GB)\n",
            "            value=ShapeDtypeStruct(shape=(2048, 18, 16384), dtype=bfloat16),\n",
            "            mesh=None,\n",
            "            sharding=('embed', 'layers', 'mlp'),\n",
            "            sharding_rules=None,\n",
            "            linen_meta_type=LogicallyPartitioned\n",
            "          )\n",
            "        },\n",
            "        'wo': {\n",
            "          'kernel': Param( # 603,979,776 (1.2 GB)\n",
            "            value=ShapeDtypeStruct(shape=(16384, 18, 2048), dtype=bfloat16),\n",
            "            mesh=None,\n",
            "            sharding=('mlp', 'layers', 'embed'),\n",
            "            sharding_rules=None,\n",
            "            linen_meta_type=LogicallyPartitioned\n",
            "          )\n",
            "        }\n",
            "      },\n",
            "      'pre_ffw_norm': {\n",
            "        'scale': Param( # 36,864 (73.7 KB)\n",
            "          value=ShapeDtypeStruct(shape=(2048, 18), dtype=bfloat16),\n",
            "          mesh=None,\n",
            "          sharding=('norm', 'layers'),\n",
            "          sharding_rules=None,\n",
            "          linen_meta_type=LogicallyPartitioned\n",
            "        )\n",
            "      },\n",
            "      'pre_self_attention_norm': {\n",
            "        'scale': Param( # 36,864 (73.7 KB)\n",
            "          value=ShapeDtypeStruct(shape=(2048, 18), dtype=bfloat16),\n",
            "          mesh=None,\n",
            "          sharding=('norm', 'layers'),\n",
            "          sharding_rules=None,\n",
            "          linen_meta_type=LogicallyPartitioned\n",
            "        )\n",
            "      },\n",
            "      'self_attention': {\n",
            "        'key': {\n",
            "          'kernel': Param( # 9,437,184 (18.9 MB)\n",
            "            value=ShapeDtypeStruct(shape=(2048, 18, 1, 256), dtype=bfloat16),\n",
            "            mesh=None,\n",
            "            sharding=('embed', 'layers', 'kv_heads', 'kv_head_dim'),\n",
            "            sharding_rules=None,\n",
            "            linen_meta_type=LogicallyPartitioned\n",
            "          )\n",
            "        },\n",
            "        'out': {\n",
            "          'kernel': Param( # 75,497,472 (151.0 MB)\n",
            "            value=ShapeDtypeStruct(shape=(8, 18, 256, 2048), dtype=bfloat16),\n",
            "            mesh=None,\n",
            "            sharding=('heads', 'layers', 'kv', 'embed'),\n",
            "            sharding_rules=None,\n",
            "            linen_meta_type=LogicallyPartitioned\n",
            "          )\n",
            "        },\n",
            "        'query': {\n",
            "          'kernel': Param( # 75,497,472 (151.0 MB)\n",
            "            value=ShapeDtypeStruct(shape=(2048, 18, 8, 256), dtype=bfloat16),\n",
            "            mesh=None,\n",
            "            sharding=('embed', 'layers', 'q_heads', 'kv'),\n",
            "            sharding_rules=None,\n",
            "            linen_meta_type=LogicallyPartitioned\n",
            "          )\n",
            "        },\n",
            "        'value': {\n",
            "          'kernel': Param( # 9,437,184 (18.9 MB)\n",
            "            value=ShapeDtypeStruct(shape=(2048, 18, 1, 256), dtype=bfloat16),\n",
            "            mesh=None,\n",
            "            sharding=('embed', 'layers', 'kv_heads', 'kv_head_dim'),\n",
            "            sharding_rules=None,\n",
            "            linen_meta_type=LogicallyPartitioned\n",
            "          )\n",
            "        }\n",
            "      }\n",
            "    },\n",
            "    'to_nnx__rngs': {\n",
            "      'dropout': {\n",
            "        'count': RngCount( # 1 (4 B)\n",
            "          value=ShapeDtypeStruct(shape=(), dtype=uint32),\n",
            "          tag='dropout'\n",
            "        ),\n",
            "        'key': RngKey( # 1 (8 B)\n",
            "          value=ShapeDtypeStruct(shape=(), dtype=key<fry>),\n",
            "          tag='dropout'\n",
            "        )\n",
            "      },\n",
            "      'params': {\n",
            "        'count': RngCount( # 1 (4 B)\n",
            "          value=ShapeDtypeStruct(shape=(), dtype=uint32),\n",
            "          tag='params'\n",
            "        ),\n",
            "        'key': RngKey( # 1 (8 B)\n",
            "          value=ShapeDtypeStruct(shape=(), dtype=key<fry>),\n",
            "          tag='params'\n",
            "        )\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  'token_embedder': {\n",
            "    'embedding': Param( # 524,550,144 (1.0 GB)\n",
            "      value=ShapeDtypeStruct(shape=(256128, 2048), dtype=bfloat16),\n",
            "      sharding=('vocab', 'embed')\n",
            "    )\n",
            "  }\n",
            "})}\n",
            "state_restored=State({\n",
            "  'decoder': {\n",
            "    'decoder_norm': {\n",
            "      'scale': Param( # 2,048 (4.1 KB)\n",
            "        value=ShapeDtypeStruct(shape=(2048,), dtype=bfloat16),\n",
            "        mesh=None,\n",
            "        sharding=('norm',),\n",
            "        sharding_rules=None,\n",
            "        linen_meta_type=LogicallyPartitioned\n",
            "      )\n",
            "    },\n",
            "    'layers': {\n",
            "      'mlp': {\n",
            "        'wi_0': {\n",
            "          'kernel': Param( # 603,979,776 (1.2 GB)\n",
            "            value=ShapeDtypeStruct(shape=(2048, 18, 16384), dtype=bfloat16),\n",
            "            mesh=None,\n",
            "            sharding=('embed', 'layers', 'mlp'),\n",
            "            sharding_rules=None,\n",
            "            linen_meta_type=LogicallyPartitioned\n",
            "          )\n",
            "        },\n",
            "        'wi_1': {\n",
            "          'kernel': Param( # 603,979,776 (1.2 GB)\n",
            "            value=ShapeDtypeStruct(shape=(2048, 18, 16384), dtype=bfloat16),\n",
            "            mesh=None,\n",
            "            sharding=('embed', 'layers', 'mlp'),\n",
            "            sharding_rules=None,\n",
            "            linen_meta_type=LogicallyPartitioned\n",
            "          )\n",
            "        },\n",
            "        'wo': {\n",
            "          'kernel': Param( # 603,979,776 (1.2 GB)\n",
            "            value=ShapeDtypeStruct(shape=(16384, 18, 2048), dtype=bfloat16),\n",
            "            mesh=None,\n",
            "            sharding=('mlp', 'layers', 'embed'),\n",
            "            sharding_rules=None,\n",
            "            linen_meta_type=LogicallyPartitioned\n",
            "          )\n",
            "        }\n",
            "      },\n",
            "      'pre_ffw_norm': {\n",
            "        'scale': Param( # 36,864 (73.7 KB)\n",
            "          value=ShapeDtypeStruct(shape=(2048, 18), dtype=bfloat16),\n",
            "          mesh=None,\n",
            "          sharding=('norm', 'layers'),\n",
            "          sharding_rules=None,\n",
            "          linen_meta_type=LogicallyPartitioned\n",
            "        )\n",
            "      },\n",
            "      'pre_self_attention_norm': {\n",
            "        'scale': Param( # 36,864 (73.7 KB)\n",
            "          value=ShapeDtypeStruct(shape=(2048, 18), dtype=bfloat16),\n",
            "          mesh=None,\n",
            "          sharding=('norm', 'layers'),\n",
            "          sharding_rules=None,\n",
            "          linen_meta_type=LogicallyPartitioned\n",
            "        )\n",
            "      },\n",
            "      'self_attention': {\n",
            "        'key': {\n",
            "          'kernel': Param( # 9,437,184 (18.9 MB)\n",
            "            value=ShapeDtypeStruct(shape=(2048, 18, 1, 256), dtype=bfloat16),\n",
            "            mesh=None,\n",
            "            sharding=('embed', 'layers', 'kv_heads', 'kv_head_dim'),\n",
            "            sharding_rules=None,\n",
            "            linen_meta_type=LogicallyPartitioned\n",
            "          )\n",
            "        },\n",
            "        'out': {\n",
            "          'kernel': Param( # 75,497,472 (151.0 MB)\n",
            "            value=ShapeDtypeStruct(shape=(8, 18, 256, 2048), dtype=bfloat16),\n",
            "            mesh=None,\n",
            "            sharding=('heads', 'layers', 'kv', 'embed'),\n",
            "            sharding_rules=None,\n",
            "            linen_meta_type=LogicallyPartitioned\n",
            "          )\n",
            "        },\n",
            "        'query': {\n",
            "          'kernel': Param( # 75,497,472 (151.0 MB)\n",
            "            value=ShapeDtypeStruct(shape=(2048, 18, 8, 256), dtype=bfloat16),\n",
            "            mesh=None,\n",
            "            sharding=('embed', 'layers', 'q_heads', 'kv'),\n",
            "            sharding_rules=None,\n",
            "            linen_meta_type=LogicallyPartitioned\n",
            "          )\n",
            "        },\n",
            "        'value': {\n",
            "          'kernel': Param( # 9,437,184 (18.9 MB)\n",
            "            value=ShapeDtypeStruct(shape=(2048, 18, 1, 256), dtype=bfloat16),\n",
            "            mesh=None,\n",
            "            sharding=('embed', 'layers', 'kv_heads', 'kv_head_dim'),\n",
            "            sharding_rules=None,\n",
            "            linen_meta_type=LogicallyPartitioned\n",
            "          )\n",
            "        }\n",
            "      }\n",
            "    },\n",
            "    'to_nnx__rngs': {\n",
            "      'dropout': {\n",
            "        'count': RngCount( # 1 (4 B)\n",
            "          value=ShapeDtypeStruct(shape=(), dtype=uint32),\n",
            "          tag='dropout'\n",
            "        ),\n",
            "        'key': RngKey( # 1 (8 B)\n",
            "          value=ShapeDtypeStruct(shape=(), dtype=key<fry>),\n",
            "          tag='dropout'\n",
            "        )\n",
            "      },\n",
            "      'params': {\n",
            "        'count': RngCount( # 1 (4 B)\n",
            "          value=ShapeDtypeStruct(shape=(), dtype=uint32),\n",
            "          tag='params'\n",
            "        ),\n",
            "        'key': RngKey( # 1 (8 B)\n",
            "          value=ShapeDtypeStruct(shape=(), dtype=key<fry>),\n",
            "          tag='params'\n",
            "        )\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  'token_embedder': {\n",
            "    'embedding': Param( # 524,550,144 (1.0 GB)\n",
            "      value=ShapeDtypeStruct(shape=(256128, 2048), dtype=bfloat16),\n",
            "      sharding=('vocab', 'embed')\n",
            "    )\n",
            "  }\n",
            "})\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Param( # 603,979,776 (1.2 GB)\n",
              "  value=ShapeDtypeStruct(shape=(2048, 18, 16384), dtype=bfloat16),\n",
              "  mesh=None,\n",
              "  sharding=('embed', 'layers', 'mlp'),\n",
              "  sharding_rules=None,\n",
              "  linen_meta_type=LogicallyPartitioned\n",
              ")"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "from MaxText import max_logging\n",
        "from etils import epath\n",
        "\n",
        "def create_model(config):\n",
        "    return mt.from_pretrained(config, rngs=nnx.Rngs(params=0, dropout=1))\n",
        "\n",
        "abstract_model = nnx.eval_shape(create_model, config=config)\n",
        "graphdef, abstract_state = nnx.split(abstract_model)\n",
        "print('The abstract NNX state (all leaves are abstract arrays):')\n",
        "nnx.display(abstract_state)\n",
        "  \n",
        "# state_restored = mt.checkpointing.load_params_from_path(\n",
        "#     load_parameters_from_path=\"gs://maxtext-gemma/2b/2025-08-05-04-37/0/items\",\n",
        "#     abstract_unboxed_params=abstract_state,\n",
        "#     checkpoint_storage_concurrent_gb=None,\n",
        "# )\n",
        "# state_restored = checkpointer\n",
        "\n",
        "state_restored = load_params_from_path(\n",
        "    load_parameters_from_path=\"gs://maxtext-gemma/2b/2025-08-05-04-37/0/items\",\n",
        "    abstract_unboxed_params=abstract_state,\n",
        "    checkpoint_storage_concurrent_gb=None,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"{state_restored=}\")\n",
        "model = nnx.merge(graphdef, state_restored)\n",
        "\n",
        "model.decoder.layers[\"mlp\"][\"wi_0\"][\"kernel\"]\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from MaxText.integration.tunix.tunix_adaptor import TunixMaxTextLlama\n",
        "from flax import linen as nn\n",
        "\n",
        "def get_ref_maxtext_model():\n",
        "\n",
        "  #python3 -m MaxText.train MaxText/configs/base.yml base_output_directory=${BASE_OUTPUT_DIRECTORY} dataset_path=${DATASET_PATH} tokenizer_path=assets/tokenizer.gemma load_parameters_path=${CONVERTED_CHECKPOINT} per_device_batch_size=1 run_name=${FINETUNE_RUN_NAME} max_target_length=8192 steps=10 async_checkpointing=false model_name=gemma-2b checkpoint_period=5\n",
        "\n",
        "  #TODO: @mazumdera: change this to use Gemma2-2b-it\n",
        "  config = pyconfig.initialize(\n",
        "      [\"\", \"../../maxtext/MaxText/configs/base.yml\"], #TODO: @mazumdera: why decode.py?\n",
        "      base_output_directory=\"gs://dummy_output_dir\",  # This is not used in Tunix.\n",
        "      run_name=\"test-tunix-maxtext-llama3-8b\",\n",
        "      # dataset_path=we use Tunix's dataset\n",
        "      # load_parameters_path=\"gs://maxtext-gemma/2b/\", #TODO: @mazumdera: change this to use checkpoint\n",
        "      # tokenizer_type=\"tiktoken\",\n",
        "      # tokenizer_path=\"assets/tokenizer_llama3.tiktoken\",\n",
        "      tokenizer_path=\"../../maxtext/assets/tokenizer.gemma\",\n",
        "      per_device_batch_size=8,\n",
        "      max_target_length=8192,\n",
        "      steps=10,\n",
        "      async_checkpointing=\"false\",\n",
        "      # model_name=\"llama3.1-8b\",\n",
        "      model_name=\"gemma-2b\",\n",
        "      checkpoint_period=5,\n",
        "      skip_jax_distributed_system=\"true\",\n",
        "      weight_dtype=\"bfloat16\",\n",
        "      attention=\"dot_product\"\n",
        "\n",
        "  )\n",
        "  \n",
        "  def create_model(config):\n",
        "    return mt.from_pretrained(config, rngs=nnx.Rngs(params=0, dropout=1))\n",
        "\n",
        "  model = nnx.eval_shape(create_model, config=config)\n",
        "\n",
        "  abstract_model = nnx.eval_shape(create_model, config=config)\n",
        "  graphdef, abstract_state = nnx.split(abstract_model)\n",
        "  print('The abstract NNX state (all leaves are abstract arrays):')\n",
        "  nnx.display(abstract_state)\n",
        "  checkpoint = mt.checkpointing.load_params_from_path(\n",
        "      load_parameters_from_path=\"gs://maxtext-gemma/2b/2025-08-05-04-37/0/items\",\n",
        "      abstract_unboxed_params=None,\n",
        "      checkpoint_storage_concurrent_gb=None,\n",
        "  )\n",
        "  print(\"{checkpoint=}\")\n",
        "  # checkpoint = {}\n",
        "\n",
        "  @nnx.jit\n",
        "  def partial_init(checkpoint, config):\n",
        "    model = create_model(config)\n",
        "    nnx.update(model, checkpoint)\n",
        "    # shard model\n",
        "    state = nnx.state(model)\n",
        "    specs = nnx.get_partition_spec(state)\n",
        "    state = jax.lax.with_sharding_constraint(state, specs)\n",
        "    nnx.update(model, state)\n",
        "    return model\n",
        "\n",
        "  with jax.sharding.use_mesh(model.mesh), nn.logical_axis_rules(config.logical_axis_rules):\n",
        "    model = partial_init(checkpoint, config)\n",
        "  print(model)\n",
        "\n",
        "  \n",
        "  tunix_model = TunixMaxTextLlama(\n",
        "        base_model=model,\n",
        "        use_attention_mask=False,  # trust Tunix loss masking\n",
        "    )\n",
        "  mesh  = tunix_model.base.mesh\n",
        "  \n",
        "  #TODO: @mazumdera: change this to use llama3.1-8b\n",
        "  # model_config = None\n",
        "  # We can continue to use Tunix's model_config\n",
        "  model_config = gemma_lib.TransformerConfig.gemma2_2b()\n",
        "\n",
        "  # Add these lines to properly get the graph definition and state\n",
        "  graphdef, state = nnx.split(tunix_model)\n",
        "  tunix_model = nnx.merge(graphdef, state)  # Recreate model in proper NNX format\n",
        "    \n",
        "  \n",
        "  return tunix_model, mesh, model_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCPPEEi3T3Uv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updating keys from env and command line: ['run_name', 'model_name', 'async_checkpointing', 'checkpoint_period', 'weight_dtype', 'attention', 'base_output_directory', 'tokenizer_path', 'per_device_batch_size', 'steps', 'skip_jax_distributed_system', 'max_target_length']\n",
            "Running Model: gemma-2b\n",
            "Updating following parameters in config\n",
            "\n",
            "base_emb_dim: 2048\n",
            "base_num_query_heads: 8\n",
            "base_num_kv_heads: 1\n",
            "base_mlp_dim: 16384\n",
            "base_num_decoder_layers: 18\n",
            "head_dim: 256\n",
            "mlp_activations: ['gelu', 'linear']\n",
            "vocab_size: 256128\n",
            "decoder_block: gemma\n",
            "normalization_layer_epsilon: 1e-06\n",
            "logits_via_embedding: True\n",
            "Updating keys from model: ['base_emb_dim', 'base_num_query_heads', 'base_num_kv_heads', 'base_mlp_dim', 'base_num_decoder_layers', 'head_dim', 'mlp_activations', 'vocab_size', 'decoder_block', 'normalization_layer_epsilon', 'logits_via_embedding']\n",
            "Skipping jax distributed system due to skip_jax_distributed_system=True flag.\n",
            "Not using emergency checkpoint, ignoring local_checkpoint_directory, local_checkpoint_period, use_replicator_service and replicator_backup_interval_minutes\n",
            "dataset_type set to tfds, will use keys['dataset_path']='' and keys['dataset_name']='c4/en:3.0.1'\n",
            "Config param activations_in_float32: False\n",
            "Config param adam_b1: 0.9\n",
            "Config param adam_b2: 0.95\n",
            "Config param adam_eps: 1e-08\n",
            "Config param adam_eps_root: 0.0\n",
            "Config param adam_weight_decay: 0.1\n",
            "Config param add_bos: True\n",
            "Config param add_eos: True\n",
            "Config param allow_split_physical_axes: False\n",
            "Config param ar_cache_axis_order: 1,2,0,3\n",
            "Config param async_checkpointing: False\n",
            "Config param attention: dot_product\n",
            "Config param attention_type: global\n",
            "Config param attn_logits_soft_cap: None\n",
            "Config param autoregressive_decode_assert: \n",
            "Config param base_emb_dim: 2048\n",
            "Config param base_mlp_dim: 16384\n",
            "Config param base_moe_mlp_dim: 7168\n",
            "Config param base_num_decoder_layers: 18\n",
            "Config param base_num_kv_heads: 1\n",
            "Config param base_num_query_heads: 8\n",
            "Config param base_output_directory: gs://dummy_output_dir\n",
            "Config param beta_fast: 32\n",
            "Config param beta_slow: 1\n",
            "Config param capacity_factor: -1.0\n",
            "Config param cast_logits_to_fp32: True\n",
            "Config param checkpoint_conversion_fn: None\n",
            "Config param checkpoint_dir: gs://dummy_output_dir/test-tunix-maxtext-llama3-8b/checkpoints/\n",
            "Config param checkpoint_is_quantized: False\n",
            "Config param checkpoint_period: 5\n",
            "Config param checkpoint_storage_concurrent_gb: 96\n",
            "Config param checkpoint_storage_target_data_file_size_bytes: 2147483648\n",
            "Config param checkpoint_storage_use_ocdbt: True\n",
            "Config param checkpoint_storage_use_zarr3: True\n",
            "Config param chunk_attn_window_size: 0\n",
            "Config param collect_stack_trace: False\n",
            "Config param colocated_python_data_input: False\n",
            "Config param compile_topology: \n",
            "Config param compile_topology_num_slices: -1\n",
            "Config param compiled_trainstep_file: \n",
            "Config param compute_axis_order: 0,1,2,3\n",
            "Config param constant_bound_config: []\n",
            "Config param context: remat\n",
            "Config param context_parallel_load_balance: True\n",
            "Config param cosine_learning_rate_final_fraction: 0.1\n",
            "Config param custom_mesh: \n",
            "Config param data_sharding: (('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'),)\n",
            "Config param data_shuffle_seed: 0\n",
            "Config param dataset_name: c4/en:3.0.1\n",
            "Config param dataset_path: \n",
            "Config param dataset_type: tfds\n",
            "Config param dcn_autoregressive_parallelism: 1\n",
            "Config param dcn_context_autoregressive_parallelism: 1\n",
            "Config param dcn_context_parallelism: 1\n",
            "Config param dcn_data_parallelism: -1\n",
            "Config param dcn_expert_parallelism: 1\n",
            "Config param dcn_fsdp_parallelism: 1\n",
            "Config param dcn_fsdp_transpose_parallelism: 1\n",
            "Config param dcn_parallelism: [-1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Config param dcn_pipeline_parallelism: 1\n",
            "Config param dcn_sequence_parallelism: 1\n",
            "Config param dcn_tensor_parallelism: 1\n",
            "Config param dcn_tensor_sequence_parallelism: 1\n",
            "Config param dcn_tensor_transpose_parallelism: 1\n",
            "Config param decode_sampling_nucleus_p: -1\n",
            "Config param decode_sampling_strategy: greedy\n",
            "Config param decode_sampling_temperature: 1.0\n",
            "Config param decode_sampling_top_k: 0\n",
            "Config param decoder_block: DecoderBlockType.GEMMA\n",
            "Config param decoder_layer_input: device\n",
            "Config param dpo_beta: 0.1\n",
            "Config param dpo_label_smoothing: 0.0\n",
            "Config param dropout_rate: 0.0\n",
            "Config param dtype: bfloat16\n",
            "Config param dtype_mm: float32\n",
            "Config param dump_hlo: False\n",
            "Config param dump_hlo_delete_local_after: True\n",
            "Config param dump_hlo_gcs_dir: \n",
            "Config param dump_hlo_local_dir: /tmp/xla_dump/\n",
            "Config param dump_hlo_module_name: jit_train_step\n",
            "Config param dump_hlo_upload_all: False\n",
            "Config param dump_hlo_xla_flags: \n",
            "Config param dump_step: -1\n",
            "Config param emb_dim: 2048\n",
            "Config param enable_checkpoint_cloud_logger: False\n",
            "Config param enable_checkpointing: True\n",
            "Config param enable_data_shuffling: True\n",
            "Config param enable_dropout: True\n",
            "Config param enable_emergency_checkpoint: False\n",
            "Config param enable_gcp_goodput_metrics: True\n",
            "Config param enable_gcp_step_deviation_metrics: True\n",
            "Config param enable_goodput_recording: False\n",
            "Config param enable_jax_profiler: False\n",
            "Config param enable_llm_inference_pool: False\n",
            "Config param enable_model_warmup: False\n",
            "Config param enable_orbax_v1: False\n",
            "Config param enable_padding_causal_mask: True\n",
            "Config param enable_pathways_goodput: False\n",
            "Config param enable_prefix_caching: False\n",
            "Config param enable_single_controller: False\n",
            "Config param enable_single_replica_ckpt_restoring: False\n",
            "Config param enable_tensorboard: True\n",
            "Config param eval_data_columns: ['text']\n",
            "Config param eval_dataset_name: c4/en:3.0.1\n",
            "Config param eval_image_column: image\n",
            "Config param eval_interval: -1\n",
            "Config param eval_per_device_batch_size: 8.0\n",
            "Config param eval_split: validation\n",
            "Config param eval_steps: -1\n",
            "Config param expansion_factor_real_data: -1\n",
            "Config param final_logits_soft_cap: None\n",
            "Config param first_num_dense_layers: 0\n",
            "Config param float32_logits: False\n",
            "Config param float32_qk_product: False\n",
            "Config param force_unroll: False\n",
            "Config param freeze_vision_encoder_params: True\n",
            "Config param fused_mlp: False\n",
            "Config param fused_qkv: False\n",
            "Config param gcs_metrics: False\n",
            "Config param generate_slice: v5e-16\n",
            "Config param global_batch_size_to_eval_on: 64\n",
            "Config param global_batch_size_to_load: 64\n",
            "Config param global_batch_size_to_load_eval: 64\n",
            "Config param global_batch_size_to_train_on: 64\n",
            "Config param global_parameter_scale: 1\n",
            "Config param goodput_upload_interval_seconds: 30\n",
            "Config param gradient_accumulation_steps: 1\n",
            "Config param gradient_clipping_threshold: 1.0\n",
            "Config param grain_eval_files: \n",
            "Config param grain_file_type: arrayrecord\n",
            "Config param grain_train_files: \n",
            "Config param grain_worker_count: 1\n",
            "Config param grain_worker_count_eval: 1\n",
            "Config param hardware: tpu\n",
            "Config param head_dim: 256\n",
            "Config param heartbeat_reporting_interval_in_seconds: 5\n",
            "Config param hf_data_dir: \n",
            "Config param hf_eval_files: \n",
            "Config param hf_eval_split: \n",
            "Config param hf_path: \n",
            "Config param hf_train_files: \n",
            "Config param hidden_size_for_vit: 1408\n",
            "Config param ici_autoregressive_parallelism: 1\n",
            "Config param ici_context_autoregressive_parallelism: 1\n",
            "Config param ici_context_parallelism: 1\n",
            "Config param ici_data_parallelism: 1\n",
            "Config param ici_expert_parallelism: 1\n",
            "Config param ici_fsdp_parallelism: -1\n",
            "Config param ici_fsdp_transpose_parallelism: 1\n",
            "Config param ici_parallelism: [1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Config param ici_pipeline_parallelism: 1\n",
            "Config param ici_sequence_parallelism: 1\n",
            "Config param ici_tensor_parallelism: 1\n",
            "Config param ici_tensor_sequence_parallelism: 1\n",
            "Config param ici_tensor_transpose_parallelism: 1\n",
            "Config param image_path: \n",
            "Config param image_placeholder: <|image|>\n",
            "Config param image_size_for_vit: 896\n",
            "Config param inference_benchmark_test: False\n",
            "Config param inference_metadata_file: \n",
            "Config param inference_microbenchmark_log_file_path: \n",
            "Config param inference_microbenchmark_loop_iters: 10\n",
            "Config param inference_microbenchmark_num_samples: [1, 2, 3, 4, 5]\n",
            "Config param inference_microbenchmark_prefill_lengths: 64,128,256,512,1024\n",
            "Config param inference_microbenchmark_stages: prefill,generate\n",
            "Config param inference_server: MaxtextInterleavedServer\n",
            "Config param inhomogeneous_layer_cycle_interval: 1\n",
            "Config param init_weights_seed: 0\n",
            "Config param input_data_sharding_logical_axes: ['activation_embed_and_logits_batch', 'activation_norm_length']\n",
            "Config param interleave_moe_layer_step: 1\n",
            "Config param intermediate_size_for_vit: 5632\n",
            "Config param jax_cache_dir: ~/jax_cache\n",
            "Config param jax_debug_log_modules: \n",
            "Config param jax_distributed_initialization_timeout: 300\n",
            "Config param jax_profiler_port: 9999\n",
            "Config param key_proj: remat\n",
            "Config param kv_lora_rank: 512\n",
            "Config param kv_quant_axis: heads_and_dkv\n",
            "Config param kv_quant_dtype: int8\n",
            "Config param learning_rate: 3e-05\n",
            "Config param learning_rate_schedule_steps: 10\n",
            "Config param load_balance_loss_weight: 0.01\n",
            "Config param load_from_prefill_dir: False\n",
            "Config param load_full_state_path: \n",
            "Config param load_parameters_path: \n",
            "Config param local_checkpoint_directory: \n",
            "Config param local_checkpoint_period: 0\n",
            "Config param local_rope_max_timescale: -1\n",
            "Config param log_config: True\n",
            "Config param log_period: 100\n",
            "Config param logical_axis_rules: (('activation_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_batch_no_exp', ('data', 'fsdp', 'fsdp_transpose')), ('activation_embed_and_logits_batch', ('data', 'stage', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence', 'autoregressive')), ('activation_kv_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_length', ('sequence', 'context')), ('prefill_activation_length', ('sequence', 'context')), ('activation_length', ('context',)), ('activation_norm_length', ('tensor_sequence', 'context', 'sequence')), ('prefill_activation_norm_length', ('tensor_sequence', 'context', 'sequence')), ('activation_q_length', ('context',)), ('activation_kv_length', ()), ('activation_embed', ('tensor', 'tensor_transpose')), ('activation_mlp', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_kv', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_prefill_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_head_dim', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose')), ('activation_vocab', 'tensor_sequence'), ('activation_vocab', ('sequence', 'context')), ('activation_stage', 'stage'), ('activation_exp', ('expert',)), ('decode_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('decode_length', ('sequence',)), ('mlp', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), ('vocab', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('q_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('kv_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'context', 'expert')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'context')), ('embed_tensor_transpose', ('tensor_transpose',)), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'expert')), ('norm', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('layers', 'stage'), ('kv', ()), ('kv_head_dim', ()), ('cache_batch_prefill', ()), ('cache_batch', ()), ('cache_heads_none', ()), ('cache_heads', ('autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence')), ('cache_heads', ('autoregressive', 'tensor', 'tensor_sequence')), ('cache_kv', ()), ('cache_sequence', ()), ('exp', 'expert'), ('paged_kv_heads', ('tensor',)), ('num_pages', ()), ('tokens_per_page', ()), ('paged_kv_head_dim_size', ()))\n",
            "Config param logits_dot_in_fp32: False\n",
            "Config param logits_via_embedding: True\n",
            "Config param lora_input_adapters_path: \n",
            "Config param matmul_precision: default\n",
            "Config param max_checkify: False\n",
            "Config param max_corpus_chars: 10000000\n",
            "Config param max_position_embeddings: 163840\n",
            "Config param max_prefill_predict_length: 64\n",
            "Config param max_target_length: 8192\n",
            "Config param megablox: True\n",
            "Config param mesh_axes: ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive']\n",
            "Config param metrics_dir: gs://dummy_output_dir/test-tunix-maxtext-llama3-8b/metrics/\n",
            "Config param metrics_file: \n",
            "Config param micro_batch_size_to_eval_on: 64\n",
            "Config param micro_batch_size_to_train_on: 64\n",
            "Config param mla_naive_kvcache: True\n",
            "Config param mlp_activations: ['gelu', 'linear']\n",
            "Config param mlp_dim: 16384\n",
            "Config param mlpwi: remat\n",
            "Config param mlpwi_0: remat\n",
            "Config param mlpwi_1: remat\n",
            "Config param mlpwo: remat\n",
            "Config param model_call_mode: \n",
            "Config param model_fsdp_ag_once: False\n",
            "Config param model_name: gemma-2b\n",
            "Config param moe_mlp_dim: 7168\n",
            "Config param monitor_goodput: False\n",
            "Config param monitor_step_time_deviation: True\n",
            "Config param mscale: 1.0\n",
            "Config param mtp_eval_target_module: 0\n",
            "Config param mtp_loss_scaling_factor: 0.1\n",
            "Config param mtp_num_layers: 0\n",
            "Config param mu_dtype: bfloat16\n",
            "Config param multi_sampling: False\n",
            "Config param n_routing_groups: -1\n",
            "Config param nope_layer_interval: -1\n",
            "Config param normalization_layer_epsilon: 1e-06\n",
            "Config param normalize_embedding_logits: True\n",
            "Config param num_attention_heads_for_vit: 16\n",
            "Config param num_channels_for_vit: 3\n",
            "Config param num_decoder_layers: 18\n",
            "Config param num_epoch: 1\n",
            "Config param num_experts: 1\n",
            "Config param num_experts_per_tok: 1\n",
            "Config param num_hidden_layers_for_vit: 34\n",
            "Config param num_kv_heads: 1\n",
            "Config param num_layers_per_pipeline_stage: 1\n",
            "Config param num_pipeline_microbatches: -1\n",
            "Config param num_pipeline_repeats: -1\n",
            "Config param num_query_heads: 8\n",
            "Config param num_slices: 1\n",
            "Config param opt_type: adamw\n",
            "Config param optimize_mesh_for_tpu_v6e: False\n",
            "Config param optimizer_memory_host_offload: False\n",
            "Config param original_max_position_embeddings: 4096\n",
            "Config param out_proj: remat\n",
            "Config param override_model_config: False\n",
            "Config param packing: True\n",
            "Config param pagedattn_head_dim_alignment: 128\n",
            "Config param pagedattn_max_pages_per_group: 256\n",
            "Config param pagedattn_num_pages: 64\n",
            "Config param pagedattn_pages_per_compute_block: 4\n",
            "Config param pagedattn_tokens_per_page: 32\n",
            "Config param param_scan_axis: 1\n",
            "Config param parameter_memory_host_offload: False\n",
            "Config param patch_size_for_vit: 14\n",
            "Config param per_device_batch_size: 8.0\n",
            "Config param pipeline_delay_activation_forwarding: False\n",
            "Config param pipeline_fsdp_ag_once: False\n",
            "Config param pipeline_parallel_layers: -1\n",
            "Config param pixel_shuffle_ratio_for_vit: 0.5\n",
            "Config param prefill_cache_axis_order: 1,2,0,3\n",
            "Config param prefill_cache_dir: \n",
            "Config param prefill_chunk_size: 256\n",
            "Config param prefill_slice: v5e-16\n",
            "Config param prefix_caching_dram_byte: 100000000000\n",
            "Config param prefix_caching_hbm_byte: 10000000000\n",
            "Config param profile_cleanly: True\n",
            "Config param profile_periodically_period: -1\n",
            "Config param profiler: \n",
            "Config param profiler_steps: 5\n",
            "Config param projector_dropout_for_vit: 0.0\n",
            "Config param projector_input_dim_for_vit: 4096\n",
            "Config param projector_output_dim_for_vit: 4096\n",
            "Config param prometheus_port: 0\n",
            "Config param prompt: I love to\n",
            "Config param q_lora_rank: 0\n",
            "Config param qk_nope_head_dim: 128\n",
            "Config param qk_rope_head_dim: 64\n",
            "Config param qkv_proj: remat\n",
            "Config param quant_cfg_path: \n",
            "Config param quantization: \n",
            "Config param quantization_local_shard_count: 1\n",
            "Config param quantize_kvcache: False\n",
            "Config param query_proj: remat\n",
            "Config param ragged_block_size: 256\n",
            "Config param record_internal_nn_metrics: 0\n",
            "Config param remat_policy: full\n",
            "Config param remat_policy_for_vit: minimal\n",
            "Config param replicate_quant_scale: False\n",
            "Config param replicator_backup_interval_minutes: 0\n",
            "Config param report_heartbeat_metric_for_gcp_monitoring: False\n",
            "Config param report_performance_metric_for_gcp_monitoring: False\n",
            "Config param reshape_q: False\n",
            "Config param return_log_prob: False\n",
            "Config param reuse_example_batch: 0\n",
            "Config param rope_factor: 40\n",
            "Config param rope_max_timescale: 10000\n",
            "Config param rope_min_timescale: 1\n",
            "Config param rope_theta_for_vit: 10000\n",
            "Config param rope_type: default\n",
            "Config param rope_use_scale: True\n",
            "Config param routed_bias: False\n",
            "Config param routed_scaling_factor: 1.0\n",
            "Config param routed_score_func: \n",
            "Config param run_name: test-tunix-maxtext-llama3-8b\n",
            "Config param sa_block_kv: 512\n",
            "Config param sa_block_kv_compute: 512\n",
            "Config param sa_block_kv_dkv: 512\n",
            "Config param sa_block_kv_dkv_compute: 512\n",
            "Config param sa_block_kv_dq: 512\n",
            "Config param sa_block_q: 512\n",
            "Config param sa_block_q_dkv: 512\n",
            "Config param sa_block_q_dq: 512\n",
            "Config param sa_k_layout: HEAD_DIM_MINOR\n",
            "Config param sa_q_layout: HEAD_DIM_MINOR\n",
            "Config param sa_use_fused_bwd_kernel: False\n",
            "Config param sa_v_layout: HEAD_DIM_MINOR\n",
            "Config param save_config_to_gcs: False\n",
            "Config param save_quantized_params_path: \n",
            "Config param scan_layers: True\n",
            "Config param scan_layers_per_stage: False\n",
            "Config param scan_pipeline_iterations: True\n",
            "Config param set_remat_policy_on_layers_per_stage: False\n",
            "Config param set_remat_policy_on_pipeline_iterations: True\n",
            "Config param sft_train_on_completion_only: False\n",
            "Config param sharding_tolerance: 0.02\n",
            "Config param shardy: True\n",
            "Config param shared_experts: 1\n",
            "Config param skip_first_n_steps_for_profiler: 1\n",
            "Config param skip_jax_distributed_system: True\n",
            "Config param sliding_window_size: 0\n",
            "Config param source_checkpoint_layout: orbax\n",
            "Config param sparse_matmul: True\n",
            "Config param stack_prefill_result_cache: False\n",
            "Config param stack_trace_interval_seconds: 600\n",
            "Config param stack_trace_to_cloud: False\n",
            "Config param step_deviation_interval_seconds: 30\n",
            "Config param steps: 10\n",
            "Config param subslice_shape: \n",
            "Config param target_eval_loss: 0.0\n",
            "Config param temperature_tuning: False\n",
            "Config param tensorboard_dir: gs://dummy_output_dir/test-tunix-maxtext-llama3-8b/tensorboard/\n",
            "Config param tile_activation_dim: 1024\n",
            "Config param tile_batch_seq: 512\n",
            "Config param tile_size_for_vit: 336\n",
            "Config param tile_weight_dim: 1024\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config param tokenize_eval_data: True\n",
            "Config param tokenize_train_data: True\n",
            "Config param tokenizer_path: ../../maxtext/assets/tokenizer.gemma\n",
            "Config param tokenizer_type: sentencepiece\n",
            "Config param topk_routing_group: -1\n",
            "Config param train_data_columns: ['text']\n",
            "Config param train_image_column: image\n",
            "Config param train_split: train\n",
            "Config param trainable_position_size: -1\n",
            "Config param upload_all_profiler_results: False\n",
            "Config param use_chat_template: False\n",
            "Config param use_chunked_prefill: False\n",
            "Config param use_dpo: False\n",
            "Config param use_iota_embed: False\n",
            "Config param use_multimodal: False\n",
            "Config param use_post_attn_norm: False\n",
            "Config param use_post_ffw_norm: False\n",
            "Config param use_qk_norm: False\n",
            "Config param use_ragged_attention: False\n",
            "Config param use_random_routing: False\n",
            "Config param use_replicator_service: False\n",
            "Config param use_sft: False\n",
            "Config param use_untrainable_positional_embedding: False\n",
            "Config param use_vertex_tensorboard: False\n",
            "Config param using_pipeline_parallelism: False\n",
            "Config param v_head_dim: 128\n",
            "Config param value_proj: remat\n",
            "Config param vertex_tensorboard_project: \n",
            "Config param vertex_tensorboard_region: \n",
            "Config param vision_output_dim_for_vit: 4096\n",
            "Config param vocab_size: 256128\n",
            "Config param warmup_steps_fraction: 0.1\n",
            "Config param weight_dtype: bfloat16\n",
            "Num_devices: 8, shape (1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
            "Num_devices: 8, shape (1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
            "The abstract NNX state (all leaves are abstract arrays):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<script> (()=>{ if (customElements.get('treescope-container') === undefined) { class TreescopeContainer extends HTMLElement { constructor() { super(); this.attachShadow({mode: \"open\"}); this.defns = {}; this.state = {}; } } customElements.define(\"treescope-container\", TreescopeContainer); } if (customElements.get('treescope-run-here') === undefined) { class RunHere extends HTMLElement { constructor() { super() } connectedCallback() { const run = child => { const fn = new Function(child.textContent); child.textContent = \"\"; fn.call(this); this.remove(); }; const child = this.querySelector(\"script\"); if (child) { run(child); } else { new MutationObserver(()=>{ run(this.querySelector(\"script\")); }).observe(this, {childList: true}); } } } customElements.define(\"treescope-run-here\", RunHere); } })(); </script> <treescope-container class=\"treescope_out_eb410f1803634228ae1c1af9ee975731\" style=\"display:block\"></treescope-container> <treescope-run-here><script type=\"application/octet-stream\"> const root = ( Array.from(document.getElementsByClassName( \"treescope_out_eb410f1803634228ae1c1af9ee975731\")) .filter((elt) => !elt.dataset.setup) )[0]; root.dataset.setup = 1; const msg = document.createElement(\"span\"); msg.style = \"color: #cccccc; font-family: monospace;\"; msg.textContent = \"(Loading...)\"; root.state.loadingMsg = msg; root.shadowRoot.appendChild(msg); root.state.chain = new Promise((resolve, reject) => { const observer = new IntersectionObserver((entries) => { for (const entry of entries) { if (entry.isIntersecting) { resolve(); observer.disconnect(); return; } } }, {rootMargin: \"1000px\"}); window.setTimeout(() => { observer.observe(root); }, 0); }); root.state.deferring = false; const _insertNode = (node) => { for (let oldScript of node.querySelectorAll(\"script\")) { let newScript = document.createElement(\"script\"); newScript.type = oldScript.type; newScript.textContent = oldScript.textContent; oldScript.parentNode.replaceChild(newScript, oldScript); } if (root.state.loadingMsg) { root.state.loadingMsg.remove(); root.state.loadingMsg = null; } root.shadowRoot.appendChild(node); }; root.defns.insertContent = ((contentNode, compressed) => { if (compressed) { root.state.deferring = true; } if (root.state.deferring) { root.state.chain = (async () => { await root.state.chain; if (compressed) { const encoded = contentNode.textContent; const blob = new Blob([ Uint8Array.from(atob(encoded), (m) => m.codePointAt(0)) ]); const reader = blob.stream().pipeThrough( new DecompressionStream(\"deflate\") ).pipeThrough( new TextDecoderStream(\"utf-8\") ).getReader(); const parts = []; while (true) { const step = await reader.read(); if (step.done) { break; } parts.push(step.value); } const tpl = document.createElement('template'); tpl.innerHTML = parts.join(\"\"); _insertNode(tpl.content); } else { _insertNode(contentNode.content); } })(); } else { _insertNode(contentNode.content); } }); </script></treescope-run-here><div style=\"display:none\"> <script type=\"application/octet-stream\" >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</script> <treescope-run-here><script type=\"application/octet-stream\"> const root = ( Array.from(document.getElementsByClassName( \"treescope_out_eb410f1803634228ae1c1af9ee975731\")) .filter((elt) => !elt.dataset['step0']) )[0]; root.dataset['step0'] = 1; root.defns.insertContent( this.parentNode.querySelector('script[type=\"application/octet-stream\"]'), true ); this.parentNode.remove(); </script></treescope-run-here> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "restoring params from gs://maxtext-gemma/2b/2025-08-05-04-37/0/items\n",
            "Creating checkpoint manager with ocdbt=True and zarr3=True\n",
            "Checkpoint manager created!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:The transformations API will eventually be replaced by an upgraded design. The current API will not be removed until this point, but it will no longer be actively worked on.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{checkpoint=}\n",
            "Num_devices: 8, shape (1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'items'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Base model\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# gemma, mesh, model_config = get_base_model(\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#     ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m gemma, mesh, model_config = \u001b[43mget_ref_maxtext_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# gemma_maxtext_nnx = nnx.bridge.ToNNX(gemma)\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Instead of:\u001b[39;00m\n\u001b[32m      9\u001b[39m nnx.display(gemma)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mget_ref_maxtext_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     57\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m jax.sharding.use_mesh(model.mesh), nn.logical_axis_rules(config.logical_axis_rules):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m   model = \u001b[43mpartial_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[32m     64\u001b[39m tunix_model = TunixMaxTextLlama(\n\u001b[32m     65\u001b[39m       base_model=model,\n\u001b[32m     66\u001b[39m       use_attention_mask=\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# trust Tunix loss masking\u001b[39;00m\n\u001b[32m     67\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/flax/nnx/transforms/compilation.py:431\u001b[39m, in \u001b[36mJitWrapped.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m graph.update_context(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    430\u001b[39m   pure_args, pure_kwargs = \u001b[38;5;28mself\u001b[39m._get_pure_args_kwargs(args, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m   pure_args_out, pure_kwargs_out, pure_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjitted_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43mpure_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpure_kwargs\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m   out = \u001b[38;5;28mself\u001b[39m._get_non_pure_out(pure_args_out, pure_kwargs_out, pure_out)\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "    \u001b[31m[... skipping hidden 14 frame]\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/flax/nnx/transforms/compilation.py:126\u001b[39m, in \u001b[36mJitFn.__call__\u001b[39m\u001b[34m(self, *pure_args, **pure_kwargs)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *pure_args, **pure_kwargs):\n\u001b[32m    119\u001b[39m   args, kwargs = extract.from_tree(\n\u001b[32m    120\u001b[39m     (pure_args, pure_kwargs),\n\u001b[32m    121\u001b[39m     merge_fn=_jit_merge_fn,\n\u001b[32m    122\u001b[39m     ctxtag=\u001b[38;5;28mself\u001b[39m.ctxtag,\n\u001b[32m    123\u001b[39m     is_inner=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    124\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m   out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m   args_out, kwargs_out = extract.clear_non_graph_nodes((args, kwargs))\n\u001b[32m    129\u001b[39m   pure_args_out, pure_kwargs_out, pure_out = extract.to_tree(\n\u001b[32m    130\u001b[39m     (args_out, kwargs_out, out),\n\u001b[32m    131\u001b[39m     prefix=(\u001b[38;5;28mself\u001b[39m.in_shardings, \u001b[38;5;28mself\u001b[39m.kwarg_shardings, \u001b[38;5;28mself\u001b[39m.out_shardings),\n\u001b[32m    132\u001b[39m     ctxtag=\u001b[38;5;28mself\u001b[39m.ctxtag,\n\u001b[32m    133\u001b[39m     split_fn=_jit_split_fn,\n\u001b[32m    134\u001b[39m   )\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mget_ref_maxtext_model.<locals>.partial_init\u001b[39m\u001b[34m(checkpoint, config)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;129m@nnx\u001b[39m.jit\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpartial_init\u001b[39m(checkpoint, config):\n\u001b[32m     50\u001b[39m   model = create_model(config)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m   \u001b[43mnnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m   \u001b[38;5;66;03m# shard model\u001b[39;00m\n\u001b[32m     53\u001b[39m   state = nnx.state(model)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/flax/nnx/graph.py:2410\u001b[39m, in \u001b[36mupdate\u001b[39m\u001b[34m(node, state, *states)\u001b[39m\n\u001b[32m   2408\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2409\u001b[39m     state = statelib.merge_state(state, *states)\n\u001b[32m-> \u001b[39m\u001b[32m2410\u001b[39m \u001b[43m_graph_update_dynamic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/flax/nnx/graph.py:1441\u001b[39m, in \u001b[36m_graph_update_dynamic\u001b[39m\u001b[34m(node, state)\u001b[39m\n\u001b[32m   1439\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mUnknown node type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(node)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m   1440\u001b[39m node_dict = node_impl.node_dict(node)\n\u001b[32m-> \u001b[39m\u001b[32m1441\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m():\n\u001b[32m   1442\u001b[39m   \u001b[38;5;66;03m# case 1: new state is being added\u001b[39;00m\n\u001b[32m   1443\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m node_dict:\n\u001b[32m   1444\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node_impl, PytreeNodeImpl):\n",
            "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'items'"
          ]
        }
      ],
      "source": [
        "# Base model\n",
        "# gemma, mesh, model_config = get_base_model(\n",
        "#     ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\n",
        "# )\n",
        "\n",
        "gemma, mesh, model_config = get_ref_maxtext_model()\n",
        "# gemma_maxtext_nnx = nnx.bridge.ToNNX(gemma)\n",
        "# Instead of:\n",
        "nnx.display(gemma)\n",
        "\n",
        "# Use:\n",
        "print(\"Model initialized successfully\")\n",
        "print(f\"Model mesh shape: {mesh.shape}\")\n",
        "print(f\"Model config: {model_config}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Param( # 603,979,776 (1.2 GB)\n",
              "  value=Array([[[-0.0349121, 0.019043, 0.0178223, ..., -0.00424194, 0.0116577,\n",
              "           -0.00518799],\n",
              "          [-0.0373535, 0.0285645, -0.00473022, ..., 0.0314941, -0.0172119,\n",
              "           0],\n",
              "          [0.0032959, 0.0111694, 0.00762939, ..., 0.0314941, -0.0148926,\n",
              "           -0.0196533],\n",
              "          ...,\n",
              "          [0.00714111, -0.046875, 0.0305176, ..., 0.0444336, 0.00376892,\n",
              "           -0.00909424],\n",
              "          [0.00714111, 0.00811768, 0.00518799, ..., -0.00376892,\n",
              "           0.00665283, -0.000938416],\n",
              "          [-0.0373535, -0.0324707, -0.000469208, ..., -0.046875,\n",
              "           0.00860596, 0.00424194]],\n",
              "  \n",
              "         [[0.0154419, -0.0148926, -0.0373535, ..., 0.00473022, 0.0216064,\n",
              "           -0.0373535],\n",
              "          [-0.00909424, -0.0245361, 0.0405273, ..., 0.00282288, -0.012207,\n",
              "           -0.000469208],\n",
              "          [0.0361328, -0.000938416, -0.02771, ..., 0.0196533, -0.00616455,\n",
              "           0.0159912],\n",
              "          ...,\n",
              "          [0.0116577, -0.0216064, -0.00234985, ..., -0.0196533,\n",
              "           -0.00762939, 0.0178223],\n",
              "          [0.000469208, -0.00665283, -0.0238037, ..., 0.0361328,\n",
              "           0.0111694, 0.00616455],\n",
              "          [0.00964355, -0.00141144, 0.012207, ..., -0.0137939, -0.0196533,\n",
              "           -0.00811768]],\n",
              "  \n",
              "         [[0.0116577, 0.0405273, -0.0285645, ..., 0.0245361, -0.0305176,\n",
              "           0.00567627],\n",
              "          [-0.0361328, -0.0132446, -0.029541, ..., -0.0106201,\n",
              "           0.000469208, -0.0361328],\n",
              "          [-0.00964355, 0.0230713, -0.0252686, ..., -0.0106201,\n",
              "           -0.0268555, 0.00665283],\n",
              "          ...,\n",
              "          [0.0305176, 0.0238037, 0.0143433, ..., -0.0032959, 0.0390625,\n",
              "           -0.00376892],\n",
              "          [0.0268555, 0.00665283, -0.0202637, ..., 0.0126953, -0.0361328,\n",
              "           0.0101318],\n",
              "          [-0.00811768, -0.0349121, 0.00424194, ..., 0.00234985,\n",
              "           0.00964355, 0.00567627]],\n",
              "  \n",
              "         ...,\n",
              "  \n",
              "         [[-0.0238037, 0.00860596, 0.00714111, ..., -0.00616455,\n",
              "           -0.0132446, -0.0252686],\n",
              "          [-0.019043, -0.0111694, 0.0172119, ..., 0.0390625, -0.0202637,\n",
              "           -0.029541],\n",
              "          [-0.0361328, 0.0032959, -0.0336914, ..., -0.00424194, 0.0349121,\n",
              "           0.0184326],\n",
              "          ...,\n",
              "          [0.0373535, 0.00811768, -0.0373535, ..., 0.0444336, 0.0424805,\n",
              "           0.02771],\n",
              "          [-0.00282288, 0.000469208, -0.0223389, ..., 0.0245361,\n",
              "           0.00665283, -0.0268555],\n",
              "          [-0.0336914, 0.00473022, 0.0126953, ..., -0.0373535,\n",
              "           -0.00424194, -0.0349121]],\n",
              "  \n",
              "         [[-0.0209961, -0.0285645, -0.00811768, ..., 0.00187683,\n",
              "           0.0285645, -0.00811768],\n",
              "          [0.00141144, -0.019043, -0.000938416, ..., 0.0216064,\n",
              "           0.00187683, 0.0305176],\n",
              "          [0.0032959, 0.026001, 0.0116577, ..., -0.0500488, 0, 0.00518799],\n",
              "          ...,\n",
              "          [0.00714111, -0.0196533, 0.0132446, ..., -0.00518799,\n",
              "           -0.0101318, 0.0202637],\n",
              "          [-0.019043, 0.0223389, 0.0336914, ..., 0.0314941, -0.0252686,\n",
              "           0.0159912],\n",
              "          [-0.0230713, -0.0101318, 0.0137939, ..., 0.00141144, -0.0444336,\n",
              "           -0.00616455]],\n",
              "  \n",
              "         [[-0.0159912, 0.0137939, -0.0101318, ..., -0.0405273, 0.0166016,\n",
              "           0.00860596],\n",
              "          [0.0159912, 0.0126953, 0.029541, ..., 0.00909424, 0.0196533,\n",
              "           -0.0132446],\n",
              "          [0.00473022, 0.0373535, 0.0111694, ..., 0.00187683, 0.012207,\n",
              "           -0.00473022],\n",
              "          ...,\n",
              "          [-0.000938416, 0.0349121, 0.0132446, ..., 0.0148926, 0.0184326,\n",
              "           0.0196533],\n",
              "          [-0.0209961, 0.00424194, -0.0336914, ..., 0.0245361, 0.029541,\n",
              "           0.0314941],\n",
              "          [0.00424194, 0.0184326, 0.0230713, ..., 0.000938416, 0.00567627,\n",
              "           0.046875]]], dtype=bfloat16),\n",
              "  mesh=None,\n",
              "  sharding=('embed', 'layers', 'mlp'),\n",
              "  sharding_rules=None,\n",
              "  linen_meta_type=LogicallyPartitioned\n",
              ")"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gemma.base.decoder.layers[\"mlp\"][\"wi_0\"][\"kernel\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00]]]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00]]]]]\n",
            "inputs=[[[0.0245361 1.15625 0.515625 ... 0.472656 1.05469 1.61719]\n",
            "  [-1 1.24219 -1.83594 ... 1.19531 -0.824219 -0.132812]\n",
            "  [0.0634766 0.585938 -2.89062 ... -0.192383 -2.89062 -0.192383]\n",
            "  ...\n",
            "  [0.472656 0.261719 -0.0147095 ... -0.171875 2.03125 0.65625]\n",
            "  [0.5625 0.632812 -0.292969 ... 2.51562 -0.333984 0.122559]\n",
            "  [-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[0.0245361 1.15625 0.515625 ... 0.472656 1.05469 1.61719]\n",
            "  [-1 1.24219 -1.83594 ... 1.19531 -0.824219 -0.132812]\n",
            "  [0.0634766 0.585938 -2.89062 ... -0.192383 -2.89062 -0.192383]\n",
            "  ...\n",
            "  [0.472656 0.261719 -0.0147095 ... -0.171875 2.03125 0.65625]\n",
            "  [0.5625 0.632812 -0.292969 ... 2.51562 -0.333984 0.122559]\n",
            "  [-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[0.0245361 1.15625 0.515625 ... 0.472656 1.05469 1.61719]\n",
            "  [-1 1.24219 -1.83594 ... 1.19531 -0.824219 -0.132812]\n",
            "  [0.0634766 0.585938 -2.89062 ... -0.192383 -2.89062 -0.192383]\n",
            "  ...\n",
            "  [0.472656 0.261719 -0.0147095 ... -0.171875 2.03125 0.65625]\n",
            "  [0.5625 0.632812 -0.292969 ... 2.51562 -0.333984 0.122559]\n",
            "  [-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.0245361 1.15625 0.515625 ... 0.472656 1.05469 1.61719]\n",
            "  [-1 1.24219 -1.83594 ... 1.19531 -0.824219 -0.132812]\n",
            "  [0.0634766 0.585938 -2.89062 ... -0.192383 -2.89062 -0.192383]\n",
            "  ...\n",
            "  [0.472656 0.261719 -0.0147095 ... -0.171875 2.03125 0.65625]\n",
            "  [0.5625 0.632812 -0.292969 ... 2.51562 -0.333984 0.122559]\n",
            "  [-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[0.0245361 1.15625 0.515625 ... 0.472656 1.05469 1.61719]\n",
            "  [-1 1.24219 -1.83594 ... 1.19531 -0.824219 -0.132812]\n",
            "  [0.0634766 0.585938 -2.89062 ... -0.192383 -2.89062 -0.192383]\n",
            "  ...\n",
            "  [0.472656 0.261719 -0.0147095 ... -0.171875 2.03125 0.65625]\n",
            "  [0.5625 0.632812 -0.292969 ... 2.51562 -0.333984 0.122559]\n",
            "  [-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[0.0245361 1.15625 0.515625 ... 0.472656 1.05469 1.61719]\n",
            "  [-1 1.24219 -1.83594 ... 1.19531 -0.824219 -0.132812]\n",
            "  [0.0634766 0.585938 -2.89062 ... -0.192383 -2.89062 -0.192383]\n",
            "  ...\n",
            "  [0.472656 0.261719 -0.0147095 ... -0.171875 2.03125 0.65625]\n",
            "  [0.5625 0.632812 -0.292969 ... 2.51562 -0.333984 0.122559]\n",
            "  [-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]]\n",
            "lnx=[[[0.024292 1.14844 0.511719 ... 0.46875 1.04688 1.60156]\n",
            "  [-1.01562 1.25781 -1.85938 ... 1.21094 -0.835938 -0.134766]\n",
            "  [0.0639648 0.589844 -2.92188 ... -0.194336 -2.92188 -0.194336]\n",
            "  ...\n",
            "  [0.474609 0.263672 -0.0147705 ... -0.172852 2.04688 0.660156]\n",
            "  [0.5625 0.632812 -0.292969 ... 2.51562 -0.333984 0.122559]\n",
            "  [-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[0.024292 1.14844 0.511719 ... 0.46875 1.04688 1.60156]\n",
            "  [-1.01562 1.25781 -1.85938 ... 1.21094 -0.835938 -0.134766]\n",
            "  [0.0639648 0.589844 -2.92188 ... -0.194336 -2.92188 -0.194336]\n",
            "  ...\n",
            "  [0.474609 0.263672 -0.0147705 ... -0.172852 2.04688 0.660156]\n",
            "  [0.5625 0.632812 -0.292969 ... 2.51562 -0.333984 0.122559]\n",
            "  [-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[0.024292 1.14844 0.511719 ... 0.46875 1.04688 1.60156]\n",
            "  [-1.01562 1.25781 -1.85938 ... 1.21094 -0.835938 -0.134766]\n",
            "  [0.0639648 0.589844 -2.92188 ... -0.194336 -2.92188 -0.194336]\n",
            "  ...\n",
            "  [0.474609 0.263672 -0.0147705 ... -0.172852 2.04688 0.660156]\n",
            "  [0.5625 0.632812 -0.292969 ... 2.51562 -0.333984 0.122559]\n",
            "  [-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.024292 1.14844 0.511719 ... 0.46875 1.04688 1.60156]\n",
            "  [-1.01562 1.25781 -1.85938 ... 1.21094 -0.835938 -0.134766]\n",
            "  [0.0639648 0.589844 -2.92188 ... -0.194336 -2.92188 -0.194336]\n",
            "  ...\n",
            "  [0.474609 0.263672 -0.0147705 ... -0.172852 2.04688 0.660156]\n",
            "  [0.5625 0.632812 -0.292969 ... 2.51562 -0.333984 0.122559]\n",
            "  [-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[0.024292 1.14844 0.511719 ... 0.46875 1.04688 1.60156]\n",
            "  [-1.01562 1.25781 -1.85938 ... 1.21094 -0.835938 -0.134766]\n",
            "  [0.0639648 0.589844 -2.92188 ... -0.194336 -2.92188 -0.194336]\n",
            "  ...\n",
            "  [0.474609 0.263672 -0.0147705 ... -0.172852 2.04688 0.660156]\n",
            "  [0.5625 0.632812 -0.292969 ... 2.51562 -0.333984 0.122559]\n",
            "  [-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[0.024292 1.14844 0.511719 ... 0.46875 1.04688 1.60156]\n",
            "  [-1.01562 1.25781 -1.85938 ... 1.21094 -0.835938 -0.134766]\n",
            "  [0.0639648 0.589844 -2.92188 ... -0.194336 -2.92188 -0.194336]\n",
            "  ...\n",
            "  [0.474609 0.263672 -0.0147705 ... -0.172852 2.04688 0.660156]\n",
            "  [0.5625 0.632812 -0.292969 ... 2.51562 -0.333984 0.122559]\n",
            "  [-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]]\n",
            "attention_lnx=[[[0.0297852 1.00781 0.439453 ... -1.02344 -0.5625 -1.04688]\n",
            "  [-0.601562 -0.283203 1.29688 ... -0.839844 -1.20312 -0.890625]\n",
            "  [-0.601562 -0.363281 -0.202148 ... -1.19531 -0.488281 -0.964844]\n",
            "  ...\n",
            "  [-0.0267334 0.10791 -0.0688477 ... -0.427734 0.414062 -0.451172]\n",
            "  [0.0854492 -0.000736237 -0.761719 ... -0.318359 0.0158691 -0.46875]\n",
            "  [0.194336 0.15918 -0.330078 ... -0.345703 -0.115234 -0.527344]]\n",
            "\n",
            " [[0.0297852 1.00781 0.439453 ... -1.02344 -0.5625 -1.04688]\n",
            "  [-0.601562 -0.283203 1.29688 ... -0.839844 -1.20312 -0.890625]\n",
            "  [-0.601562 -0.363281 -0.202148 ... -1.19531 -0.488281 -0.964844]\n",
            "  ...\n",
            "  [-0.0267334 0.10791 -0.0688477 ... -0.427734 0.414062 -0.451172]\n",
            "  [0.0854492 -0.000736237 -0.761719 ... -0.318359 0.0158691 -0.46875]\n",
            "  [0.194336 0.15918 -0.330078 ... -0.345703 -0.115234 -0.527344]]\n",
            "\n",
            " [[0.0297852 1.00781 0.439453 ... -1.02344 -0.5625 -1.04688]\n",
            "  [-0.601562 -0.283203 1.29688 ... -0.839844 -1.20312 -0.890625]\n",
            "  [-0.601562 -0.363281 -0.202148 ... -1.19531 -0.488281 -0.964844]\n",
            "  ...\n",
            "  [-0.0267334 0.10791 -0.0688477 ... -0.427734 0.414062 -0.451172]\n",
            "  [0.0854492 -0.000736237 -0.761719 ... -0.318359 0.0158691 -0.46875]\n",
            "  [0.194336 0.15918 -0.330078 ... -0.345703 -0.115234 -0.527344]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.0297852 1.00781 0.439453 ... -1.02344 -0.5625 -1.04688]\n",
            "  [-0.601562 -0.283203 1.29688 ... -0.839844 -1.20312 -0.890625]\n",
            "  [-0.601562 -0.363281 -0.202148 ... -1.19531 -0.488281 -0.964844]\n",
            "  ...\n",
            "  [-0.0267334 0.10791 -0.0688477 ... -0.427734 0.414062 -0.451172]\n",
            "  [0.0854492 -0.000736237 -0.761719 ... -0.318359 0.0158691 -0.46875]\n",
            "  [0.194336 0.15918 -0.330078 ... -0.345703 -0.115234 -0.527344]]\n",
            "\n",
            " [[0.0297852 1.00781 0.439453 ... -1.02344 -0.5625 -1.04688]\n",
            "  [-0.601562 -0.283203 1.29688 ... -0.839844 -1.20312 -0.890625]\n",
            "  [-0.601562 -0.363281 -0.202148 ... -1.19531 -0.488281 -0.964844]\n",
            "  ...\n",
            "  [-0.0267334 0.10791 -0.0688477 ... -0.427734 0.414062 -0.451172]\n",
            "  [0.0854492 -0.000736237 -0.761719 ... -0.318359 0.0158691 -0.46875]\n",
            "  [0.194336 0.15918 -0.330078 ... -0.345703 -0.115234 -0.527344]]\n",
            "\n",
            " [[0.0297852 1.00781 0.439453 ... -1.02344 -0.5625 -1.04688]\n",
            "  [-0.601562 -0.283203 1.29688 ... -0.839844 -1.20312 -0.890625]\n",
            "  [-0.601562 -0.363281 -0.202148 ... -1.19531 -0.488281 -0.964844]\n",
            "  ...\n",
            "  [-0.0267334 0.10791 -0.0688477 ... -0.427734 0.414062 -0.451172]\n",
            "  [0.0854492 -0.000736237 -0.761719 ... -0.318359 0.0158691 -0.46875]\n",
            "  [0.194336 0.15918 -0.330078 ... -0.345703 -0.115234 -0.527344]]]\n",
            "attn_output=[[[0.0383301 1.53125 0.671875 ... -0.388672 0.349609 0.402344]\n",
            "  [-1.25 0.746094 -0.417969 ... 0.277344 -1.58594 -0.796875]\n",
            "  [-0.457031 0.189453 -2.625 ... -1.17969 -2.85938 -0.980469]\n",
            "  ...\n",
            "  [0.404297 0.335938 -0.0756836 ... -0.546875 2.21875 0.186523]\n",
            "  [0.609375 0.59375 -0.992188 ... 2.07812 -0.298828 -0.324219]\n",
            "  [0.0578613 -0.789062 -0.582031 ... -0.507812 0.847656 0.707031]]\n",
            "\n",
            " [[0.0383301 1.53125 0.671875 ... -0.388672 0.349609 0.402344]\n",
            "  [-1.25 0.746094 -0.417969 ... 0.277344 -1.58594 -0.796875]\n",
            "  [-0.457031 0.189453 -2.625 ... -1.17969 -2.85938 -0.980469]\n",
            "  ...\n",
            "  [0.404297 0.335938 -0.0756836 ... -0.546875 2.21875 0.186523]\n",
            "  [0.609375 0.59375 -0.992188 ... 2.07812 -0.298828 -0.324219]\n",
            "  [0.0578613 -0.789062 -0.582031 ... -0.507812 0.847656 0.707031]]\n",
            "\n",
            " [[0.0383301 1.53125 0.671875 ... -0.388672 0.349609 0.402344]\n",
            "  [-1.25 0.746094 -0.417969 ... 0.277344 -1.58594 -0.796875]\n",
            "  [-0.457031 0.189453 -2.625 ... -1.17969 -2.85938 -0.980469]\n",
            "  ...\n",
            "  [0.404297 0.335938 -0.0756836 ... -0.546875 2.21875 0.186523]\n",
            "  [0.609375 0.59375 -0.992188 ... 2.07812 -0.298828 -0.324219]\n",
            "  [0.0578613 -0.789062 -0.582031 ... -0.507812 0.847656 0.707031]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.0383301 1.53125 0.671875 ... -0.388672 0.349609 0.402344]\n",
            "  [-1.25 0.746094 -0.417969 ... 0.277344 -1.58594 -0.796875]\n",
            "  [-0.457031 0.189453 -2.625 ... -1.17969 -2.85938 -0.980469]\n",
            "  ...\n",
            "  [0.404297 0.335938 -0.0756836 ... -0.546875 2.21875 0.186523]\n",
            "  [0.609375 0.59375 -0.992188 ... 2.07812 -0.298828 -0.324219]\n",
            "  [0.0578613 -0.789062 -0.582031 ... -0.507812 0.847656 0.707031]]\n",
            "\n",
            " [[0.0383301 1.53125 0.671875 ... -0.388672 0.349609 0.402344]\n",
            "  [-1.25 0.746094 -0.417969 ... 0.277344 -1.58594 -0.796875]\n",
            "  [-0.457031 0.189453 -2.625 ... -1.17969 -2.85938 -0.980469]\n",
            "  ...\n",
            "  [0.404297 0.335938 -0.0756836 ... -0.546875 2.21875 0.186523]\n",
            "  [0.609375 0.59375 -0.992188 ... 2.07812 -0.298828 -0.324219]\n",
            "  [0.0578613 -0.789062 -0.582031 ... -0.507812 0.847656 0.707031]]\n",
            "\n",
            " [[0.0383301 1.53125 0.671875 ... -0.388672 0.349609 0.402344]\n",
            "  [-1.25 0.746094 -0.417969 ... 0.277344 -1.58594 -0.796875]\n",
            "  [-0.457031 0.189453 -2.625 ... -1.17969 -2.85938 -0.980469]\n",
            "  ...\n",
            "  [0.404297 0.335938 -0.0756836 ... -0.546875 2.21875 0.186523]\n",
            "  [0.609375 0.59375 -0.992188 ... 2.07812 -0.298828 -0.324219]\n",
            "  [0.0578613 -0.789062 -0.582031 ... -0.507812 0.847656 0.707031]]]\n",
            "next_layer_addition_dropped_out=[[[0.226562 1.53125 2.125 ... -1.13281 0.816406 1.65625]\n",
            "  [-2.625 0.427734 -0.542969 ... 1.01562 -2.29688 -0.792969]\n",
            "  [-0.558594 -0.0908203 -3.59375 ... -2.15625 -3.82812 -1.42969]\n",
            "  ...\n",
            "  [0.902344 0.0427246 0.449219 ... 1.46875 1.89844 0.00164032]\n",
            "  [-0.851562 0.0581055 -1.02344 ... 2.23438 0.380859 0.699219]\n",
            "  [-0.710938 -0.59375 0.0961914 ... 0.570312 1.3125 -0.519531]]\n",
            "\n",
            " [[0.226562 1.53125 2.125 ... -1.13281 0.816406 1.65625]\n",
            "  [-2.625 0.427734 -0.542969 ... 1.01562 -2.29688 -0.792969]\n",
            "  [-0.558594 -0.0908203 -3.59375 ... -2.15625 -3.82812 -1.42969]\n",
            "  ...\n",
            "  [0.902344 0.0427246 0.449219 ... 1.46875 1.89844 0.00164032]\n",
            "  [-0.851562 0.0581055 -1.02344 ... 2.23438 0.380859 0.699219]\n",
            "  [-0.710938 -0.59375 0.0961914 ... 0.570312 1.3125 -0.519531]]\n",
            "\n",
            " [[0.226562 1.53125 2.125 ... -1.13281 0.816406 1.65625]\n",
            "  [-2.625 0.427734 -0.542969 ... 1.01562 -2.29688 -0.792969]\n",
            "  [-0.558594 -0.0908203 -3.59375 ... -2.15625 -3.82812 -1.42969]\n",
            "  ...\n",
            "  [0.902344 0.0427246 0.449219 ... 1.46875 1.89844 0.00164032]\n",
            "  [-0.851562 0.0581055 -1.02344 ... 2.23438 0.380859 0.699219]\n",
            "  [-0.710938 -0.59375 0.0961914 ... 0.570312 1.3125 -0.519531]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.226562 1.53125 2.125 ... -1.13281 0.816406 1.65625]\n",
            "  [-2.625 0.427734 -0.542969 ... 1.01562 -2.29688 -0.792969]\n",
            "  [-0.558594 -0.0908203 -3.59375 ... -2.15625 -3.82812 -1.42969]\n",
            "  ...\n",
            "  [0.902344 0.0427246 0.449219 ... 1.46875 1.89844 0.00164032]\n",
            "  [-0.851562 0.0581055 -1.02344 ... 2.23438 0.380859 0.699219]\n",
            "  [-0.710938 -0.59375 0.0961914 ... 0.570312 1.3125 -0.519531]]\n",
            "\n",
            " [[0.226562 1.53125 2.125 ... -1.13281 0.816406 1.65625]\n",
            "  [-2.625 0.427734 -0.542969 ... 1.01562 -2.29688 -0.792969]\n",
            "  [-0.558594 -0.0908203 -3.59375 ... -2.15625 -3.82812 -1.42969]\n",
            "  ...\n",
            "  [0.902344 0.0427246 0.449219 ... 1.46875 1.89844 0.00164032]\n",
            "  [-0.851562 0.0581055 -1.02344 ... 2.23438 0.380859 0.699219]\n",
            "  [-0.710938 -0.59375 0.0961914 ... 0.570312 1.3125 -0.519531]]\n",
            "\n",
            " [[0.226562 1.53125 2.125 ... -1.13281 0.816406 1.65625]\n",
            "  [-2.625 0.427734 -0.542969 ... 1.01562 -2.29688 -0.792969]\n",
            "  [-0.558594 -0.0908203 -3.59375 ... -2.15625 -3.82812 -1.42969]\n",
            "  ...\n",
            "  [0.902344 0.0427246 0.449219 ... 1.46875 1.89844 0.00164032]\n",
            "  [-0.851562 0.0581055 -1.02344 ... 2.23438 0.380859 0.699219]\n",
            "  [-0.710938 -0.59375 0.0961914 ... 0.570312 1.3125 -0.519531]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00]]]]]\n",
            "inputs=[[[0.226562 1.53125 2.125 ... -1.13281 0.816406 1.65625]\n",
            "  [-2.625 0.427734 -0.542969 ... 1.01562 -2.29688 -0.792969]\n",
            "  [-0.558594 -0.0908203 -3.59375 ... -2.15625 -3.82812 -1.42969]\n",
            "  ...\n",
            "  [0.902344 0.0427246 0.449219 ... 1.46875 1.89844 0.00164032]\n",
            "  [-0.851562 0.0581055 -1.02344 ... 2.23438 0.380859 0.699219]\n",
            "  [-0.710938 -0.59375 0.0961914 ... 0.570312 1.3125 -0.519531]]\n",
            "\n",
            " [[0.226562 1.53125 2.125 ... -1.13281 0.816406 1.65625]\n",
            "  [-2.625 0.427734 -0.542969 ... 1.01562 -2.29688 -0.792969]\n",
            "  [-0.558594 -0.0908203 -3.59375 ... -2.15625 -3.82812 -1.42969]\n",
            "  ...\n",
            "  [0.902344 0.0427246 0.449219 ... 1.46875 1.89844 0.00164032]\n",
            "  [-0.851562 0.0581055 -1.02344 ... 2.23438 0.380859 0.699219]\n",
            "  [-0.710938 -0.59375 0.0961914 ... 0.570312 1.3125 -0.519531]]\n",
            "\n",
            " [[0.226562 1.53125 2.125 ... -1.13281 0.816406 1.65625]\n",
            "  [-2.625 0.427734 -0.542969 ... 1.01562 -2.29688 -0.792969]\n",
            "  [-0.558594 -0.0908203 -3.59375 ... -2.15625 -3.82812 -1.42969]\n",
            "  ...\n",
            "  [0.902344 0.0427246 0.449219 ... 1.46875 1.89844 0.00164032]\n",
            "  [-0.851562 0.0581055 -1.02344 ... 2.23438 0.380859 0.699219]\n",
            "  [-0.710938 -0.59375 0.0961914 ... 0.570312 1.3125 -0.519531]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.226562 1.53125 2.125 ... -1.13281 0.816406 1.65625]\n",
            "  [-2.625 0.427734 -0.542969 ... 1.01562 -2.29688 -0.792969]\n",
            "  [-0.558594 -0.0908203 -3.59375 ... -2.15625 -3.82812 -1.42969]\n",
            "  ...\n",
            "  [0.902344 0.0427246 0.449219 ... 1.46875 1.89844 0.00164032]\n",
            "  [-0.851562 0.0581055 -1.02344 ... 2.23438 0.380859 0.699219]\n",
            "  [-0.710938 -0.59375 0.0961914 ... 0.570312 1.3125 -0.519531]]\n",
            "\n",
            " [[0.226562 1.53125 2.125 ... -1.13281 0.816406 1.65625]\n",
            "  [-2.625 0.427734 -0.542969 ... 1.01562 -2.29688 -0.792969]\n",
            "  [-0.558594 -0.0908203 -3.59375 ... -2.15625 -3.82812 -1.42969]\n",
            "  ...\n",
            "  [0.902344 0.0427246 0.449219 ... 1.46875 1.89844 0.00164032]\n",
            "  [-0.851562 0.0581055 -1.02344 ... 2.23438 0.380859 0.699219]\n",
            "  [-0.710938 -0.59375 0.0961914 ... 0.570312 1.3125 -0.519531]]\n",
            "\n",
            " [[0.226562 1.53125 2.125 ... -1.13281 0.816406 1.65625]\n",
            "  [-2.625 0.427734 -0.542969 ... 1.01562 -2.29688 -0.792969]\n",
            "  [-0.558594 -0.0908203 -3.59375 ... -2.15625 -3.82812 -1.42969]\n",
            "  ...\n",
            "  [0.902344 0.0427246 0.449219 ... 1.46875 1.89844 0.00164032]\n",
            "  [-0.851562 0.0581055 -1.02344 ... 2.23438 0.380859 0.699219]\n",
            "  [-0.710938 -0.59375 0.0961914 ... 0.570312 1.3125 -0.519531]]]\n",
            "lnx=[[[0.145508 0.980469 1.36719 ... -0.726562 0.523438 1.0625]\n",
            "  [-1.85156 0.302734 -0.382812 ... 0.71875 -1.625 -0.558594]\n",
            "  [-0.416016 -0.0678711 -2.67188 ... -1.60938 -2.85938 -1.0625]\n",
            "  ...\n",
            "  [0.707031 0.0334473 0.351562 ... 1.14844 1.48438 0.00128174]\n",
            "  [-0.671875 0.0458984 -0.804688 ... 1.75781 0.300781 0.550781]\n",
            "  [-0.574219 -0.478516 0.0776367 ... 0.458984 1.05469 -0.417969]]\n",
            "\n",
            " [[0.145508 0.980469 1.36719 ... -0.726562 0.523438 1.0625]\n",
            "  [-1.85156 0.302734 -0.382812 ... 0.71875 -1.625 -0.558594]\n",
            "  [-0.416016 -0.0678711 -2.67188 ... -1.60938 -2.85938 -1.0625]\n",
            "  ...\n",
            "  [0.707031 0.0334473 0.351562 ... 1.14844 1.48438 0.00128174]\n",
            "  [-0.671875 0.0458984 -0.804688 ... 1.75781 0.300781 0.550781]\n",
            "  [-0.574219 -0.478516 0.0776367 ... 0.458984 1.05469 -0.417969]]\n",
            "\n",
            " [[0.145508 0.980469 1.36719 ... -0.726562 0.523438 1.0625]\n",
            "  [-1.85156 0.302734 -0.382812 ... 0.71875 -1.625 -0.558594]\n",
            "  [-0.416016 -0.0678711 -2.67188 ... -1.60938 -2.85938 -1.0625]\n",
            "  ...\n",
            "  [0.707031 0.0334473 0.351562 ... 1.14844 1.48438 0.00128174]\n",
            "  [-0.671875 0.0458984 -0.804688 ... 1.75781 0.300781 0.550781]\n",
            "  [-0.574219 -0.478516 0.0776367 ... 0.458984 1.05469 -0.417969]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.145508 0.980469 1.36719 ... -0.726562 0.523438 1.0625]\n",
            "  [-1.85156 0.302734 -0.382812 ... 0.71875 -1.625 -0.558594]\n",
            "  [-0.416016 -0.0678711 -2.67188 ... -1.60938 -2.85938 -1.0625]\n",
            "  ...\n",
            "  [0.707031 0.0334473 0.351562 ... 1.14844 1.48438 0.00128174]\n",
            "  [-0.671875 0.0458984 -0.804688 ... 1.75781 0.300781 0.550781]\n",
            "  [-0.574219 -0.478516 0.0776367 ... 0.458984 1.05469 -0.417969]]\n",
            "\n",
            " [[0.145508 0.980469 1.36719 ... -0.726562 0.523438 1.0625]\n",
            "  [-1.85156 0.302734 -0.382812 ... 0.71875 -1.625 -0.558594]\n",
            "  [-0.416016 -0.0678711 -2.67188 ... -1.60938 -2.85938 -1.0625]\n",
            "  ...\n",
            "  [0.707031 0.0334473 0.351562 ... 1.14844 1.48438 0.00128174]\n",
            "  [-0.671875 0.0458984 -0.804688 ... 1.75781 0.300781 0.550781]\n",
            "  [-0.574219 -0.478516 0.0776367 ... 0.458984 1.05469 -0.417969]]\n",
            "\n",
            " [[0.145508 0.980469 1.36719 ... -0.726562 0.523438 1.0625]\n",
            "  [-1.85156 0.302734 -0.382812 ... 0.71875 -1.625 -0.558594]\n",
            "  [-0.416016 -0.0678711 -2.67188 ... -1.60938 -2.85938 -1.0625]\n",
            "  ...\n",
            "  [0.707031 0.0334473 0.351562 ... 1.14844 1.48438 0.00128174]\n",
            "  [-0.671875 0.0458984 -0.804688 ... 1.75781 0.300781 0.550781]\n",
            "  [-0.574219 -0.478516 0.0776367 ... 0.458984 1.05469 -0.417969]]]\n",
            "attention_lnx=[[[-0.816406 0.5 1.83594 ... -1.42188 -0.621094 0.636719]\n",
            "  [-0.0349121 0.503906 1.40625 ... -0.855469 -0.667969 0.0211182]\n",
            "  [-0.0279541 0.730469 1.35938 ... -1.42969 -0.345703 0.730469]\n",
            "  ...\n",
            "  [0.361328 0.330078 0.867188 ... -0.746094 0.441406 -0.263672]\n",
            "  [0.298828 0.179688 0.847656 ... -1.03125 0.410156 -0.102051]\n",
            "  [-0.294922 0.24707 0.675781 ... -0.291016 -0.131836 -0.367188]]\n",
            "\n",
            " [[-0.816406 0.5 1.83594 ... -1.42188 -0.621094 0.636719]\n",
            "  [-0.0349121 0.503906 1.40625 ... -0.855469 -0.667969 0.0211182]\n",
            "  [-0.0279541 0.730469 1.35938 ... -1.42969 -0.345703 0.730469]\n",
            "  ...\n",
            "  [0.361328 0.330078 0.867188 ... -0.746094 0.441406 -0.263672]\n",
            "  [0.298828 0.179688 0.847656 ... -1.03125 0.410156 -0.102051]\n",
            "  [-0.294922 0.24707 0.675781 ... -0.291016 -0.131836 -0.367188]]\n",
            "\n",
            " [[-0.816406 0.5 1.83594 ... -1.42188 -0.621094 0.636719]\n",
            "  [-0.0349121 0.503906 1.40625 ... -0.855469 -0.667969 0.0211182]\n",
            "  [-0.0279541 0.730469 1.35938 ... -1.42969 -0.345703 0.730469]\n",
            "  ...\n",
            "  [0.361328 0.330078 0.867188 ... -0.746094 0.441406 -0.263672]\n",
            "  [0.298828 0.179688 0.847656 ... -1.03125 0.410156 -0.102051]\n",
            "  [-0.294922 0.24707 0.675781 ... -0.291016 -0.131836 -0.367188]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.816406 0.5 1.83594 ... -1.42188 -0.621094 0.636719]\n",
            "  [-0.0349121 0.503906 1.40625 ... -0.855469 -0.667969 0.0211182]\n",
            "  [-0.0279541 0.730469 1.35938 ... -1.42969 -0.345703 0.730469]\n",
            "  ...\n",
            "  [0.361328 0.330078 0.867188 ... -0.746094 0.441406 -0.263672]\n",
            "  [0.298828 0.179688 0.847656 ... -1.03125 0.410156 -0.102051]\n",
            "  [-0.294922 0.24707 0.675781 ... -0.291016 -0.131836 -0.367188]]\n",
            "\n",
            " [[-0.816406 0.5 1.83594 ... -1.42188 -0.621094 0.636719]\n",
            "  [-0.0349121 0.503906 1.40625 ... -0.855469 -0.667969 0.0211182]\n",
            "  [-0.0279541 0.730469 1.35938 ... -1.42969 -0.345703 0.730469]\n",
            "  ...\n",
            "  [0.361328 0.330078 0.867188 ... -0.746094 0.441406 -0.263672]\n",
            "  [0.298828 0.179688 0.847656 ... -1.03125 0.410156 -0.102051]\n",
            "  [-0.294922 0.24707 0.675781 ... -0.291016 -0.131836 -0.367188]]\n",
            "\n",
            " [[-0.816406 0.5 1.83594 ... -1.42188 -0.621094 0.636719]\n",
            "  [-0.0349121 0.503906 1.40625 ... -0.855469 -0.667969 0.0211182]\n",
            "  [-0.0279541 0.730469 1.35938 ... -1.42969 -0.345703 0.730469]\n",
            "  ...\n",
            "  [0.361328 0.330078 0.867188 ... -0.746094 0.441406 -0.263672]\n",
            "  [0.298828 0.179688 0.847656 ... -1.03125 0.410156 -0.102051]\n",
            "  [-0.294922 0.24707 0.675781 ... -0.291016 -0.131836 -0.367188]]]\n",
            "attn_output=[[[-0.320312 1.10156 2.14062 ... -1.39062 0.106445 1.24219]\n",
            "  [-1.64844 0.574219 0.535156 ... 0.0986328 -1.83594 -0.478516]\n",
            "  [-0.396484 0.431641 -1.50781 ... -2.42188 -2.82812 -0.472656]\n",
            "  ...\n",
            "  [0.945312 0.279297 0.980469 ... 0.539062 1.75 -0.196289]\n",
            "  [-0.408203 0.175781 -0.129883 ... 0.890625 0.582031 0.441406]\n",
            "  [-0.773438 -0.265625 0.59375 ... 0.213867 0.902344 -0.679688]]\n",
            "\n",
            " [[-0.320312 1.10156 2.14062 ... -1.39062 0.106445 1.24219]\n",
            "  [-1.64844 0.574219 0.535156 ... 0.0986328 -1.83594 -0.478516]\n",
            "  [-0.396484 0.431641 -1.50781 ... -2.42188 -2.82812 -0.472656]\n",
            "  ...\n",
            "  [0.945312 0.279297 0.980469 ... 0.539062 1.75 -0.196289]\n",
            "  [-0.408203 0.175781 -0.129883 ... 0.890625 0.582031 0.441406]\n",
            "  [-0.773438 -0.265625 0.59375 ... 0.213867 0.902344 -0.679688]]\n",
            "\n",
            " [[-0.320312 1.10156 2.14062 ... -1.39062 0.106445 1.24219]\n",
            "  [-1.64844 0.574219 0.535156 ... 0.0986328 -1.83594 -0.478516]\n",
            "  [-0.396484 0.431641 -1.50781 ... -2.42188 -2.82812 -0.472656]\n",
            "  ...\n",
            "  [0.945312 0.279297 0.980469 ... 0.539062 1.75 -0.196289]\n",
            "  [-0.408203 0.175781 -0.129883 ... 0.890625 0.582031 0.441406]\n",
            "  [-0.773438 -0.265625 0.59375 ... 0.213867 0.902344 -0.679688]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.320312 1.10156 2.14062 ... -1.39062 0.106445 1.24219]\n",
            "  [-1.64844 0.574219 0.535156 ... 0.0986328 -1.83594 -0.478516]\n",
            "  [-0.396484 0.431641 -1.50781 ... -2.42188 -2.82812 -0.472656]\n",
            "  ...\n",
            "  [0.945312 0.279297 0.980469 ... 0.539062 1.75 -0.196289]\n",
            "  [-0.408203 0.175781 -0.129883 ... 0.890625 0.582031 0.441406]\n",
            "  [-0.773438 -0.265625 0.59375 ... 0.213867 0.902344 -0.679688]]\n",
            "\n",
            " [[-0.320312 1.10156 2.14062 ... -1.39062 0.106445 1.24219]\n",
            "  [-1.64844 0.574219 0.535156 ... 0.0986328 -1.83594 -0.478516]\n",
            "  [-0.396484 0.431641 -1.50781 ... -2.42188 -2.82812 -0.472656]\n",
            "  ...\n",
            "  [0.945312 0.279297 0.980469 ... 0.539062 1.75 -0.196289]\n",
            "  [-0.408203 0.175781 -0.129883 ... 0.890625 0.582031 0.441406]\n",
            "  [-0.773438 -0.265625 0.59375 ... 0.213867 0.902344 -0.679688]]\n",
            "\n",
            " [[-0.320312 1.10156 2.14062 ... -1.39062 0.106445 1.24219]\n",
            "  [-1.64844 0.574219 0.535156 ... 0.0986328 -1.83594 -0.478516]\n",
            "  [-0.396484 0.431641 -1.50781 ... -2.42188 -2.82812 -0.472656]\n",
            "  ...\n",
            "  [0.945312 0.279297 0.980469 ... 0.539062 1.75 -0.196289]\n",
            "  [-0.408203 0.175781 -0.129883 ... 0.890625 0.582031 0.441406]\n",
            "  [-0.773438 -0.265625 0.59375 ... 0.213867 0.902344 -0.679688]]]\n",
            "next_layer_addition_dropped_out=[[[0.169922 1.61719 3.60938 ... -2.4375 0.628906 2.4375]\n",
            "  [-1.9375 0.28125 1.23438 ... 0.941406 -2.26562 -0.402344]\n",
            "  [0.255859 -0.235352 -2.6875 ... -2.92188 -4.125 -0.121094]\n",
            "  ...\n",
            "  [1.64062 -0.15332 1.45312 ... 0.851562 2.35938 -0.902344]\n",
            "  [-0.451172 0.166992 0.203125 ... 1.5 1.07031 1.05469]\n",
            "  [-0.683594 -0.746094 0.824219 ... -0.730469 1.80469 -2.09375]]\n",
            "\n",
            " [[0.169922 1.61719 3.60938 ... -2.4375 0.628906 2.4375]\n",
            "  [-1.9375 0.28125 1.23438 ... 0.941406 -2.26562 -0.402344]\n",
            "  [0.255859 -0.235352 -2.6875 ... -2.92188 -4.125 -0.121094]\n",
            "  ...\n",
            "  [1.64062 -0.15332 1.45312 ... 0.851562 2.35938 -0.902344]\n",
            "  [-0.451172 0.166992 0.203125 ... 1.5 1.07031 1.05469]\n",
            "  [-0.683594 -0.746094 0.824219 ... -0.730469 1.80469 -2.09375]]\n",
            "\n",
            " [[0.169922 1.61719 3.60938 ... -2.4375 0.628906 2.4375]\n",
            "  [-1.9375 0.28125 1.23438 ... 0.941406 -2.26562 -0.402344]\n",
            "  [0.255859 -0.235352 -2.6875 ... -2.92188 -4.125 -0.121094]\n",
            "  ...\n",
            "  [1.64062 -0.15332 1.45312 ... 0.851562 2.35938 -0.902344]\n",
            "  [-0.451172 0.166992 0.203125 ... 1.5 1.07031 1.05469]\n",
            "  [-0.683594 -0.746094 0.824219 ... -0.730469 1.80469 -2.09375]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.169922 1.61719 3.60938 ... -2.4375 0.628906 2.4375]\n",
            "  [-1.9375 0.28125 1.23438 ... 0.941406 -2.26562 -0.402344]\n",
            "  [0.255859 -0.235352 -2.6875 ... -2.92188 -4.125 -0.121094]\n",
            "  ...\n",
            "  [1.64062 -0.15332 1.45312 ... 0.851562 2.35938 -0.902344]\n",
            "  [-0.451172 0.166992 0.203125 ... 1.5 1.07031 1.05469]\n",
            "  [-0.683594 -0.746094 0.824219 ... -0.730469 1.80469 -2.09375]]\n",
            "\n",
            " [[0.169922 1.61719 3.60938 ... -2.4375 0.628906 2.4375]\n",
            "  [-1.9375 0.28125 1.23438 ... 0.941406 -2.26562 -0.402344]\n",
            "  [0.255859 -0.235352 -2.6875 ... -2.92188 -4.125 -0.121094]\n",
            "  ...\n",
            "  [1.64062 -0.15332 1.45312 ... 0.851562 2.35938 -0.902344]\n",
            "  [-0.451172 0.166992 0.203125 ... 1.5 1.07031 1.05469]\n",
            "  [-0.683594 -0.746094 0.824219 ... -0.730469 1.80469 -2.09375]]\n",
            "\n",
            " [[0.169922 1.61719 3.60938 ... -2.4375 0.628906 2.4375]\n",
            "  [-1.9375 0.28125 1.23438 ... 0.941406 -2.26562 -0.402344]\n",
            "  [0.255859 -0.235352 -2.6875 ... -2.92188 -4.125 -0.121094]\n",
            "  ...\n",
            "  [1.64062 -0.15332 1.45312 ... 0.851562 2.35938 -0.902344]\n",
            "  [-0.451172 0.166992 0.203125 ... 1.5 1.07031 1.05469]\n",
            "  [-0.683594 -0.746094 0.824219 ... -0.730469 1.80469 -2.09375]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00]]]]]\n",
            "Successfully ran the model!\n",
            "Output shape: (8, 15, 256128)\n",
            "Tokens: [49688, 736, 1280, 6987, 235292, 108, 4521, 235269, 970, 1503, 603, 20189, 235249, 235265, 108]\n",
            "inputs=[[[0.169922 1.61719 3.60938 ... -2.4375 0.628906 2.4375]\n",
            "  [-1.9375 0.28125 1.23438 ... 0.941406 -2.26562 -0.402344]\n",
            "  [0.255859 -0.235352 -2.6875 ... -2.92188 -4.125 -0.121094]\n",
            "  ...\n",
            "  [1.64062 -0.15332 1.45312 ... 0.851562 2.35938 -0.902344]\n",
            "  [-0.451172 0.166992 0.203125 ... 1.5 1.07031 1.05469]\n",
            "  [-0.683594 -0.746094 0.824219 ... -0.730469 1.80469 -2.09375]]\n",
            "\n",
            " [[0.169922 1.61719 3.60938 ... -2.4375 0.628906 2.4375]\n",
            "  [-1.9375 0.28125 1.23438 ... 0.941406 -2.26562 -0.402344]\n",
            "  [0.255859 -0.235352 -2.6875 ... -2.92188 -4.125 -0.121094]\n",
            "  ...\n",
            "  [1.64062 -0.15332 1.45312 ... 0.851562 2.35938 -0.902344]\n",
            "  [-0.451172 0.166992 0.203125 ... 1.5 1.07031 1.05469]\n",
            "  [-0.683594 -0.746094 0.824219 ... -0.730469 1.80469 -2.09375]]\n",
            "\n",
            " [[0.169922 1.61719 3.60938 ... -2.4375 0.628906 2.4375]\n",
            "  [-1.9375 0.28125 1.23438 ... 0.941406 -2.26562 -0.402344]\n",
            "  [0.255859 -0.235352 -2.6875 ... -2.92188 -4.125 -0.121094]\n",
            "  ...\n",
            "  [1.64062 -0.15332 1.45312 ... 0.851562 2.35938 -0.902344]\n",
            "  [-0.451172 0.166992 0.203125 ... 1.5 1.07031 1.05469]\n",
            "  [-0.683594 -0.746094 0.824219 ... -0.730469 1.80469 -2.09375]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.169922 1.61719 3.60938 ... -2.4375 0.628906 2.4375]\n",
            "  [-1.9375 0.28125 1.23438 ... 0.941406 -2.26562 -0.402344]\n",
            "  [0.255859 -0.235352 -2.6875 ... -2.92188 -4.125 -0.121094]\n",
            "  ...\n",
            "  [1.64062 -0.15332 1.45312 ... 0.851562 2.35938 -0.902344]\n",
            "  [-0.451172 0.166992 0.203125 ... 1.5 1.07031 1.05469]\n",
            "  [-0.683594 -0.746094 0.824219 ... -0.730469 1.80469 -2.09375]]\n",
            "\n",
            " [[0.169922 1.61719 3.60938 ... -2.4375 0.628906 2.4375]\n",
            "  [-1.9375 0.28125 1.23438 ... 0.941406 -2.26562 -0.402344]\n",
            "  [0.255859 -0.235352 -2.6875 ... -2.92188 -4.125 -0.121094]\n",
            "  ...\n",
            "  [1.64062 -0.15332 1.45312 ... 0.851562 2.35938 -0.902344]\n",
            "  [-0.451172 0.166992 0.203125 ... 1.5 1.07031 1.05469]\n",
            "  [-0.683594 -0.746094 0.824219 ... -0.730469 1.80469 -2.09375]]\n",
            "\n",
            " [[0.169922 1.61719 3.60938 ... -2.4375 0.628906 2.4375]\n",
            "  [-1.9375 0.28125 1.23438 ... 0.941406 -2.26562 -0.402344]\n",
            "  [0.255859 -0.235352 -2.6875 ... -2.92188 -4.125 -0.121094]\n",
            "  ...\n",
            "  [1.64062 -0.15332 1.45312 ... 0.851562 2.35938 -0.902344]\n",
            "  [-0.451172 0.166992 0.203125 ... 1.5 1.07031 1.05469]\n",
            "  [-0.683594 -0.746094 0.824219 ... -0.730469 1.80469 -2.09375]]]\n",
            "lnx=[[[0.0864258 0.820312 1.83594 ... -1.24219 0.320312 1.24219]\n",
            "  [-1.10938 0.161133 0.707031 ... 0.539062 -1.29688 -0.230469]\n",
            "  [0.15918 -0.146484 -1.67969 ... -1.82031 -2.57812 -0.0756836]\n",
            "  ...\n",
            "  [1.11719 -0.104492 0.988281 ... 0.582031 1.60938 -0.613281]\n",
            "  [-0.294922 0.109375 0.132812 ... 0.984375 0.703125 0.691406]\n",
            "  [-0.472656 -0.515625 0.570312 ... -0.503906 1.24219 -1.44531]]\n",
            "\n",
            " [[0.0864258 0.820312 1.83594 ... -1.24219 0.320312 1.24219]\n",
            "  [-1.10938 0.161133 0.707031 ... 0.539062 -1.29688 -0.230469]\n",
            "  [0.15918 -0.146484 -1.67969 ... -1.82031 -2.57812 -0.0756836]\n",
            "  ...\n",
            "  [1.11719 -0.104492 0.988281 ... 0.582031 1.60938 -0.613281]\n",
            "  [-0.294922 0.109375 0.132812 ... 0.984375 0.703125 0.691406]\n",
            "  [-0.472656 -0.515625 0.570312 ... -0.503906 1.24219 -1.44531]]\n",
            "\n",
            " [[0.0864258 0.820312 1.83594 ... -1.24219 0.320312 1.24219]\n",
            "  [-1.10938 0.161133 0.707031 ... 0.539062 -1.29688 -0.230469]\n",
            "  [0.15918 -0.146484 -1.67969 ... -1.82031 -2.57812 -0.0756836]\n",
            "  ...\n",
            "  [1.11719 -0.104492 0.988281 ... 0.582031 1.60938 -0.613281]\n",
            "  [-0.294922 0.109375 0.132812 ... 0.984375 0.703125 0.691406]\n",
            "  [-0.472656 -0.515625 0.570312 ... -0.503906 1.24219 -1.44531]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.0864258 0.820312 1.83594 ... -1.24219 0.320312 1.24219]\n",
            "  [-1.10938 0.161133 0.707031 ... 0.539062 -1.29688 -0.230469]\n",
            "  [0.15918 -0.146484 -1.67969 ... -1.82031 -2.57812 -0.0756836]\n",
            "  ...\n",
            "  [1.11719 -0.104492 0.988281 ... 0.582031 1.60938 -0.613281]\n",
            "  [-0.294922 0.109375 0.132812 ... 0.984375 0.703125 0.691406]\n",
            "  [-0.472656 -0.515625 0.570312 ... -0.503906 1.24219 -1.44531]]\n",
            "\n",
            " [[0.0864258 0.820312 1.83594 ... -1.24219 0.320312 1.24219]\n",
            "  [-1.10938 0.161133 0.707031 ... 0.539062 -1.29688 -0.230469]\n",
            "  [0.15918 -0.146484 -1.67969 ... -1.82031 -2.57812 -0.0756836]\n",
            "  ...\n",
            "  [1.11719 -0.104492 0.988281 ... 0.582031 1.60938 -0.613281]\n",
            "  [-0.294922 0.109375 0.132812 ... 0.984375 0.703125 0.691406]\n",
            "  [-0.472656 -0.515625 0.570312 ... -0.503906 1.24219 -1.44531]]\n",
            "\n",
            " [[0.0864258 0.820312 1.83594 ... -1.24219 0.320312 1.24219]\n",
            "  [-1.10938 0.161133 0.707031 ... 0.539062 -1.29688 -0.230469]\n",
            "  [0.15918 -0.146484 -1.67969 ... -1.82031 -2.57812 -0.0756836]\n",
            "  ...\n",
            "  [1.11719 -0.104492 0.988281 ... 0.582031 1.60938 -0.613281]\n",
            "  [-0.294922 0.109375 0.132812 ... 0.984375 0.703125 0.691406]\n",
            "  [-0.472656 -0.515625 0.570312 ... -0.503906 1.24219 -1.44531]]]\n",
            "attention_lnx=[[[0.208984 2.29688 1.25781 ... 1.14844 0.757812 0.126953]\n",
            "  [-0.398438 1.60938 -0.123047 ... 1.65625 0.738281 0.789062]\n",
            "  [0.224609 1.4375 -0.0622559 ... 1.52344 0.660156 0.160156]\n",
            "  ...\n",
            "  [0.0878906 0.960938 0.183594 ... -0.166016 0.152344 -0.275391]\n",
            "  [0.464844 0.53125 0.431641 ... -0.160156 0.175781 -0.507812]\n",
            "  [0.310547 0.9375 0.310547 ... -0.125977 0.0437012 -0.0212402]]\n",
            "\n",
            " [[0.208984 2.29688 1.25781 ... 1.14844 0.757812 0.126953]\n",
            "  [-0.398438 1.60938 -0.123047 ... 1.65625 0.738281 0.789062]\n",
            "  [0.224609 1.4375 -0.0622559 ... 1.52344 0.660156 0.160156]\n",
            "  ...\n",
            "  [0.0878906 0.960938 0.183594 ... -0.166016 0.152344 -0.275391]\n",
            "  [0.464844 0.53125 0.431641 ... -0.160156 0.175781 -0.507812]\n",
            "  [0.310547 0.9375 0.310547 ... -0.125977 0.0437012 -0.0212402]]\n",
            "\n",
            " [[0.208984 2.29688 1.25781 ... 1.14844 0.757812 0.126953]\n",
            "  [-0.398438 1.60938 -0.123047 ... 1.65625 0.738281 0.789062]\n",
            "  [0.224609 1.4375 -0.0622559 ... 1.52344 0.660156 0.160156]\n",
            "  ...\n",
            "  [0.0878906 0.960938 0.183594 ... -0.166016 0.152344 -0.275391]\n",
            "  [0.464844 0.53125 0.431641 ... -0.160156 0.175781 -0.507812]\n",
            "  [0.310547 0.9375 0.310547 ... -0.125977 0.0437012 -0.0212402]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.208984 2.29688 1.25781 ... 1.14844 0.757812 0.126953]\n",
            "  [-0.398438 1.60938 -0.123047 ... 1.65625 0.738281 0.789062]\n",
            "  [0.224609 1.4375 -0.0622559 ... 1.52344 0.660156 0.160156]\n",
            "  ...\n",
            "  [0.0878906 0.960938 0.183594 ... -0.166016 0.152344 -0.275391]\n",
            "  [0.464844 0.53125 0.431641 ... -0.160156 0.175781 -0.507812]\n",
            "  [0.310547 0.9375 0.310547 ... -0.125977 0.0437012 -0.0212402]]\n",
            "\n",
            " [[0.208984 2.29688 1.25781 ... 1.14844 0.757812 0.126953]\n",
            "  [-0.398438 1.60938 -0.123047 ... 1.65625 0.738281 0.789062]\n",
            "  [0.224609 1.4375 -0.0622559 ... 1.52344 0.660156 0.160156]\n",
            "  ...\n",
            "  [0.0878906 0.960938 0.183594 ... -0.166016 0.152344 -0.275391]\n",
            "  [0.464844 0.53125 0.431641 ... -0.160156 0.175781 -0.507812]\n",
            "  [0.310547 0.9375 0.310547 ... -0.125977 0.0437012 -0.0212402]]\n",
            "\n",
            " [[0.208984 2.29688 1.25781 ... 1.14844 0.757812 0.126953]\n",
            "  [-0.398438 1.60938 -0.123047 ... 1.65625 0.738281 0.789062]\n",
            "  [0.224609 1.4375 -0.0622559 ... 1.52344 0.660156 0.160156]\n",
            "  ...\n",
            "  [0.0878906 0.960938 0.183594 ... -0.166016 0.152344 -0.275391]\n",
            "  [0.464844 0.53125 0.431641 ... -0.160156 0.175781 -0.507812]\n",
            "  [0.310547 0.9375 0.310547 ... -0.125977 0.0437012 -0.0212402]]]\n",
            "attn_output=[[[0.171875 1.77344 2.20312 ... -0.582031 0.625 1.15625]\n",
            "  [-1.20312 0.976562 0.574219 ... 1.34375 -0.792969 0.200195]\n",
            "  [0.275391 0.691406 -1.57812 ... -0.800781 -1.99219 0.0227051]\n",
            "  ...\n",
            "  [1.125 0.527344 1.0625 ... 0.447266 1.64062 -0.769531]\n",
            "  [0.00805664 0.435547 0.396484 ... 0.835938 0.777344 0.341797]\n",
            "  [-0.245117 0.124512 0.742188 ... -0.5625 1.21094 -1.38281]]\n",
            "\n",
            " [[0.171875 1.77344 2.20312 ... -0.582031 0.625 1.15625]\n",
            "  [-1.20312 0.976562 0.574219 ... 1.34375 -0.792969 0.200195]\n",
            "  [0.275391 0.691406 -1.57812 ... -0.800781 -1.99219 0.0227051]\n",
            "  ...\n",
            "  [1.125 0.527344 1.0625 ... 0.447266 1.64062 -0.769531]\n",
            "  [0.00805664 0.435547 0.396484 ... 0.835938 0.777344 0.341797]\n",
            "  [-0.245117 0.124512 0.742188 ... -0.5625 1.21094 -1.38281]]\n",
            "\n",
            " [[0.171875 1.77344 2.20312 ... -0.582031 0.625 1.15625]\n",
            "  [-1.20312 0.976562 0.574219 ... 1.34375 -0.792969 0.200195]\n",
            "  [0.275391 0.691406 -1.57812 ... -0.800781 -1.99219 0.0227051]\n",
            "  ...\n",
            "  [1.125 0.527344 1.0625 ... 0.447266 1.64062 -0.769531]\n",
            "  [0.00805664 0.435547 0.396484 ... 0.835938 0.777344 0.341797]\n",
            "  [-0.245117 0.124512 0.742188 ... -0.5625 1.21094 -1.38281]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.171875 1.77344 2.20312 ... -0.582031 0.625 1.15625]\n",
            "  [-1.20312 0.976562 0.574219 ... 1.34375 -0.792969 0.200195]\n",
            "  [0.275391 0.691406 -1.57812 ... -0.800781 -1.99219 0.0227051]\n",
            "  ...\n",
            "  [1.125 0.527344 1.0625 ... 0.447266 1.64062 -0.769531]\n",
            "  [0.00805664 0.435547 0.396484 ... 0.835938 0.777344 0.341797]\n",
            "  [-0.245117 0.124512 0.742188 ... -0.5625 1.21094 -1.38281]]\n",
            "\n",
            " [[0.171875 1.77344 2.20312 ... -0.582031 0.625 1.15625]\n",
            "  [-1.20312 0.976562 0.574219 ... 1.34375 -0.792969 0.200195]\n",
            "  [0.275391 0.691406 -1.57812 ... -0.800781 -1.99219 0.0227051]\n",
            "  ...\n",
            "  [1.125 0.527344 1.0625 ... 0.447266 1.64062 -0.769531]\n",
            "  [0.00805664 0.435547 0.396484 ... 0.835938 0.777344 0.341797]\n",
            "  [-0.245117 0.124512 0.742188 ... -0.5625 1.21094 -1.38281]]\n",
            "\n",
            " [[0.171875 1.77344 2.20312 ... -0.582031 0.625 1.15625]\n",
            "  [-1.20312 0.976562 0.574219 ... 1.34375 -0.792969 0.200195]\n",
            "  [0.275391 0.691406 -1.57812 ... -0.800781 -1.99219 0.0227051]\n",
            "  ...\n",
            "  [1.125 0.527344 1.0625 ... 0.447266 1.64062 -0.769531]\n",
            "  [0.00805664 0.435547 0.396484 ... 0.835938 0.777344 0.341797]\n",
            "  [-0.245117 0.124512 0.742188 ... -0.5625 1.21094 -1.38281]]]\n",
            "next_layer_addition_dropped_out=[[[-0.742188 4.15625 4.3125 ... -1.48438 2.48438 2.15625]\n",
            "  [-2.84375 2.14062 0.945312 ... 2.57812 -1.57812 0.980469]\n",
            "  [0.75 2.32812 -2.48438 ... -1.32812 -3.01562 0.816406]\n",
            "  ...\n",
            "  [2.85938 -0.0493164 1.82812 ... 0.71875 2.23438 -1.15625]\n",
            "  [-0.380859 0.765625 0.972656 ... 1.10156 0.726562 0.292969]\n",
            "  [-1.10938 -0.460938 -0.229492 ... -1.67188 1.42188 -3.375]]\n",
            "\n",
            " [[-0.742188 4.15625 4.3125 ... -1.48438 2.48438 2.15625]\n",
            "  [-2.84375 2.14062 0.945312 ... 2.57812 -1.57812 0.980469]\n",
            "  [0.75 2.32812 -2.48438 ... -1.32812 -3.01562 0.816406]\n",
            "  ...\n",
            "  [2.85938 -0.0493164 1.82812 ... 0.71875 2.23438 -1.15625]\n",
            "  [-0.380859 0.765625 0.972656 ... 1.10156 0.726562 0.292969]\n",
            "  [-1.10938 -0.460938 -0.229492 ... -1.67188 1.42188 -3.375]]\n",
            "\n",
            " [[-0.742188 4.15625 4.3125 ... -1.48438 2.48438 2.15625]\n",
            "  [-2.84375 2.14062 0.945312 ... 2.57812 -1.57812 0.980469]\n",
            "  [0.75 2.32812 -2.48438 ... -1.32812 -3.01562 0.816406]\n",
            "  ...\n",
            "  [2.85938 -0.0493164 1.82812 ... 0.71875 2.23438 -1.15625]\n",
            "  [-0.380859 0.765625 0.972656 ... 1.10156 0.726562 0.292969]\n",
            "  [-1.10938 -0.460938 -0.229492 ... -1.67188 1.42188 -3.375]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.742188 4.15625 4.3125 ... -1.48438 2.48438 2.15625]\n",
            "  [-2.84375 2.14062 0.945312 ... 2.57812 -1.57812 0.980469]\n",
            "  [0.75 2.32812 -2.48438 ... -1.32812 -3.01562 0.816406]\n",
            "  ...\n",
            "  [2.85938 -0.0493164 1.82812 ... 0.71875 2.23438 -1.15625]\n",
            "  [-0.380859 0.765625 0.972656 ... 1.10156 0.726562 0.292969]\n",
            "  [-1.10938 -0.460938 -0.229492 ... -1.67188 1.42188 -3.375]]\n",
            "\n",
            " [[-0.742188 4.15625 4.3125 ... -1.48438 2.48438 2.15625]\n",
            "  [-2.84375 2.14062 0.945312 ... 2.57812 -1.57812 0.980469]\n",
            "  [0.75 2.32812 -2.48438 ... -1.32812 -3.01562 0.816406]\n",
            "  ...\n",
            "  [2.85938 -0.0493164 1.82812 ... 0.71875 2.23438 -1.15625]\n",
            "  [-0.380859 0.765625 0.972656 ... 1.10156 0.726562 0.292969]\n",
            "  [-1.10938 -0.460938 -0.229492 ... -1.67188 1.42188 -3.375]]\n",
            "\n",
            " [[-0.742188 4.15625 4.3125 ... -1.48438 2.48438 2.15625]\n",
            "  [-2.84375 2.14062 0.945312 ... 2.57812 -1.57812 0.980469]\n",
            "  [0.75 2.32812 -2.48438 ... -1.32812 -3.01562 0.816406]\n",
            "  ...\n",
            "  [2.85938 -0.0493164 1.82812 ... 0.71875 2.23438 -1.15625]\n",
            "  [-0.380859 0.765625 0.972656 ... 1.10156 0.726562 0.292969]\n",
            "  [-1.10938 -0.460938 -0.229492 ... -1.67188 1.42188 -3.375]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00]]]]]\n",
            "inputs=[[[-0.742188 4.15625 4.3125 ... -1.48438 2.48438 2.15625]\n",
            "  [-2.84375 2.14062 0.945312 ... 2.57812 -1.57812 0.980469]\n",
            "  [0.75 2.32812 -2.48438 ... -1.32812 -3.01562 0.816406]\n",
            "  ...\n",
            "  [2.85938 -0.0493164 1.82812 ... 0.71875 2.23438 -1.15625]\n",
            "  [-0.380859 0.765625 0.972656 ... 1.10156 0.726562 0.292969]\n",
            "  [-1.10938 -0.460938 -0.229492 ... -1.67188 1.42188 -3.375]]\n",
            "\n",
            " [[-0.742188 4.15625 4.3125 ... -1.48438 2.48438 2.15625]\n",
            "  [-2.84375 2.14062 0.945312 ... 2.57812 -1.57812 0.980469]\n",
            "  [0.75 2.32812 -2.48438 ... -1.32812 -3.01562 0.816406]\n",
            "  ...\n",
            "  [2.85938 -0.0493164 1.82812 ... 0.71875 2.23438 -1.15625]\n",
            "  [-0.380859 0.765625 0.972656 ... 1.10156 0.726562 0.292969]\n",
            "  [-1.10938 -0.460938 -0.229492 ... -1.67188 1.42188 -3.375]]\n",
            "\n",
            " [[-0.742188 4.15625 4.3125 ... -1.48438 2.48438 2.15625]\n",
            "  [-2.84375 2.14062 0.945312 ... 2.57812 -1.57812 0.980469]\n",
            "  [0.75 2.32812 -2.48438 ... -1.32812 -3.01562 0.816406]\n",
            "  ...\n",
            "  [2.85938 -0.0493164 1.82812 ... 0.71875 2.23438 -1.15625]\n",
            "  [-0.380859 0.765625 0.972656 ... 1.10156 0.726562 0.292969]\n",
            "  [-1.10938 -0.460938 -0.229492 ... -1.67188 1.42188 -3.375]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.742188 4.15625 4.3125 ... -1.48438 2.48438 2.15625]\n",
            "  [-2.84375 2.14062 0.945312 ... 2.57812 -1.57812 0.980469]\n",
            "  [0.75 2.32812 -2.48438 ... -1.32812 -3.01562 0.816406]\n",
            "  ...\n",
            "  [2.85938 -0.0493164 1.82812 ... 0.71875 2.23438 -1.15625]\n",
            "  [-0.380859 0.765625 0.972656 ... 1.10156 0.726562 0.292969]\n",
            "  [-1.10938 -0.460938 -0.229492 ... -1.67188 1.42188 -3.375]]\n",
            "\n",
            " [[-0.742188 4.15625 4.3125 ... -1.48438 2.48438 2.15625]\n",
            "  [-2.84375 2.14062 0.945312 ... 2.57812 -1.57812 0.980469]\n",
            "  [0.75 2.32812 -2.48438 ... -1.32812 -3.01562 0.816406]\n",
            "  ...\n",
            "  [2.85938 -0.0493164 1.82812 ... 0.71875 2.23438 -1.15625]\n",
            "  [-0.380859 0.765625 0.972656 ... 1.10156 0.726562 0.292969]\n",
            "  [-1.10938 -0.460938 -0.229492 ... -1.67188 1.42188 -3.375]]\n",
            "\n",
            " [[-0.742188 4.15625 4.3125 ... -1.48438 2.48438 2.15625]\n",
            "  [-2.84375 2.14062 0.945312 ... 2.57812 -1.57812 0.980469]\n",
            "  [0.75 2.32812 -2.48438 ... -1.32812 -3.01562 0.816406]\n",
            "  ...\n",
            "  [2.85938 -0.0493164 1.82812 ... 0.71875 2.23438 -1.15625]\n",
            "  [-0.380859 0.765625 0.972656 ... 1.10156 0.726562 0.292969]\n",
            "  [-1.10938 -0.460938 -0.229492 ... -1.67188 1.42188 -3.375]]]\n",
            "lnx=[[[-0.322266 1.8125 1.875 ... -0.644531 1.07812 0.9375]\n",
            "  [-1.39844 1.05469 0.466797 ... 1.27344 -0.777344 0.484375]\n",
            "  [0.404297 1.25781 -1.34375 ... -0.714844 -1.625 0.441406]\n",
            "  ...\n",
            "  [1.71875 -0.0296631 1.10156 ... 0.431641 1.34375 -0.695312]\n",
            "  [-0.21582 0.435547 0.550781 ... 0.625 0.412109 0.166016]\n",
            "  [-0.660156 -0.275391 -0.136719 ... -0.996094 0.847656 -2.01562]]\n",
            "\n",
            " [[-0.322266 1.8125 1.875 ... -0.644531 1.07812 0.9375]\n",
            "  [-1.39844 1.05469 0.466797 ... 1.27344 -0.777344 0.484375]\n",
            "  [0.404297 1.25781 -1.34375 ... -0.714844 -1.625 0.441406]\n",
            "  ...\n",
            "  [1.71875 -0.0296631 1.10156 ... 0.431641 1.34375 -0.695312]\n",
            "  [-0.21582 0.435547 0.550781 ... 0.625 0.412109 0.166016]\n",
            "  [-0.660156 -0.275391 -0.136719 ... -0.996094 0.847656 -2.01562]]\n",
            "\n",
            " [[-0.322266 1.8125 1.875 ... -0.644531 1.07812 0.9375]\n",
            "  [-1.39844 1.05469 0.466797 ... 1.27344 -0.777344 0.484375]\n",
            "  [0.404297 1.25781 -1.34375 ... -0.714844 -1.625 0.441406]\n",
            "  ...\n",
            "  [1.71875 -0.0296631 1.10156 ... 0.431641 1.34375 -0.695312]\n",
            "  [-0.21582 0.435547 0.550781 ... 0.625 0.412109 0.166016]\n",
            "  [-0.660156 -0.275391 -0.136719 ... -0.996094 0.847656 -2.01562]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.322266 1.8125 1.875 ... -0.644531 1.07812 0.9375]\n",
            "  [-1.39844 1.05469 0.466797 ... 1.27344 -0.777344 0.484375]\n",
            "  [0.404297 1.25781 -1.34375 ... -0.714844 -1.625 0.441406]\n",
            "  ...\n",
            "  [1.71875 -0.0296631 1.10156 ... 0.431641 1.34375 -0.695312]\n",
            "  [-0.21582 0.435547 0.550781 ... 0.625 0.412109 0.166016]\n",
            "  [-0.660156 -0.275391 -0.136719 ... -0.996094 0.847656 -2.01562]]\n",
            "\n",
            " [[-0.322266 1.8125 1.875 ... -0.644531 1.07812 0.9375]\n",
            "  [-1.39844 1.05469 0.466797 ... 1.27344 -0.777344 0.484375]\n",
            "  [0.404297 1.25781 -1.34375 ... -0.714844 -1.625 0.441406]\n",
            "  ...\n",
            "  [1.71875 -0.0296631 1.10156 ... 0.431641 1.34375 -0.695312]\n",
            "  [-0.21582 0.435547 0.550781 ... 0.625 0.412109 0.166016]\n",
            "  [-0.660156 -0.275391 -0.136719 ... -0.996094 0.847656 -2.01562]]\n",
            "\n",
            " [[-0.322266 1.8125 1.875 ... -0.644531 1.07812 0.9375]\n",
            "  [-1.39844 1.05469 0.466797 ... 1.27344 -0.777344 0.484375]\n",
            "  [0.404297 1.25781 -1.34375 ... -0.714844 -1.625 0.441406]\n",
            "  ...\n",
            "  [1.71875 -0.0296631 1.10156 ... 0.431641 1.34375 -0.695312]\n",
            "  [-0.21582 0.435547 0.550781 ... 0.625 0.412109 0.166016]\n",
            "  [-0.660156 -0.275391 -0.136719 ... -0.996094 0.847656 -2.01562]]]\n",
            "positions: [[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
            " [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
            " [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
            " [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
            " [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
            " [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
            " [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
            " [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]]\n",
            "attention_lnx=[[[-0.416016 2.07812 -1.79688 ... -1.35938 0.28125 -0.302734]\n",
            "  [0.191406 1.40625 -1.22656 ... -0.15625 0.412109 0.161133]\n",
            "  [0.226562 1.6875 -0.875 ... -0.542969 0.451172 0.378906]\n",
            "  ...\n",
            "  [0.114746 0.917969 0.196289 ... -0.515625 -0.230469 0.168945]\n",
            "  [-0.0791016 1.10938 0.0332031 ... -0.285156 0.152344 -0.00793457]\n",
            "  [0.125 1.30469 0.443359 ... -0.609375 -0.21875 -0.0576172]]\n",
            "\n",
            " [[-0.416016 2.07812 -1.79688 ... -1.35938 0.28125 -0.302734]\n",
            "  [0.191406 1.40625 -1.22656 ... -0.15625 0.412109 0.161133]\n",
            "  [0.226562 1.6875 -0.875 ... -0.542969 0.451172 0.378906]\n",
            "  ...\n",
            "  [0.114746 0.917969 0.196289 ... -0.515625 -0.230469 0.168945]\n",
            "  [-0.0791016 1.10938 0.0332031 ... -0.285156 0.152344 -0.00793457]\n",
            "  [0.125 1.30469 0.443359 ... -0.609375 -0.21875 -0.0576172]]\n",
            "\n",
            " [[-0.416016 2.07812 -1.79688 ... -1.35938 0.28125 -0.302734]\n",
            "  [0.191406 1.40625 -1.22656 ... -0.15625 0.412109 0.161133]\n",
            "  [0.226562 1.6875 -0.875 ... -0.542969 0.451172 0.378906]\n",
            "  ...\n",
            "  [0.114746 0.917969 0.196289 ... -0.515625 -0.230469 0.168945]\n",
            "  [-0.0791016 1.10938 0.0332031 ... -0.285156 0.152344 -0.00793457]\n",
            "  [0.125 1.30469 0.443359 ... -0.609375 -0.21875 -0.0576172]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.416016 2.07812 -1.79688 ... -1.35938 0.28125 -0.302734]\n",
            "  [0.191406 1.40625 -1.22656 ... -0.15625 0.412109 0.161133]\n",
            "  [0.226562 1.6875 -0.875 ... -0.542969 0.451172 0.378906]\n",
            "  ...\n",
            "  [0.114746 0.917969 0.196289 ... -0.515625 -0.230469 0.168945]\n",
            "  [-0.0791016 1.10938 0.0332031 ... -0.285156 0.152344 -0.00793457]\n",
            "  [0.125 1.30469 0.443359 ... -0.609375 -0.21875 -0.0576172]]\n",
            "\n",
            " [[-0.416016 2.07812 -1.79688 ... -1.35938 0.28125 -0.302734]\n",
            "  [0.191406 1.40625 -1.22656 ... -0.15625 0.412109 0.161133]\n",
            "  [0.226562 1.6875 -0.875 ... -0.542969 0.451172 0.378906]\n",
            "  ...\n",
            "  [0.114746 0.917969 0.196289 ... -0.515625 -0.230469 0.168945]\n",
            "  [-0.0791016 1.10938 0.0332031 ... -0.285156 0.152344 -0.00793457]\n",
            "  [0.125 1.30469 0.443359 ... -0.609375 -0.21875 -0.0576172]]\n",
            "\n",
            " [[-0.416016 2.07812 -1.79688 ... -1.35938 0.28125 -0.302734]\n",
            "  [0.191406 1.40625 -1.22656 ... -0.15625 0.412109 0.161133]\n",
            "  [0.226562 1.6875 -0.875 ... -0.542969 0.451172 0.378906]\n",
            "  ...\n",
            "  [0.114746 0.917969 0.196289 ... -0.515625 -0.230469 0.168945]\n",
            "  [-0.0791016 1.10938 0.0332031 ... -0.285156 0.152344 -0.00793457]\n",
            "  [0.125 1.30469 0.443359 ... -0.609375 -0.21875 -0.0576172]]]\n",
            "attn_output=[[[-0.458984 2.46875 0.996094 ... -1.125 1.09375 0.734375]\n",
            "  [-1.17969 1.57812 -0.126953 ... 1.07812 -0.519531 0.507812]\n",
            "  [0.472656 1.95312 -1.625 ... -0.910156 -1.24219 0.578125]\n",
            "  ...\n",
            "  [1.6875 0.492188 1.15625 ... 0.114746 1.13281 -0.5625]\n",
            "  [-0.248047 1.00781 0.542969 ... 0.439453 0.474609 0.154297]\n",
            "  [-0.558594 0.476562 0.120605 ... -1.28906 0.683594 -1.94531]]\n",
            "\n",
            " [[-0.458984 2.46875 0.996094 ... -1.125 1.09375 0.734375]\n",
            "  [-1.17969 1.57812 -0.126953 ... 1.07812 -0.519531 0.507812]\n",
            "  [0.472656 1.95312 -1.625 ... -0.910156 -1.24219 0.578125]\n",
            "  ...\n",
            "  [1.6875 0.492188 1.15625 ... 0.114746 1.13281 -0.5625]\n",
            "  [-0.248047 1.00781 0.542969 ... 0.439453 0.474609 0.154297]\n",
            "  [-0.558594 0.476562 0.120605 ... -1.28906 0.683594 -1.94531]]\n",
            "\n",
            " [[-0.458984 2.46875 0.996094 ... -1.125 1.09375 0.734375]\n",
            "  [-1.17969 1.57812 -0.126953 ... 1.07812 -0.519531 0.507812]\n",
            "  [0.472656 1.95312 -1.625 ... -0.910156 -1.24219 0.578125]\n",
            "  ...\n",
            "  [1.6875 0.492188 1.15625 ... 0.114746 1.13281 -0.5625]\n",
            "  [-0.248047 1.00781 0.542969 ... 0.439453 0.474609 0.154297]\n",
            "  [-0.558594 0.476562 0.120605 ... -1.28906 0.683594 -1.94531]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.458984 2.46875 0.996094 ... -1.125 1.09375 0.734375]\n",
            "  [-1.17969 1.57812 -0.126953 ... 1.07812 -0.519531 0.507812]\n",
            "  [0.472656 1.95312 -1.625 ... -0.910156 -1.24219 0.578125]\n",
            "  ...\n",
            "  [1.6875 0.492188 1.15625 ... 0.114746 1.13281 -0.5625]\n",
            "  [-0.248047 1.00781 0.542969 ... 0.439453 0.474609 0.154297]\n",
            "  [-0.558594 0.476562 0.120605 ... -1.28906 0.683594 -1.94531]]\n",
            "\n",
            " [[-0.458984 2.46875 0.996094 ... -1.125 1.09375 0.734375]\n",
            "  [-1.17969 1.57812 -0.126953 ... 1.07812 -0.519531 0.507812]\n",
            "  [0.472656 1.95312 -1.625 ... -0.910156 -1.24219 0.578125]\n",
            "  ...\n",
            "  [1.6875 0.492188 1.15625 ... 0.114746 1.13281 -0.5625]\n",
            "  [-0.248047 1.00781 0.542969 ... 0.439453 0.474609 0.154297]\n",
            "  [-0.558594 0.476562 0.120605 ... -1.28906 0.683594 -1.94531]]\n",
            "\n",
            " [[-0.458984 2.46875 0.996094 ... -1.125 1.09375 0.734375]\n",
            "  [-1.17969 1.57812 -0.126953 ... 1.07812 -0.519531 0.507812]\n",
            "  [0.472656 1.95312 -1.625 ... -0.910156 -1.24219 0.578125]\n",
            "  ...\n",
            "  [1.6875 0.492188 1.15625 ... 0.114746 1.13281 -0.5625]\n",
            "  [-0.248047 1.00781 0.542969 ... 0.439453 0.474609 0.154297]\n",
            "  [-0.558594 0.476562 0.120605 ... -1.28906 0.683594 -1.94531]]]\n",
            "next_layer_addition_dropped_out=[[[-0.976562 6.21875 2.90625 ... -2.8125 3.14062 1.25781]\n",
            "  [-1.64844 3.625 -0.259766 ... 2.21875 -1.57812 -0.213867]\n",
            "  [1.01562 3.95312 -2.4375 ... -2.125 -2.39062 0.929688]\n",
            "  ...\n",
            "  [3.57812 0.988281 2.03125 ... 0.460938 2.25 -0.318359]\n",
            "  [-0.738281 1.67188 -0.212891 ... 1.29688 0.667969 0.542969]\n",
            "  [-1.08594 1.78125 1.03125 ... -1.92969 0.792969 -4.34375]]\n",
            "\n",
            " [[-0.976562 6.21875 2.90625 ... -2.8125 3.14062 1.25781]\n",
            "  [-1.64844 3.625 -0.259766 ... 2.21875 -1.57812 -0.213867]\n",
            "  [1.01562 3.95312 -2.4375 ... -2.125 -2.39062 0.929688]\n",
            "  ...\n",
            "  [3.57812 0.988281 2.03125 ... 0.460938 2.25 -0.318359]\n",
            "  [-0.738281 1.67188 -0.212891 ... 1.29688 0.667969 0.542969]\n",
            "  [-1.08594 1.78125 1.03125 ... -1.92969 0.792969 -4.34375]]\n",
            "\n",
            " [[-0.976562 6.21875 2.90625 ... -2.8125 3.14062 1.25781]\n",
            "  [-1.64844 3.625 -0.259766 ... 2.21875 -1.57812 -0.213867]\n",
            "  [1.01562 3.95312 -2.4375 ... -2.125 -2.39062 0.929688]\n",
            "  ...\n",
            "  [3.57812 0.988281 2.03125 ... 0.460938 2.25 -0.318359]\n",
            "  [-0.738281 1.67188 -0.212891 ... 1.29688 0.667969 0.542969]\n",
            "  [-1.08594 1.78125 1.03125 ... -1.92969 0.792969 -4.34375]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.976562 6.21875 2.90625 ... -2.8125 3.14062 1.25781]\n",
            "  [-1.64844 3.625 -0.259766 ... 2.21875 -1.57812 -0.213867]\n",
            "  [1.01562 3.95312 -2.4375 ... -2.125 -2.39062 0.929688]\n",
            "  ...\n",
            "  [3.57812 0.988281 2.03125 ... 0.460938 2.25 -0.318359]\n",
            "  [-0.738281 1.67188 -0.212891 ... 1.29688 0.667969 0.542969]\n",
            "  [-1.08594 1.78125 1.03125 ... -1.92969 0.792969 -4.34375]]\n",
            "\n",
            " [[-0.976562 6.21875 2.90625 ... -2.8125 3.14062 1.25781]\n",
            "  [-1.64844 3.625 -0.259766 ... 2.21875 -1.57812 -0.213867]\n",
            "  [1.01562 3.95312 -2.4375 ... -2.125 -2.39062 0.929688]\n",
            "  ...\n",
            "  [3.57812 0.988281 2.03125 ... 0.460938 2.25 -0.318359]\n",
            "  [-0.738281 1.67188 -0.212891 ... 1.29688 0.667969 0.542969]\n",
            "  [-1.08594 1.78125 1.03125 ... -1.92969 0.792969 -4.34375]]\n",
            "\n",
            " [[-0.976562 6.21875 2.90625 ... -2.8125 3.14062 1.25781]\n",
            "  [-1.64844 3.625 -0.259766 ... 2.21875 -1.57812 -0.213867]\n",
            "  [1.01562 3.95312 -2.4375 ... -2.125 -2.39062 0.929688]\n",
            "  ...\n",
            "  [3.57812 0.988281 2.03125 ... 0.460938 2.25 -0.318359]\n",
            "  [-0.738281 1.67188 -0.212891 ... 1.29688 0.667969 0.542969]\n",
            "  [-1.08594 1.78125 1.03125 ... -1.92969 0.792969 -4.34375]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00]]]]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs=[[[-0.976562 6.21875 2.90625 ... -2.8125 3.14062 1.25781]\n",
            "  [-1.64844 3.625 -0.259766 ... 2.21875 -1.57812 -0.213867]\n",
            "  [1.01562 3.95312 -2.4375 ... -2.125 -2.39062 0.929688]\n",
            "  ...\n",
            "  [3.57812 0.988281 2.03125 ... 0.460938 2.25 -0.318359]\n",
            "  [-0.738281 1.67188 -0.212891 ... 1.29688 0.667969 0.542969]\n",
            "  [-1.08594 1.78125 1.03125 ... -1.92969 0.792969 -4.34375]]\n",
            "\n",
            " [[-0.976562 6.21875 2.90625 ... -2.8125 3.14062 1.25781]\n",
            "  [-1.64844 3.625 -0.259766 ... 2.21875 -1.57812 -0.213867]\n",
            "  [1.01562 3.95312 -2.4375 ... -2.125 -2.39062 0.929688]\n",
            "  ...\n",
            "  [3.57812 0.988281 2.03125 ... 0.460938 2.25 -0.318359]\n",
            "  [-0.738281 1.67188 -0.212891 ... 1.29688 0.667969 0.542969]\n",
            "  [-1.08594 1.78125 1.03125 ... -1.92969 0.792969 -4.34375]]\n",
            "\n",
            " [[-0.976562 6.21875 2.90625 ... -2.8125 3.14062 1.25781]\n",
            "  [-1.64844 3.625 -0.259766 ... 2.21875 -1.57812 -0.213867]\n",
            "  [1.01562 3.95312 -2.4375 ... -2.125 -2.39062 0.929688]\n",
            "  ...\n",
            "  [3.57812 0.988281 2.03125 ... 0.460938 2.25 -0.318359]\n",
            "  [-0.738281 1.67188 -0.212891 ... 1.29688 0.667969 0.542969]\n",
            "  [-1.08594 1.78125 1.03125 ... -1.92969 0.792969 -4.34375]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.976562 6.21875 2.90625 ... -2.8125 3.14062 1.25781]\n",
            "  [-1.64844 3.625 -0.259766 ... 2.21875 -1.57812 -0.213867]\n",
            "  [1.01562 3.95312 -2.4375 ... -2.125 -2.39062 0.929688]\n",
            "  ...\n",
            "  [3.57812 0.988281 2.03125 ... 0.460938 2.25 -0.318359]\n",
            "  [-0.738281 1.67188 -0.212891 ... 1.29688 0.667969 0.542969]\n",
            "  [-1.08594 1.78125 1.03125 ... -1.92969 0.792969 -4.34375]]\n",
            "\n",
            " [[-0.976562 6.21875 2.90625 ... -2.8125 3.14062 1.25781]\n",
            "  [-1.64844 3.625 -0.259766 ... 2.21875 -1.57812 -0.213867]\n",
            "  [1.01562 3.95312 -2.4375 ... -2.125 -2.39062 0.929688]\n",
            "  ...\n",
            "  [3.57812 0.988281 2.03125 ... 0.460938 2.25 -0.318359]\n",
            "  [-0.738281 1.67188 -0.212891 ... 1.29688 0.667969 0.542969]\n",
            "  [-1.08594 1.78125 1.03125 ... -1.92969 0.792969 -4.34375]]\n",
            "\n",
            " [[-0.976562 6.21875 2.90625 ... -2.8125 3.14062 1.25781]\n",
            "  [-1.64844 3.625 -0.259766 ... 2.21875 -1.57812 -0.213867]\n",
            "  [1.01562 3.95312 -2.4375 ... -2.125 -2.39062 0.929688]\n",
            "  ...\n",
            "  [3.57812 0.988281 2.03125 ... 0.460938 2.25 -0.318359]\n",
            "  [-0.738281 1.67188 -0.212891 ... 1.29688 0.667969 0.542969]\n",
            "  [-1.08594 1.78125 1.03125 ... -1.92969 0.792969 -4.34375]]]\n",
            "lnx=[[[-0.375 2.39062 1.11719 ... -1.07812 1.21094 0.484375]\n",
            "  [-0.714844 1.57031 -0.112793 ... 0.964844 -0.683594 -0.0927734]\n",
            "  [0.470703 1.83594 -1.13281 ... -0.984375 -1.10938 0.431641]\n",
            "  ...\n",
            "  [1.88281 0.519531 1.07031 ... 0.242188 1.17969 -0.166992]\n",
            "  [-0.376953 0.851562 -0.108398 ... 0.660156 0.339844 0.277344]\n",
            "  [-0.574219 0.941406 0.546875 ... -1.02344 0.419922 -2.29688]]\n",
            "\n",
            " [[-0.375 2.39062 1.11719 ... -1.07812 1.21094 0.484375]\n",
            "  [-0.714844 1.57031 -0.112793 ... 0.964844 -0.683594 -0.0927734]\n",
            "  [0.470703 1.83594 -1.13281 ... -0.984375 -1.10938 0.431641]\n",
            "  ...\n",
            "  [1.88281 0.519531 1.07031 ... 0.242188 1.17969 -0.166992]\n",
            "  [-0.376953 0.851562 -0.108398 ... 0.660156 0.339844 0.277344]\n",
            "  [-0.574219 0.941406 0.546875 ... -1.02344 0.419922 -2.29688]]\n",
            "\n",
            " [[-0.375 2.39062 1.11719 ... -1.07812 1.21094 0.484375]\n",
            "  [-0.714844 1.57031 -0.112793 ... 0.964844 -0.683594 -0.0927734]\n",
            "  [0.470703 1.83594 -1.13281 ... -0.984375 -1.10938 0.431641]\n",
            "  ...\n",
            "  [1.88281 0.519531 1.07031 ... 0.242188 1.17969 -0.166992]\n",
            "  [-0.376953 0.851562 -0.108398 ... 0.660156 0.339844 0.277344]\n",
            "  [-0.574219 0.941406 0.546875 ... -1.02344 0.419922 -2.29688]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.375 2.39062 1.11719 ... -1.07812 1.21094 0.484375]\n",
            "  [-0.714844 1.57031 -0.112793 ... 0.964844 -0.683594 -0.0927734]\n",
            "  [0.470703 1.83594 -1.13281 ... -0.984375 -1.10938 0.431641]\n",
            "  ...\n",
            "  [1.88281 0.519531 1.07031 ... 0.242188 1.17969 -0.166992]\n",
            "  [-0.376953 0.851562 -0.108398 ... 0.660156 0.339844 0.277344]\n",
            "  [-0.574219 0.941406 0.546875 ... -1.02344 0.419922 -2.29688]]\n",
            "\n",
            " [[-0.375 2.39062 1.11719 ... -1.07812 1.21094 0.484375]\n",
            "  [-0.714844 1.57031 -0.112793 ... 0.964844 -0.683594 -0.0927734]\n",
            "  [0.470703 1.83594 -1.13281 ... -0.984375 -1.10938 0.431641]\n",
            "  ...\n",
            "  [1.88281 0.519531 1.07031 ... 0.242188 1.17969 -0.166992]\n",
            "  [-0.376953 0.851562 -0.108398 ... 0.660156 0.339844 0.277344]\n",
            "  [-0.574219 0.941406 0.546875 ... -1.02344 0.419922 -2.29688]]\n",
            "\n",
            " [[-0.375 2.39062 1.11719 ... -1.07812 1.21094 0.484375]\n",
            "  [-0.714844 1.57031 -0.112793 ... 0.964844 -0.683594 -0.0927734]\n",
            "  [0.470703 1.83594 -1.13281 ... -0.984375 -1.10938 0.431641]\n",
            "  ...\n",
            "  [1.88281 0.519531 1.07031 ... 0.242188 1.17969 -0.166992]\n",
            "  [-0.376953 0.851562 -0.108398 ... 0.660156 0.339844 0.277344]\n",
            "  [-0.574219 0.941406 0.546875 ... -1.02344 0.419922 -2.29688]]]\n",
            "attention_lnx=[[[-0.933594 -0.357422 -0.636719 ... 0.773438 0.355469 1.69531]\n",
            "  [-0.773438 -0.133789 -0.200195 ... -0.457031 1.10156 0.90625]\n",
            "  [-0.527344 0.337891 -0.464844 ... -0.333984 1.22656 1.27344]\n",
            "  ...\n",
            "  [-0.181641 0.640625 -0.769531 ... 0.287109 0.675781 0.5625]\n",
            "  [-0.261719 0.103027 -0.660156 ... -0.283203 0.601562 0.173828]\n",
            "  [-0.208984 0.498047 -0.632812 ... 0.261719 0.349609 0.130859]]\n",
            "\n",
            " [[-0.933594 -0.357422 -0.636719 ... 0.773438 0.355469 1.69531]\n",
            "  [-0.773438 -0.133789 -0.200195 ... -0.457031 1.10156 0.90625]\n",
            "  [-0.527344 0.337891 -0.464844 ... -0.333984 1.22656 1.27344]\n",
            "  ...\n",
            "  [-0.181641 0.640625 -0.769531 ... 0.287109 0.675781 0.5625]\n",
            "  [-0.261719 0.103027 -0.660156 ... -0.283203 0.601562 0.173828]\n",
            "  [-0.208984 0.498047 -0.632812 ... 0.261719 0.349609 0.130859]]\n",
            "\n",
            " [[-0.933594 -0.357422 -0.636719 ... 0.773438 0.355469 1.69531]\n",
            "  [-0.773438 -0.133789 -0.200195 ... -0.457031 1.10156 0.90625]\n",
            "  [-0.527344 0.337891 -0.464844 ... -0.333984 1.22656 1.27344]\n",
            "  ...\n",
            "  [-0.181641 0.640625 -0.769531 ... 0.287109 0.675781 0.5625]\n",
            "  [-0.261719 0.103027 -0.660156 ... -0.283203 0.601562 0.173828]\n",
            "  [-0.208984 0.498047 -0.632812 ... 0.261719 0.349609 0.130859]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.933594 -0.357422 -0.636719 ... 0.773438 0.355469 1.69531]\n",
            "  [-0.773438 -0.133789 -0.200195 ... -0.457031 1.10156 0.90625]\n",
            "  [-0.527344 0.337891 -0.464844 ... -0.333984 1.22656 1.27344]\n",
            "  ...\n",
            "  [-0.181641 0.640625 -0.769531 ... 0.287109 0.675781 0.5625]\n",
            "  [-0.261719 0.103027 -0.660156 ... -0.283203 0.601562 0.173828]\n",
            "  [-0.208984 0.498047 -0.632812 ... 0.261719 0.349609 0.130859]]\n",
            "\n",
            " [[-0.933594 -0.357422 -0.636719 ... 0.773438 0.355469 1.69531]\n",
            "  [-0.773438 -0.133789 -0.200195 ... -0.457031 1.10156 0.90625]\n",
            "  [-0.527344 0.337891 -0.464844 ... -0.333984 1.22656 1.27344]\n",
            "  ...\n",
            "  [-0.181641 0.640625 -0.769531 ... 0.287109 0.675781 0.5625]\n",
            "  [-0.261719 0.103027 -0.660156 ... -0.283203 0.601562 0.173828]\n",
            "  [-0.208984 0.498047 -0.632812 ... 0.261719 0.349609 0.130859]]\n",
            "\n",
            " [[-0.933594 -0.357422 -0.636719 ... 0.773438 0.355469 1.69531]\n",
            "  [-0.773438 -0.133789 -0.200195 ... -0.457031 1.10156 0.90625]\n",
            "  [-0.527344 0.337891 -0.464844 ... -0.333984 1.22656 1.27344]\n",
            "  ...\n",
            "  [-0.181641 0.640625 -0.769531 ... 0.287109 0.675781 0.5625]\n",
            "  [-0.261719 0.103027 -0.660156 ... -0.283203 0.601562 0.173828]\n",
            "  [-0.208984 0.498047 -0.632812 ... 0.261719 0.349609 0.130859]]]\n",
            "attn_output=[[[-0.683594 2.10938 0.8125 ... -0.734375 1.25781 1.0625]\n",
            "  [-1 1.4375 -0.19043 ... 0.730469 -0.196289 0.287109]\n",
            "  [0.21582 1.89844 -1.28906 ... -1.08594 -0.515625 0.976562]\n",
            "  ...\n",
            "  [1.71875 0.824219 0.644531 ... 0.380859 1.48438 0.124512]\n",
            "  [-0.490234 0.867188 -0.427734 ... 0.498047 0.621094 0.351562]\n",
            "  [-0.664062 1.17188 0.204102 ... -0.851562 0.585938 -2.15625]]\n",
            "\n",
            " [[-0.683594 2.10938 0.8125 ... -0.734375 1.25781 1.0625]\n",
            "  [-1 1.4375 -0.19043 ... 0.730469 -0.196289 0.287109]\n",
            "  [0.21582 1.89844 -1.28906 ... -1.08594 -0.515625 0.976562]\n",
            "  ...\n",
            "  [1.71875 0.824219 0.644531 ... 0.380859 1.48438 0.124512]\n",
            "  [-0.490234 0.867188 -0.427734 ... 0.498047 0.621094 0.351562]\n",
            "  [-0.664062 1.17188 0.204102 ... -0.851562 0.585938 -2.15625]]\n",
            "\n",
            " [[-0.683594 2.10938 0.8125 ... -0.734375 1.25781 1.0625]\n",
            "  [-1 1.4375 -0.19043 ... 0.730469 -0.196289 0.287109]\n",
            "  [0.21582 1.89844 -1.28906 ... -1.08594 -0.515625 0.976562]\n",
            "  ...\n",
            "  [1.71875 0.824219 0.644531 ... 0.380859 1.48438 0.124512]\n",
            "  [-0.490234 0.867188 -0.427734 ... 0.498047 0.621094 0.351562]\n",
            "  [-0.664062 1.17188 0.204102 ... -0.851562 0.585938 -2.15625]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.683594 2.10938 0.8125 ... -0.734375 1.25781 1.0625]\n",
            "  [-1 1.4375 -0.19043 ... 0.730469 -0.196289 0.287109]\n",
            "  [0.21582 1.89844 -1.28906 ... -1.08594 -0.515625 0.976562]\n",
            "  ...\n",
            "  [1.71875 0.824219 0.644531 ... 0.380859 1.48438 0.124512]\n",
            "  [-0.490234 0.867188 -0.427734 ... 0.498047 0.621094 0.351562]\n",
            "  [-0.664062 1.17188 0.204102 ... -0.851562 0.585938 -2.15625]]\n",
            "\n",
            " [[-0.683594 2.10938 0.8125 ... -0.734375 1.25781 1.0625]\n",
            "  [-1 1.4375 -0.19043 ... 0.730469 -0.196289 0.287109]\n",
            "  [0.21582 1.89844 -1.28906 ... -1.08594 -0.515625 0.976562]\n",
            "  ...\n",
            "  [1.71875 0.824219 0.644531 ... 0.380859 1.48438 0.124512]\n",
            "  [-0.490234 0.867188 -0.427734 ... 0.498047 0.621094 0.351562]\n",
            "  [-0.664062 1.17188 0.204102 ... -0.851562 0.585938 -2.15625]]\n",
            "\n",
            " [[-0.683594 2.10938 0.8125 ... -0.734375 1.25781 1.0625]\n",
            "  [-1 1.4375 -0.19043 ... 0.730469 -0.196289 0.287109]\n",
            "  [0.21582 1.89844 -1.28906 ... -1.08594 -0.515625 0.976562]\n",
            "  ...\n",
            "  [1.71875 0.824219 0.644531 ... 0.380859 1.48438 0.124512]\n",
            "  [-0.490234 0.867188 -0.427734 ... 0.498047 0.621094 0.351562]\n",
            "  [-0.664062 1.17188 0.204102 ... -0.851562 0.585938 -2.15625]]]\n",
            "next_layer_addition_dropped_out=[[[-1.04688 6.46875 2.40625 ... -1.5625 4.46875 3.17188]\n",
            "  [-3.23438 3.3125 -0.859375 ... 2.8125 0.203125 1.42188]\n",
            "  [-0.917969 4.1875 -2.73438 ... -2.82812 -0.722656 2.0625]\n",
            "  ...\n",
            "  [3.59375 2.09375 0.242188 ... 0.847656 2.01562 -0.148438]\n",
            "  [-0.488281 1.00781 0.0305176 ... 0.992188 0.9375 -0.0629883]\n",
            "  [-1.5 2.5625 1.29688 ... 0.220703 0.667969 -3.51562]]\n",
            "\n",
            " [[-1.04688 6.46875 2.40625 ... -1.5625 4.46875 3.17188]\n",
            "  [-3.23438 3.3125 -0.859375 ... 2.8125 0.203125 1.42188]\n",
            "  [-0.917969 4.1875 -2.73438 ... -2.82812 -0.722656 2.0625]\n",
            "  ...\n",
            "  [3.59375 2.09375 0.242188 ... 0.847656 2.01562 -0.148438]\n",
            "  [-0.488281 1.00781 0.0305176 ... 0.992188 0.9375 -0.0629883]\n",
            "  [-1.5 2.5625 1.29688 ... 0.220703 0.667969 -3.51562]]\n",
            "\n",
            " [[-1.04688 6.46875 2.40625 ... -1.5625 4.46875 3.17188]\n",
            "  [-3.23438 3.3125 -0.859375 ... 2.8125 0.203125 1.42188]\n",
            "  [-0.917969 4.1875 -2.73438 ... -2.82812 -0.722656 2.0625]\n",
            "  ...\n",
            "  [3.59375 2.09375 0.242188 ... 0.847656 2.01562 -0.148438]\n",
            "  [-0.488281 1.00781 0.0305176 ... 0.992188 0.9375 -0.0629883]\n",
            "  [-1.5 2.5625 1.29688 ... 0.220703 0.667969 -3.51562]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.04688 6.46875 2.40625 ... -1.5625 4.46875 3.17188]\n",
            "  [-3.23438 3.3125 -0.859375 ... 2.8125 0.203125 1.42188]\n",
            "  [-0.917969 4.1875 -2.73438 ... -2.82812 -0.722656 2.0625]\n",
            "  ...\n",
            "  [3.59375 2.09375 0.242188 ... 0.847656 2.01562 -0.148438]\n",
            "  [-0.488281 1.00781 0.0305176 ... 0.992188 0.9375 -0.0629883]\n",
            "  [-1.5 2.5625 1.29688 ... 0.220703 0.667969 -3.51562]]\n",
            "\n",
            " [[-1.04688 6.46875 2.40625 ... -1.5625 4.46875 3.17188]\n",
            "  [-3.23438 3.3125 -0.859375 ... 2.8125 0.203125 1.42188]\n",
            "  [-0.917969 4.1875 -2.73438 ... -2.82812 -0.722656 2.0625]\n",
            "  ...\n",
            "  [3.59375 2.09375 0.242188 ... 0.847656 2.01562 -0.148438]\n",
            "  [-0.488281 1.00781 0.0305176 ... 0.992188 0.9375 -0.0629883]\n",
            "  [-1.5 2.5625 1.29688 ... 0.220703 0.667969 -3.51562]]\n",
            "\n",
            " [[-1.04688 6.46875 2.40625 ... -1.5625 4.46875 3.17188]\n",
            "  [-3.23438 3.3125 -0.859375 ... 2.8125 0.203125 1.42188]\n",
            "  [-0.917969 4.1875 -2.73438 ... -2.82812 -0.722656 2.0625]\n",
            "  ...\n",
            "  [3.59375 2.09375 0.242188 ... 0.847656 2.01562 -0.148438]\n",
            "  [-0.488281 1.00781 0.0305176 ... 0.992188 0.9375 -0.0629883]\n",
            "  [-1.5 2.5625 1.29688 ... 0.220703 0.667969 -3.51562]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00]]]]]\n",
            "inputs=[[[-1.04688 6.46875 2.40625 ... -1.5625 4.46875 3.17188]\n",
            "  [-3.23438 3.3125 -0.859375 ... 2.8125 0.203125 1.42188]\n",
            "  [-0.917969 4.1875 -2.73438 ... -2.82812 -0.722656 2.0625]\n",
            "  ...\n",
            "  [3.59375 2.09375 0.242188 ... 0.847656 2.01562 -0.148438]\n",
            "  [-0.488281 1.00781 0.0305176 ... 0.992188 0.9375 -0.0629883]\n",
            "  [-1.5 2.5625 1.29688 ... 0.220703 0.667969 -3.51562]]\n",
            "\n",
            " [[-1.04688 6.46875 2.40625 ... -1.5625 4.46875 3.17188]\n",
            "  [-3.23438 3.3125 -0.859375 ... 2.8125 0.203125 1.42188]\n",
            "  [-0.917969 4.1875 -2.73438 ... -2.82812 -0.722656 2.0625]\n",
            "  ...\n",
            "  [3.59375 2.09375 0.242188 ... 0.847656 2.01562 -0.148438]\n",
            "  [-0.488281 1.00781 0.0305176 ... 0.992188 0.9375 -0.0629883]\n",
            "  [-1.5 2.5625 1.29688 ... 0.220703 0.667969 -3.51562]]\n",
            "\n",
            " [[-1.04688 6.46875 2.40625 ... -1.5625 4.46875 3.17188]\n",
            "  [-3.23438 3.3125 -0.859375 ... 2.8125 0.203125 1.42188]\n",
            "  [-0.917969 4.1875 -2.73438 ... -2.82812 -0.722656 2.0625]\n",
            "  ...\n",
            "  [3.59375 2.09375 0.242188 ... 0.847656 2.01562 -0.148438]\n",
            "  [-0.488281 1.00781 0.0305176 ... 0.992188 0.9375 -0.0629883]\n",
            "  [-1.5 2.5625 1.29688 ... 0.220703 0.667969 -3.51562]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.04688 6.46875 2.40625 ... -1.5625 4.46875 3.17188]\n",
            "  [-3.23438 3.3125 -0.859375 ... 2.8125 0.203125 1.42188]\n",
            "  [-0.917969 4.1875 -2.73438 ... -2.82812 -0.722656 2.0625]\n",
            "  ...\n",
            "  [3.59375 2.09375 0.242188 ... 0.847656 2.01562 -0.148438]\n",
            "  [-0.488281 1.00781 0.0305176 ... 0.992188 0.9375 -0.0629883]\n",
            "  [-1.5 2.5625 1.29688 ... 0.220703 0.667969 -3.51562]]\n",
            "\n",
            " [[-1.04688 6.46875 2.40625 ... -1.5625 4.46875 3.17188]\n",
            "  [-3.23438 3.3125 -0.859375 ... 2.8125 0.203125 1.42188]\n",
            "  [-0.917969 4.1875 -2.73438 ... -2.82812 -0.722656 2.0625]\n",
            "  ...\n",
            "  [3.59375 2.09375 0.242188 ... 0.847656 2.01562 -0.148438]\n",
            "  [-0.488281 1.00781 0.0305176 ... 0.992188 0.9375 -0.0629883]\n",
            "  [-1.5 2.5625 1.29688 ... 0.220703 0.667969 -3.51562]]\n",
            "\n",
            " [[-1.04688 6.46875 2.40625 ... -1.5625 4.46875 3.17188]\n",
            "  [-3.23438 3.3125 -0.859375 ... 2.8125 0.203125 1.42188]\n",
            "  [-0.917969 4.1875 -2.73438 ... -2.82812 -0.722656 2.0625]\n",
            "  ...\n",
            "  [3.59375 2.09375 0.242188 ... 0.847656 2.01562 -0.148438]\n",
            "  [-0.488281 1.00781 0.0305176 ... 0.992188 0.9375 -0.0629883]\n",
            "  [-1.5 2.5625 1.29688 ... 0.220703 0.667969 -3.51562]]]\n",
            "lnx=[[[-0.367188 2.26562 0.84375 ... -0.546875 1.57031 1.10938]\n",
            "  [-1.29688 1.32812 -0.34375 ... 1.125 0.0810547 0.570312]\n",
            "  [-0.388672 1.77344 -1.15625 ... -1.19531 -0.306641 0.875]\n",
            "  ...\n",
            "  [1.73438 1.00781 0.116699 ... 0.408203 0.972656 -0.0717773]\n",
            "  [-0.225586 0.464844 0.0140991 ... 0.457031 0.431641 -0.0290527]\n",
            "  [-0.71875 1.22656 0.621094 ... 0.105957 0.320312 -1.6875]]\n",
            "\n",
            " [[-0.367188 2.26562 0.84375 ... -0.546875 1.57031 1.10938]\n",
            "  [-1.29688 1.32812 -0.34375 ... 1.125 0.0810547 0.570312]\n",
            "  [-0.388672 1.77344 -1.15625 ... -1.19531 -0.306641 0.875]\n",
            "  ...\n",
            "  [1.73438 1.00781 0.116699 ... 0.408203 0.972656 -0.0717773]\n",
            "  [-0.225586 0.464844 0.0140991 ... 0.457031 0.431641 -0.0290527]\n",
            "  [-0.71875 1.22656 0.621094 ... 0.105957 0.320312 -1.6875]]\n",
            "\n",
            " [[-0.367188 2.26562 0.84375 ... -0.546875 1.57031 1.10938]\n",
            "  [-1.29688 1.32812 -0.34375 ... 1.125 0.0810547 0.570312]\n",
            "  [-0.388672 1.77344 -1.15625 ... -1.19531 -0.306641 0.875]\n",
            "  ...\n",
            "  [1.73438 1.00781 0.116699 ... 0.408203 0.972656 -0.0717773]\n",
            "  [-0.225586 0.464844 0.0140991 ... 0.457031 0.431641 -0.0290527]\n",
            "  [-0.71875 1.22656 0.621094 ... 0.105957 0.320312 -1.6875]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.367188 2.26562 0.84375 ... -0.546875 1.57031 1.10938]\n",
            "  [-1.29688 1.32812 -0.34375 ... 1.125 0.0810547 0.570312]\n",
            "  [-0.388672 1.77344 -1.15625 ... -1.19531 -0.306641 0.875]\n",
            "  ...\n",
            "  [1.73438 1.00781 0.116699 ... 0.408203 0.972656 -0.0717773]\n",
            "  [-0.225586 0.464844 0.0140991 ... 0.457031 0.431641 -0.0290527]\n",
            "  [-0.71875 1.22656 0.621094 ... 0.105957 0.320312 -1.6875]]\n",
            "\n",
            " [[-0.367188 2.26562 0.84375 ... -0.546875 1.57031 1.10938]\n",
            "  [-1.29688 1.32812 -0.34375 ... 1.125 0.0810547 0.570312]\n",
            "  [-0.388672 1.77344 -1.15625 ... -1.19531 -0.306641 0.875]\n",
            "  ...\n",
            "  [1.73438 1.00781 0.116699 ... 0.408203 0.972656 -0.0717773]\n",
            "  [-0.225586 0.464844 0.0140991 ... 0.457031 0.431641 -0.0290527]\n",
            "  [-0.71875 1.22656 0.621094 ... 0.105957 0.320312 -1.6875]]\n",
            "\n",
            " [[-0.367188 2.26562 0.84375 ... -0.546875 1.57031 1.10938]\n",
            "  [-1.29688 1.32812 -0.34375 ... 1.125 0.0810547 0.570312]\n",
            "  [-0.388672 1.77344 -1.15625 ... -1.19531 -0.306641 0.875]\n",
            "  ...\n",
            "  [1.73438 1.00781 0.116699 ... 0.408203 0.972656 -0.0717773]\n",
            "  [-0.225586 0.464844 0.0140991 ... 0.457031 0.431641 -0.0290527]\n",
            "  [-0.71875 1.22656 0.621094 ... 0.105957 0.320312 -1.6875]]]\n",
            "attention_lnx=[[[0.400391 0.617188 -0.351562 ... -0.474609 -1.5625 0.124512]\n",
            "  [-0.0549316 1.28125 -0.273438 ... -1.32812 -0.183594 0.330078]\n",
            "  [0.546875 1.16406 0.0932617 ... -0.546875 -0.00668335 0.757812]\n",
            "  ...\n",
            "  [0.855469 1.32031 0.251953 ... -0.357422 0.25 0.404297]\n",
            "  [0.237305 1.48438 0.539062 ... -0.243164 0.132812 0.396484]\n",
            "  [0.375 1.38281 0.523438 ... -0.558594 0.390625 0.423828]]\n",
            "\n",
            " [[0.400391 0.617188 -0.351562 ... -0.474609 -1.5625 0.124512]\n",
            "  [-0.0549316 1.28125 -0.273438 ... -1.32812 -0.183594 0.330078]\n",
            "  [0.546875 1.16406 0.0932617 ... -0.546875 -0.00668335 0.757812]\n",
            "  ...\n",
            "  [0.855469 1.32031 0.251953 ... -0.357422 0.25 0.404297]\n",
            "  [0.237305 1.48438 0.539062 ... -0.243164 0.132812 0.396484]\n",
            "  [0.375 1.38281 0.523438 ... -0.558594 0.390625 0.423828]]\n",
            "\n",
            " [[0.400391 0.617188 -0.351562 ... -0.474609 -1.5625 0.124512]\n",
            "  [-0.0549316 1.28125 -0.273438 ... -1.32812 -0.183594 0.330078]\n",
            "  [0.546875 1.16406 0.0932617 ... -0.546875 -0.00668335 0.757812]\n",
            "  ...\n",
            "  [0.855469 1.32031 0.251953 ... -0.357422 0.25 0.404297]\n",
            "  [0.237305 1.48438 0.539062 ... -0.243164 0.132812 0.396484]\n",
            "  [0.375 1.38281 0.523438 ... -0.558594 0.390625 0.423828]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.400391 0.617188 -0.351562 ... -0.474609 -1.5625 0.124512]\n",
            "  [-0.0549316 1.28125 -0.273438 ... -1.32812 -0.183594 0.330078]\n",
            "  [0.546875 1.16406 0.0932617 ... -0.546875 -0.00668335 0.757812]\n",
            "  ...\n",
            "  [0.855469 1.32031 0.251953 ... -0.357422 0.25 0.404297]\n",
            "  [0.237305 1.48438 0.539062 ... -0.243164 0.132812 0.396484]\n",
            "  [0.375 1.38281 0.523438 ... -0.558594 0.390625 0.423828]]\n",
            "\n",
            " [[0.400391 0.617188 -0.351562 ... -0.474609 -1.5625 0.124512]\n",
            "  [-0.0549316 1.28125 -0.273438 ... -1.32812 -0.183594 0.330078]\n",
            "  [0.546875 1.16406 0.0932617 ... -0.546875 -0.00668335 0.757812]\n",
            "  ...\n",
            "  [0.855469 1.32031 0.251953 ... -0.357422 0.25 0.404297]\n",
            "  [0.237305 1.48438 0.539062 ... -0.243164 0.132812 0.396484]\n",
            "  [0.375 1.38281 0.523438 ... -0.558594 0.390625 0.423828]]\n",
            "\n",
            " [[0.400391 0.617188 -0.351562 ... -0.474609 -1.5625 0.124512]\n",
            "  [-0.0549316 1.28125 -0.273438 ... -1.32812 -0.183594 0.330078]\n",
            "  [0.546875 1.16406 0.0932617 ... -0.546875 -0.00668335 0.757812]\n",
            "  ...\n",
            "  [0.855469 1.32031 0.251953 ... -0.357422 0.25 0.404297]\n",
            "  [0.237305 1.48438 0.539062 ... -0.243164 0.132812 0.396484]\n",
            "  [0.375 1.38281 0.523438 ... -0.558594 0.390625 0.423828]]]\n",
            "attn_output=[[[-0.211914 2.32812 0.679688 ... -0.667969 0.957031 1.08594]\n",
            "  [-1.23438 1.71875 -0.423828 ... 0.554688 0.00741577 0.65625]\n",
            "  [-0.149414 2.15625 -1.0625 ... -1.35938 -0.292969 1.13281]\n",
            "  ...\n",
            "  [2.04688 1.58594 0.228516 ... 0.226562 1.04688 0.118164]\n",
            "  [-0.11084 1.10938 0.251953 ... 0.332031 0.472656 0.146484]\n",
            "  [-0.519531 1.8125 0.839844 ... -0.15625 0.486328 -1.42969]]\n",
            "\n",
            " [[-0.211914 2.32812 0.679688 ... -0.667969 0.957031 1.08594]\n",
            "  [-1.23438 1.71875 -0.423828 ... 0.554688 0.00741577 0.65625]\n",
            "  [-0.149414 2.15625 -1.0625 ... -1.35938 -0.292969 1.13281]\n",
            "  ...\n",
            "  [2.04688 1.58594 0.228516 ... 0.226562 1.04688 0.118164]\n",
            "  [-0.11084 1.10938 0.251953 ... 0.332031 0.472656 0.146484]\n",
            "  [-0.519531 1.8125 0.839844 ... -0.15625 0.486328 -1.42969]]\n",
            "\n",
            " [[-0.211914 2.32812 0.679688 ... -0.667969 0.957031 1.08594]\n",
            "  [-1.23438 1.71875 -0.423828 ... 0.554688 0.00741577 0.65625]\n",
            "  [-0.149414 2.15625 -1.0625 ... -1.35938 -0.292969 1.13281]\n",
            "  ...\n",
            "  [2.04688 1.58594 0.228516 ... 0.226562 1.04688 0.118164]\n",
            "  [-0.11084 1.10938 0.251953 ... 0.332031 0.472656 0.146484]\n",
            "  [-0.519531 1.8125 0.839844 ... -0.15625 0.486328 -1.42969]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.211914 2.32812 0.679688 ... -0.667969 0.957031 1.08594]\n",
            "  [-1.23438 1.71875 -0.423828 ... 0.554688 0.00741577 0.65625]\n",
            "  [-0.149414 2.15625 -1.0625 ... -1.35938 -0.292969 1.13281]\n",
            "  ...\n",
            "  [2.04688 1.58594 0.228516 ... 0.226562 1.04688 0.118164]\n",
            "  [-0.11084 1.10938 0.251953 ... 0.332031 0.472656 0.146484]\n",
            "  [-0.519531 1.8125 0.839844 ... -0.15625 0.486328 -1.42969]]\n",
            "\n",
            " [[-0.211914 2.32812 0.679688 ... -0.667969 0.957031 1.08594]\n",
            "  [-1.23438 1.71875 -0.423828 ... 0.554688 0.00741577 0.65625]\n",
            "  [-0.149414 2.15625 -1.0625 ... -1.35938 -0.292969 1.13281]\n",
            "  ...\n",
            "  [2.04688 1.58594 0.228516 ... 0.226562 1.04688 0.118164]\n",
            "  [-0.11084 1.10938 0.251953 ... 0.332031 0.472656 0.146484]\n",
            "  [-0.519531 1.8125 0.839844 ... -0.15625 0.486328 -1.42969]]\n",
            "\n",
            " [[-0.211914 2.32812 0.679688 ... -0.667969 0.957031 1.08594]\n",
            "  [-1.23438 1.71875 -0.423828 ... 0.554688 0.00741577 0.65625]\n",
            "  [-0.149414 2.15625 -1.0625 ... -1.35938 -0.292969 1.13281]\n",
            "  ...\n",
            "  [2.04688 1.58594 0.228516 ... 0.226562 1.04688 0.118164]\n",
            "  [-0.11084 1.10938 0.251953 ... 0.332031 0.472656 0.146484]\n",
            "  [-0.519531 1.8125 0.839844 ... -0.15625 0.486328 -1.42969]]]\n",
            "next_layer_addition_dropped_out=[[[-1.15625 7.4375 1.5625 ... -1.82031 2.0625 2.5625]\n",
            "  [-3.60938 4.5 -0.179688 ... 1.44531 0.00106812 1.14844]\n",
            "  [-1.32031 6.40625 -2.70312 ... -4.0625 -0.490234 2.8125]\n",
            "  ...\n",
            "  [4.09375 5.3125 0.404297 ... 0.988281 3.39062 0.535156]\n",
            "  [0.120605 3.59375 0.722656 ... 0.220703 1.25 0.71875]\n",
            "  [-0.550781 4.25 2.07812 ... -0.427734 0.90625 -3.10938]]\n",
            "\n",
            " [[-1.15625 7.4375 1.5625 ... -1.82031 2.0625 2.5625]\n",
            "  [-3.60938 4.5 -0.179688 ... 1.44531 0.00106812 1.14844]\n",
            "  [-1.32031 6.40625 -2.70312 ... -4.0625 -0.490234 2.8125]\n",
            "  ...\n",
            "  [4.09375 5.3125 0.404297 ... 0.988281 3.39062 0.535156]\n",
            "  [0.120605 3.59375 0.722656 ... 0.220703 1.25 0.71875]\n",
            "  [-0.550781 4.25 2.07812 ... -0.427734 0.90625 -3.10938]]\n",
            "\n",
            " [[-1.15625 7.4375 1.5625 ... -1.82031 2.0625 2.5625]\n",
            "  [-3.60938 4.5 -0.179688 ... 1.44531 0.00106812 1.14844]\n",
            "  [-1.32031 6.40625 -2.70312 ... -4.0625 -0.490234 2.8125]\n",
            "  ...\n",
            "  [4.09375 5.3125 0.404297 ... 0.988281 3.39062 0.535156]\n",
            "  [0.120605 3.59375 0.722656 ... 0.220703 1.25 0.71875]\n",
            "  [-0.550781 4.25 2.07812 ... -0.427734 0.90625 -3.10938]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.15625 7.4375 1.5625 ... -1.82031 2.0625 2.5625]\n",
            "  [-3.60938 4.5 -0.179688 ... 1.44531 0.00106812 1.14844]\n",
            "  [-1.32031 6.40625 -2.70312 ... -4.0625 -0.490234 2.8125]\n",
            "  ...\n",
            "  [4.09375 5.3125 0.404297 ... 0.988281 3.39062 0.535156]\n",
            "  [0.120605 3.59375 0.722656 ... 0.220703 1.25 0.71875]\n",
            "  [-0.550781 4.25 2.07812 ... -0.427734 0.90625 -3.10938]]\n",
            "\n",
            " [[-1.15625 7.4375 1.5625 ... -1.82031 2.0625 2.5625]\n",
            "  [-3.60938 4.5 -0.179688 ... 1.44531 0.00106812 1.14844]\n",
            "  [-1.32031 6.40625 -2.70312 ... -4.0625 -0.490234 2.8125]\n",
            "  ...\n",
            "  [4.09375 5.3125 0.404297 ... 0.988281 3.39062 0.535156]\n",
            "  [0.120605 3.59375 0.722656 ... 0.220703 1.25 0.71875]\n",
            "  [-0.550781 4.25 2.07812 ... -0.427734 0.90625 -3.10938]]\n",
            "\n",
            " [[-1.15625 7.4375 1.5625 ... -1.82031 2.0625 2.5625]\n",
            "  [-3.60938 4.5 -0.179688 ... 1.44531 0.00106812 1.14844]\n",
            "  [-1.32031 6.40625 -2.70312 ... -4.0625 -0.490234 2.8125]\n",
            "  ...\n",
            "  [4.09375 5.3125 0.404297 ... 0.988281 3.39062 0.535156]\n",
            "  [0.120605 3.59375 0.722656 ... 0.220703 1.25 0.71875]\n",
            "  [-0.550781 4.25 2.07812 ... -0.427734 0.90625 -3.10938]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00]]]]]\n",
            "inputs=[[[-1.15625 7.4375 1.5625 ... -1.82031 2.0625 2.5625]\n",
            "  [-3.60938 4.5 -0.179688 ... 1.44531 0.00106812 1.14844]\n",
            "  [-1.32031 6.40625 -2.70312 ... -4.0625 -0.490234 2.8125]\n",
            "  ...\n",
            "  [4.09375 5.3125 0.404297 ... 0.988281 3.39062 0.535156]\n",
            "  [0.120605 3.59375 0.722656 ... 0.220703 1.25 0.71875]\n",
            "  [-0.550781 4.25 2.07812 ... -0.427734 0.90625 -3.10938]]\n",
            "\n",
            " [[-1.15625 7.4375 1.5625 ... -1.82031 2.0625 2.5625]\n",
            "  [-3.60938 4.5 -0.179688 ... 1.44531 0.00106812 1.14844]\n",
            "  [-1.32031 6.40625 -2.70312 ... -4.0625 -0.490234 2.8125]\n",
            "  ...\n",
            "  [4.09375 5.3125 0.404297 ... 0.988281 3.39062 0.535156]\n",
            "  [0.120605 3.59375 0.722656 ... 0.220703 1.25 0.71875]\n",
            "  [-0.550781 4.25 2.07812 ... -0.427734 0.90625 -3.10938]]\n",
            "\n",
            " [[-1.15625 7.4375 1.5625 ... -1.82031 2.0625 2.5625]\n",
            "  [-3.60938 4.5 -0.179688 ... 1.44531 0.00106812 1.14844]\n",
            "  [-1.32031 6.40625 -2.70312 ... -4.0625 -0.490234 2.8125]\n",
            "  ...\n",
            "  [4.09375 5.3125 0.404297 ... 0.988281 3.39062 0.535156]\n",
            "  [0.120605 3.59375 0.722656 ... 0.220703 1.25 0.71875]\n",
            "  [-0.550781 4.25 2.07812 ... -0.427734 0.90625 -3.10938]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.15625 7.4375 1.5625 ... -1.82031 2.0625 2.5625]\n",
            "  [-3.60938 4.5 -0.179688 ... 1.44531 0.00106812 1.14844]\n",
            "  [-1.32031 6.40625 -2.70312 ... -4.0625 -0.490234 2.8125]\n",
            "  ...\n",
            "  [4.09375 5.3125 0.404297 ... 0.988281 3.39062 0.535156]\n",
            "  [0.120605 3.59375 0.722656 ... 0.220703 1.25 0.71875]\n",
            "  [-0.550781 4.25 2.07812 ... -0.427734 0.90625 -3.10938]]\n",
            "\n",
            " [[-1.15625 7.4375 1.5625 ... -1.82031 2.0625 2.5625]\n",
            "  [-3.60938 4.5 -0.179688 ... 1.44531 0.00106812 1.14844]\n",
            "  [-1.32031 6.40625 -2.70312 ... -4.0625 -0.490234 2.8125]\n",
            "  ...\n",
            "  [4.09375 5.3125 0.404297 ... 0.988281 3.39062 0.535156]\n",
            "  [0.120605 3.59375 0.722656 ... 0.220703 1.25 0.71875]\n",
            "  [-0.550781 4.25 2.07812 ... -0.427734 0.90625 -3.10938]]\n",
            "\n",
            " [[-1.15625 7.4375 1.5625 ... -1.82031 2.0625 2.5625]\n",
            "  [-3.60938 4.5 -0.179688 ... 1.44531 0.00106812 1.14844]\n",
            "  [-1.32031 6.40625 -2.70312 ... -4.0625 -0.490234 2.8125]\n",
            "  ...\n",
            "  [4.09375 5.3125 0.404297 ... 0.988281 3.39062 0.535156]\n",
            "  [0.120605 3.59375 0.722656 ... 0.220703 1.25 0.71875]\n",
            "  [-0.550781 4.25 2.07812 ... -0.427734 0.90625 -3.10938]]]\n",
            "lnx=[[[-0.371094 2.39062 0.503906 ... -0.585938 0.664062 0.824219]\n",
            "  [-1.3125 1.63281 -0.0654297 ... 0.523438 0.000387192 0.416016]\n",
            "  [-0.515625 2.5 -1.05469 ... -1.58594 -0.191406 1.09375]\n",
            "  ...\n",
            "  [1.8125 2.34375 0.178711 ... 0.4375 1.5 0.236328]\n",
            "  [0.0500488 1.49219 0.300781 ... 0.0917969 0.519531 0.298828]\n",
            "  [-0.240234 1.85938 0.90625 ... -0.186523 0.396484 -1.35938]]\n",
            "\n",
            " [[-0.371094 2.39062 0.503906 ... -0.585938 0.664062 0.824219]\n",
            "  [-1.3125 1.63281 -0.0654297 ... 0.523438 0.000387192 0.416016]\n",
            "  [-0.515625 2.5 -1.05469 ... -1.58594 -0.191406 1.09375]\n",
            "  ...\n",
            "  [1.8125 2.34375 0.178711 ... 0.4375 1.5 0.236328]\n",
            "  [0.0500488 1.49219 0.300781 ... 0.0917969 0.519531 0.298828]\n",
            "  [-0.240234 1.85938 0.90625 ... -0.186523 0.396484 -1.35938]]\n",
            "\n",
            " [[-0.371094 2.39062 0.503906 ... -0.585938 0.664062 0.824219]\n",
            "  [-1.3125 1.63281 -0.0654297 ... 0.523438 0.000387192 0.416016]\n",
            "  [-0.515625 2.5 -1.05469 ... -1.58594 -0.191406 1.09375]\n",
            "  ...\n",
            "  [1.8125 2.34375 0.178711 ... 0.4375 1.5 0.236328]\n",
            "  [0.0500488 1.49219 0.300781 ... 0.0917969 0.519531 0.298828]\n",
            "  [-0.240234 1.85938 0.90625 ... -0.186523 0.396484 -1.35938]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.371094 2.39062 0.503906 ... -0.585938 0.664062 0.824219]\n",
            "  [-1.3125 1.63281 -0.0654297 ... 0.523438 0.000387192 0.416016]\n",
            "  [-0.515625 2.5 -1.05469 ... -1.58594 -0.191406 1.09375]\n",
            "  ...\n",
            "  [1.8125 2.34375 0.178711 ... 0.4375 1.5 0.236328]\n",
            "  [0.0500488 1.49219 0.300781 ... 0.0917969 0.519531 0.298828]\n",
            "  [-0.240234 1.85938 0.90625 ... -0.186523 0.396484 -1.35938]]\n",
            "\n",
            " [[-0.371094 2.39062 0.503906 ... -0.585938 0.664062 0.824219]\n",
            "  [-1.3125 1.63281 -0.0654297 ... 0.523438 0.000387192 0.416016]\n",
            "  [-0.515625 2.5 -1.05469 ... -1.58594 -0.191406 1.09375]\n",
            "  ...\n",
            "  [1.8125 2.34375 0.178711 ... 0.4375 1.5 0.236328]\n",
            "  [0.0500488 1.49219 0.300781 ... 0.0917969 0.519531 0.298828]\n",
            "  [-0.240234 1.85938 0.90625 ... -0.186523 0.396484 -1.35938]]\n",
            "\n",
            " [[-0.371094 2.39062 0.503906 ... -0.585938 0.664062 0.824219]\n",
            "  [-1.3125 1.63281 -0.0654297 ... 0.523438 0.000387192 0.416016]\n",
            "  [-0.515625 2.5 -1.05469 ... -1.58594 -0.191406 1.09375]\n",
            "  ...\n",
            "  [1.8125 2.34375 0.178711 ... 0.4375 1.5 0.236328]\n",
            "  [0.0500488 1.49219 0.300781 ... 0.0917969 0.519531 0.298828]\n",
            "  [-0.240234 1.85938 0.90625 ... -0.186523 0.396484 -1.35938]]]\n",
            "attention_lnx=[[[-1.88281 -0.482422 -1 ... -1.19531 0.878906 -0.020874]\n",
            "  [-1.86719 -0.839844 -0.976562 ... -1.85156 1.30469 0.480469]\n",
            "  [-1.38281 0.168945 -1.32031 ... -1.25781 1.13281 0.310547]\n",
            "  ...\n",
            "  [-1.55469 0.375 -0.890625 ... -1.08594 0.445312 0.558594]\n",
            "  [-1.58594 0.414062 -0.9375 ... -1.27344 0.0461426 0.216797]\n",
            "  [-1.53906 0.151367 -0.910156 ... -1.23438 0.226562 0.464844]]\n",
            "\n",
            " [[-1.88281 -0.482422 -1 ... -1.19531 0.878906 -0.020874]\n",
            "  [-1.86719 -0.839844 -0.976562 ... -1.85156 1.30469 0.480469]\n",
            "  [-1.38281 0.168945 -1.32031 ... -1.25781 1.13281 0.310547]\n",
            "  ...\n",
            "  [-1.55469 0.375 -0.890625 ... -1.08594 0.445312 0.558594]\n",
            "  [-1.58594 0.414062 -0.9375 ... -1.27344 0.0461426 0.216797]\n",
            "  [-1.53906 0.151367 -0.910156 ... -1.23438 0.226562 0.464844]]\n",
            "\n",
            " [[-1.88281 -0.482422 -1 ... -1.19531 0.878906 -0.020874]\n",
            "  [-1.86719 -0.839844 -0.976562 ... -1.85156 1.30469 0.480469]\n",
            "  [-1.38281 0.168945 -1.32031 ... -1.25781 1.13281 0.310547]\n",
            "  ...\n",
            "  [-1.55469 0.375 -0.890625 ... -1.08594 0.445312 0.558594]\n",
            "  [-1.58594 0.414062 -0.9375 ... -1.27344 0.0461426 0.216797]\n",
            "  [-1.53906 0.151367 -0.910156 ... -1.23438 0.226562 0.464844]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.88281 -0.482422 -1 ... -1.19531 0.878906 -0.020874]\n",
            "  [-1.86719 -0.839844 -0.976562 ... -1.85156 1.30469 0.480469]\n",
            "  [-1.38281 0.168945 -1.32031 ... -1.25781 1.13281 0.310547]\n",
            "  ...\n",
            "  [-1.55469 0.375 -0.890625 ... -1.08594 0.445312 0.558594]\n",
            "  [-1.58594 0.414062 -0.9375 ... -1.27344 0.0461426 0.216797]\n",
            "  [-1.53906 0.151367 -0.910156 ... -1.23438 0.226562 0.464844]]\n",
            "\n",
            " [[-1.88281 -0.482422 -1 ... -1.19531 0.878906 -0.020874]\n",
            "  [-1.86719 -0.839844 -0.976562 ... -1.85156 1.30469 0.480469]\n",
            "  [-1.38281 0.168945 -1.32031 ... -1.25781 1.13281 0.310547]\n",
            "  ...\n",
            "  [-1.55469 0.375 -0.890625 ... -1.08594 0.445312 0.558594]\n",
            "  [-1.58594 0.414062 -0.9375 ... -1.27344 0.0461426 0.216797]\n",
            "  [-1.53906 0.151367 -0.910156 ... -1.23438 0.226562 0.464844]]\n",
            "\n",
            " [[-1.88281 -0.482422 -1 ... -1.19531 0.878906 -0.020874]\n",
            "  [-1.86719 -0.839844 -0.976562 ... -1.85156 1.30469 0.480469]\n",
            "  [-1.38281 0.168945 -1.32031 ... -1.25781 1.13281 0.310547]\n",
            "  ...\n",
            "  [-1.55469 0.375 -0.890625 ... -1.08594 0.445312 0.558594]\n",
            "  [-1.58594 0.414062 -0.9375 ... -1.27344 0.0461426 0.216797]\n",
            "  [-1.53906 0.151367 -0.910156 ... -1.23438 0.226562 0.464844]]]\n",
            "attn_output=[[[-0.925781 2.125 0.170898 ... -0.921875 0.898438 0.777344]\n",
            "  [-1.89844 1.26562 -0.400391 ... -0.139648 0.453125 0.5625]\n",
            "  [-1.00781 2.45312 -1.50781 ... -1.98438 0.240234 1.16406]\n",
            "  ...\n",
            "  [1.09375 2.4375 -0.208984 ... -0.0407715 1.64844 0.470703]\n",
            "  [-0.589844 1.61719 -0.0869141 ... -0.423828 0.523438 0.376953]\n",
            "  [-0.890625 1.875 0.494141 ... -0.703125 0.480469 -1.125]]\n",
            "\n",
            " [[-0.925781 2.125 0.170898 ... -0.921875 0.898438 0.777344]\n",
            "  [-1.89844 1.26562 -0.400391 ... -0.139648 0.453125 0.5625]\n",
            "  [-1.00781 2.45312 -1.50781 ... -1.98438 0.240234 1.16406]\n",
            "  ...\n",
            "  [1.09375 2.4375 -0.208984 ... -0.0407715 1.64844 0.470703]\n",
            "  [-0.589844 1.61719 -0.0869141 ... -0.423828 0.523438 0.376953]\n",
            "  [-0.890625 1.875 0.494141 ... -0.703125 0.480469 -1.125]]\n",
            "\n",
            " [[-0.925781 2.125 0.170898 ... -0.921875 0.898438 0.777344]\n",
            "  [-1.89844 1.26562 -0.400391 ... -0.139648 0.453125 0.5625]\n",
            "  [-1.00781 2.45312 -1.50781 ... -1.98438 0.240234 1.16406]\n",
            "  ...\n",
            "  [1.09375 2.4375 -0.208984 ... -0.0407715 1.64844 0.470703]\n",
            "  [-0.589844 1.61719 -0.0869141 ... -0.423828 0.523438 0.376953]\n",
            "  [-0.890625 1.875 0.494141 ... -0.703125 0.480469 -1.125]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.925781 2.125 0.170898 ... -0.921875 0.898438 0.777344]\n",
            "  [-1.89844 1.26562 -0.400391 ... -0.139648 0.453125 0.5625]\n",
            "  [-1.00781 2.45312 -1.50781 ... -1.98438 0.240234 1.16406]\n",
            "  ...\n",
            "  [1.09375 2.4375 -0.208984 ... -0.0407715 1.64844 0.470703]\n",
            "  [-0.589844 1.61719 -0.0869141 ... -0.423828 0.523438 0.376953]\n",
            "  [-0.890625 1.875 0.494141 ... -0.703125 0.480469 -1.125]]\n",
            "\n",
            " [[-0.925781 2.125 0.170898 ... -0.921875 0.898438 0.777344]\n",
            "  [-1.89844 1.26562 -0.400391 ... -0.139648 0.453125 0.5625]\n",
            "  [-1.00781 2.45312 -1.50781 ... -1.98438 0.240234 1.16406]\n",
            "  ...\n",
            "  [1.09375 2.4375 -0.208984 ... -0.0407715 1.64844 0.470703]\n",
            "  [-0.589844 1.61719 -0.0869141 ... -0.423828 0.523438 0.376953]\n",
            "  [-0.890625 1.875 0.494141 ... -0.703125 0.480469 -1.125]]\n",
            "\n",
            " [[-0.925781 2.125 0.170898 ... -0.921875 0.898438 0.777344]\n",
            "  [-1.89844 1.26562 -0.400391 ... -0.139648 0.453125 0.5625]\n",
            "  [-1.00781 2.45312 -1.50781 ... -1.98438 0.240234 1.16406]\n",
            "  ...\n",
            "  [1.09375 2.4375 -0.208984 ... -0.0407715 1.64844 0.470703]\n",
            "  [-0.589844 1.61719 -0.0869141 ... -0.423828 0.523438 0.376953]\n",
            "  [-0.890625 1.875 0.494141 ... -0.703125 0.480469 -1.125]]]\n",
            "next_layer_addition_dropped_out=[[[-2.95312 7 0.714844 ... -3.53125 4.59375 2.78125]\n",
            "  [-6.0625 4.3125 -1.53125 ... -0.402344 1.9375 1.79688]\n",
            "  [-1.99219 6.09375 -4.46875 ... -4.9375 1.82031 2.125]\n",
            "  ...\n",
            "  [3.625 5.3125 -0.6875 ... 0.00799561 3.10938 1.17188]\n",
            "  [-0.988281 4.0625 0.511719 ... -0.10498 0.585938 -0.488281]\n",
            "  [-2.15625 5.84375 1.53906 ... -2.125 1.00781 -1.92969]]\n",
            "\n",
            " [[-2.95312 7 0.714844 ... -3.53125 4.59375 2.78125]\n",
            "  [-6.0625 4.3125 -1.53125 ... -0.402344 1.9375 1.79688]\n",
            "  [-1.99219 6.09375 -4.46875 ... -4.9375 1.82031 2.125]\n",
            "  ...\n",
            "  [3.625 5.3125 -0.6875 ... 0.00799561 3.10938 1.17188]\n",
            "  [-0.988281 4.0625 0.511719 ... -0.10498 0.585938 -0.488281]\n",
            "  [-2.15625 5.84375 1.53906 ... -2.125 1.00781 -1.92969]]\n",
            "\n",
            " [[-2.95312 7 0.714844 ... -3.53125 4.59375 2.78125]\n",
            "  [-6.0625 4.3125 -1.53125 ... -0.402344 1.9375 1.79688]\n",
            "  [-1.99219 6.09375 -4.46875 ... -4.9375 1.82031 2.125]\n",
            "  ...\n",
            "  [3.625 5.3125 -0.6875 ... 0.00799561 3.10938 1.17188]\n",
            "  [-0.988281 4.0625 0.511719 ... -0.10498 0.585938 -0.488281]\n",
            "  [-2.15625 5.84375 1.53906 ... -2.125 1.00781 -1.92969]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-2.95312 7 0.714844 ... -3.53125 4.59375 2.78125]\n",
            "  [-6.0625 4.3125 -1.53125 ... -0.402344 1.9375 1.79688]\n",
            "  [-1.99219 6.09375 -4.46875 ... -4.9375 1.82031 2.125]\n",
            "  ...\n",
            "  [3.625 5.3125 -0.6875 ... 0.00799561 3.10938 1.17188]\n",
            "  [-0.988281 4.0625 0.511719 ... -0.10498 0.585938 -0.488281]\n",
            "  [-2.15625 5.84375 1.53906 ... -2.125 1.00781 -1.92969]]\n",
            "\n",
            " [[-2.95312 7 0.714844 ... -3.53125 4.59375 2.78125]\n",
            "  [-6.0625 4.3125 -1.53125 ... -0.402344 1.9375 1.79688]\n",
            "  [-1.99219 6.09375 -4.46875 ... -4.9375 1.82031 2.125]\n",
            "  ...\n",
            "  [3.625 5.3125 -0.6875 ... 0.00799561 3.10938 1.17188]\n",
            "  [-0.988281 4.0625 0.511719 ... -0.10498 0.585938 -0.488281]\n",
            "  [-2.15625 5.84375 1.53906 ... -2.125 1.00781 -1.92969]]\n",
            "\n",
            " [[-2.95312 7 0.714844 ... -3.53125 4.59375 2.78125]\n",
            "  [-6.0625 4.3125 -1.53125 ... -0.402344 1.9375 1.79688]\n",
            "  [-1.99219 6.09375 -4.46875 ... -4.9375 1.82031 2.125]\n",
            "  ...\n",
            "  [3.625 5.3125 -0.6875 ... 0.00799561 3.10938 1.17188]\n",
            "  [-0.988281 4.0625 0.511719 ... -0.10498 0.585938 -0.488281]\n",
            "  [-2.15625 5.84375 1.53906 ... -2.125 1.00781 -1.92969]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00]]]]]\n",
            "inputs=[[[-2.95312 7 0.714844 ... -3.53125 4.59375 2.78125]\n",
            "  [-6.0625 4.3125 -1.53125 ... -0.402344 1.9375 1.79688]\n",
            "  [-1.99219 6.09375 -4.46875 ... -4.9375 1.82031 2.125]\n",
            "  ...\n",
            "  [3.625 5.3125 -0.6875 ... 0.00799561 3.10938 1.17188]\n",
            "  [-0.988281 4.0625 0.511719 ... -0.10498 0.585938 -0.488281]\n",
            "  [-2.15625 5.84375 1.53906 ... -2.125 1.00781 -1.92969]]\n",
            "\n",
            " [[-2.95312 7 0.714844 ... -3.53125 4.59375 2.78125]\n",
            "  [-6.0625 4.3125 -1.53125 ... -0.402344 1.9375 1.79688]\n",
            "  [-1.99219 6.09375 -4.46875 ... -4.9375 1.82031 2.125]\n",
            "  ...\n",
            "  [3.625 5.3125 -0.6875 ... 0.00799561 3.10938 1.17188]\n",
            "  [-0.988281 4.0625 0.511719 ... -0.10498 0.585938 -0.488281]\n",
            "  [-2.15625 5.84375 1.53906 ... -2.125 1.00781 -1.92969]]\n",
            "\n",
            " [[-2.95312 7 0.714844 ... -3.53125 4.59375 2.78125]\n",
            "  [-6.0625 4.3125 -1.53125 ... -0.402344 1.9375 1.79688]\n",
            "  [-1.99219 6.09375 -4.46875 ... -4.9375 1.82031 2.125]\n",
            "  ...\n",
            "  [3.625 5.3125 -0.6875 ... 0.00799561 3.10938 1.17188]\n",
            "  [-0.988281 4.0625 0.511719 ... -0.10498 0.585938 -0.488281]\n",
            "  [-2.15625 5.84375 1.53906 ... -2.125 1.00781 -1.92969]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-2.95312 7 0.714844 ... -3.53125 4.59375 2.78125]\n",
            "  [-6.0625 4.3125 -1.53125 ... -0.402344 1.9375 1.79688]\n",
            "  [-1.99219 6.09375 -4.46875 ... -4.9375 1.82031 2.125]\n",
            "  ...\n",
            "  [3.625 5.3125 -0.6875 ... 0.00799561 3.10938 1.17188]\n",
            "  [-0.988281 4.0625 0.511719 ... -0.10498 0.585938 -0.488281]\n",
            "  [-2.15625 5.84375 1.53906 ... -2.125 1.00781 -1.92969]]\n",
            "\n",
            " [[-2.95312 7 0.714844 ... -3.53125 4.59375 2.78125]\n",
            "  [-6.0625 4.3125 -1.53125 ... -0.402344 1.9375 1.79688]\n",
            "  [-1.99219 6.09375 -4.46875 ... -4.9375 1.82031 2.125]\n",
            "  ...\n",
            "  [3.625 5.3125 -0.6875 ... 0.00799561 3.10938 1.17188]\n",
            "  [-0.988281 4.0625 0.511719 ... -0.10498 0.585938 -0.488281]\n",
            "  [-2.15625 5.84375 1.53906 ... -2.125 1.00781 -1.92969]]\n",
            "\n",
            " [[-2.95312 7 0.714844 ... -3.53125 4.59375 2.78125]\n",
            "  [-6.0625 4.3125 -1.53125 ... -0.402344 1.9375 1.79688]\n",
            "  [-1.99219 6.09375 -4.46875 ... -4.9375 1.82031 2.125]\n",
            "  ...\n",
            "  [3.625 5.3125 -0.6875 ... 0.00799561 3.10938 1.17188]\n",
            "  [-0.988281 4.0625 0.511719 ... -0.10498 0.585938 -0.488281]\n",
            "  [-2.15625 5.84375 1.53906 ... -2.125 1.00781 -1.92969]]]\n",
            "lnx=[[[-0.894531 2.125 0.216797 ... -1.07031 1.39062 0.84375]\n",
            "  [-2.0625 1.46875 -0.519531 ... -0.136719 0.660156 0.609375]\n",
            "  [-0.726562 2.21875 -1.63281 ... -1.80469 0.664062 0.777344]\n",
            "  ...\n",
            "  [1.5 2.1875 -0.283203 ... 0.0032959 1.28125 0.484375]\n",
            "  [-0.375 1.54688 0.194336 ... -0.0397949 0.222656 -0.185547]\n",
            "  [-0.871094 2.35938 0.621094 ... -0.859375 0.408203 -0.78125]]\n",
            "\n",
            " [[-0.894531 2.125 0.216797 ... -1.07031 1.39062 0.84375]\n",
            "  [-2.0625 1.46875 -0.519531 ... -0.136719 0.660156 0.609375]\n",
            "  [-0.726562 2.21875 -1.63281 ... -1.80469 0.664062 0.777344]\n",
            "  ...\n",
            "  [1.5 2.1875 -0.283203 ... 0.0032959 1.28125 0.484375]\n",
            "  [-0.375 1.54688 0.194336 ... -0.0397949 0.222656 -0.185547]\n",
            "  [-0.871094 2.35938 0.621094 ... -0.859375 0.408203 -0.78125]]\n",
            "\n",
            " [[-0.894531 2.125 0.216797 ... -1.07031 1.39062 0.84375]\n",
            "  [-2.0625 1.46875 -0.519531 ... -0.136719 0.660156 0.609375]\n",
            "  [-0.726562 2.21875 -1.63281 ... -1.80469 0.664062 0.777344]\n",
            "  ...\n",
            "  [1.5 2.1875 -0.283203 ... 0.0032959 1.28125 0.484375]\n",
            "  [-0.375 1.54688 0.194336 ... -0.0397949 0.222656 -0.185547]\n",
            "  [-0.871094 2.35938 0.621094 ... -0.859375 0.408203 -0.78125]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.894531 2.125 0.216797 ... -1.07031 1.39062 0.84375]\n",
            "  [-2.0625 1.46875 -0.519531 ... -0.136719 0.660156 0.609375]\n",
            "  [-0.726562 2.21875 -1.63281 ... -1.80469 0.664062 0.777344]\n",
            "  ...\n",
            "  [1.5 2.1875 -0.283203 ... 0.0032959 1.28125 0.484375]\n",
            "  [-0.375 1.54688 0.194336 ... -0.0397949 0.222656 -0.185547]\n",
            "  [-0.871094 2.35938 0.621094 ... -0.859375 0.408203 -0.78125]]\n",
            "\n",
            " [[-0.894531 2.125 0.216797 ... -1.07031 1.39062 0.84375]\n",
            "  [-2.0625 1.46875 -0.519531 ... -0.136719 0.660156 0.609375]\n",
            "  [-0.726562 2.21875 -1.63281 ... -1.80469 0.664062 0.777344]\n",
            "  ...\n",
            "  [1.5 2.1875 -0.283203 ... 0.0032959 1.28125 0.484375]\n",
            "  [-0.375 1.54688 0.194336 ... -0.0397949 0.222656 -0.185547]\n",
            "  [-0.871094 2.35938 0.621094 ... -0.859375 0.408203 -0.78125]]\n",
            "\n",
            " [[-0.894531 2.125 0.216797 ... -1.07031 1.39062 0.84375]\n",
            "  [-2.0625 1.46875 -0.519531 ... -0.136719 0.660156 0.609375]\n",
            "  [-0.726562 2.21875 -1.63281 ... -1.80469 0.664062 0.777344]\n",
            "  ...\n",
            "  [1.5 2.1875 -0.283203 ... 0.0032959 1.28125 0.484375]\n",
            "  [-0.375 1.54688 0.194336 ... -0.0397949 0.222656 -0.185547]\n",
            "  [-0.871094 2.35938 0.621094 ... -0.859375 0.408203 -0.78125]]]\n",
            "attention_lnx=[[[0.000648499 0.40625 0.186523 ... -0.133789 0.652344 -0.394531]\n",
            "  [0.628906 0.314453 0.010498 ... -0.210938 0.882812 0.0549316]\n",
            "  [-0.0339355 0.644531 -0.28125 ... 0.652344 0.310547 -0.0761719]\n",
            "  ...\n",
            "  [0.208008 0.902344 -1.47656 ... -0.464844 0.574219 0.466797]\n",
            "  [-0.204102 1.17969 -1.60156 ... -0.800781 0.320312 0.871094]\n",
            "  [0.300781 0.90625 -1 ... -0.265625 0.441406 0.824219]]\n",
            "\n",
            " [[0.000648499 0.40625 0.186523 ... -0.133789 0.652344 -0.394531]\n",
            "  [0.628906 0.314453 0.010498 ... -0.210938 0.882812 0.0549316]\n",
            "  [-0.0339355 0.644531 -0.28125 ... 0.652344 0.310547 -0.0761719]\n",
            "  ...\n",
            "  [0.208008 0.902344 -1.47656 ... -0.464844 0.574219 0.466797]\n",
            "  [-0.204102 1.17969 -1.60156 ... -0.800781 0.320312 0.871094]\n",
            "  [0.300781 0.90625 -1 ... -0.265625 0.441406 0.824219]]\n",
            "\n",
            " [[0.000648499 0.40625 0.186523 ... -0.133789 0.652344 -0.394531]\n",
            "  [0.628906 0.314453 0.010498 ... -0.210938 0.882812 0.0549316]\n",
            "  [-0.0339355 0.644531 -0.28125 ... 0.652344 0.310547 -0.0761719]\n",
            "  ...\n",
            "  [0.208008 0.902344 -1.47656 ... -0.464844 0.574219 0.466797]\n",
            "  [-0.204102 1.17969 -1.60156 ... -0.800781 0.320312 0.871094]\n",
            "  [0.300781 0.90625 -1 ... -0.265625 0.441406 0.824219]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.000648499 0.40625 0.186523 ... -0.133789 0.652344 -0.394531]\n",
            "  [0.628906 0.314453 0.010498 ... -0.210938 0.882812 0.0549316]\n",
            "  [-0.0339355 0.644531 -0.28125 ... 0.652344 0.310547 -0.0761719]\n",
            "  ...\n",
            "  [0.208008 0.902344 -1.47656 ... -0.464844 0.574219 0.466797]\n",
            "  [-0.204102 1.17969 -1.60156 ... -0.800781 0.320312 0.871094]\n",
            "  [0.300781 0.90625 -1 ... -0.265625 0.441406 0.824219]]\n",
            "\n",
            " [[0.000648499 0.40625 0.186523 ... -0.133789 0.652344 -0.394531]\n",
            "  [0.628906 0.314453 0.010498 ... -0.210938 0.882812 0.0549316]\n",
            "  [-0.0339355 0.644531 -0.28125 ... 0.652344 0.310547 -0.0761719]\n",
            "  ...\n",
            "  [0.208008 0.902344 -1.47656 ... -0.464844 0.574219 0.466797]\n",
            "  [-0.204102 1.17969 -1.60156 ... -0.800781 0.320312 0.871094]\n",
            "  [0.300781 0.90625 -1 ... -0.265625 0.441406 0.824219]]\n",
            "\n",
            " [[0.000648499 0.40625 0.186523 ... -0.133789 0.652344 -0.394531]\n",
            "  [0.628906 0.314453 0.010498 ... -0.210938 0.882812 0.0549316]\n",
            "  [-0.0339355 0.644531 -0.28125 ... 0.652344 0.310547 -0.0761719]\n",
            "  ...\n",
            "  [0.208008 0.902344 -1.47656 ... -0.464844 0.574219 0.466797]\n",
            "  [-0.204102 1.17969 -1.60156 ... -0.800781 0.320312 0.871094]\n",
            "  [0.300781 0.90625 -1 ... -0.265625 0.441406 0.824219]]]\n",
            "attn_output=[[[-0.851562 2.14062 0.259766 ... -1.0625 1.51562 0.691406]\n",
            "  [-1.75781 1.49219 -0.492188 ... -0.198242 0.910156 0.597656]\n",
            "  [-0.710938 2.35938 -1.65625 ... -1.49219 0.742188 0.714844]\n",
            "  ...\n",
            "  [1.53125 2.48438 -0.859375 ... -0.182617 1.47656 0.65625]\n",
            "  [-0.441406 1.9375 -0.400391 ... -0.333984 0.333984 0.140625]\n",
            "  [-0.726562 2.64062 0.210938 ... -0.9375 0.566406 -0.433594]]\n",
            "\n",
            " [[-0.851562 2.14062 0.259766 ... -1.0625 1.51562 0.691406]\n",
            "  [-1.75781 1.49219 -0.492188 ... -0.198242 0.910156 0.597656]\n",
            "  [-0.710938 2.35938 -1.65625 ... -1.49219 0.742188 0.714844]\n",
            "  ...\n",
            "  [1.53125 2.48438 -0.859375 ... -0.182617 1.47656 0.65625]\n",
            "  [-0.441406 1.9375 -0.400391 ... -0.333984 0.333984 0.140625]\n",
            "  [-0.726562 2.64062 0.210938 ... -0.9375 0.566406 -0.433594]]\n",
            "\n",
            " [[-0.851562 2.14062 0.259766 ... -1.0625 1.51562 0.691406]\n",
            "  [-1.75781 1.49219 -0.492188 ... -0.198242 0.910156 0.597656]\n",
            "  [-0.710938 2.35938 -1.65625 ... -1.49219 0.742188 0.714844]\n",
            "  ...\n",
            "  [1.53125 2.48438 -0.859375 ... -0.182617 1.47656 0.65625]\n",
            "  [-0.441406 1.9375 -0.400391 ... -0.333984 0.333984 0.140625]\n",
            "  [-0.726562 2.64062 0.210938 ... -0.9375 0.566406 -0.433594]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.851562 2.14062 0.259766 ... -1.0625 1.51562 0.691406]\n",
            "  [-1.75781 1.49219 -0.492188 ... -0.198242 0.910156 0.597656]\n",
            "  [-0.710938 2.35938 -1.65625 ... -1.49219 0.742188 0.714844]\n",
            "  ...\n",
            "  [1.53125 2.48438 -0.859375 ... -0.182617 1.47656 0.65625]\n",
            "  [-0.441406 1.9375 -0.400391 ... -0.333984 0.333984 0.140625]\n",
            "  [-0.726562 2.64062 0.210938 ... -0.9375 0.566406 -0.433594]]\n",
            "\n",
            " [[-0.851562 2.14062 0.259766 ... -1.0625 1.51562 0.691406]\n",
            "  [-1.75781 1.49219 -0.492188 ... -0.198242 0.910156 0.597656]\n",
            "  [-0.710938 2.35938 -1.65625 ... -1.49219 0.742188 0.714844]\n",
            "  ...\n",
            "  [1.53125 2.48438 -0.859375 ... -0.182617 1.47656 0.65625]\n",
            "  [-0.441406 1.9375 -0.400391 ... -0.333984 0.333984 0.140625]\n",
            "  [-0.726562 2.64062 0.210938 ... -0.9375 0.566406 -0.433594]]\n",
            "\n",
            " [[-0.851562 2.14062 0.259766 ... -1.0625 1.51562 0.691406]\n",
            "  [-1.75781 1.49219 -0.492188 ... -0.198242 0.910156 0.597656]\n",
            "  [-0.710938 2.35938 -1.65625 ... -1.49219 0.742188 0.714844]\n",
            "  ...\n",
            "  [1.53125 2.48438 -0.859375 ... -0.182617 1.47656 0.65625]\n",
            "  [-0.441406 1.9375 -0.400391 ... -0.333984 0.333984 0.140625]\n",
            "  [-0.726562 2.64062 0.210938 ... -0.9375 0.566406 -0.433594]]]\n",
            "next_layer_addition_dropped_out=[[[-2.92188 7.34375 1.09375 ... -2.90625 6.5625 1.76562]\n",
            "  [-5.21875 4.78125 -1.66406 ... -0.285156 2.95312 2.78125]\n",
            "  [-1.375 7 -5.21875 ... -3.46875 2.75 2.48438]\n",
            "  ...\n",
            "  [4.03125 6.4375 -2.39062 ... 0.125977 3.84375 1.20312]\n",
            "  [-1.63281 4.625 -1.09375 ... -1.17188 0.386719 0.0247803]\n",
            "  [-2.53125 7.03125 0.396484 ... -2.96875 0.878906 -2.26562]]\n",
            "\n",
            " [[-2.92188 7.34375 1.09375 ... -2.90625 6.5625 1.76562]\n",
            "  [-5.21875 4.78125 -1.66406 ... -0.285156 2.95312 2.78125]\n",
            "  [-1.375 7 -5.21875 ... -3.46875 2.75 2.48438]\n",
            "  ...\n",
            "  [4.03125 6.4375 -2.39062 ... 0.125977 3.84375 1.20312]\n",
            "  [-1.63281 4.625 -1.09375 ... -1.17188 0.386719 0.0247803]\n",
            "  [-2.53125 7.03125 0.396484 ... -2.96875 0.878906 -2.26562]]\n",
            "\n",
            " [[-2.92188 7.34375 1.09375 ... -2.90625 6.5625 1.76562]\n",
            "  [-5.21875 4.78125 -1.66406 ... -0.285156 2.95312 2.78125]\n",
            "  [-1.375 7 -5.21875 ... -3.46875 2.75 2.48438]\n",
            "  ...\n",
            "  [4.03125 6.4375 -2.39062 ... 0.125977 3.84375 1.20312]\n",
            "  [-1.63281 4.625 -1.09375 ... -1.17188 0.386719 0.0247803]\n",
            "  [-2.53125 7.03125 0.396484 ... -2.96875 0.878906 -2.26562]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-2.92188 7.34375 1.09375 ... -2.90625 6.5625 1.76562]\n",
            "  [-5.21875 4.78125 -1.66406 ... -0.285156 2.95312 2.78125]\n",
            "  [-1.375 7 -5.21875 ... -3.46875 2.75 2.48438]\n",
            "  ...\n",
            "  [4.03125 6.4375 -2.39062 ... 0.125977 3.84375 1.20312]\n",
            "  [-1.63281 4.625 -1.09375 ... -1.17188 0.386719 0.0247803]\n",
            "  [-2.53125 7.03125 0.396484 ... -2.96875 0.878906 -2.26562]]\n",
            "\n",
            " [[-2.92188 7.34375 1.09375 ... -2.90625 6.5625 1.76562]\n",
            "  [-5.21875 4.78125 -1.66406 ... -0.285156 2.95312 2.78125]\n",
            "  [-1.375 7 -5.21875 ... -3.46875 2.75 2.48438]\n",
            "  ...\n",
            "  [4.03125 6.4375 -2.39062 ... 0.125977 3.84375 1.20312]\n",
            "  [-1.63281 4.625 -1.09375 ... -1.17188 0.386719 0.0247803]\n",
            "  [-2.53125 7.03125 0.396484 ... -2.96875 0.878906 -2.26562]]\n",
            "\n",
            " [[-2.92188 7.34375 1.09375 ... -2.90625 6.5625 1.76562]\n",
            "  [-5.21875 4.78125 -1.66406 ... -0.285156 2.95312 2.78125]\n",
            "  [-1.375 7 -5.21875 ... -3.46875 2.75 2.48438]\n",
            "  ...\n",
            "  [4.03125 6.4375 -2.39062 ... 0.125977 3.84375 1.20312]\n",
            "  [-1.63281 4.625 -1.09375 ... -1.17188 0.386719 0.0247803]\n",
            "  [-2.53125 7.03125 0.396484 ... -2.96875 0.878906 -2.26562]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00]]]]]\n",
            "inputs=[[[-2.92188 7.34375 1.09375 ... -2.90625 6.5625 1.76562]\n",
            "  [-5.21875 4.78125 -1.66406 ... -0.285156 2.95312 2.78125]\n",
            "  [-1.375 7 -5.21875 ... -3.46875 2.75 2.48438]\n",
            "  ...\n",
            "  [4.03125 6.4375 -2.39062 ... 0.125977 3.84375 1.20312]\n",
            "  [-1.63281 4.625 -1.09375 ... -1.17188 0.386719 0.0247803]\n",
            "  [-2.53125 7.03125 0.396484 ... -2.96875 0.878906 -2.26562]]\n",
            "\n",
            " [[-2.92188 7.34375 1.09375 ... -2.90625 6.5625 1.76562]\n",
            "  [-5.21875 4.78125 -1.66406 ... -0.285156 2.95312 2.78125]\n",
            "  [-1.375 7 -5.21875 ... -3.46875 2.75 2.48438]\n",
            "  ...\n",
            "  [4.03125 6.4375 -2.39062 ... 0.125977 3.84375 1.20312]\n",
            "  [-1.63281 4.625 -1.09375 ... -1.17188 0.386719 0.0247803]\n",
            "  [-2.53125 7.03125 0.396484 ... -2.96875 0.878906 -2.26562]]\n",
            "\n",
            " [[-2.92188 7.34375 1.09375 ... -2.90625 6.5625 1.76562]\n",
            "  [-5.21875 4.78125 -1.66406 ... -0.285156 2.95312 2.78125]\n",
            "  [-1.375 7 -5.21875 ... -3.46875 2.75 2.48438]\n",
            "  ...\n",
            "  [4.03125 6.4375 -2.39062 ... 0.125977 3.84375 1.20312]\n",
            "  [-1.63281 4.625 -1.09375 ... -1.17188 0.386719 0.0247803]\n",
            "  [-2.53125 7.03125 0.396484 ... -2.96875 0.878906 -2.26562]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-2.92188 7.34375 1.09375 ... -2.90625 6.5625 1.76562]\n",
            "  [-5.21875 4.78125 -1.66406 ... -0.285156 2.95312 2.78125]\n",
            "  [-1.375 7 -5.21875 ... -3.46875 2.75 2.48438]\n",
            "  ...\n",
            "  [4.03125 6.4375 -2.39062 ... 0.125977 3.84375 1.20312]\n",
            "  [-1.63281 4.625 -1.09375 ... -1.17188 0.386719 0.0247803]\n",
            "  [-2.53125 7.03125 0.396484 ... -2.96875 0.878906 -2.26562]]\n",
            "\n",
            " [[-2.92188 7.34375 1.09375 ... -2.90625 6.5625 1.76562]\n",
            "  [-5.21875 4.78125 -1.66406 ... -0.285156 2.95312 2.78125]\n",
            "  [-1.375 7 -5.21875 ... -3.46875 2.75 2.48438]\n",
            "  ...\n",
            "  [4.03125 6.4375 -2.39062 ... 0.125977 3.84375 1.20312]\n",
            "  [-1.63281 4.625 -1.09375 ... -1.17188 0.386719 0.0247803]\n",
            "  [-2.53125 7.03125 0.396484 ... -2.96875 0.878906 -2.26562]]\n",
            "\n",
            " [[-2.92188 7.34375 1.09375 ... -2.90625 6.5625 1.76562]\n",
            "  [-5.21875 4.78125 -1.66406 ... -0.285156 2.95312 2.78125]\n",
            "  [-1.375 7 -5.21875 ... -3.46875 2.75 2.48438]\n",
            "  ...\n",
            "  [4.03125 6.4375 -2.39062 ... 0.125977 3.84375 1.20312]\n",
            "  [-1.63281 4.625 -1.09375 ... -1.17188 0.386719 0.0247803]\n",
            "  [-2.53125 7.03125 0.396484 ... -2.96875 0.878906 -2.26562]]]\n",
            "lnx=[[[-0.824219 2.07812 0.308594 ... -0.820312 1.85156 0.498047]\n",
            "  [-1.65625 1.51562 -0.527344 ... -0.090332 0.9375 0.882812]\n",
            "  [-0.466797 2.375 -1.77344 ... -1.17969 0.933594 0.84375]\n",
            "  ...\n",
            "  [1.54688 2.46875 -0.917969 ... 0.0483398 1.47656 0.460938]\n",
            "  [-0.574219 1.625 -0.384766 ... -0.412109 0.135742 0.00872803]\n",
            "  [-0.945312 2.625 0.147461 ... -1.10938 0.328125 -0.84375]]\n",
            "\n",
            " [[-0.824219 2.07812 0.308594 ... -0.820312 1.85156 0.498047]\n",
            "  [-1.65625 1.51562 -0.527344 ... -0.090332 0.9375 0.882812]\n",
            "  [-0.466797 2.375 -1.77344 ... -1.17969 0.933594 0.84375]\n",
            "  ...\n",
            "  [1.54688 2.46875 -0.917969 ... 0.0483398 1.47656 0.460938]\n",
            "  [-0.574219 1.625 -0.384766 ... -0.412109 0.135742 0.00872803]\n",
            "  [-0.945312 2.625 0.147461 ... -1.10938 0.328125 -0.84375]]\n",
            "\n",
            " [[-0.824219 2.07812 0.308594 ... -0.820312 1.85156 0.498047]\n",
            "  [-1.65625 1.51562 -0.527344 ... -0.090332 0.9375 0.882812]\n",
            "  [-0.466797 2.375 -1.77344 ... -1.17969 0.933594 0.84375]\n",
            "  ...\n",
            "  [1.54688 2.46875 -0.917969 ... 0.0483398 1.47656 0.460938]\n",
            "  [-0.574219 1.625 -0.384766 ... -0.412109 0.135742 0.00872803]\n",
            "  [-0.945312 2.625 0.147461 ... -1.10938 0.328125 -0.84375]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.824219 2.07812 0.308594 ... -0.820312 1.85156 0.498047]\n",
            "  [-1.65625 1.51562 -0.527344 ... -0.090332 0.9375 0.882812]\n",
            "  [-0.466797 2.375 -1.77344 ... -1.17969 0.933594 0.84375]\n",
            "  ...\n",
            "  [1.54688 2.46875 -0.917969 ... 0.0483398 1.47656 0.460938]\n",
            "  [-0.574219 1.625 -0.384766 ... -0.412109 0.135742 0.00872803]\n",
            "  [-0.945312 2.625 0.147461 ... -1.10938 0.328125 -0.84375]]\n",
            "\n",
            " [[-0.824219 2.07812 0.308594 ... -0.820312 1.85156 0.498047]\n",
            "  [-1.65625 1.51562 -0.527344 ... -0.090332 0.9375 0.882812]\n",
            "  [-0.466797 2.375 -1.77344 ... -1.17969 0.933594 0.84375]\n",
            "  ...\n",
            "  [1.54688 2.46875 -0.917969 ... 0.0483398 1.47656 0.460938]\n",
            "  [-0.574219 1.625 -0.384766 ... -0.412109 0.135742 0.00872803]\n",
            "  [-0.945312 2.625 0.147461 ... -1.10938 0.328125 -0.84375]]\n",
            "\n",
            " [[-0.824219 2.07812 0.308594 ... -0.820312 1.85156 0.498047]\n",
            "  [-1.65625 1.51562 -0.527344 ... -0.090332 0.9375 0.882812]\n",
            "  [-0.466797 2.375 -1.77344 ... -1.17969 0.933594 0.84375]\n",
            "  ...\n",
            "  [1.54688 2.46875 -0.917969 ... 0.0483398 1.47656 0.460938]\n",
            "  [-0.574219 1.625 -0.384766 ... -0.412109 0.135742 0.00872803]\n",
            "  [-0.945312 2.625 0.147461 ... -1.10938 0.328125 -0.84375]]]\n",
            "attention_lnx=[[[-0.0991211 -0.628906 0.111816 ... -0.294922 -0.785156 0.457031]\n",
            "  [-0.118164 -0.539062 0.09375 ... 0.0563965 -0.34375 0.59375]\n",
            "  [0.339844 -1.03906 0.165039 ... 0.228516 -0.103516 0.3125]\n",
            "  ...\n",
            "  [0.417969 -0.882812 -0.375 ... 0.332031 -0.235352 -0.0368652]\n",
            "  [-0.339844 -0.582031 -0.550781 ... -0.097168 -0.324219 -0.165039]\n",
            "  [-0.0598145 -0.589844 -0.134766 ... 0.0598145 -0.539062 -0.133789]]\n",
            "\n",
            " [[-0.0991211 -0.628906 0.111816 ... -0.294922 -0.785156 0.457031]\n",
            "  [-0.118164 -0.539062 0.09375 ... 0.0563965 -0.34375 0.59375]\n",
            "  [0.339844 -1.03906 0.165039 ... 0.228516 -0.103516 0.3125]\n",
            "  ...\n",
            "  [0.417969 -0.882812 -0.375 ... 0.332031 -0.235352 -0.0368652]\n",
            "  [-0.339844 -0.582031 -0.550781 ... -0.097168 -0.324219 -0.165039]\n",
            "  [-0.0598145 -0.589844 -0.134766 ... 0.0598145 -0.539062 -0.133789]]\n",
            "\n",
            " [[-0.0991211 -0.628906 0.111816 ... -0.294922 -0.785156 0.457031]\n",
            "  [-0.118164 -0.539062 0.09375 ... 0.0563965 -0.34375 0.59375]\n",
            "  [0.339844 -1.03906 0.165039 ... 0.228516 -0.103516 0.3125]\n",
            "  ...\n",
            "  [0.417969 -0.882812 -0.375 ... 0.332031 -0.235352 -0.0368652]\n",
            "  [-0.339844 -0.582031 -0.550781 ... -0.097168 -0.324219 -0.165039]\n",
            "  [-0.0598145 -0.589844 -0.134766 ... 0.0598145 -0.539062 -0.133789]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.0991211 -0.628906 0.111816 ... -0.294922 -0.785156 0.457031]\n",
            "  [-0.118164 -0.539062 0.09375 ... 0.0563965 -0.34375 0.59375]\n",
            "  [0.339844 -1.03906 0.165039 ... 0.228516 -0.103516 0.3125]\n",
            "  ...\n",
            "  [0.417969 -0.882812 -0.375 ... 0.332031 -0.235352 -0.0368652]\n",
            "  [-0.339844 -0.582031 -0.550781 ... -0.097168 -0.324219 -0.165039]\n",
            "  [-0.0598145 -0.589844 -0.134766 ... 0.0598145 -0.539062 -0.133789]]\n",
            "\n",
            " [[-0.0991211 -0.628906 0.111816 ... -0.294922 -0.785156 0.457031]\n",
            "  [-0.118164 -0.539062 0.09375 ... 0.0563965 -0.34375 0.59375]\n",
            "  [0.339844 -1.03906 0.165039 ... 0.228516 -0.103516 0.3125]\n",
            "  ...\n",
            "  [0.417969 -0.882812 -0.375 ... 0.332031 -0.235352 -0.0368652]\n",
            "  [-0.339844 -0.582031 -0.550781 ... -0.097168 -0.324219 -0.165039]\n",
            "  [-0.0598145 -0.589844 -0.134766 ... 0.0598145 -0.539062 -0.133789]]\n",
            "\n",
            " [[-0.0991211 -0.628906 0.111816 ... -0.294922 -0.785156 0.457031]\n",
            "  [-0.118164 -0.539062 0.09375 ... 0.0563965 -0.34375 0.59375]\n",
            "  [0.339844 -1.03906 0.165039 ... 0.228516 -0.103516 0.3125]\n",
            "  ...\n",
            "  [0.417969 -0.882812 -0.375 ... 0.332031 -0.235352 -0.0368652]\n",
            "  [-0.339844 -0.582031 -0.550781 ... -0.097168 -0.324219 -0.165039]\n",
            "  [-0.0598145 -0.589844 -0.134766 ... 0.0598145 -0.539062 -0.133789]]]\n",
            "attn_output=[[[-0.820312 1.82812 0.328125 ... -0.871094 1.57031 0.605469]\n",
            "  [-1.63281 1.29688 -0.478516 ... -0.0698242 0.796875 1.03125]\n",
            "  [-0.337891 1.96094 -1.66406 ... -1.0625 0.867188 0.917969]\n",
            "  ...\n",
            "  [1.65625 2.07812 -1.03125 ... 0.170898 1.34375 0.433594]\n",
            "  [-0.679688 1.39062 -0.566406 ... -0.435547 0.0214844 -0.0480957]\n",
            "  [-0.949219 2.35938 0.0957031 ... -1.0625 0.123535 -0.878906]]\n",
            "\n",
            " [[-0.820312 1.82812 0.328125 ... -0.871094 1.57031 0.605469]\n",
            "  [-1.63281 1.29688 -0.478516 ... -0.0698242 0.796875 1.03125]\n",
            "  [-0.337891 1.96094 -1.66406 ... -1.0625 0.867188 0.917969]\n",
            "  ...\n",
            "  [1.65625 2.07812 -1.03125 ... 0.170898 1.34375 0.433594]\n",
            "  [-0.679688 1.39062 -0.566406 ... -0.435547 0.0214844 -0.0480957]\n",
            "  [-0.949219 2.35938 0.0957031 ... -1.0625 0.123535 -0.878906]]\n",
            "\n",
            " [[-0.820312 1.82812 0.328125 ... -0.871094 1.57031 0.605469]\n",
            "  [-1.63281 1.29688 -0.478516 ... -0.0698242 0.796875 1.03125]\n",
            "  [-0.337891 1.96094 -1.66406 ... -1.0625 0.867188 0.917969]\n",
            "  ...\n",
            "  [1.65625 2.07812 -1.03125 ... 0.170898 1.34375 0.433594]\n",
            "  [-0.679688 1.39062 -0.566406 ... -0.435547 0.0214844 -0.0480957]\n",
            "  [-0.949219 2.35938 0.0957031 ... -1.0625 0.123535 -0.878906]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.820312 1.82812 0.328125 ... -0.871094 1.57031 0.605469]\n",
            "  [-1.63281 1.29688 -0.478516 ... -0.0698242 0.796875 1.03125]\n",
            "  [-0.337891 1.96094 -1.66406 ... -1.0625 0.867188 0.917969]\n",
            "  ...\n",
            "  [1.65625 2.07812 -1.03125 ... 0.170898 1.34375 0.433594]\n",
            "  [-0.679688 1.39062 -0.566406 ... -0.435547 0.0214844 -0.0480957]\n",
            "  [-0.949219 2.35938 0.0957031 ... -1.0625 0.123535 -0.878906]]\n",
            "\n",
            " [[-0.820312 1.82812 0.328125 ... -0.871094 1.57031 0.605469]\n",
            "  [-1.63281 1.29688 -0.478516 ... -0.0698242 0.796875 1.03125]\n",
            "  [-0.337891 1.96094 -1.66406 ... -1.0625 0.867188 0.917969]\n",
            "  ...\n",
            "  [1.65625 2.07812 -1.03125 ... 0.170898 1.34375 0.433594]\n",
            "  [-0.679688 1.39062 -0.566406 ... -0.435547 0.0214844 -0.0480957]\n",
            "  [-0.949219 2.35938 0.0957031 ... -1.0625 0.123535 -0.878906]]\n",
            "\n",
            " [[-0.820312 1.82812 0.328125 ... -0.871094 1.57031 0.605469]\n",
            "  [-1.63281 1.29688 -0.478516 ... -0.0698242 0.796875 1.03125]\n",
            "  [-0.337891 1.96094 -1.66406 ... -1.0625 0.867188 0.917969]\n",
            "  ...\n",
            "  [1.65625 2.07812 -1.03125 ... 0.170898 1.34375 0.433594]\n",
            "  [-0.679688 1.39062 -0.566406 ... -0.435547 0.0214844 -0.0480957]\n",
            "  [-0.949219 2.35938 0.0957031 ... -1.0625 0.123535 -0.878906]]]\n",
            "next_layer_addition_dropped_out=[[[-2.03125 6.9375 0.535156 ... -2.34375 5.875 3.0625]\n",
            "  [-4.40625 4.15625 -1.58594 ... 0.992188 2.375 2.98438]\n",
            "  [-1.33594 6.40625 -5.15625 ... -2.40625 3.40625 3.0625]\n",
            "  ...\n",
            "  [5.25 6.3125 -1.84375 ... -0.0373535 2.60938 2.625]\n",
            "  [-1.89062 4.3125 -2.0625 ... -2.01562 -0.229492 -0.133789]\n",
            "  [-2.54688 6.3125 0.236328 ... -3.5625 0.112793 -2.17188]]\n",
            "\n",
            " [[-2.03125 6.9375 0.535156 ... -2.34375 5.875 3.0625]\n",
            "  [-4.40625 4.15625 -1.58594 ... 0.992188 2.375 2.98438]\n",
            "  [-1.33594 6.40625 -5.15625 ... -2.40625 3.40625 3.0625]\n",
            "  ...\n",
            "  [5.25 6.3125 -1.84375 ... -0.0373535 2.60938 2.625]\n",
            "  [-1.89062 4.3125 -2.0625 ... -2.01562 -0.229492 -0.133789]\n",
            "  [-2.54688 6.3125 0.236328 ... -3.5625 0.112793 -2.17188]]\n",
            "\n",
            " [[-2.03125 6.9375 0.535156 ... -2.34375 5.875 3.0625]\n",
            "  [-4.40625 4.15625 -1.58594 ... 0.992188 2.375 2.98438]\n",
            "  [-1.33594 6.40625 -5.15625 ... -2.40625 3.40625 3.0625]\n",
            "  ...\n",
            "  [5.25 6.3125 -1.84375 ... -0.0373535 2.60938 2.625]\n",
            "  [-1.89062 4.3125 -2.0625 ... -2.01562 -0.229492 -0.133789]\n",
            "  [-2.54688 6.3125 0.236328 ... -3.5625 0.112793 -2.17188]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-2.03125 6.9375 0.535156 ... -2.34375 5.875 3.0625]\n",
            "  [-4.40625 4.15625 -1.58594 ... 0.992188 2.375 2.98438]\n",
            "  [-1.33594 6.40625 -5.15625 ... -2.40625 3.40625 3.0625]\n",
            "  ...\n",
            "  [5.25 6.3125 -1.84375 ... -0.0373535 2.60938 2.625]\n",
            "  [-1.89062 4.3125 -2.0625 ... -2.01562 -0.229492 -0.133789]\n",
            "  [-2.54688 6.3125 0.236328 ... -3.5625 0.112793 -2.17188]]\n",
            "\n",
            " [[-2.03125 6.9375 0.535156 ... -2.34375 5.875 3.0625]\n",
            "  [-4.40625 4.15625 -1.58594 ... 0.992188 2.375 2.98438]\n",
            "  [-1.33594 6.40625 -5.15625 ... -2.40625 3.40625 3.0625]\n",
            "  ...\n",
            "  [5.25 6.3125 -1.84375 ... -0.0373535 2.60938 2.625]\n",
            "  [-1.89062 4.3125 -2.0625 ... -2.01562 -0.229492 -0.133789]\n",
            "  [-2.54688 6.3125 0.236328 ... -3.5625 0.112793 -2.17188]]\n",
            "\n",
            " [[-2.03125 6.9375 0.535156 ... -2.34375 5.875 3.0625]\n",
            "  [-4.40625 4.15625 -1.58594 ... 0.992188 2.375 2.98438]\n",
            "  [-1.33594 6.40625 -5.15625 ... -2.40625 3.40625 3.0625]\n",
            "  ...\n",
            "  [5.25 6.3125 -1.84375 ... -0.0373535 2.60938 2.625]\n",
            "  [-1.89062 4.3125 -2.0625 ... -2.01562 -0.229492 -0.133789]\n",
            "  [-2.54688 6.3125 0.236328 ... -3.5625 0.112793 -2.17188]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00]]]]]\n",
            "inputs=[[[-2.03125 6.9375 0.535156 ... -2.34375 5.875 3.0625]\n",
            "  [-4.40625 4.15625 -1.58594 ... 0.992188 2.375 2.98438]\n",
            "  [-1.33594 6.40625 -5.15625 ... -2.40625 3.40625 3.0625]\n",
            "  ...\n",
            "  [5.25 6.3125 -1.84375 ... -0.0373535 2.60938 2.625]\n",
            "  [-1.89062 4.3125 -2.0625 ... -2.01562 -0.229492 -0.133789]\n",
            "  [-2.54688 6.3125 0.236328 ... -3.5625 0.112793 -2.17188]]\n",
            "\n",
            " [[-2.03125 6.9375 0.535156 ... -2.34375 5.875 3.0625]\n",
            "  [-4.40625 4.15625 -1.58594 ... 0.992188 2.375 2.98438]\n",
            "  [-1.33594 6.40625 -5.15625 ... -2.40625 3.40625 3.0625]\n",
            "  ...\n",
            "  [5.25 6.3125 -1.84375 ... -0.0373535 2.60938 2.625]\n",
            "  [-1.89062 4.3125 -2.0625 ... -2.01562 -0.229492 -0.133789]\n",
            "  [-2.54688 6.3125 0.236328 ... -3.5625 0.112793 -2.17188]]\n",
            "\n",
            " [[-2.03125 6.9375 0.535156 ... -2.34375 5.875 3.0625]\n",
            "  [-4.40625 4.15625 -1.58594 ... 0.992188 2.375 2.98438]\n",
            "  [-1.33594 6.40625 -5.15625 ... -2.40625 3.40625 3.0625]\n",
            "  ...\n",
            "  [5.25 6.3125 -1.84375 ... -0.0373535 2.60938 2.625]\n",
            "  [-1.89062 4.3125 -2.0625 ... -2.01562 -0.229492 -0.133789]\n",
            "  [-2.54688 6.3125 0.236328 ... -3.5625 0.112793 -2.17188]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-2.03125 6.9375 0.535156 ... -2.34375 5.875 3.0625]\n",
            "  [-4.40625 4.15625 -1.58594 ... 0.992188 2.375 2.98438]\n",
            "  [-1.33594 6.40625 -5.15625 ... -2.40625 3.40625 3.0625]\n",
            "  ...\n",
            "  [5.25 6.3125 -1.84375 ... -0.0373535 2.60938 2.625]\n",
            "  [-1.89062 4.3125 -2.0625 ... -2.01562 -0.229492 -0.133789]\n",
            "  [-2.54688 6.3125 0.236328 ... -3.5625 0.112793 -2.17188]]\n",
            "\n",
            " [[-2.03125 6.9375 0.535156 ... -2.34375 5.875 3.0625]\n",
            "  [-4.40625 4.15625 -1.58594 ... 0.992188 2.375 2.98438]\n",
            "  [-1.33594 6.40625 -5.15625 ... -2.40625 3.40625 3.0625]\n",
            "  ...\n",
            "  [5.25 6.3125 -1.84375 ... -0.0373535 2.60938 2.625]\n",
            "  [-1.89062 4.3125 -2.0625 ... -2.01562 -0.229492 -0.133789]\n",
            "  [-2.54688 6.3125 0.236328 ... -3.5625 0.112793 -2.17188]]\n",
            "\n",
            " [[-2.03125 6.9375 0.535156 ... -2.34375 5.875 3.0625]\n",
            "  [-4.40625 4.15625 -1.58594 ... 0.992188 2.375 2.98438]\n",
            "  [-1.33594 6.40625 -5.15625 ... -2.40625 3.40625 3.0625]\n",
            "  ...\n",
            "  [5.25 6.3125 -1.84375 ... -0.0373535 2.60938 2.625]\n",
            "  [-1.89062 4.3125 -2.0625 ... -2.01562 -0.229492 -0.133789]\n",
            "  [-2.54688 6.3125 0.236328 ... -3.5625 0.112793 -2.17188]]]\n",
            "lnx=[[[-0.546875 1.86719 0.143555 ... -0.628906 1.57812 0.824219]\n",
            "  [-1.32031 1.24219 -0.474609 ... 0.296875 0.710938 0.894531]\n",
            "  [-0.427734 2.04688 -1.64844 ... -0.769531 1.09375 0.980469]\n",
            "  ...\n",
            "  [1.89844 2.28125 -0.667969 ... -0.0135498 0.945312 0.949219]\n",
            "  [-0.625 1.42188 -0.679688 ... -0.664062 -0.0756836 -0.0441895]\n",
            "  [-0.878906 2.1875 0.081543 ... -1.23438 0.0390625 -0.75]]\n",
            "\n",
            " [[-0.546875 1.86719 0.143555 ... -0.628906 1.57812 0.824219]\n",
            "  [-1.32031 1.24219 -0.474609 ... 0.296875 0.710938 0.894531]\n",
            "  [-0.427734 2.04688 -1.64844 ... -0.769531 1.09375 0.980469]\n",
            "  ...\n",
            "  [1.89844 2.28125 -0.667969 ... -0.0135498 0.945312 0.949219]\n",
            "  [-0.625 1.42188 -0.679688 ... -0.664062 -0.0756836 -0.0441895]\n",
            "  [-0.878906 2.1875 0.081543 ... -1.23438 0.0390625 -0.75]]\n",
            "\n",
            " [[-0.546875 1.86719 0.143555 ... -0.628906 1.57812 0.824219]\n",
            "  [-1.32031 1.24219 -0.474609 ... 0.296875 0.710938 0.894531]\n",
            "  [-0.427734 2.04688 -1.64844 ... -0.769531 1.09375 0.980469]\n",
            "  ...\n",
            "  [1.89844 2.28125 -0.667969 ... -0.0135498 0.945312 0.949219]\n",
            "  [-0.625 1.42188 -0.679688 ... -0.664062 -0.0756836 -0.0441895]\n",
            "  [-0.878906 2.1875 0.081543 ... -1.23438 0.0390625 -0.75]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.546875 1.86719 0.143555 ... -0.628906 1.57812 0.824219]\n",
            "  [-1.32031 1.24219 -0.474609 ... 0.296875 0.710938 0.894531]\n",
            "  [-0.427734 2.04688 -1.64844 ... -0.769531 1.09375 0.980469]\n",
            "  ...\n",
            "  [1.89844 2.28125 -0.667969 ... -0.0135498 0.945312 0.949219]\n",
            "  [-0.625 1.42188 -0.679688 ... -0.664062 -0.0756836 -0.0441895]\n",
            "  [-0.878906 2.1875 0.081543 ... -1.23438 0.0390625 -0.75]]\n",
            "\n",
            " [[-0.546875 1.86719 0.143555 ... -0.628906 1.57812 0.824219]\n",
            "  [-1.32031 1.24219 -0.474609 ... 0.296875 0.710938 0.894531]\n",
            "  [-0.427734 2.04688 -1.64844 ... -0.769531 1.09375 0.980469]\n",
            "  ...\n",
            "  [1.89844 2.28125 -0.667969 ... -0.0135498 0.945312 0.949219]\n",
            "  [-0.625 1.42188 -0.679688 ... -0.664062 -0.0756836 -0.0441895]\n",
            "  [-0.878906 2.1875 0.081543 ... -1.23438 0.0390625 -0.75]]\n",
            "\n",
            " [[-0.546875 1.86719 0.143555 ... -0.628906 1.57812 0.824219]\n",
            "  [-1.32031 1.24219 -0.474609 ... 0.296875 0.710938 0.894531]\n",
            "  [-0.427734 2.04688 -1.64844 ... -0.769531 1.09375 0.980469]\n",
            "  ...\n",
            "  [1.89844 2.28125 -0.667969 ... -0.0135498 0.945312 0.949219]\n",
            "  [-0.625 1.42188 -0.679688 ... -0.664062 -0.0756836 -0.0441895]\n",
            "  [-0.878906 2.1875 0.081543 ... -1.23438 0.0390625 -0.75]]]\n",
            "attention_lnx=[[[-0.742188 1.07031 -0.482422 ... -0.570312 1.8125 -0.320312]\n",
            "  [-0.443359 0.980469 -0.9375 ... -0.341797 1.32031 -0.457031]\n",
            "  [-0.0776367 0.96875 -1.17188 ... -0.110352 1.27344 0.0410156]\n",
            "  ...\n",
            "  [-0.322266 0.90625 -1.64062 ... 0.271484 -0.149414 -0.558594]\n",
            "  [-0.523438 0.12793 -1.33594 ... -0.263672 0.015564 -0.261719]\n",
            "  [-0.388672 0.199219 -1.10938 ... 0.326172 0.136719 -0.431641]]\n",
            "\n",
            " [[-0.742188 1.07031 -0.482422 ... -0.570312 1.8125 -0.320312]\n",
            "  [-0.443359 0.980469 -0.9375 ... -0.341797 1.32031 -0.457031]\n",
            "  [-0.0776367 0.96875 -1.17188 ... -0.110352 1.27344 0.0410156]\n",
            "  ...\n",
            "  [-0.322266 0.90625 -1.64062 ... 0.271484 -0.149414 -0.558594]\n",
            "  [-0.523438 0.12793 -1.33594 ... -0.263672 0.015564 -0.261719]\n",
            "  [-0.388672 0.199219 -1.10938 ... 0.326172 0.136719 -0.431641]]\n",
            "\n",
            " [[-0.742188 1.07031 -0.482422 ... -0.570312 1.8125 -0.320312]\n",
            "  [-0.443359 0.980469 -0.9375 ... -0.341797 1.32031 -0.457031]\n",
            "  [-0.0776367 0.96875 -1.17188 ... -0.110352 1.27344 0.0410156]\n",
            "  ...\n",
            "  [-0.322266 0.90625 -1.64062 ... 0.271484 -0.149414 -0.558594]\n",
            "  [-0.523438 0.12793 -1.33594 ... -0.263672 0.015564 -0.261719]\n",
            "  [-0.388672 0.199219 -1.10938 ... 0.326172 0.136719 -0.431641]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.742188 1.07031 -0.482422 ... -0.570312 1.8125 -0.320312]\n",
            "  [-0.443359 0.980469 -0.9375 ... -0.341797 1.32031 -0.457031]\n",
            "  [-0.0776367 0.96875 -1.17188 ... -0.110352 1.27344 0.0410156]\n",
            "  ...\n",
            "  [-0.322266 0.90625 -1.64062 ... 0.271484 -0.149414 -0.558594]\n",
            "  [-0.523438 0.12793 -1.33594 ... -0.263672 0.015564 -0.261719]\n",
            "  [-0.388672 0.199219 -1.10938 ... 0.326172 0.136719 -0.431641]]\n",
            "\n",
            " [[-0.742188 1.07031 -0.482422 ... -0.570312 1.8125 -0.320312]\n",
            "  [-0.443359 0.980469 -0.9375 ... -0.341797 1.32031 -0.457031]\n",
            "  [-0.0776367 0.96875 -1.17188 ... -0.110352 1.27344 0.0410156]\n",
            "  ...\n",
            "  [-0.322266 0.90625 -1.64062 ... 0.271484 -0.149414 -0.558594]\n",
            "  [-0.523438 0.12793 -1.33594 ... -0.263672 0.015564 -0.261719]\n",
            "  [-0.388672 0.199219 -1.10938 ... 0.326172 0.136719 -0.431641]]\n",
            "\n",
            " [[-0.742188 1.07031 -0.482422 ... -0.570312 1.8125 -0.320312]\n",
            "  [-0.443359 0.980469 -0.9375 ... -0.341797 1.32031 -0.457031]\n",
            "  [-0.0776367 0.96875 -1.17188 ... -0.110352 1.27344 0.0410156]\n",
            "  ...\n",
            "  [-0.322266 0.90625 -1.64062 ... 0.271484 -0.149414 -0.558594]\n",
            "  [-0.523438 0.12793 -1.33594 ... -0.263672 0.015564 -0.261719]\n",
            "  [-0.388672 0.199219 -1.10938 ... 0.326172 0.136719 -0.431641]]]\n",
            "attn_output=[[[-0.71875 2.07812 0.013855 ... -0.753906 1.99219 0.714844]\n",
            "  [-1.39062 1.46875 -0.722656 ... 0.1875 1.0625 0.726562]\n",
            "  [-0.4375 2.28125 -1.96094 ... -0.777344 1.45312 0.964844]\n",
            "  ...\n",
            "  [1.73438 2.53125 -1.22656 ... 0.0825195 0.863281 0.726562]\n",
            "  [-0.773438 1.42969 -1.09375 ... -0.734375 -0.0688477 -0.126953]\n",
            "  [-0.992188 2.20312 -0.294922 ... -1.09375 0.0844727 -0.882812]]\n",
            "\n",
            " [[-0.71875 2.07812 0.013855 ... -0.753906 1.99219 0.714844]\n",
            "  [-1.39062 1.46875 -0.722656 ... 0.1875 1.0625 0.726562]\n",
            "  [-0.4375 2.28125 -1.96094 ... -0.777344 1.45312 0.964844]\n",
            "  ...\n",
            "  [1.73438 2.53125 -1.22656 ... 0.0825195 0.863281 0.726562]\n",
            "  [-0.773438 1.42969 -1.09375 ... -0.734375 -0.0688477 -0.126953]\n",
            "  [-0.992188 2.20312 -0.294922 ... -1.09375 0.0844727 -0.882812]]\n",
            "\n",
            " [[-0.71875 2.07812 0.013855 ... -0.753906 1.99219 0.714844]\n",
            "  [-1.39062 1.46875 -0.722656 ... 0.1875 1.0625 0.726562]\n",
            "  [-0.4375 2.28125 -1.96094 ... -0.777344 1.45312 0.964844]\n",
            "  ...\n",
            "  [1.73438 2.53125 -1.22656 ... 0.0825195 0.863281 0.726562]\n",
            "  [-0.773438 1.42969 -1.09375 ... -0.734375 -0.0688477 -0.126953]\n",
            "  [-0.992188 2.20312 -0.294922 ... -1.09375 0.0844727 -0.882812]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.71875 2.07812 0.013855 ... -0.753906 1.99219 0.714844]\n",
            "  [-1.39062 1.46875 -0.722656 ... 0.1875 1.0625 0.726562]\n",
            "  [-0.4375 2.28125 -1.96094 ... -0.777344 1.45312 0.964844]\n",
            "  ...\n",
            "  [1.73438 2.53125 -1.22656 ... 0.0825195 0.863281 0.726562]\n",
            "  [-0.773438 1.42969 -1.09375 ... -0.734375 -0.0688477 -0.126953]\n",
            "  [-0.992188 2.20312 -0.294922 ... -1.09375 0.0844727 -0.882812]]\n",
            "\n",
            " [[-0.71875 2.07812 0.013855 ... -0.753906 1.99219 0.714844]\n",
            "  [-1.39062 1.46875 -0.722656 ... 0.1875 1.0625 0.726562]\n",
            "  [-0.4375 2.28125 -1.96094 ... -0.777344 1.45312 0.964844]\n",
            "  ...\n",
            "  [1.73438 2.53125 -1.22656 ... 0.0825195 0.863281 0.726562]\n",
            "  [-0.773438 1.42969 -1.09375 ... -0.734375 -0.0688477 -0.126953]\n",
            "  [-0.992188 2.20312 -0.294922 ... -1.09375 0.0844727 -0.882812]]\n",
            "\n",
            " [[-0.71875 2.07812 0.013855 ... -0.753906 1.99219 0.714844]\n",
            "  [-1.39062 1.46875 -0.722656 ... 0.1875 1.0625 0.726562]\n",
            "  [-0.4375 2.28125 -1.96094 ... -0.777344 1.45312 0.964844]\n",
            "  ...\n",
            "  [1.73438 2.53125 -1.22656 ... 0.0825195 0.863281 0.726562]\n",
            "  [-0.773438 1.42969 -1.09375 ... -0.734375 -0.0688477 -0.126953]\n",
            "  [-0.992188 2.20312 -0.294922 ... -1.09375 0.0844727 -0.882812]]]\n",
            "next_layer_addition_dropped_out=[[[-3.48438 7.46875 -0.78125 ... -3.10938 8.375 2.14062]\n",
            "  [-5.25 5 -2.71875 ... 0.765625 4.9375 2.73438]\n",
            "  [-2.3125 6.5 -6.1875 ... -2.60938 5.96875 2.17188]\n",
            "  ...\n",
            "  [4.375 6.5625 -3.84375 ... 0.492188 2.59375 2.9375]\n",
            "  [-2.84375 5.21875 -2.6875 ... -2.9375 0.332031 -0.296875]\n",
            "  [-4.40625 5.03125 -1.6875 ... -2.26562 -1.41406 -3.4375]]\n",
            "\n",
            " [[-3.48438 7.46875 -0.78125 ... -3.10938 8.375 2.14062]\n",
            "  [-5.25 5 -2.71875 ... 0.765625 4.9375 2.73438]\n",
            "  [-2.3125 6.5 -6.1875 ... -2.60938 5.96875 2.17188]\n",
            "  ...\n",
            "  [4.375 6.5625 -3.84375 ... 0.492188 2.59375 2.9375]\n",
            "  [-2.84375 5.21875 -2.6875 ... -2.9375 0.332031 -0.296875]\n",
            "  [-4.40625 5.03125 -1.6875 ... -2.26562 -1.41406 -3.4375]]\n",
            "\n",
            " [[-3.48438 7.46875 -0.78125 ... -3.10938 8.375 2.14062]\n",
            "  [-5.25 5 -2.71875 ... 0.765625 4.9375 2.73438]\n",
            "  [-2.3125 6.5 -6.1875 ... -2.60938 5.96875 2.17188]\n",
            "  ...\n",
            "  [4.375 6.5625 -3.84375 ... 0.492188 2.59375 2.9375]\n",
            "  [-2.84375 5.21875 -2.6875 ... -2.9375 0.332031 -0.296875]\n",
            "  [-4.40625 5.03125 -1.6875 ... -2.26562 -1.41406 -3.4375]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-3.48438 7.46875 -0.78125 ... -3.10938 8.375 2.14062]\n",
            "  [-5.25 5 -2.71875 ... 0.765625 4.9375 2.73438]\n",
            "  [-2.3125 6.5 -6.1875 ... -2.60938 5.96875 2.17188]\n",
            "  ...\n",
            "  [4.375 6.5625 -3.84375 ... 0.492188 2.59375 2.9375]\n",
            "  [-2.84375 5.21875 -2.6875 ... -2.9375 0.332031 -0.296875]\n",
            "  [-4.40625 5.03125 -1.6875 ... -2.26562 -1.41406 -3.4375]]\n",
            "\n",
            " [[-3.48438 7.46875 -0.78125 ... -3.10938 8.375 2.14062]\n",
            "  [-5.25 5 -2.71875 ... 0.765625 4.9375 2.73438]\n",
            "  [-2.3125 6.5 -6.1875 ... -2.60938 5.96875 2.17188]\n",
            "  ...\n",
            "  [4.375 6.5625 -3.84375 ... 0.492188 2.59375 2.9375]\n",
            "  [-2.84375 5.21875 -2.6875 ... -2.9375 0.332031 -0.296875]\n",
            "  [-4.40625 5.03125 -1.6875 ... -2.26562 -1.41406 -3.4375]]\n",
            "\n",
            " [[-3.48438 7.46875 -0.78125 ... -3.10938 8.375 2.14062]\n",
            "  [-5.25 5 -2.71875 ... 0.765625 4.9375 2.73438]\n",
            "  [-2.3125 6.5 -6.1875 ... -2.60938 5.96875 2.17188]\n",
            "  ...\n",
            "  [4.375 6.5625 -3.84375 ... 0.492188 2.59375 2.9375]\n",
            "  [-2.84375 5.21875 -2.6875 ... -2.9375 0.332031 -0.296875]\n",
            "  [-4.40625 5.03125 -1.6875 ... -2.26562 -1.41406 -3.4375]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00]]]]]\n",
            "inputs=[[[-3.48438 7.46875 -0.78125 ... -3.10938 8.375 2.14062]\n",
            "  [-5.25 5 -2.71875 ... 0.765625 4.9375 2.73438]\n",
            "  [-2.3125 6.5 -6.1875 ... -2.60938 5.96875 2.17188]\n",
            "  ...\n",
            "  [4.375 6.5625 -3.84375 ... 0.492188 2.59375 2.9375]\n",
            "  [-2.84375 5.21875 -2.6875 ... -2.9375 0.332031 -0.296875]\n",
            "  [-4.40625 5.03125 -1.6875 ... -2.26562 -1.41406 -3.4375]]\n",
            "\n",
            " [[-3.48438 7.46875 -0.78125 ... -3.10938 8.375 2.14062]\n",
            "  [-5.25 5 -2.71875 ... 0.765625 4.9375 2.73438]\n",
            "  [-2.3125 6.5 -6.1875 ... -2.60938 5.96875 2.17188]\n",
            "  ...\n",
            "  [4.375 6.5625 -3.84375 ... 0.492188 2.59375 2.9375]\n",
            "  [-2.84375 5.21875 -2.6875 ... -2.9375 0.332031 -0.296875]\n",
            "  [-4.40625 5.03125 -1.6875 ... -2.26562 -1.41406 -3.4375]]\n",
            "\n",
            " [[-3.48438 7.46875 -0.78125 ... -3.10938 8.375 2.14062]\n",
            "  [-5.25 5 -2.71875 ... 0.765625 4.9375 2.73438]\n",
            "  [-2.3125 6.5 -6.1875 ... -2.60938 5.96875 2.17188]\n",
            "  ...\n",
            "  [4.375 6.5625 -3.84375 ... 0.492188 2.59375 2.9375]\n",
            "  [-2.84375 5.21875 -2.6875 ... -2.9375 0.332031 -0.296875]\n",
            "  [-4.40625 5.03125 -1.6875 ... -2.26562 -1.41406 -3.4375]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-3.48438 7.46875 -0.78125 ... -3.10938 8.375 2.14062]\n",
            "  [-5.25 5 -2.71875 ... 0.765625 4.9375 2.73438]\n",
            "  [-2.3125 6.5 -6.1875 ... -2.60938 5.96875 2.17188]\n",
            "  ...\n",
            "  [4.375 6.5625 -3.84375 ... 0.492188 2.59375 2.9375]\n",
            "  [-2.84375 5.21875 -2.6875 ... -2.9375 0.332031 -0.296875]\n",
            "  [-4.40625 5.03125 -1.6875 ... -2.26562 -1.41406 -3.4375]]\n",
            "\n",
            " [[-3.48438 7.46875 -0.78125 ... -3.10938 8.375 2.14062]\n",
            "  [-5.25 5 -2.71875 ... 0.765625 4.9375 2.73438]\n",
            "  [-2.3125 6.5 -6.1875 ... -2.60938 5.96875 2.17188]\n",
            "  ...\n",
            "  [4.375 6.5625 -3.84375 ... 0.492188 2.59375 2.9375]\n",
            "  [-2.84375 5.21875 -2.6875 ... -2.9375 0.332031 -0.296875]\n",
            "  [-4.40625 5.03125 -1.6875 ... -2.26562 -1.41406 -3.4375]]\n",
            "\n",
            " [[-3.48438 7.46875 -0.78125 ... -3.10938 8.375 2.14062]\n",
            "  [-5.25 5 -2.71875 ... 0.765625 4.9375 2.73438]\n",
            "  [-2.3125 6.5 -6.1875 ... -2.60938 5.96875 2.17188]\n",
            "  ...\n",
            "  [4.375 6.5625 -3.84375 ... 0.492188 2.59375 2.9375]\n",
            "  [-2.84375 5.21875 -2.6875 ... -2.9375 0.332031 -0.296875]\n",
            "  [-4.40625 5.03125 -1.6875 ... -2.26562 -1.41406 -3.4375]]]\n",
            "lnx=[[[-0.890625 1.90625 -0.199219 ... -0.792969 2.14062 0.546875]\n",
            "  [-1.48438 1.41406 -0.765625 ... 0.21582 1.39062 0.773438]\n",
            "  [-0.699219 1.96875 -1.86719 ... -0.789062 1.80469 0.65625]\n",
            "  ...\n",
            "  [1.5 2.25 -1.32031 ... 0.168945 0.890625 1.00781]\n",
            "  [-0.871094 1.59375 -0.824219 ... -0.898438 0.101562 -0.0908203]\n",
            "  [-1.42969 1.625 -0.546875 ... -0.734375 -0.457031 -1.10938]]\n",
            "\n",
            " [[-0.890625 1.90625 -0.199219 ... -0.792969 2.14062 0.546875]\n",
            "  [-1.48438 1.41406 -0.765625 ... 0.21582 1.39062 0.773438]\n",
            "  [-0.699219 1.96875 -1.86719 ... -0.789062 1.80469 0.65625]\n",
            "  ...\n",
            "  [1.5 2.25 -1.32031 ... 0.168945 0.890625 1.00781]\n",
            "  [-0.871094 1.59375 -0.824219 ... -0.898438 0.101562 -0.0908203]\n",
            "  [-1.42969 1.625 -0.546875 ... -0.734375 -0.457031 -1.10938]]\n",
            "\n",
            " [[-0.890625 1.90625 -0.199219 ... -0.792969 2.14062 0.546875]\n",
            "  [-1.48438 1.41406 -0.765625 ... 0.21582 1.39062 0.773438]\n",
            "  [-0.699219 1.96875 -1.86719 ... -0.789062 1.80469 0.65625]\n",
            "  ...\n",
            "  [1.5 2.25 -1.32031 ... 0.168945 0.890625 1.00781]\n",
            "  [-0.871094 1.59375 -0.824219 ... -0.898438 0.101562 -0.0908203]\n",
            "  [-1.42969 1.625 -0.546875 ... -0.734375 -0.457031 -1.10938]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.890625 1.90625 -0.199219 ... -0.792969 2.14062 0.546875]\n",
            "  [-1.48438 1.41406 -0.765625 ... 0.21582 1.39062 0.773438]\n",
            "  [-0.699219 1.96875 -1.86719 ... -0.789062 1.80469 0.65625]\n",
            "  ...\n",
            "  [1.5 2.25 -1.32031 ... 0.168945 0.890625 1.00781]\n",
            "  [-0.871094 1.59375 -0.824219 ... -0.898438 0.101562 -0.0908203]\n",
            "  [-1.42969 1.625 -0.546875 ... -0.734375 -0.457031 -1.10938]]\n",
            "\n",
            " [[-0.890625 1.90625 -0.199219 ... -0.792969 2.14062 0.546875]\n",
            "  [-1.48438 1.41406 -0.765625 ... 0.21582 1.39062 0.773438]\n",
            "  [-0.699219 1.96875 -1.86719 ... -0.789062 1.80469 0.65625]\n",
            "  ...\n",
            "  [1.5 2.25 -1.32031 ... 0.168945 0.890625 1.00781]\n",
            "  [-0.871094 1.59375 -0.824219 ... -0.898438 0.101562 -0.0908203]\n",
            "  [-1.42969 1.625 -0.546875 ... -0.734375 -0.457031 -1.10938]]\n",
            "\n",
            " [[-0.890625 1.90625 -0.199219 ... -0.792969 2.14062 0.546875]\n",
            "  [-1.48438 1.41406 -0.765625 ... 0.21582 1.39062 0.773438]\n",
            "  [-0.699219 1.96875 -1.86719 ... -0.789062 1.80469 0.65625]\n",
            "  ...\n",
            "  [1.5 2.25 -1.32031 ... 0.168945 0.890625 1.00781]\n",
            "  [-0.871094 1.59375 -0.824219 ... -0.898438 0.101562 -0.0908203]\n",
            "  [-1.42969 1.625 -0.546875 ... -0.734375 -0.457031 -1.10938]]]\n",
            "attention_lnx=[[[-1.30469 0.6875 -1.28906 ... -1.07031 -0.554688 -0.761719]\n",
            "  [-0.78125 0.484375 -1.66406 ... -0.546875 -0.695312 -0.636719]\n",
            "  [-0.988281 0.476562 -1.57812 ... -0.515625 -0.800781 -0.157227]\n",
            "  ...\n",
            "  [-1.04688 0.245117 -0.542969 ... 0.162109 -0.664062 0.578125]\n",
            "  [-1.60938 0.0722656 -0.102539 ... 0.112305 -0.808594 1.16406]\n",
            "  [-1.14062 0.308594 -0.341797 ... -0.0131226 -0.847656 1.25]]\n",
            "\n",
            " [[-1.30469 0.6875 -1.28906 ... -1.07031 -0.554688 -0.761719]\n",
            "  [-0.78125 0.484375 -1.66406 ... -0.546875 -0.695312 -0.636719]\n",
            "  [-0.988281 0.476562 -1.57812 ... -0.515625 -0.800781 -0.157227]\n",
            "  ...\n",
            "  [-1.04688 0.245117 -0.542969 ... 0.162109 -0.664062 0.578125]\n",
            "  [-1.60938 0.0722656 -0.102539 ... 0.112305 -0.808594 1.16406]\n",
            "  [-1.14062 0.308594 -0.341797 ... -0.0131226 -0.847656 1.25]]\n",
            "\n",
            " [[-1.30469 0.6875 -1.28906 ... -1.07031 -0.554688 -0.761719]\n",
            "  [-0.78125 0.484375 -1.66406 ... -0.546875 -0.695312 -0.636719]\n",
            "  [-0.988281 0.476562 -1.57812 ... -0.515625 -0.800781 -0.157227]\n",
            "  ...\n",
            "  [-1.04688 0.245117 -0.542969 ... 0.162109 -0.664062 0.578125]\n",
            "  [-1.60938 0.0722656 -0.102539 ... 0.112305 -0.808594 1.16406]\n",
            "  [-1.14062 0.308594 -0.341797 ... -0.0131226 -0.847656 1.25]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.30469 0.6875 -1.28906 ... -1.07031 -0.554688 -0.761719]\n",
            "  [-0.78125 0.484375 -1.66406 ... -0.546875 -0.695312 -0.636719]\n",
            "  [-0.988281 0.476562 -1.57812 ... -0.515625 -0.800781 -0.157227]\n",
            "  ...\n",
            "  [-1.04688 0.245117 -0.542969 ... 0.162109 -0.664062 0.578125]\n",
            "  [-1.60938 0.0722656 -0.102539 ... 0.112305 -0.808594 1.16406]\n",
            "  [-1.14062 0.308594 -0.341797 ... -0.0131226 -0.847656 1.25]]\n",
            "\n",
            " [[-1.30469 0.6875 -1.28906 ... -1.07031 -0.554688 -0.761719]\n",
            "  [-0.78125 0.484375 -1.66406 ... -0.546875 -0.695312 -0.636719]\n",
            "  [-0.988281 0.476562 -1.57812 ... -0.515625 -0.800781 -0.157227]\n",
            "  ...\n",
            "  [-1.04688 0.245117 -0.542969 ... 0.162109 -0.664062 0.578125]\n",
            "  [-1.60938 0.0722656 -0.102539 ... 0.112305 -0.808594 1.16406]\n",
            "  [-1.14062 0.308594 -0.341797 ... -0.0131226 -0.847656 1.25]]\n",
            "\n",
            " [[-1.30469 0.6875 -1.28906 ... -1.07031 -0.554688 -0.761719]\n",
            "  [-0.78125 0.484375 -1.66406 ... -0.546875 -0.695312 -0.636719]\n",
            "  [-0.988281 0.476562 -1.57812 ... -0.515625 -0.800781 -0.157227]\n",
            "  ...\n",
            "  [-1.04688 0.245117 -0.542969 ... 0.162109 -0.664062 0.578125]\n",
            "  [-1.60938 0.0722656 -0.102539 ... 0.112305 -0.808594 1.16406]\n",
            "  [-1.14062 0.308594 -0.341797 ... -0.0131226 -0.847656 1.25]]]\n",
            "attn_output=[[[-1.17188 2.01562 -0.511719 ... -1.03125 1.92188 0.339844]\n",
            "  [-1.64844 1.50781 -1.19531 ... 0.0605469 1.16406 0.574219]\n",
            "  [-0.96875 2.04688 -2.28125 ... -0.917969 1.51562 0.589844]\n",
            "  ...\n",
            "  [1.10938 2.28125 -1.46094 ... 0.21875 0.644531 1.17188]\n",
            "  [-1.32031 1.57031 -0.832031 ... -0.839844 -0.142578 0.255859]\n",
            "  [-1.74219 1.67969 -0.640625 ... -0.71875 -0.714844 -0.6875]]\n",
            "\n",
            " [[-1.17188 2.01562 -0.511719 ... -1.03125 1.92188 0.339844]\n",
            "  [-1.64844 1.50781 -1.19531 ... 0.0605469 1.16406 0.574219]\n",
            "  [-0.96875 2.04688 -2.28125 ... -0.917969 1.51562 0.589844]\n",
            "  ...\n",
            "  [1.10938 2.28125 -1.46094 ... 0.21875 0.644531 1.17188]\n",
            "  [-1.32031 1.57031 -0.832031 ... -0.839844 -0.142578 0.255859]\n",
            "  [-1.74219 1.67969 -0.640625 ... -0.71875 -0.714844 -0.6875]]\n",
            "\n",
            " [[-1.17188 2.01562 -0.511719 ... -1.03125 1.92188 0.339844]\n",
            "  [-1.64844 1.50781 -1.19531 ... 0.0605469 1.16406 0.574219]\n",
            "  [-0.96875 2.04688 -2.28125 ... -0.917969 1.51562 0.589844]\n",
            "  ...\n",
            "  [1.10938 2.28125 -1.46094 ... 0.21875 0.644531 1.17188]\n",
            "  [-1.32031 1.57031 -0.832031 ... -0.839844 -0.142578 0.255859]\n",
            "  [-1.74219 1.67969 -0.640625 ... -0.71875 -0.714844 -0.6875]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.17188 2.01562 -0.511719 ... -1.03125 1.92188 0.339844]\n",
            "  [-1.64844 1.50781 -1.19531 ... 0.0605469 1.16406 0.574219]\n",
            "  [-0.96875 2.04688 -2.28125 ... -0.917969 1.51562 0.589844]\n",
            "  ...\n",
            "  [1.10938 2.28125 -1.46094 ... 0.21875 0.644531 1.17188]\n",
            "  [-1.32031 1.57031 -0.832031 ... -0.839844 -0.142578 0.255859]\n",
            "  [-1.74219 1.67969 -0.640625 ... -0.71875 -0.714844 -0.6875]]\n",
            "\n",
            " [[-1.17188 2.01562 -0.511719 ... -1.03125 1.92188 0.339844]\n",
            "  [-1.64844 1.50781 -1.19531 ... 0.0605469 1.16406 0.574219]\n",
            "  [-0.96875 2.04688 -2.28125 ... -0.917969 1.51562 0.589844]\n",
            "  ...\n",
            "  [1.10938 2.28125 -1.46094 ... 0.21875 0.644531 1.17188]\n",
            "  [-1.32031 1.57031 -0.832031 ... -0.839844 -0.142578 0.255859]\n",
            "  [-1.74219 1.67969 -0.640625 ... -0.71875 -0.714844 -0.6875]]\n",
            "\n",
            " [[-1.17188 2.01562 -0.511719 ... -1.03125 1.92188 0.339844]\n",
            "  [-1.64844 1.50781 -1.19531 ... 0.0605469 1.16406 0.574219]\n",
            "  [-0.96875 2.04688 -2.28125 ... -0.917969 1.51562 0.589844]\n",
            "  ...\n",
            "  [1.10938 2.28125 -1.46094 ... 0.21875 0.644531 1.17188]\n",
            "  [-1.32031 1.57031 -0.832031 ... -0.839844 -0.142578 0.255859]\n",
            "  [-1.74219 1.67969 -0.640625 ... -0.71875 -0.714844 -0.6875]]]\n",
            "next_layer_addition_dropped_out=[[[-3.92188 7.5 -1.14844 ... -3.625 6.90625 2.20312]\n",
            "  [-5.90625 4.71875 -3.375 ... -0.390625 3.40625 2.3125]\n",
            "  [-3.65625 7.15625 -8.0625 ... -3.96875 4.96875 2.28125]\n",
            "  ...\n",
            "  [4.0625 7.1875 -3.15625 ... 1.14844 2.4375 4.65625]\n",
            "  [-3.39062 3.59375 -3.10938 ... -2.60938 -0.851562 0.914062]\n",
            "  [-5.09375 5.15625 -1.25781 ... -3.0625 -1.71094 -3.26562]]\n",
            "\n",
            " [[-3.92188 7.5 -1.14844 ... -3.625 6.90625 2.20312]\n",
            "  [-5.90625 4.71875 -3.375 ... -0.390625 3.40625 2.3125]\n",
            "  [-3.65625 7.15625 -8.0625 ... -3.96875 4.96875 2.28125]\n",
            "  ...\n",
            "  [4.0625 7.1875 -3.15625 ... 1.14844 2.4375 4.65625]\n",
            "  [-3.39062 3.59375 -3.10938 ... -2.60938 -0.851562 0.914062]\n",
            "  [-5.09375 5.15625 -1.25781 ... -3.0625 -1.71094 -3.26562]]\n",
            "\n",
            " [[-3.92188 7.5 -1.14844 ... -3.625 6.90625 2.20312]\n",
            "  [-5.90625 4.71875 -3.375 ... -0.390625 3.40625 2.3125]\n",
            "  [-3.65625 7.15625 -8.0625 ... -3.96875 4.96875 2.28125]\n",
            "  ...\n",
            "  [4.0625 7.1875 -3.15625 ... 1.14844 2.4375 4.65625]\n",
            "  [-3.39062 3.59375 -3.10938 ... -2.60938 -0.851562 0.914062]\n",
            "  [-5.09375 5.15625 -1.25781 ... -3.0625 -1.71094 -3.26562]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-3.92188 7.5 -1.14844 ... -3.625 6.90625 2.20312]\n",
            "  [-5.90625 4.71875 -3.375 ... -0.390625 3.40625 2.3125]\n",
            "  [-3.65625 7.15625 -8.0625 ... -3.96875 4.96875 2.28125]\n",
            "  ...\n",
            "  [4.0625 7.1875 -3.15625 ... 1.14844 2.4375 4.65625]\n",
            "  [-3.39062 3.59375 -3.10938 ... -2.60938 -0.851562 0.914062]\n",
            "  [-5.09375 5.15625 -1.25781 ... -3.0625 -1.71094 -3.26562]]\n",
            "\n",
            " [[-3.92188 7.5 -1.14844 ... -3.625 6.90625 2.20312]\n",
            "  [-5.90625 4.71875 -3.375 ... -0.390625 3.40625 2.3125]\n",
            "  [-3.65625 7.15625 -8.0625 ... -3.96875 4.96875 2.28125]\n",
            "  ...\n",
            "  [4.0625 7.1875 -3.15625 ... 1.14844 2.4375 4.65625]\n",
            "  [-3.39062 3.59375 -3.10938 ... -2.60938 -0.851562 0.914062]\n",
            "  [-5.09375 5.15625 -1.25781 ... -3.0625 -1.71094 -3.26562]]\n",
            "\n",
            " [[-3.92188 7.5 -1.14844 ... -3.625 6.90625 2.20312]\n",
            "  [-5.90625 4.71875 -3.375 ... -0.390625 3.40625 2.3125]\n",
            "  [-3.65625 7.15625 -8.0625 ... -3.96875 4.96875 2.28125]\n",
            "  ...\n",
            "  [4.0625 7.1875 -3.15625 ... 1.14844 2.4375 4.65625]\n",
            "  [-3.39062 3.59375 -3.10938 ... -2.60938 -0.851562 0.914062]\n",
            "  [-5.09375 5.15625 -1.25781 ... -3.0625 -1.71094 -3.26562]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00]]]]]\n",
            "inputs=[[[-3.92188 7.5 -1.14844 ... -3.625 6.90625 2.20312]\n",
            "  [-5.90625 4.71875 -3.375 ... -0.390625 3.40625 2.3125]\n",
            "  [-3.65625 7.15625 -8.0625 ... -3.96875 4.96875 2.28125]\n",
            "  ...\n",
            "  [4.0625 7.1875 -3.15625 ... 1.14844 2.4375 4.65625]\n",
            "  [-3.39062 3.59375 -3.10938 ... -2.60938 -0.851562 0.914062]\n",
            "  [-5.09375 5.15625 -1.25781 ... -3.0625 -1.71094 -3.26562]]\n",
            "\n",
            " [[-3.92188 7.5 -1.14844 ... -3.625 6.90625 2.20312]\n",
            "  [-5.90625 4.71875 -3.375 ... -0.390625 3.40625 2.3125]\n",
            "  [-3.65625 7.15625 -8.0625 ... -3.96875 4.96875 2.28125]\n",
            "  ...\n",
            "  [4.0625 7.1875 -3.15625 ... 1.14844 2.4375 4.65625]\n",
            "  [-3.39062 3.59375 -3.10938 ... -2.60938 -0.851562 0.914062]\n",
            "  [-5.09375 5.15625 -1.25781 ... -3.0625 -1.71094 -3.26562]]\n",
            "\n",
            " [[-3.92188 7.5 -1.14844 ... -3.625 6.90625 2.20312]\n",
            "  [-5.90625 4.71875 -3.375 ... -0.390625 3.40625 2.3125]\n",
            "  [-3.65625 7.15625 -8.0625 ... -3.96875 4.96875 2.28125]\n",
            "  ...\n",
            "  [4.0625 7.1875 -3.15625 ... 1.14844 2.4375 4.65625]\n",
            "  [-3.39062 3.59375 -3.10938 ... -2.60938 -0.851562 0.914062]\n",
            "  [-5.09375 5.15625 -1.25781 ... -3.0625 -1.71094 -3.26562]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-3.92188 7.5 -1.14844 ... -3.625 6.90625 2.20312]\n",
            "  [-5.90625 4.71875 -3.375 ... -0.390625 3.40625 2.3125]\n",
            "  [-3.65625 7.15625 -8.0625 ... -3.96875 4.96875 2.28125]\n",
            "  ...\n",
            "  [4.0625 7.1875 -3.15625 ... 1.14844 2.4375 4.65625]\n",
            "  [-3.39062 3.59375 -3.10938 ... -2.60938 -0.851562 0.914062]\n",
            "  [-5.09375 5.15625 -1.25781 ... -3.0625 -1.71094 -3.26562]]\n",
            "\n",
            " [[-3.92188 7.5 -1.14844 ... -3.625 6.90625 2.20312]\n",
            "  [-5.90625 4.71875 -3.375 ... -0.390625 3.40625 2.3125]\n",
            "  [-3.65625 7.15625 -8.0625 ... -3.96875 4.96875 2.28125]\n",
            "  ...\n",
            "  [4.0625 7.1875 -3.15625 ... 1.14844 2.4375 4.65625]\n",
            "  [-3.39062 3.59375 -3.10938 ... -2.60938 -0.851562 0.914062]\n",
            "  [-5.09375 5.15625 -1.25781 ... -3.0625 -1.71094 -3.26562]]\n",
            "\n",
            " [[-3.92188 7.5 -1.14844 ... -3.625 6.90625 2.20312]\n",
            "  [-5.90625 4.71875 -3.375 ... -0.390625 3.40625 2.3125]\n",
            "  [-3.65625 7.15625 -8.0625 ... -3.96875 4.96875 2.28125]\n",
            "  ...\n",
            "  [4.0625 7.1875 -3.15625 ... 1.14844 2.4375 4.65625]\n",
            "  [-3.39062 3.59375 -3.10938 ... -2.60938 -0.851562 0.914062]\n",
            "  [-5.09375 5.15625 -1.25781 ... -3.0625 -1.71094 -3.26562]]]\n",
            "lnx=[[[-0.949219 1.82031 -0.279297 ... -0.878906 1.67188 0.535156]\n",
            "  [-1.58594 1.27344 -0.90625 ... -0.10498 0.917969 0.621094]\n",
            "  [-1.0625 2.07812 -2.34375 ... -1.14844 1.4375 0.660156]\n",
            "  ...\n",
            "  [1.32812 2.35938 -1.03125 ... 0.375 0.796875 1.52344]\n",
            "  [-0.941406 1 -0.863281 ... -0.726562 -0.236328 0.253906]\n",
            "  [-1.53906 1.5625 -0.380859 ... -0.925781 -0.519531 -0.988281]]\n",
            "\n",
            " [[-0.949219 1.82031 -0.279297 ... -0.878906 1.67188 0.535156]\n",
            "  [-1.58594 1.27344 -0.90625 ... -0.10498 0.917969 0.621094]\n",
            "  [-1.0625 2.07812 -2.34375 ... -1.14844 1.4375 0.660156]\n",
            "  ...\n",
            "  [1.32812 2.35938 -1.03125 ... 0.375 0.796875 1.52344]\n",
            "  [-0.941406 1 -0.863281 ... -0.726562 -0.236328 0.253906]\n",
            "  [-1.53906 1.5625 -0.380859 ... -0.925781 -0.519531 -0.988281]]\n",
            "\n",
            " [[-0.949219 1.82031 -0.279297 ... -0.878906 1.67188 0.535156]\n",
            "  [-1.58594 1.27344 -0.90625 ... -0.10498 0.917969 0.621094]\n",
            "  [-1.0625 2.07812 -2.34375 ... -1.14844 1.4375 0.660156]\n",
            "  ...\n",
            "  [1.32812 2.35938 -1.03125 ... 0.375 0.796875 1.52344]\n",
            "  [-0.941406 1 -0.863281 ... -0.726562 -0.236328 0.253906]\n",
            "  [-1.53906 1.5625 -0.380859 ... -0.925781 -0.519531 -0.988281]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.949219 1.82031 -0.279297 ... -0.878906 1.67188 0.535156]\n",
            "  [-1.58594 1.27344 -0.90625 ... -0.10498 0.917969 0.621094]\n",
            "  [-1.0625 2.07812 -2.34375 ... -1.14844 1.4375 0.660156]\n",
            "  ...\n",
            "  [1.32812 2.35938 -1.03125 ... 0.375 0.796875 1.52344]\n",
            "  [-0.941406 1 -0.863281 ... -0.726562 -0.236328 0.253906]\n",
            "  [-1.53906 1.5625 -0.380859 ... -0.925781 -0.519531 -0.988281]]\n",
            "\n",
            " [[-0.949219 1.82031 -0.279297 ... -0.878906 1.67188 0.535156]\n",
            "  [-1.58594 1.27344 -0.90625 ... -0.10498 0.917969 0.621094]\n",
            "  [-1.0625 2.07812 -2.34375 ... -1.14844 1.4375 0.660156]\n",
            "  ...\n",
            "  [1.32812 2.35938 -1.03125 ... 0.375 0.796875 1.52344]\n",
            "  [-0.941406 1 -0.863281 ... -0.726562 -0.236328 0.253906]\n",
            "  [-1.53906 1.5625 -0.380859 ... -0.925781 -0.519531 -0.988281]]\n",
            "\n",
            " [[-0.949219 1.82031 -0.279297 ... -0.878906 1.67188 0.535156]\n",
            "  [-1.58594 1.27344 -0.90625 ... -0.10498 0.917969 0.621094]\n",
            "  [-1.0625 2.07812 -2.34375 ... -1.14844 1.4375 0.660156]\n",
            "  ...\n",
            "  [1.32812 2.35938 -1.03125 ... 0.375 0.796875 1.52344]\n",
            "  [-0.941406 1 -0.863281 ... -0.726562 -0.236328 0.253906]\n",
            "  [-1.53906 1.5625 -0.380859 ... -0.925781 -0.519531 -0.988281]]]\n",
            "attention_lnx=[[[-0.507812 0.0505371 0.738281 ... 0.976562 -0.527344 0.0341797]\n",
            "  [0.0373535 0.189453 0.0349121 ... 0.648438 -0.0957031 -1.09375]\n",
            "  [-0.180664 0.265625 -0.949219 ... 0.605469 -0.235352 -0.839844]\n",
            "  ...\n",
            "  [-0.229492 0.300781 -1.03906 ... 0.245117 0.0554199 -0.765625]\n",
            "  [-0.351562 0.228516 -1.28125 ... -0.316406 0.271484 -0.960938]\n",
            "  [-0.474609 0.449219 -1.28125 ... -0.12793 0.0610352 -0.53125]]\n",
            "\n",
            " [[-0.507812 0.0505371 0.738281 ... 0.976562 -0.527344 0.0341797]\n",
            "  [0.0373535 0.189453 0.0349121 ... 0.648438 -0.0957031 -1.09375]\n",
            "  [-0.180664 0.265625 -0.949219 ... 0.605469 -0.235352 -0.839844]\n",
            "  ...\n",
            "  [-0.229492 0.300781 -1.03906 ... 0.245117 0.0554199 -0.765625]\n",
            "  [-0.351562 0.228516 -1.28125 ... -0.316406 0.271484 -0.960938]\n",
            "  [-0.474609 0.449219 -1.28125 ... -0.12793 0.0610352 -0.53125]]\n",
            "\n",
            " [[-0.507812 0.0505371 0.738281 ... 0.976562 -0.527344 0.0341797]\n",
            "  [0.0373535 0.189453 0.0349121 ... 0.648438 -0.0957031 -1.09375]\n",
            "  [-0.180664 0.265625 -0.949219 ... 0.605469 -0.235352 -0.839844]\n",
            "  ...\n",
            "  [-0.229492 0.300781 -1.03906 ... 0.245117 0.0554199 -0.765625]\n",
            "  [-0.351562 0.228516 -1.28125 ... -0.316406 0.271484 -0.960938]\n",
            "  [-0.474609 0.449219 -1.28125 ... -0.12793 0.0610352 -0.53125]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.507812 0.0505371 0.738281 ... 0.976562 -0.527344 0.0341797]\n",
            "  [0.0373535 0.189453 0.0349121 ... 0.648438 -0.0957031 -1.09375]\n",
            "  [-0.180664 0.265625 -0.949219 ... 0.605469 -0.235352 -0.839844]\n",
            "  ...\n",
            "  [-0.229492 0.300781 -1.03906 ... 0.245117 0.0554199 -0.765625]\n",
            "  [-0.351562 0.228516 -1.28125 ... -0.316406 0.271484 -0.960938]\n",
            "  [-0.474609 0.449219 -1.28125 ... -0.12793 0.0610352 -0.53125]]\n",
            "\n",
            " [[-0.507812 0.0505371 0.738281 ... 0.976562 -0.527344 0.0341797]\n",
            "  [0.0373535 0.189453 0.0349121 ... 0.648438 -0.0957031 -1.09375]\n",
            "  [-0.180664 0.265625 -0.949219 ... 0.605469 -0.235352 -0.839844]\n",
            "  ...\n",
            "  [-0.229492 0.300781 -1.03906 ... 0.245117 0.0554199 -0.765625]\n",
            "  [-0.351562 0.228516 -1.28125 ... -0.316406 0.271484 -0.960938]\n",
            "  [-0.474609 0.449219 -1.28125 ... -0.12793 0.0610352 -0.53125]]\n",
            "\n",
            " [[-0.507812 0.0505371 0.738281 ... 0.976562 -0.527344 0.0341797]\n",
            "  [0.0373535 0.189453 0.0349121 ... 0.648438 -0.0957031 -1.09375]\n",
            "  [-0.180664 0.265625 -0.949219 ... 0.605469 -0.235352 -0.839844]\n",
            "  ...\n",
            "  [-0.229492 0.300781 -1.03906 ... 0.245117 0.0554199 -0.765625]\n",
            "  [-0.351562 0.228516 -1.28125 ... -0.316406 0.271484 -0.960938]\n",
            "  [-0.474609 0.449219 -1.28125 ... -0.12793 0.0610352 -0.53125]]]\n",
            "attn_output=[[[-1.04688 1.78906 -0.097168 ... -0.625 1.50781 0.527344]\n",
            "  [-1.53906 1.28125 -0.875 ... 0.0668945 0.867188 0.318359]\n",
            "  [-1.08594 2.09375 -2.54688 ... -0.949219 1.32812 0.40625]\n",
            "  ...\n",
            "  [1.22656 2.40625 -1.34375 ... 0.447266 0.804688 1.25]\n",
            "  [-1.02344 1.03906 -1.1875 ... -0.796875 -0.158203 -0.0126343]\n",
            "  [-1.64844 1.65625 -0.753906 ... -0.945312 -0.488281 -1.125]]\n",
            "\n",
            " [[-1.04688 1.78906 -0.097168 ... -0.625 1.50781 0.527344]\n",
            "  [-1.53906 1.28125 -0.875 ... 0.0668945 0.867188 0.318359]\n",
            "  [-1.08594 2.09375 -2.54688 ... -0.949219 1.32812 0.40625]\n",
            "  ...\n",
            "  [1.22656 2.40625 -1.34375 ... 0.447266 0.804688 1.25]\n",
            "  [-1.02344 1.03906 -1.1875 ... -0.796875 -0.158203 -0.0126343]\n",
            "  [-1.64844 1.65625 -0.753906 ... -0.945312 -0.488281 -1.125]]\n",
            "\n",
            " [[-1.04688 1.78906 -0.097168 ... -0.625 1.50781 0.527344]\n",
            "  [-1.53906 1.28125 -0.875 ... 0.0668945 0.867188 0.318359]\n",
            "  [-1.08594 2.09375 -2.54688 ... -0.949219 1.32812 0.40625]\n",
            "  ...\n",
            "  [1.22656 2.40625 -1.34375 ... 0.447266 0.804688 1.25]\n",
            "  [-1.02344 1.03906 -1.1875 ... -0.796875 -0.158203 -0.0126343]\n",
            "  [-1.64844 1.65625 -0.753906 ... -0.945312 -0.488281 -1.125]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.04688 1.78906 -0.097168 ... -0.625 1.50781 0.527344]\n",
            "  [-1.53906 1.28125 -0.875 ... 0.0668945 0.867188 0.318359]\n",
            "  [-1.08594 2.09375 -2.54688 ... -0.949219 1.32812 0.40625]\n",
            "  ...\n",
            "  [1.22656 2.40625 -1.34375 ... 0.447266 0.804688 1.25]\n",
            "  [-1.02344 1.03906 -1.1875 ... -0.796875 -0.158203 -0.0126343]\n",
            "  [-1.64844 1.65625 -0.753906 ... -0.945312 -0.488281 -1.125]]\n",
            "\n",
            " [[-1.04688 1.78906 -0.097168 ... -0.625 1.50781 0.527344]\n",
            "  [-1.53906 1.28125 -0.875 ... 0.0668945 0.867188 0.318359]\n",
            "  [-1.08594 2.09375 -2.54688 ... -0.949219 1.32812 0.40625]\n",
            "  ...\n",
            "  [1.22656 2.40625 -1.34375 ... 0.447266 0.804688 1.25]\n",
            "  [-1.02344 1.03906 -1.1875 ... -0.796875 -0.158203 -0.0126343]\n",
            "  [-1.64844 1.65625 -0.753906 ... -0.945312 -0.488281 -1.125]]\n",
            "\n",
            " [[-1.04688 1.78906 -0.097168 ... -0.625 1.50781 0.527344]\n",
            "  [-1.53906 1.28125 -0.875 ... 0.0668945 0.867188 0.318359]\n",
            "  [-1.08594 2.09375 -2.54688 ... -0.949219 1.32812 0.40625]\n",
            "  ...\n",
            "  [1.22656 2.40625 -1.34375 ... 0.447266 0.804688 1.25]\n",
            "  [-1.02344 1.03906 -1.1875 ... -0.796875 -0.158203 -0.0126343]\n",
            "  [-1.64844 1.65625 -0.753906 ... -0.945312 -0.488281 -1.125]]]\n",
            "next_layer_addition_dropped_out=[[[-4.875 7.6875 -0.498047 ... -2.71875 6.125 1.85156]\n",
            "  [-5.28125 5.03125 -2.32812 ... 1.23438 4.4375 1.49219]\n",
            "  [-4.625 6.59375 -9.25 ... -3.57812 4.625 2.34375]\n",
            "  ...\n",
            "  [5.40625 6.75 -3.85938 ... 1.23438 2.54688 3.82812]\n",
            "  [-4.375 2.84375 -5.59375 ... -3.59375 -1.875 -0.168945]\n",
            "  [-5.5 4.8125 -2.84375 ... -5.125 -1.72656 -4.34375]]\n",
            "\n",
            " [[-4.875 7.6875 -0.498047 ... -2.71875 6.125 1.85156]\n",
            "  [-5.28125 5.03125 -2.32812 ... 1.23438 4.4375 1.49219]\n",
            "  [-4.625 6.59375 -9.25 ... -3.57812 4.625 2.34375]\n",
            "  ...\n",
            "  [5.40625 6.75 -3.85938 ... 1.23438 2.54688 3.82812]\n",
            "  [-4.375 2.84375 -5.59375 ... -3.59375 -1.875 -0.168945]\n",
            "  [-5.5 4.8125 -2.84375 ... -5.125 -1.72656 -4.34375]]\n",
            "\n",
            " [[-4.875 7.6875 -0.498047 ... -2.71875 6.125 1.85156]\n",
            "  [-5.28125 5.03125 -2.32812 ... 1.23438 4.4375 1.49219]\n",
            "  [-4.625 6.59375 -9.25 ... -3.57812 4.625 2.34375]\n",
            "  ...\n",
            "  [5.40625 6.75 -3.85938 ... 1.23438 2.54688 3.82812]\n",
            "  [-4.375 2.84375 -5.59375 ... -3.59375 -1.875 -0.168945]\n",
            "  [-5.5 4.8125 -2.84375 ... -5.125 -1.72656 -4.34375]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-4.875 7.6875 -0.498047 ... -2.71875 6.125 1.85156]\n",
            "  [-5.28125 5.03125 -2.32812 ... 1.23438 4.4375 1.49219]\n",
            "  [-4.625 6.59375 -9.25 ... -3.57812 4.625 2.34375]\n",
            "  ...\n",
            "  [5.40625 6.75 -3.85938 ... 1.23438 2.54688 3.82812]\n",
            "  [-4.375 2.84375 -5.59375 ... -3.59375 -1.875 -0.168945]\n",
            "  [-5.5 4.8125 -2.84375 ... -5.125 -1.72656 -4.34375]]\n",
            "\n",
            " [[-4.875 7.6875 -0.498047 ... -2.71875 6.125 1.85156]\n",
            "  [-5.28125 5.03125 -2.32812 ... 1.23438 4.4375 1.49219]\n",
            "  [-4.625 6.59375 -9.25 ... -3.57812 4.625 2.34375]\n",
            "  ...\n",
            "  [5.40625 6.75 -3.85938 ... 1.23438 2.54688 3.82812]\n",
            "  [-4.375 2.84375 -5.59375 ... -3.59375 -1.875 -0.168945]\n",
            "  [-5.5 4.8125 -2.84375 ... -5.125 -1.72656 -4.34375]]\n",
            "\n",
            " [[-4.875 7.6875 -0.498047 ... -2.71875 6.125 1.85156]\n",
            "  [-5.28125 5.03125 -2.32812 ... 1.23438 4.4375 1.49219]\n",
            "  [-4.625 6.59375 -9.25 ... -3.57812 4.625 2.34375]\n",
            "  ...\n",
            "  [5.40625 6.75 -3.85938 ... 1.23438 2.54688 3.82812]\n",
            "  [-4.375 2.84375 -5.59375 ... -3.59375 -1.875 -0.168945]\n",
            "  [-5.5 4.8125 -2.84375 ... -5.125 -1.72656 -4.34375]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00]]]]]\n",
            "inputs=[[[-4.875 7.6875 -0.498047 ... -2.71875 6.125 1.85156]\n",
            "  [-5.28125 5.03125 -2.32812 ... 1.23438 4.4375 1.49219]\n",
            "  [-4.625 6.59375 -9.25 ... -3.57812 4.625 2.34375]\n",
            "  ...\n",
            "  [5.40625 6.75 -3.85938 ... 1.23438 2.54688 3.82812]\n",
            "  [-4.375 2.84375 -5.59375 ... -3.59375 -1.875 -0.168945]\n",
            "  [-5.5 4.8125 -2.84375 ... -5.125 -1.72656 -4.34375]]\n",
            "\n",
            " [[-4.875 7.6875 -0.498047 ... -2.71875 6.125 1.85156]\n",
            "  [-5.28125 5.03125 -2.32812 ... 1.23438 4.4375 1.49219]\n",
            "  [-4.625 6.59375 -9.25 ... -3.57812 4.625 2.34375]\n",
            "  ...\n",
            "  [5.40625 6.75 -3.85938 ... 1.23438 2.54688 3.82812]\n",
            "  [-4.375 2.84375 -5.59375 ... -3.59375 -1.875 -0.168945]\n",
            "  [-5.5 4.8125 -2.84375 ... -5.125 -1.72656 -4.34375]]\n",
            "\n",
            " [[-4.875 7.6875 -0.498047 ... -2.71875 6.125 1.85156]\n",
            "  [-5.28125 5.03125 -2.32812 ... 1.23438 4.4375 1.49219]\n",
            "  [-4.625 6.59375 -9.25 ... -3.57812 4.625 2.34375]\n",
            "  ...\n",
            "  [5.40625 6.75 -3.85938 ... 1.23438 2.54688 3.82812]\n",
            "  [-4.375 2.84375 -5.59375 ... -3.59375 -1.875 -0.168945]\n",
            "  [-5.5 4.8125 -2.84375 ... -5.125 -1.72656 -4.34375]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-4.875 7.6875 -0.498047 ... -2.71875 6.125 1.85156]\n",
            "  [-5.28125 5.03125 -2.32812 ... 1.23438 4.4375 1.49219]\n",
            "  [-4.625 6.59375 -9.25 ... -3.57812 4.625 2.34375]\n",
            "  ...\n",
            "  [5.40625 6.75 -3.85938 ... 1.23438 2.54688 3.82812]\n",
            "  [-4.375 2.84375 -5.59375 ... -3.59375 -1.875 -0.168945]\n",
            "  [-5.5 4.8125 -2.84375 ... -5.125 -1.72656 -4.34375]]\n",
            "\n",
            " [[-4.875 7.6875 -0.498047 ... -2.71875 6.125 1.85156]\n",
            "  [-5.28125 5.03125 -2.32812 ... 1.23438 4.4375 1.49219]\n",
            "  [-4.625 6.59375 -9.25 ... -3.57812 4.625 2.34375]\n",
            "  ...\n",
            "  [5.40625 6.75 -3.85938 ... 1.23438 2.54688 3.82812]\n",
            "  [-4.375 2.84375 -5.59375 ... -3.59375 -1.875 -0.168945]\n",
            "  [-5.5 4.8125 -2.84375 ... -5.125 -1.72656 -4.34375]]\n",
            "\n",
            " [[-4.875 7.6875 -0.498047 ... -2.71875 6.125 1.85156]\n",
            "  [-5.28125 5.03125 -2.32812 ... 1.23438 4.4375 1.49219]\n",
            "  [-4.625 6.59375 -9.25 ... -3.57812 4.625 2.34375]\n",
            "  ...\n",
            "  [5.40625 6.75 -3.85938 ... 1.23438 2.54688 3.82812]\n",
            "  [-4.375 2.84375 -5.59375 ... -3.59375 -1.875 -0.168945]\n",
            "  [-5.5 4.8125 -2.84375 ... -5.125 -1.72656 -4.34375]]]\n",
            "lnx=[[[-1.14844 1.80469 -0.117188 ... -0.640625 1.4375 0.435547]\n",
            "  [-1.35938 1.29688 -0.601562 ... 0.318359 1.14062 0.384766]\n",
            "  [-1.27344 1.82031 -2.54688 ... -0.988281 1.27344 0.648438]\n",
            "  ...\n",
            "  [1.70312 2.125 -1.21875 ... 0.390625 0.804688 1.21094]\n",
            "  [-1.11719 0.726562 -1.42969 ... -0.917969 -0.478516 -0.0432129]\n",
            "  [-1.55469 1.35938 -0.804688 ... -1.45312 -0.488281 -1.22656]]\n",
            "\n",
            " [[-1.14844 1.80469 -0.117188 ... -0.640625 1.4375 0.435547]\n",
            "  [-1.35938 1.29688 -0.601562 ... 0.318359 1.14062 0.384766]\n",
            "  [-1.27344 1.82031 -2.54688 ... -0.988281 1.27344 0.648438]\n",
            "  ...\n",
            "  [1.70312 2.125 -1.21875 ... 0.390625 0.804688 1.21094]\n",
            "  [-1.11719 0.726562 -1.42969 ... -0.917969 -0.478516 -0.0432129]\n",
            "  [-1.55469 1.35938 -0.804688 ... -1.45312 -0.488281 -1.22656]]\n",
            "\n",
            " [[-1.14844 1.80469 -0.117188 ... -0.640625 1.4375 0.435547]\n",
            "  [-1.35938 1.29688 -0.601562 ... 0.318359 1.14062 0.384766]\n",
            "  [-1.27344 1.82031 -2.54688 ... -0.988281 1.27344 0.648438]\n",
            "  ...\n",
            "  [1.70312 2.125 -1.21875 ... 0.390625 0.804688 1.21094]\n",
            "  [-1.11719 0.726562 -1.42969 ... -0.917969 -0.478516 -0.0432129]\n",
            "  [-1.55469 1.35938 -0.804688 ... -1.45312 -0.488281 -1.22656]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.14844 1.80469 -0.117188 ... -0.640625 1.4375 0.435547]\n",
            "  [-1.35938 1.29688 -0.601562 ... 0.318359 1.14062 0.384766]\n",
            "  [-1.27344 1.82031 -2.54688 ... -0.988281 1.27344 0.648438]\n",
            "  ...\n",
            "  [1.70312 2.125 -1.21875 ... 0.390625 0.804688 1.21094]\n",
            "  [-1.11719 0.726562 -1.42969 ... -0.917969 -0.478516 -0.0432129]\n",
            "  [-1.55469 1.35938 -0.804688 ... -1.45312 -0.488281 -1.22656]]\n",
            "\n",
            " [[-1.14844 1.80469 -0.117188 ... -0.640625 1.4375 0.435547]\n",
            "  [-1.35938 1.29688 -0.601562 ... 0.318359 1.14062 0.384766]\n",
            "  [-1.27344 1.82031 -2.54688 ... -0.988281 1.27344 0.648438]\n",
            "  ...\n",
            "  [1.70312 2.125 -1.21875 ... 0.390625 0.804688 1.21094]\n",
            "  [-1.11719 0.726562 -1.42969 ... -0.917969 -0.478516 -0.0432129]\n",
            "  [-1.55469 1.35938 -0.804688 ... -1.45312 -0.488281 -1.22656]]\n",
            "\n",
            " [[-1.14844 1.80469 -0.117188 ... -0.640625 1.4375 0.435547]\n",
            "  [-1.35938 1.29688 -0.601562 ... 0.318359 1.14062 0.384766]\n",
            "  [-1.27344 1.82031 -2.54688 ... -0.988281 1.27344 0.648438]\n",
            "  ...\n",
            "  [1.70312 2.125 -1.21875 ... 0.390625 0.804688 1.21094]\n",
            "  [-1.11719 0.726562 -1.42969 ... -0.917969 -0.478516 -0.0432129]\n",
            "  [-1.55469 1.35938 -0.804688 ... -1.45312 -0.488281 -1.22656]]]\n",
            "attention_lnx=[[[-0.00610352 -0.0664062 -0.359375 ... 1.66406 -0.953125 -0.408203]\n",
            "  [0.507812 0.275391 -0.124023 ... 1.5 -0.0668945 0.488281]\n",
            "  [0.792969 0.890625 -0.820312 ... 1.72656 0.03125 -0.291016]\n",
            "  ...\n",
            "  [0.065918 0.176758 0.773438 ... 1.80469 -0.0869141 0.320312]\n",
            "  [-0.503906 0.458984 0.652344 ... 1.73438 0.122559 -0.0683594]\n",
            "  [-0.492188 0.478516 0.628906 ... 2.14062 0.0476074 0.318359]]\n",
            "\n",
            " [[-0.00610352 -0.0664062 -0.359375 ... 1.66406 -0.953125 -0.408203]\n",
            "  [0.507812 0.275391 -0.124023 ... 1.5 -0.0668945 0.488281]\n",
            "  [0.792969 0.890625 -0.820312 ... 1.72656 0.03125 -0.291016]\n",
            "  ...\n",
            "  [0.065918 0.176758 0.773438 ... 1.80469 -0.0869141 0.320312]\n",
            "  [-0.503906 0.458984 0.652344 ... 1.73438 0.122559 -0.0683594]\n",
            "  [-0.492188 0.478516 0.628906 ... 2.14062 0.0476074 0.318359]]\n",
            "\n",
            " [[-0.00610352 -0.0664062 -0.359375 ... 1.66406 -0.953125 -0.408203]\n",
            "  [0.507812 0.275391 -0.124023 ... 1.5 -0.0668945 0.488281]\n",
            "  [0.792969 0.890625 -0.820312 ... 1.72656 0.03125 -0.291016]\n",
            "  ...\n",
            "  [0.065918 0.176758 0.773438 ... 1.80469 -0.0869141 0.320312]\n",
            "  [-0.503906 0.458984 0.652344 ... 1.73438 0.122559 -0.0683594]\n",
            "  [-0.492188 0.478516 0.628906 ... 2.14062 0.0476074 0.318359]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.00610352 -0.0664062 -0.359375 ... 1.66406 -0.953125 -0.408203]\n",
            "  [0.507812 0.275391 -0.124023 ... 1.5 -0.0668945 0.488281]\n",
            "  [0.792969 0.890625 -0.820312 ... 1.72656 0.03125 -0.291016]\n",
            "  ...\n",
            "  [0.065918 0.176758 0.773438 ... 1.80469 -0.0869141 0.320312]\n",
            "  [-0.503906 0.458984 0.652344 ... 1.73438 0.122559 -0.0683594]\n",
            "  [-0.492188 0.478516 0.628906 ... 2.14062 0.0476074 0.318359]]\n",
            "\n",
            " [[-0.00610352 -0.0664062 -0.359375 ... 1.66406 -0.953125 -0.408203]\n",
            "  [0.507812 0.275391 -0.124023 ... 1.5 -0.0668945 0.488281]\n",
            "  [0.792969 0.890625 -0.820312 ... 1.72656 0.03125 -0.291016]\n",
            "  ...\n",
            "  [0.065918 0.176758 0.773438 ... 1.80469 -0.0869141 0.320312]\n",
            "  [-0.503906 0.458984 0.652344 ... 1.73438 0.122559 -0.0683594]\n",
            "  [-0.492188 0.478516 0.628906 ... 2.14062 0.0476074 0.318359]]\n",
            "\n",
            " [[-0.00610352 -0.0664062 -0.359375 ... 1.66406 -0.953125 -0.408203]\n",
            "  [0.507812 0.275391 -0.124023 ... 1.5 -0.0668945 0.488281]\n",
            "  [0.792969 0.890625 -0.820312 ... 1.72656 0.03125 -0.291016]\n",
            "  ...\n",
            "  [0.065918 0.176758 0.773438 ... 1.80469 -0.0869141 0.320312]\n",
            "  [-0.503906 0.458984 0.652344 ... 1.73438 0.122559 -0.0683594]\n",
            "  [-0.492188 0.478516 0.628906 ... 2.14062 0.0476074 0.318359]]]\n",
            "attn_output=[[[-1.10938 1.74219 -0.195312 ... -0.240234 1.17969 0.330078]\n",
            "  [-1.19531 1.32812 -0.613281 ... 0.683594 1.09375 0.494141]\n",
            "  [-1.02344 1.99219 -2.6875 ... -0.494141 1.24219 0.546875]\n",
            "  ...\n",
            "  [1.6875 2.14062 -0.949219 ... 0.933594 0.757812 1.28125]\n",
            "  [-1.21875 0.824219 -1.23438 ... -0.464844 -0.4375 -0.0593262]\n",
            "  [-1.65625 1.46094 -0.613281 ... -0.824219 -0.464844 -1.11719]]\n",
            "\n",
            " [[-1.10938 1.74219 -0.195312 ... -0.240234 1.17969 0.330078]\n",
            "  [-1.19531 1.32812 -0.613281 ... 0.683594 1.09375 0.494141]\n",
            "  [-1.02344 1.99219 -2.6875 ... -0.494141 1.24219 0.546875]\n",
            "  ...\n",
            "  [1.6875 2.14062 -0.949219 ... 0.933594 0.757812 1.28125]\n",
            "  [-1.21875 0.824219 -1.23438 ... -0.464844 -0.4375 -0.0593262]\n",
            "  [-1.65625 1.46094 -0.613281 ... -0.824219 -0.464844 -1.11719]]\n",
            "\n",
            " [[-1.10938 1.74219 -0.195312 ... -0.240234 1.17969 0.330078]\n",
            "  [-1.19531 1.32812 -0.613281 ... 0.683594 1.09375 0.494141]\n",
            "  [-1.02344 1.99219 -2.6875 ... -0.494141 1.24219 0.546875]\n",
            "  ...\n",
            "  [1.6875 2.14062 -0.949219 ... 0.933594 0.757812 1.28125]\n",
            "  [-1.21875 0.824219 -1.23438 ... -0.464844 -0.4375 -0.0593262]\n",
            "  [-1.65625 1.46094 -0.613281 ... -0.824219 -0.464844 -1.11719]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.10938 1.74219 -0.195312 ... -0.240234 1.17969 0.330078]\n",
            "  [-1.19531 1.32812 -0.613281 ... 0.683594 1.09375 0.494141]\n",
            "  [-1.02344 1.99219 -2.6875 ... -0.494141 1.24219 0.546875]\n",
            "  ...\n",
            "  [1.6875 2.14062 -0.949219 ... 0.933594 0.757812 1.28125]\n",
            "  [-1.21875 0.824219 -1.23438 ... -0.464844 -0.4375 -0.0593262]\n",
            "  [-1.65625 1.46094 -0.613281 ... -0.824219 -0.464844 -1.11719]]\n",
            "\n",
            " [[-1.10938 1.74219 -0.195312 ... -0.240234 1.17969 0.330078]\n",
            "  [-1.19531 1.32812 -0.613281 ... 0.683594 1.09375 0.494141]\n",
            "  [-1.02344 1.99219 -2.6875 ... -0.494141 1.24219 0.546875]\n",
            "  ...\n",
            "  [1.6875 2.14062 -0.949219 ... 0.933594 0.757812 1.28125]\n",
            "  [-1.21875 0.824219 -1.23438 ... -0.464844 -0.4375 -0.0593262]\n",
            "  [-1.65625 1.46094 -0.613281 ... -0.824219 -0.464844 -1.11719]]\n",
            "\n",
            " [[-1.10938 1.74219 -0.195312 ... -0.240234 1.17969 0.330078]\n",
            "  [-1.19531 1.32812 -0.613281 ... 0.683594 1.09375 0.494141]\n",
            "  [-1.02344 1.99219 -2.6875 ... -0.494141 1.24219 0.546875]\n",
            "  ...\n",
            "  [1.6875 2.14062 -0.949219 ... 0.933594 0.757812 1.28125]\n",
            "  [-1.21875 0.824219 -1.23438 ... -0.464844 -0.4375 -0.0593262]\n",
            "  [-1.65625 1.46094 -0.613281 ... -0.824219 -0.464844 -1.11719]]]\n",
            "next_layer_addition_dropped_out=[[[-4.9375 9 -0.186523 ... -1.32812 5.1875 1.89062]\n",
            "  [-5.0625 5.71875 -2.57812 ... 2.875 3.85938 1.89062]\n",
            "  [-4.65625 9.0625 -9.0625 ... -2.26562 5.46875 3.03125]\n",
            "  ...\n",
            "  [4.9375 7.28125 -3.09375 ... 3.46875 3.57812 4.625]\n",
            "  [-5.3125 3.32812 -5.4375 ... -4.90625 -1.52344 0.0195312]\n",
            "  [-7.5 5.46875 -1.29688 ... -4.59375 -2.29688 -4.21875]]\n",
            "\n",
            " [[-4.9375 9 -0.186523 ... -1.32812 5.1875 1.89062]\n",
            "  [-5.0625 5.71875 -2.57812 ... 2.875 3.85938 1.89062]\n",
            "  [-4.65625 9.0625 -9.0625 ... -2.26562 5.46875 3.03125]\n",
            "  ...\n",
            "  [4.9375 7.28125 -3.09375 ... 3.46875 3.57812 4.625]\n",
            "  [-5.3125 3.32812 -5.4375 ... -4.90625 -1.52344 0.0195312]\n",
            "  [-7.5 5.46875 -1.29688 ... -4.59375 -2.29688 -4.21875]]\n",
            "\n",
            " [[-4.9375 9 -0.186523 ... -1.32812 5.1875 1.89062]\n",
            "  [-5.0625 5.71875 -2.57812 ... 2.875 3.85938 1.89062]\n",
            "  [-4.65625 9.0625 -9.0625 ... -2.26562 5.46875 3.03125]\n",
            "  ...\n",
            "  [4.9375 7.28125 -3.09375 ... 3.46875 3.57812 4.625]\n",
            "  [-5.3125 3.32812 -5.4375 ... -4.90625 -1.52344 0.0195312]\n",
            "  [-7.5 5.46875 -1.29688 ... -4.59375 -2.29688 -4.21875]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-4.9375 9 -0.186523 ... -1.32812 5.1875 1.89062]\n",
            "  [-5.0625 5.71875 -2.57812 ... 2.875 3.85938 1.89062]\n",
            "  [-4.65625 9.0625 -9.0625 ... -2.26562 5.46875 3.03125]\n",
            "  ...\n",
            "  [4.9375 7.28125 -3.09375 ... 3.46875 3.57812 4.625]\n",
            "  [-5.3125 3.32812 -5.4375 ... -4.90625 -1.52344 0.0195312]\n",
            "  [-7.5 5.46875 -1.29688 ... -4.59375 -2.29688 -4.21875]]\n",
            "\n",
            " [[-4.9375 9 -0.186523 ... -1.32812 5.1875 1.89062]\n",
            "  [-5.0625 5.71875 -2.57812 ... 2.875 3.85938 1.89062]\n",
            "  [-4.65625 9.0625 -9.0625 ... -2.26562 5.46875 3.03125]\n",
            "  ...\n",
            "  [4.9375 7.28125 -3.09375 ... 3.46875 3.57812 4.625]\n",
            "  [-5.3125 3.32812 -5.4375 ... -4.90625 -1.52344 0.0195312]\n",
            "  [-7.5 5.46875 -1.29688 ... -4.59375 -2.29688 -4.21875]]\n",
            "\n",
            " [[-4.9375 9 -0.186523 ... -1.32812 5.1875 1.89062]\n",
            "  [-5.0625 5.71875 -2.57812 ... 2.875 3.85938 1.89062]\n",
            "  [-4.65625 9.0625 -9.0625 ... -2.26562 5.46875 3.03125]\n",
            "  ...\n",
            "  [4.9375 7.28125 -3.09375 ... 3.46875 3.57812 4.625]\n",
            "  [-5.3125 3.32812 -5.4375 ... -4.90625 -1.52344 0.0195312]\n",
            "  [-7.5 5.46875 -1.29688 ... -4.59375 -2.29688 -4.21875]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00]]]]]\n",
            "inputs=[[[-4.9375 9 -0.186523 ... -1.32812 5.1875 1.89062]\n",
            "  [-5.0625 5.71875 -2.57812 ... 2.875 3.85938 1.89062]\n",
            "  [-4.65625 9.0625 -9.0625 ... -2.26562 5.46875 3.03125]\n",
            "  ...\n",
            "  [4.9375 7.28125 -3.09375 ... 3.46875 3.57812 4.625]\n",
            "  [-5.3125 3.32812 -5.4375 ... -4.90625 -1.52344 0.0195312]\n",
            "  [-7.5 5.46875 -1.29688 ... -4.59375 -2.29688 -4.21875]]\n",
            "\n",
            " [[-4.9375 9 -0.186523 ... -1.32812 5.1875 1.89062]\n",
            "  [-5.0625 5.71875 -2.57812 ... 2.875 3.85938 1.89062]\n",
            "  [-4.65625 9.0625 -9.0625 ... -2.26562 5.46875 3.03125]\n",
            "  ...\n",
            "  [4.9375 7.28125 -3.09375 ... 3.46875 3.57812 4.625]\n",
            "  [-5.3125 3.32812 -5.4375 ... -4.90625 -1.52344 0.0195312]\n",
            "  [-7.5 5.46875 -1.29688 ... -4.59375 -2.29688 -4.21875]]\n",
            "\n",
            " [[-4.9375 9 -0.186523 ... -1.32812 5.1875 1.89062]\n",
            "  [-5.0625 5.71875 -2.57812 ... 2.875 3.85938 1.89062]\n",
            "  [-4.65625 9.0625 -9.0625 ... -2.26562 5.46875 3.03125]\n",
            "  ...\n",
            "  [4.9375 7.28125 -3.09375 ... 3.46875 3.57812 4.625]\n",
            "  [-5.3125 3.32812 -5.4375 ... -4.90625 -1.52344 0.0195312]\n",
            "  [-7.5 5.46875 -1.29688 ... -4.59375 -2.29688 -4.21875]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-4.9375 9 -0.186523 ... -1.32812 5.1875 1.89062]\n",
            "  [-5.0625 5.71875 -2.57812 ... 2.875 3.85938 1.89062]\n",
            "  [-4.65625 9.0625 -9.0625 ... -2.26562 5.46875 3.03125]\n",
            "  ...\n",
            "  [4.9375 7.28125 -3.09375 ... 3.46875 3.57812 4.625]\n",
            "  [-5.3125 3.32812 -5.4375 ... -4.90625 -1.52344 0.0195312]\n",
            "  [-7.5 5.46875 -1.29688 ... -4.59375 -2.29688 -4.21875]]\n",
            "\n",
            " [[-4.9375 9 -0.186523 ... -1.32812 5.1875 1.89062]\n",
            "  [-5.0625 5.71875 -2.57812 ... 2.875 3.85938 1.89062]\n",
            "  [-4.65625 9.0625 -9.0625 ... -2.26562 5.46875 3.03125]\n",
            "  ...\n",
            "  [4.9375 7.28125 -3.09375 ... 3.46875 3.57812 4.625]\n",
            "  [-5.3125 3.32812 -5.4375 ... -4.90625 -1.52344 0.0195312]\n",
            "  [-7.5 5.46875 -1.29688 ... -4.59375 -2.29688 -4.21875]]\n",
            "\n",
            " [[-4.9375 9 -0.186523 ... -1.32812 5.1875 1.89062]\n",
            "  [-5.0625 5.71875 -2.57812 ... 2.875 3.85938 1.89062]\n",
            "  [-4.65625 9.0625 -9.0625 ... -2.26562 5.46875 3.03125]\n",
            "  ...\n",
            "  [4.9375 7.28125 -3.09375 ... 3.46875 3.57812 4.625]\n",
            "  [-5.3125 3.32812 -5.4375 ... -4.90625 -1.52344 0.0195312]\n",
            "  [-7.5 5.46875 -1.29688 ... -4.59375 -2.29688 -4.21875]]]\n",
            "lnx=[[[-1.11719 2.03125 -0.0419922 ... -0.298828 1.17188 0.425781]\n",
            "  [-1.25 1.41406 -0.636719 ... 0.710938 0.953125 0.466797]\n",
            "  [-1.22656 2.39062 -2.39062 ... -0.597656 1.4375 0.796875]\n",
            "  ...\n",
            "  [1.49219 2.20312 -0.933594 ... 1.04688 1.07812 1.39844]\n",
            "  [-1.24219 0.777344 -1.27344 ... -1.14844 -0.355469 0.00457764]\n",
            "  [-1.96094 1.42969 -0.339844 ... -1.20312 -0.601562 -1.10156]]\n",
            "\n",
            " [[-1.11719 2.03125 -0.0419922 ... -0.298828 1.17188 0.425781]\n",
            "  [-1.25 1.41406 -0.636719 ... 0.710938 0.953125 0.466797]\n",
            "  [-1.22656 2.39062 -2.39062 ... -0.597656 1.4375 0.796875]\n",
            "  ...\n",
            "  [1.49219 2.20312 -0.933594 ... 1.04688 1.07812 1.39844]\n",
            "  [-1.24219 0.777344 -1.27344 ... -1.14844 -0.355469 0.00457764]\n",
            "  [-1.96094 1.42969 -0.339844 ... -1.20312 -0.601562 -1.10156]]\n",
            "\n",
            " [[-1.11719 2.03125 -0.0419922 ... -0.298828 1.17188 0.425781]\n",
            "  [-1.25 1.41406 -0.636719 ... 0.710938 0.953125 0.466797]\n",
            "  [-1.22656 2.39062 -2.39062 ... -0.597656 1.4375 0.796875]\n",
            "  ...\n",
            "  [1.49219 2.20312 -0.933594 ... 1.04688 1.07812 1.39844]\n",
            "  [-1.24219 0.777344 -1.27344 ... -1.14844 -0.355469 0.00457764]\n",
            "  [-1.96094 1.42969 -0.339844 ... -1.20312 -0.601562 -1.10156]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.11719 2.03125 -0.0419922 ... -0.298828 1.17188 0.425781]\n",
            "  [-1.25 1.41406 -0.636719 ... 0.710938 0.953125 0.466797]\n",
            "  [-1.22656 2.39062 -2.39062 ... -0.597656 1.4375 0.796875]\n",
            "  ...\n",
            "  [1.49219 2.20312 -0.933594 ... 1.04688 1.07812 1.39844]\n",
            "  [-1.24219 0.777344 -1.27344 ... -1.14844 -0.355469 0.00457764]\n",
            "  [-1.96094 1.42969 -0.339844 ... -1.20312 -0.601562 -1.10156]]\n",
            "\n",
            " [[-1.11719 2.03125 -0.0419922 ... -0.298828 1.17188 0.425781]\n",
            "  [-1.25 1.41406 -0.636719 ... 0.710938 0.953125 0.466797]\n",
            "  [-1.22656 2.39062 -2.39062 ... -0.597656 1.4375 0.796875]\n",
            "  ...\n",
            "  [1.49219 2.20312 -0.933594 ... 1.04688 1.07812 1.39844]\n",
            "  [-1.24219 0.777344 -1.27344 ... -1.14844 -0.355469 0.00457764]\n",
            "  [-1.96094 1.42969 -0.339844 ... -1.20312 -0.601562 -1.10156]]\n",
            "\n",
            " [[-1.11719 2.03125 -0.0419922 ... -0.298828 1.17188 0.425781]\n",
            "  [-1.25 1.41406 -0.636719 ... 0.710938 0.953125 0.466797]\n",
            "  [-1.22656 2.39062 -2.39062 ... -0.597656 1.4375 0.796875]\n",
            "  ...\n",
            "  [1.49219 2.20312 -0.933594 ... 1.04688 1.07812 1.39844]\n",
            "  [-1.24219 0.777344 -1.27344 ... -1.14844 -0.355469 0.00457764]\n",
            "  [-1.96094 1.42969 -0.339844 ... -1.20312 -0.601562 -1.10156]]]\n",
            "attention_lnx=[[[0.0615234 2.20312 -0.0358887 ... 0.0539551 -0.988281 1.14062]\n",
            "  [0.457031 0.566406 -0.148438 ... 0.447266 -0.151367 1.49219]\n",
            "  [0.353516 0.507812 -0.194336 ... 0.441406 -0.285156 0.875]\n",
            "  ...\n",
            "  [0.878906 -0.353516 -0.00056076 ... 0.714844 -0.0151367 0.107422]\n",
            "  [0.660156 -0.882812 1.01562 ... 0.925781 -0.204102 0.036377]\n",
            "  [0.589844 -0.765625 0.726562 ... 0.847656 -0.105957 0.133789]]\n",
            "\n",
            " [[0.0615234 2.20312 -0.0358887 ... 0.0539551 -0.988281 1.14062]\n",
            "  [0.457031 0.566406 -0.148438 ... 0.447266 -0.151367 1.49219]\n",
            "  [0.353516 0.507812 -0.194336 ... 0.441406 -0.285156 0.875]\n",
            "  ...\n",
            "  [0.878906 -0.353516 -0.00056076 ... 0.714844 -0.0151367 0.107422]\n",
            "  [0.660156 -0.882812 1.01562 ... 0.925781 -0.204102 0.036377]\n",
            "  [0.589844 -0.765625 0.726562 ... 0.847656 -0.105957 0.133789]]\n",
            "\n",
            " [[0.0615234 2.20312 -0.0358887 ... 0.0539551 -0.988281 1.14062]\n",
            "  [0.457031 0.566406 -0.148438 ... 0.447266 -0.151367 1.49219]\n",
            "  [0.353516 0.507812 -0.194336 ... 0.441406 -0.285156 0.875]\n",
            "  ...\n",
            "  [0.878906 -0.353516 -0.00056076 ... 0.714844 -0.0151367 0.107422]\n",
            "  [0.660156 -0.882812 1.01562 ... 0.925781 -0.204102 0.036377]\n",
            "  [0.589844 -0.765625 0.726562 ... 0.847656 -0.105957 0.133789]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.0615234 2.20312 -0.0358887 ... 0.0539551 -0.988281 1.14062]\n",
            "  [0.457031 0.566406 -0.148438 ... 0.447266 -0.151367 1.49219]\n",
            "  [0.353516 0.507812 -0.194336 ... 0.441406 -0.285156 0.875]\n",
            "  ...\n",
            "  [0.878906 -0.353516 -0.00056076 ... 0.714844 -0.0151367 0.107422]\n",
            "  [0.660156 -0.882812 1.01562 ... 0.925781 -0.204102 0.036377]\n",
            "  [0.589844 -0.765625 0.726562 ... 0.847656 -0.105957 0.133789]]\n",
            "\n",
            " [[0.0615234 2.20312 -0.0358887 ... 0.0539551 -0.988281 1.14062]\n",
            "  [0.457031 0.566406 -0.148438 ... 0.447266 -0.151367 1.49219]\n",
            "  [0.353516 0.507812 -0.194336 ... 0.441406 -0.285156 0.875]\n",
            "  ...\n",
            "  [0.878906 -0.353516 -0.00056076 ... 0.714844 -0.0151367 0.107422]\n",
            "  [0.660156 -0.882812 1.01562 ... 0.925781 -0.204102 0.036377]\n",
            "  [0.589844 -0.765625 0.726562 ... 0.847656 -0.105957 0.133789]]\n",
            "\n",
            " [[0.0615234 2.20312 -0.0358887 ... 0.0539551 -0.988281 1.14062]\n",
            "  [0.457031 0.566406 -0.148438 ... 0.447266 -0.151367 1.49219]\n",
            "  [0.353516 0.507812 -0.194336 ... 0.441406 -0.285156 0.875]\n",
            "  ...\n",
            "  [0.878906 -0.353516 -0.00056076 ... 0.714844 -0.0151367 0.107422]\n",
            "  [0.660156 -0.882812 1.01562 ... 0.925781 -0.204102 0.036377]\n",
            "  [0.589844 -0.765625 0.726562 ... 0.847656 -0.105957 0.133789]]]\n",
            "attn_output=[[[-1.07812 2.46875 -0.0493164 ... -0.28125 0.925781 0.671875]\n",
            "  [-1.10938 1.51562 -0.660156 ... 0.804688 0.894531 0.816406]\n",
            "  [-1.11719 2.48438 -2.40625 ... -0.472656 1.34375 1.01562]\n",
            "  ...\n",
            "  [1.72656 2.0625 -0.917969 ... 1.24219 1.0625 1.40625]\n",
            "  [-1.07031 0.558594 -1.01562 ... -0.914062 -0.396484 0.0128174]\n",
            "  [-1.78906 1.21875 -0.147461 ... -0.96875 -0.621094 -1.0625]]\n",
            "\n",
            " [[-1.07812 2.46875 -0.0493164 ... -0.28125 0.925781 0.671875]\n",
            "  [-1.10938 1.51562 -0.660156 ... 0.804688 0.894531 0.816406]\n",
            "  [-1.11719 2.48438 -2.40625 ... -0.472656 1.34375 1.01562]\n",
            "  ...\n",
            "  [1.72656 2.0625 -0.917969 ... 1.24219 1.0625 1.40625]\n",
            "  [-1.07031 0.558594 -1.01562 ... -0.914062 -0.396484 0.0128174]\n",
            "  [-1.78906 1.21875 -0.147461 ... -0.96875 -0.621094 -1.0625]]\n",
            "\n",
            " [[-1.07812 2.46875 -0.0493164 ... -0.28125 0.925781 0.671875]\n",
            "  [-1.10938 1.51562 -0.660156 ... 0.804688 0.894531 0.816406]\n",
            "  [-1.11719 2.48438 -2.40625 ... -0.472656 1.34375 1.01562]\n",
            "  ...\n",
            "  [1.72656 2.0625 -0.917969 ... 1.24219 1.0625 1.40625]\n",
            "  [-1.07031 0.558594 -1.01562 ... -0.914062 -0.396484 0.0128174]\n",
            "  [-1.78906 1.21875 -0.147461 ... -0.96875 -0.621094 -1.0625]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.07812 2.46875 -0.0493164 ... -0.28125 0.925781 0.671875]\n",
            "  [-1.10938 1.51562 -0.660156 ... 0.804688 0.894531 0.816406]\n",
            "  [-1.11719 2.48438 -2.40625 ... -0.472656 1.34375 1.01562]\n",
            "  ...\n",
            "  [1.72656 2.0625 -0.917969 ... 1.24219 1.0625 1.40625]\n",
            "  [-1.07031 0.558594 -1.01562 ... -0.914062 -0.396484 0.0128174]\n",
            "  [-1.78906 1.21875 -0.147461 ... -0.96875 -0.621094 -1.0625]]\n",
            "\n",
            " [[-1.07812 2.46875 -0.0493164 ... -0.28125 0.925781 0.671875]\n",
            "  [-1.10938 1.51562 -0.660156 ... 0.804688 0.894531 0.816406]\n",
            "  [-1.11719 2.48438 -2.40625 ... -0.472656 1.34375 1.01562]\n",
            "  ...\n",
            "  [1.72656 2.0625 -0.917969 ... 1.24219 1.0625 1.40625]\n",
            "  [-1.07031 0.558594 -1.01562 ... -0.914062 -0.396484 0.0128174]\n",
            "  [-1.78906 1.21875 -0.147461 ... -0.96875 -0.621094 -1.0625]]\n",
            "\n",
            " [[-1.07812 2.46875 -0.0493164 ... -0.28125 0.925781 0.671875]\n",
            "  [-1.10938 1.51562 -0.660156 ... 0.804688 0.894531 0.816406]\n",
            "  [-1.11719 2.48438 -2.40625 ... -0.472656 1.34375 1.01562]\n",
            "  ...\n",
            "  [1.72656 2.0625 -0.917969 ... 1.24219 1.0625 1.40625]\n",
            "  [-1.07031 0.558594 -1.01562 ... -0.914062 -0.396484 0.0128174]\n",
            "  [-1.78906 1.21875 -0.147461 ... -0.96875 -0.621094 -1.0625]]]\n",
            "next_layer_addition_dropped_out=[[[-5.5625 9.75 0.902344 ... -0.605469 4 3.23438]\n",
            "  [-4.53125 6.5625 -1.6875 ... 2.89062 3.65625 3.25]\n",
            "  [-4.28125 8.625 -9.5625 ... -3.32812 5.46875 3.0625]\n",
            "  ...\n",
            "  [4.96875 5.5 -3 ... 3.96875 3.4375 5.3125]\n",
            "  [-6.375 1.39062 -3.34375 ... -4.9375 -2.92188 -0.542969]\n",
            "  [-6.75 4.3125 -1.0625 ... -3.40625 -2.89062 -3.53125]]\n",
            "\n",
            " [[-5.5625 9.75 0.902344 ... -0.605469 4 3.23438]\n",
            "  [-4.53125 6.5625 -1.6875 ... 2.89062 3.65625 3.25]\n",
            "  [-4.28125 8.625 -9.5625 ... -3.32812 5.46875 3.0625]\n",
            "  ...\n",
            "  [4.96875 5.5 -3 ... 3.96875 3.4375 5.3125]\n",
            "  [-6.375 1.39062 -3.34375 ... -4.9375 -2.92188 -0.542969]\n",
            "  [-6.75 4.3125 -1.0625 ... -3.40625 -2.89062 -3.53125]]\n",
            "\n",
            " [[-5.5625 9.75 0.902344 ... -0.605469 4 3.23438]\n",
            "  [-4.53125 6.5625 -1.6875 ... 2.89062 3.65625 3.25]\n",
            "  [-4.28125 8.625 -9.5625 ... -3.32812 5.46875 3.0625]\n",
            "  ...\n",
            "  [4.96875 5.5 -3 ... 3.96875 3.4375 5.3125]\n",
            "  [-6.375 1.39062 -3.34375 ... -4.9375 -2.92188 -0.542969]\n",
            "  [-6.75 4.3125 -1.0625 ... -3.40625 -2.89062 -3.53125]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-5.5625 9.75 0.902344 ... -0.605469 4 3.23438]\n",
            "  [-4.53125 6.5625 -1.6875 ... 2.89062 3.65625 3.25]\n",
            "  [-4.28125 8.625 -9.5625 ... -3.32812 5.46875 3.0625]\n",
            "  ...\n",
            "  [4.96875 5.5 -3 ... 3.96875 3.4375 5.3125]\n",
            "  [-6.375 1.39062 -3.34375 ... -4.9375 -2.92188 -0.542969]\n",
            "  [-6.75 4.3125 -1.0625 ... -3.40625 -2.89062 -3.53125]]\n",
            "\n",
            " [[-5.5625 9.75 0.902344 ... -0.605469 4 3.23438]\n",
            "  [-4.53125 6.5625 -1.6875 ... 2.89062 3.65625 3.25]\n",
            "  [-4.28125 8.625 -9.5625 ... -3.32812 5.46875 3.0625]\n",
            "  ...\n",
            "  [4.96875 5.5 -3 ... 3.96875 3.4375 5.3125]\n",
            "  [-6.375 1.39062 -3.34375 ... -4.9375 -2.92188 -0.542969]\n",
            "  [-6.75 4.3125 -1.0625 ... -3.40625 -2.89062 -3.53125]]\n",
            "\n",
            " [[-5.5625 9.75 0.902344 ... -0.605469 4 3.23438]\n",
            "  [-4.53125 6.5625 -1.6875 ... 2.89062 3.65625 3.25]\n",
            "  [-4.28125 8.625 -9.5625 ... -3.32812 5.46875 3.0625]\n",
            "  ...\n",
            "  [4.96875 5.5 -3 ... 3.96875 3.4375 5.3125]\n",
            "  [-6.375 1.39062 -3.34375 ... -4.9375 -2.92188 -0.542969]\n",
            "  [-6.75 4.3125 -1.0625 ... -3.40625 -2.89062 -3.53125]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00]]]]]\n",
            "inputs=[[[-5.5625 9.75 0.902344 ... -0.605469 4 3.23438]\n",
            "  [-4.53125 6.5625 -1.6875 ... 2.89062 3.65625 3.25]\n",
            "  [-4.28125 8.625 -9.5625 ... -3.32812 5.46875 3.0625]\n",
            "  ...\n",
            "  [4.96875 5.5 -3 ... 3.96875 3.4375 5.3125]\n",
            "  [-6.375 1.39062 -3.34375 ... -4.9375 -2.92188 -0.542969]\n",
            "  [-6.75 4.3125 -1.0625 ... -3.40625 -2.89062 -3.53125]]\n",
            "\n",
            " [[-5.5625 9.75 0.902344 ... -0.605469 4 3.23438]\n",
            "  [-4.53125 6.5625 -1.6875 ... 2.89062 3.65625 3.25]\n",
            "  [-4.28125 8.625 -9.5625 ... -3.32812 5.46875 3.0625]\n",
            "  ...\n",
            "  [4.96875 5.5 -3 ... 3.96875 3.4375 5.3125]\n",
            "  [-6.375 1.39062 -3.34375 ... -4.9375 -2.92188 -0.542969]\n",
            "  [-6.75 4.3125 -1.0625 ... -3.40625 -2.89062 -3.53125]]\n",
            "\n",
            " [[-5.5625 9.75 0.902344 ... -0.605469 4 3.23438]\n",
            "  [-4.53125 6.5625 -1.6875 ... 2.89062 3.65625 3.25]\n",
            "  [-4.28125 8.625 -9.5625 ... -3.32812 5.46875 3.0625]\n",
            "  ...\n",
            "  [4.96875 5.5 -3 ... 3.96875 3.4375 5.3125]\n",
            "  [-6.375 1.39062 -3.34375 ... -4.9375 -2.92188 -0.542969]\n",
            "  [-6.75 4.3125 -1.0625 ... -3.40625 -2.89062 -3.53125]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-5.5625 9.75 0.902344 ... -0.605469 4 3.23438]\n",
            "  [-4.53125 6.5625 -1.6875 ... 2.89062 3.65625 3.25]\n",
            "  [-4.28125 8.625 -9.5625 ... -3.32812 5.46875 3.0625]\n",
            "  ...\n",
            "  [4.96875 5.5 -3 ... 3.96875 3.4375 5.3125]\n",
            "  [-6.375 1.39062 -3.34375 ... -4.9375 -2.92188 -0.542969]\n",
            "  [-6.75 4.3125 -1.0625 ... -3.40625 -2.89062 -3.53125]]\n",
            "\n",
            " [[-5.5625 9.75 0.902344 ... -0.605469 4 3.23438]\n",
            "  [-4.53125 6.5625 -1.6875 ... 2.89062 3.65625 3.25]\n",
            "  [-4.28125 8.625 -9.5625 ... -3.32812 5.46875 3.0625]\n",
            "  ...\n",
            "  [4.96875 5.5 -3 ... 3.96875 3.4375 5.3125]\n",
            "  [-6.375 1.39062 -3.34375 ... -4.9375 -2.92188 -0.542969]\n",
            "  [-6.75 4.3125 -1.0625 ... -3.40625 -2.89062 -3.53125]]\n",
            "\n",
            " [[-5.5625 9.75 0.902344 ... -0.605469 4 3.23438]\n",
            "  [-4.53125 6.5625 -1.6875 ... 2.89062 3.65625 3.25]\n",
            "  [-4.28125 8.625 -9.5625 ... -3.32812 5.46875 3.0625]\n",
            "  ...\n",
            "  [4.96875 5.5 -3 ... 3.96875 3.4375 5.3125]\n",
            "  [-6.375 1.39062 -3.34375 ... -4.9375 -2.92188 -0.542969]\n",
            "  [-6.75 4.3125 -1.0625 ... -3.40625 -2.89062 -3.53125]]]\n",
            "lnx=[[[-1.21094 2.125 0.196289 ... -0.131836 0.871094 0.703125]\n",
            "  [-1.07812 1.5625 -0.402344 ... 0.6875 0.871094 0.773438]\n",
            "  [-1.09375 2.20312 -2.4375 ... -0.847656 1.39844 0.78125]\n",
            "  ...\n",
            "  [1.45312 1.60938 -0.875 ... 1.15625 1 1.55469]\n",
            "  [-1.32812 0.291016 -0.699219 ... -1.03125 -0.609375 -0.113281]\n",
            "  [-1.63281 1.03906 -0.255859 ... -0.824219 -0.699219 -0.851562]]\n",
            "\n",
            " [[-1.21094 2.125 0.196289 ... -0.131836 0.871094 0.703125]\n",
            "  [-1.07812 1.5625 -0.402344 ... 0.6875 0.871094 0.773438]\n",
            "  [-1.09375 2.20312 -2.4375 ... -0.847656 1.39844 0.78125]\n",
            "  ...\n",
            "  [1.45312 1.60938 -0.875 ... 1.15625 1 1.55469]\n",
            "  [-1.32812 0.291016 -0.699219 ... -1.03125 -0.609375 -0.113281]\n",
            "  [-1.63281 1.03906 -0.255859 ... -0.824219 -0.699219 -0.851562]]\n",
            "\n",
            " [[-1.21094 2.125 0.196289 ... -0.131836 0.871094 0.703125]\n",
            "  [-1.07812 1.5625 -0.402344 ... 0.6875 0.871094 0.773438]\n",
            "  [-1.09375 2.20312 -2.4375 ... -0.847656 1.39844 0.78125]\n",
            "  ...\n",
            "  [1.45312 1.60938 -0.875 ... 1.15625 1 1.55469]\n",
            "  [-1.32812 0.291016 -0.699219 ... -1.03125 -0.609375 -0.113281]\n",
            "  [-1.63281 1.03906 -0.255859 ... -0.824219 -0.699219 -0.851562]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.21094 2.125 0.196289 ... -0.131836 0.871094 0.703125]\n",
            "  [-1.07812 1.5625 -0.402344 ... 0.6875 0.871094 0.773438]\n",
            "  [-1.09375 2.20312 -2.4375 ... -0.847656 1.39844 0.78125]\n",
            "  ...\n",
            "  [1.45312 1.60938 -0.875 ... 1.15625 1 1.55469]\n",
            "  [-1.32812 0.291016 -0.699219 ... -1.03125 -0.609375 -0.113281]\n",
            "  [-1.63281 1.03906 -0.255859 ... -0.824219 -0.699219 -0.851562]]\n",
            "\n",
            " [[-1.21094 2.125 0.196289 ... -0.131836 0.871094 0.703125]\n",
            "  [-1.07812 1.5625 -0.402344 ... 0.6875 0.871094 0.773438]\n",
            "  [-1.09375 2.20312 -2.4375 ... -0.847656 1.39844 0.78125]\n",
            "  ...\n",
            "  [1.45312 1.60938 -0.875 ... 1.15625 1 1.55469]\n",
            "  [-1.32812 0.291016 -0.699219 ... -1.03125 -0.609375 -0.113281]\n",
            "  [-1.63281 1.03906 -0.255859 ... -0.824219 -0.699219 -0.851562]]\n",
            "\n",
            " [[-1.21094 2.125 0.196289 ... -0.131836 0.871094 0.703125]\n",
            "  [-1.07812 1.5625 -0.402344 ... 0.6875 0.871094 0.773438]\n",
            "  [-1.09375 2.20312 -2.4375 ... -0.847656 1.39844 0.78125]\n",
            "  ...\n",
            "  [1.45312 1.60938 -0.875 ... 1.15625 1 1.55469]\n",
            "  [-1.32812 0.291016 -0.699219 ... -1.03125 -0.609375 -0.113281]\n",
            "  [-1.63281 1.03906 -0.255859 ... -0.824219 -0.699219 -0.851562]]]\n",
            "attention_lnx=[[[-0.490234 -0.439453 -1.57812 ... -0.519531 0.205078 0.022583]\n",
            "  [-0.90625 0.277344 -0.898438 ... -1 -0.753906 0.0625]\n",
            "  [-1.23438 0.445312 -0.917969 ... -0.5 -0.333984 0.243164]\n",
            "  ...\n",
            "  [-0.363281 -0.248047 0.164062 ... -0.550781 -0.925781 0.0234375]\n",
            "  [-0.416016 -1.91406 0.277344 ... 0.222656 -0.488281 0.191406]\n",
            "  [-0.236328 -1.91406 0.425781 ... 0.197266 -0.601562 0.462891]]\n",
            "\n",
            " [[-0.490234 -0.439453 -1.57812 ... -0.519531 0.205078 0.022583]\n",
            "  [-0.90625 0.277344 -0.898438 ... -1 -0.753906 0.0625]\n",
            "  [-1.23438 0.445312 -0.917969 ... -0.5 -0.333984 0.243164]\n",
            "  ...\n",
            "  [-0.363281 -0.248047 0.164062 ... -0.550781 -0.925781 0.0234375]\n",
            "  [-0.416016 -1.91406 0.277344 ... 0.222656 -0.488281 0.191406]\n",
            "  [-0.236328 -1.91406 0.425781 ... 0.197266 -0.601562 0.462891]]\n",
            "\n",
            " [[-0.490234 -0.439453 -1.57812 ... -0.519531 0.205078 0.022583]\n",
            "  [-0.90625 0.277344 -0.898438 ... -1 -0.753906 0.0625]\n",
            "  [-1.23438 0.445312 -0.917969 ... -0.5 -0.333984 0.243164]\n",
            "  ...\n",
            "  [-0.363281 -0.248047 0.164062 ... -0.550781 -0.925781 0.0234375]\n",
            "  [-0.416016 -1.91406 0.277344 ... 0.222656 -0.488281 0.191406]\n",
            "  [-0.236328 -1.91406 0.425781 ... 0.197266 -0.601562 0.462891]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.490234 -0.439453 -1.57812 ... -0.519531 0.205078 0.022583]\n",
            "  [-0.90625 0.277344 -0.898438 ... -1 -0.753906 0.0625]\n",
            "  [-1.23438 0.445312 -0.917969 ... -0.5 -0.333984 0.243164]\n",
            "  ...\n",
            "  [-0.363281 -0.248047 0.164062 ... -0.550781 -0.925781 0.0234375]\n",
            "  [-0.416016 -1.91406 0.277344 ... 0.222656 -0.488281 0.191406]\n",
            "  [-0.236328 -1.91406 0.425781 ... 0.197266 -0.601562 0.462891]]\n",
            "\n",
            " [[-0.490234 -0.439453 -1.57812 ... -0.519531 0.205078 0.022583]\n",
            "  [-0.90625 0.277344 -0.898438 ... -1 -0.753906 0.0625]\n",
            "  [-1.23438 0.445312 -0.917969 ... -0.5 -0.333984 0.243164]\n",
            "  ...\n",
            "  [-0.363281 -0.248047 0.164062 ... -0.550781 -0.925781 0.0234375]\n",
            "  [-0.416016 -1.91406 0.277344 ... 0.222656 -0.488281 0.191406]\n",
            "  [-0.236328 -1.91406 0.425781 ... 0.197266 -0.601562 0.462891]]\n",
            "\n",
            " [[-0.490234 -0.439453 -1.57812 ... -0.519531 0.205078 0.022583]\n",
            "  [-0.90625 0.277344 -0.898438 ... -1 -0.753906 0.0625]\n",
            "  [-1.23438 0.445312 -0.917969 ... -0.5 -0.333984 0.243164]\n",
            "  ...\n",
            "  [-0.363281 -0.248047 0.164062 ... -0.550781 -0.925781 0.0234375]\n",
            "  [-0.416016 -1.91406 0.277344 ... 0.222656 -0.488281 0.191406]\n",
            "  [-0.236328 -1.91406 0.425781 ... 0.197266 -0.601562 0.462891]]]\n",
            "attn_output=[[[-1.28906 1.98438 -0.143555 ... -0.239258 0.898438 0.691406]\n",
            "  [-1.27344 1.60156 -0.605469 ... 0.443359 0.679688 0.777344]\n",
            "  [-1.38281 2.26562 -2.625 ... -0.957031 1.28125 0.828125]\n",
            "  ...\n",
            "  [1.32031 1.50781 -0.8125 ... 0.984375 0.722656 1.53125]\n",
            "  [-1.39062 -0.10791 -0.628906 ... -0.964844 -0.699219 -0.0717773]\n",
            "  [-1.64844 0.5625 -0.149414 ... -0.753906 -0.824219 -0.722656]]\n",
            "\n",
            " [[-1.28906 1.98438 -0.143555 ... -0.239258 0.898438 0.691406]\n",
            "  [-1.27344 1.60156 -0.605469 ... 0.443359 0.679688 0.777344]\n",
            "  [-1.38281 2.26562 -2.625 ... -0.957031 1.28125 0.828125]\n",
            "  ...\n",
            "  [1.32031 1.50781 -0.8125 ... 0.984375 0.722656 1.53125]\n",
            "  [-1.39062 -0.10791 -0.628906 ... -0.964844 -0.699219 -0.0717773]\n",
            "  [-1.64844 0.5625 -0.149414 ... -0.753906 -0.824219 -0.722656]]\n",
            "\n",
            " [[-1.28906 1.98438 -0.143555 ... -0.239258 0.898438 0.691406]\n",
            "  [-1.27344 1.60156 -0.605469 ... 0.443359 0.679688 0.777344]\n",
            "  [-1.38281 2.26562 -2.625 ... -0.957031 1.28125 0.828125]\n",
            "  ...\n",
            "  [1.32031 1.50781 -0.8125 ... 0.984375 0.722656 1.53125]\n",
            "  [-1.39062 -0.10791 -0.628906 ... -0.964844 -0.699219 -0.0717773]\n",
            "  [-1.64844 0.5625 -0.149414 ... -0.753906 -0.824219 -0.722656]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.28906 1.98438 -0.143555 ... -0.239258 0.898438 0.691406]\n",
            "  [-1.27344 1.60156 -0.605469 ... 0.443359 0.679688 0.777344]\n",
            "  [-1.38281 2.26562 -2.625 ... -0.957031 1.28125 0.828125]\n",
            "  ...\n",
            "  [1.32031 1.50781 -0.8125 ... 0.984375 0.722656 1.53125]\n",
            "  [-1.39062 -0.10791 -0.628906 ... -0.964844 -0.699219 -0.0717773]\n",
            "  [-1.64844 0.5625 -0.149414 ... -0.753906 -0.824219 -0.722656]]\n",
            "\n",
            " [[-1.28906 1.98438 -0.143555 ... -0.239258 0.898438 0.691406]\n",
            "  [-1.27344 1.60156 -0.605469 ... 0.443359 0.679688 0.777344]\n",
            "  [-1.38281 2.26562 -2.625 ... -0.957031 1.28125 0.828125]\n",
            "  ...\n",
            "  [1.32031 1.50781 -0.8125 ... 0.984375 0.722656 1.53125]\n",
            "  [-1.39062 -0.10791 -0.628906 ... -0.964844 -0.699219 -0.0717773]\n",
            "  [-1.64844 0.5625 -0.149414 ... -0.753906 -0.824219 -0.722656]]\n",
            "\n",
            " [[-1.28906 1.98438 -0.143555 ... -0.239258 0.898438 0.691406]\n",
            "  [-1.27344 1.60156 -0.605469 ... 0.443359 0.679688 0.777344]\n",
            "  [-1.38281 2.26562 -2.625 ... -0.957031 1.28125 0.828125]\n",
            "  ...\n",
            "  [1.32031 1.50781 -0.8125 ... 0.984375 0.722656 1.53125]\n",
            "  [-1.39062 -0.10791 -0.628906 ... -0.964844 -0.699219 -0.0717773]\n",
            "  [-1.64844 0.5625 -0.149414 ... -0.753906 -0.824219 -0.722656]]]\n",
            "next_layer_addition_dropped_out=[[[-7 9.0625 -1.54688 ... -0.738281 3.45312 3.29688]\n",
            "  [-5.40625 6.65625 -2.5625 ... 1.75781 2.59375 2.42188]\n",
            "  [-6.8125 8.625 -11 ... -3.73438 4.28125 3.04688]\n",
            "  ...\n",
            "  [5.3125 4.71875 -2.8125 ... 4.5625 2.625 5.75]\n",
            "  [-8.1875 0.0644531 -5.9375 ... -4.3125 -2.9375 0.326172]\n",
            "  [-8.3125 0.294922 -3.17188 ... -3.04688 -0.867188 -3.01562]]\n",
            "\n",
            " [[-7 9.0625 -1.54688 ... -0.738281 3.45312 3.29688]\n",
            "  [-5.40625 6.65625 -2.5625 ... 1.75781 2.59375 2.42188]\n",
            "  [-6.8125 8.625 -11 ... -3.73438 4.28125 3.04688]\n",
            "  ...\n",
            "  [5.3125 4.71875 -2.8125 ... 4.5625 2.625 5.75]\n",
            "  [-8.1875 0.0644531 -5.9375 ... -4.3125 -2.9375 0.326172]\n",
            "  [-8.3125 0.294922 -3.17188 ... -3.04688 -0.867188 -3.01562]]\n",
            "\n",
            " [[-7 9.0625 -1.54688 ... -0.738281 3.45312 3.29688]\n",
            "  [-5.40625 6.65625 -2.5625 ... 1.75781 2.59375 2.42188]\n",
            "  [-6.8125 8.625 -11 ... -3.73438 4.28125 3.04688]\n",
            "  ...\n",
            "  [5.3125 4.71875 -2.8125 ... 4.5625 2.625 5.75]\n",
            "  [-8.1875 0.0644531 -5.9375 ... -4.3125 -2.9375 0.326172]\n",
            "  [-8.3125 0.294922 -3.17188 ... -3.04688 -0.867188 -3.01562]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-7 9.0625 -1.54688 ... -0.738281 3.45312 3.29688]\n",
            "  [-5.40625 6.65625 -2.5625 ... 1.75781 2.59375 2.42188]\n",
            "  [-6.8125 8.625 -11 ... -3.73438 4.28125 3.04688]\n",
            "  ...\n",
            "  [5.3125 4.71875 -2.8125 ... 4.5625 2.625 5.75]\n",
            "  [-8.1875 0.0644531 -5.9375 ... -4.3125 -2.9375 0.326172]\n",
            "  [-8.3125 0.294922 -3.17188 ... -3.04688 -0.867188 -3.01562]]\n",
            "\n",
            " [[-7 9.0625 -1.54688 ... -0.738281 3.45312 3.29688]\n",
            "  [-5.40625 6.65625 -2.5625 ... 1.75781 2.59375 2.42188]\n",
            "  [-6.8125 8.625 -11 ... -3.73438 4.28125 3.04688]\n",
            "  ...\n",
            "  [5.3125 4.71875 -2.8125 ... 4.5625 2.625 5.75]\n",
            "  [-8.1875 0.0644531 -5.9375 ... -4.3125 -2.9375 0.326172]\n",
            "  [-8.3125 0.294922 -3.17188 ... -3.04688 -0.867188 -3.01562]]\n",
            "\n",
            " [[-7 9.0625 -1.54688 ... -0.738281 3.45312 3.29688]\n",
            "  [-5.40625 6.65625 -2.5625 ... 1.75781 2.59375 2.42188]\n",
            "  [-6.8125 8.625 -11 ... -3.73438 4.28125 3.04688]\n",
            "  ...\n",
            "  [5.3125 4.71875 -2.8125 ... 4.5625 2.625 5.75]\n",
            "  [-8.1875 0.0644531 -5.9375 ... -4.3125 -2.9375 0.326172]\n",
            "  [-8.3125 0.294922 -3.17188 ... -3.04688 -0.867188 -3.01562]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00]]]]]\n",
            "inputs=[[[-7 9.0625 -1.54688 ... -0.738281 3.45312 3.29688]\n",
            "  [-5.40625 6.65625 -2.5625 ... 1.75781 2.59375 2.42188]\n",
            "  [-6.8125 8.625 -11 ... -3.73438 4.28125 3.04688]\n",
            "  ...\n",
            "  [5.3125 4.71875 -2.8125 ... 4.5625 2.625 5.75]\n",
            "  [-8.1875 0.0644531 -5.9375 ... -4.3125 -2.9375 0.326172]\n",
            "  [-8.3125 0.294922 -3.17188 ... -3.04688 -0.867188 -3.01562]]\n",
            "\n",
            " [[-7 9.0625 -1.54688 ... -0.738281 3.45312 3.29688]\n",
            "  [-5.40625 6.65625 -2.5625 ... 1.75781 2.59375 2.42188]\n",
            "  [-6.8125 8.625 -11 ... -3.73438 4.28125 3.04688]\n",
            "  ...\n",
            "  [5.3125 4.71875 -2.8125 ... 4.5625 2.625 5.75]\n",
            "  [-8.1875 0.0644531 -5.9375 ... -4.3125 -2.9375 0.326172]\n",
            "  [-8.3125 0.294922 -3.17188 ... -3.04688 -0.867188 -3.01562]]\n",
            "\n",
            " [[-7 9.0625 -1.54688 ... -0.738281 3.45312 3.29688]\n",
            "  [-5.40625 6.65625 -2.5625 ... 1.75781 2.59375 2.42188]\n",
            "  [-6.8125 8.625 -11 ... -3.73438 4.28125 3.04688]\n",
            "  ...\n",
            "  [5.3125 4.71875 -2.8125 ... 4.5625 2.625 5.75]\n",
            "  [-8.1875 0.0644531 -5.9375 ... -4.3125 -2.9375 0.326172]\n",
            "  [-8.3125 0.294922 -3.17188 ... -3.04688 -0.867188 -3.01562]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-7 9.0625 -1.54688 ... -0.738281 3.45312 3.29688]\n",
            "  [-5.40625 6.65625 -2.5625 ... 1.75781 2.59375 2.42188]\n",
            "  [-6.8125 8.625 -11 ... -3.73438 4.28125 3.04688]\n",
            "  ...\n",
            "  [5.3125 4.71875 -2.8125 ... 4.5625 2.625 5.75]\n",
            "  [-8.1875 0.0644531 -5.9375 ... -4.3125 -2.9375 0.326172]\n",
            "  [-8.3125 0.294922 -3.17188 ... -3.04688 -0.867188 -3.01562]]\n",
            "\n",
            " [[-7 9.0625 -1.54688 ... -0.738281 3.45312 3.29688]\n",
            "  [-5.40625 6.65625 -2.5625 ... 1.75781 2.59375 2.42188]\n",
            "  [-6.8125 8.625 -11 ... -3.73438 4.28125 3.04688]\n",
            "  ...\n",
            "  [5.3125 4.71875 -2.8125 ... 4.5625 2.625 5.75]\n",
            "  [-8.1875 0.0644531 -5.9375 ... -4.3125 -2.9375 0.326172]\n",
            "  [-8.3125 0.294922 -3.17188 ... -3.04688 -0.867188 -3.01562]]\n",
            "\n",
            " [[-7 9.0625 -1.54688 ... -0.738281 3.45312 3.29688]\n",
            "  [-5.40625 6.65625 -2.5625 ... 1.75781 2.59375 2.42188]\n",
            "  [-6.8125 8.625 -11 ... -3.73438 4.28125 3.04688]\n",
            "  ...\n",
            "  [5.3125 4.71875 -2.8125 ... 4.5625 2.625 5.75]\n",
            "  [-8.1875 0.0644531 -5.9375 ... -4.3125 -2.9375 0.326172]\n",
            "  [-8.3125 0.294922 -3.17188 ... -3.04688 -0.867188 -3.01562]]]\n",
            "lnx=[[[-1.47656 1.91406 -0.326172 ... -0.155273 0.726562 0.695312]\n",
            "  [-1.25 1.53906 -0.59375 ... 0.40625 0.601562 0.558594]\n",
            "  [-1.67188 2.10938 -2.70312 ... -0.914062 1.04688 0.746094]\n",
            "  ...\n",
            "  [1.49219 1.32812 -0.789062 ... 1.28125 0.738281 1.61719]\n",
            "  [-1.51562 0.0119019 -1.10156 ... -0.796875 -0.542969 0.0603027]\n",
            "  [-1.79688 0.0634766 -0.683594 ... -0.65625 -0.1875 -0.652344]]\n",
            "\n",
            " [[-1.47656 1.91406 -0.326172 ... -0.155273 0.726562 0.695312]\n",
            "  [-1.25 1.53906 -0.59375 ... 0.40625 0.601562 0.558594]\n",
            "  [-1.67188 2.10938 -2.70312 ... -0.914062 1.04688 0.746094]\n",
            "  ...\n",
            "  [1.49219 1.32812 -0.789062 ... 1.28125 0.738281 1.61719]\n",
            "  [-1.51562 0.0119019 -1.10156 ... -0.796875 -0.542969 0.0603027]\n",
            "  [-1.79688 0.0634766 -0.683594 ... -0.65625 -0.1875 -0.652344]]\n",
            "\n",
            " [[-1.47656 1.91406 -0.326172 ... -0.155273 0.726562 0.695312]\n",
            "  [-1.25 1.53906 -0.59375 ... 0.40625 0.601562 0.558594]\n",
            "  [-1.67188 2.10938 -2.70312 ... -0.914062 1.04688 0.746094]\n",
            "  ...\n",
            "  [1.49219 1.32812 -0.789062 ... 1.28125 0.738281 1.61719]\n",
            "  [-1.51562 0.0119019 -1.10156 ... -0.796875 -0.542969 0.0603027]\n",
            "  [-1.79688 0.0634766 -0.683594 ... -0.65625 -0.1875 -0.652344]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.47656 1.91406 -0.326172 ... -0.155273 0.726562 0.695312]\n",
            "  [-1.25 1.53906 -0.59375 ... 0.40625 0.601562 0.558594]\n",
            "  [-1.67188 2.10938 -2.70312 ... -0.914062 1.04688 0.746094]\n",
            "  ...\n",
            "  [1.49219 1.32812 -0.789062 ... 1.28125 0.738281 1.61719]\n",
            "  [-1.51562 0.0119019 -1.10156 ... -0.796875 -0.542969 0.0603027]\n",
            "  [-1.79688 0.0634766 -0.683594 ... -0.65625 -0.1875 -0.652344]]\n",
            "\n",
            " [[-1.47656 1.91406 -0.326172 ... -0.155273 0.726562 0.695312]\n",
            "  [-1.25 1.53906 -0.59375 ... 0.40625 0.601562 0.558594]\n",
            "  [-1.67188 2.10938 -2.70312 ... -0.914062 1.04688 0.746094]\n",
            "  ...\n",
            "  [1.49219 1.32812 -0.789062 ... 1.28125 0.738281 1.61719]\n",
            "  [-1.51562 0.0119019 -1.10156 ... -0.796875 -0.542969 0.0603027]\n",
            "  [-1.79688 0.0634766 -0.683594 ... -0.65625 -0.1875 -0.652344]]\n",
            "\n",
            " [[-1.47656 1.91406 -0.326172 ... -0.155273 0.726562 0.695312]\n",
            "  [-1.25 1.53906 -0.59375 ... 0.40625 0.601562 0.558594]\n",
            "  [-1.67188 2.10938 -2.70312 ... -0.914062 1.04688 0.746094]\n",
            "  ...\n",
            "  [1.49219 1.32812 -0.789062 ... 1.28125 0.738281 1.61719]\n",
            "  [-1.51562 0.0119019 -1.10156 ... -0.796875 -0.542969 0.0603027]\n",
            "  [-1.79688 0.0634766 -0.683594 ... -0.65625 -0.1875 -0.652344]]]\n",
            "attention_lnx=[[[0.515625 0.235352 -1.78906 ... 0.292969 -1.41406 -0.675781]\n",
            "  [1.33594 0.785156 -1.44531 ... 0.0742188 -0.867188 -0.160156]\n",
            "  [1.47656 1.21094 -1.35938 ... 0.137695 -0.769531 -0.074707]\n",
            "  ...\n",
            "  [0.710938 1.15625 -0.427734 ... -0.314453 -0.378906 -0.00765991]\n",
            "  [-0.0422363 1.24219 0.765625 ... -0.25 -0.855469 -0.162109]\n",
            "  [0.251953 1.22656 0.365234 ... -0.582031 -1.22656 0.129883]]\n",
            "\n",
            " [[0.515625 0.235352 -1.78906 ... 0.292969 -1.41406 -0.675781]\n",
            "  [1.33594 0.785156 -1.44531 ... 0.0742188 -0.867188 -0.160156]\n",
            "  [1.47656 1.21094 -1.35938 ... 0.137695 -0.769531 -0.074707]\n",
            "  ...\n",
            "  [0.710938 1.15625 -0.427734 ... -0.314453 -0.378906 -0.00765991]\n",
            "  [-0.0422363 1.24219 0.765625 ... -0.25 -0.855469 -0.162109]\n",
            "  [0.251953 1.22656 0.365234 ... -0.582031 -1.22656 0.129883]]\n",
            "\n",
            " [[0.515625 0.235352 -1.78906 ... 0.292969 -1.41406 -0.675781]\n",
            "  [1.33594 0.785156 -1.44531 ... 0.0742188 -0.867188 -0.160156]\n",
            "  [1.47656 1.21094 -1.35938 ... 0.137695 -0.769531 -0.074707]\n",
            "  ...\n",
            "  [0.710938 1.15625 -0.427734 ... -0.314453 -0.378906 -0.00765991]\n",
            "  [-0.0422363 1.24219 0.765625 ... -0.25 -0.855469 -0.162109]\n",
            "  [0.251953 1.22656 0.365234 ... -0.582031 -1.22656 0.129883]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.515625 0.235352 -1.78906 ... 0.292969 -1.41406 -0.675781]\n",
            "  [1.33594 0.785156 -1.44531 ... 0.0742188 -0.867188 -0.160156]\n",
            "  [1.47656 1.21094 -1.35938 ... 0.137695 -0.769531 -0.074707]\n",
            "  ...\n",
            "  [0.710938 1.15625 -0.427734 ... -0.314453 -0.378906 -0.00765991]\n",
            "  [-0.0422363 1.24219 0.765625 ... -0.25 -0.855469 -0.162109]\n",
            "  [0.251953 1.22656 0.365234 ... -0.582031 -1.22656 0.129883]]\n",
            "\n",
            " [[0.515625 0.235352 -1.78906 ... 0.292969 -1.41406 -0.675781]\n",
            "  [1.33594 0.785156 -1.44531 ... 0.0742188 -0.867188 -0.160156]\n",
            "  [1.47656 1.21094 -1.35938 ... 0.137695 -0.769531 -0.074707]\n",
            "  ...\n",
            "  [0.710938 1.15625 -0.427734 ... -0.314453 -0.378906 -0.00765991]\n",
            "  [-0.0422363 1.24219 0.765625 ... -0.25 -0.855469 -0.162109]\n",
            "  [0.251953 1.22656 0.365234 ... -0.582031 -1.22656 0.129883]]\n",
            "\n",
            " [[0.515625 0.235352 -1.78906 ... 0.292969 -1.41406 -0.675781]\n",
            "  [1.33594 0.785156 -1.44531 ... 0.0742188 -0.867188 -0.160156]\n",
            "  [1.47656 1.21094 -1.35938 ... 0.137695 -0.769531 -0.074707]\n",
            "  ...\n",
            "  [0.710938 1.15625 -0.427734 ... -0.314453 -0.378906 -0.00765991]\n",
            "  [-0.0422363 1.24219 0.765625 ... -0.25 -0.855469 -0.162109]\n",
            "  [0.251953 1.22656 0.365234 ... -0.582031 -1.22656 0.129883]]]\n",
            "attn_output=[[[-1.32812 1.90625 -0.683594 ... -0.0913086 0.419922 0.539062]\n",
            "  [-0.921875 1.6875 -0.90625 ... 0.416016 0.390625 0.511719]\n",
            "  [-1.27344 2.34375 -2.95312 ... -0.859375 0.839844 0.710938]\n",
            "  ...\n",
            "  [1.66406 1.625 -0.894531 ... 1.17188 0.621094 1.58594]\n",
            "  [-1.48438 0.234375 -0.925781 ... -0.820312 -0.679688 0.0294189]\n",
            "  [-1.70312 0.320312 -0.59375 ... -0.765625 -0.441406 -0.609375]]\n",
            "\n",
            " [[-1.32812 1.90625 -0.683594 ... -0.0913086 0.419922 0.539062]\n",
            "  [-0.921875 1.6875 -0.90625 ... 0.416016 0.390625 0.511719]\n",
            "  [-1.27344 2.34375 -2.95312 ... -0.859375 0.839844 0.710938]\n",
            "  ...\n",
            "  [1.66406 1.625 -0.894531 ... 1.17188 0.621094 1.58594]\n",
            "  [-1.48438 0.234375 -0.925781 ... -0.820312 -0.679688 0.0294189]\n",
            "  [-1.70312 0.320312 -0.59375 ... -0.765625 -0.441406 -0.609375]]\n",
            "\n",
            " [[-1.32812 1.90625 -0.683594 ... -0.0913086 0.419922 0.539062]\n",
            "  [-0.921875 1.6875 -0.90625 ... 0.416016 0.390625 0.511719]\n",
            "  [-1.27344 2.34375 -2.95312 ... -0.859375 0.839844 0.710938]\n",
            "  ...\n",
            "  [1.66406 1.625 -0.894531 ... 1.17188 0.621094 1.58594]\n",
            "  [-1.48438 0.234375 -0.925781 ... -0.820312 -0.679688 0.0294189]\n",
            "  [-1.70312 0.320312 -0.59375 ... -0.765625 -0.441406 -0.609375]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.32812 1.90625 -0.683594 ... -0.0913086 0.419922 0.539062]\n",
            "  [-0.921875 1.6875 -0.90625 ... 0.416016 0.390625 0.511719]\n",
            "  [-1.27344 2.34375 -2.95312 ... -0.859375 0.839844 0.710938]\n",
            "  ...\n",
            "  [1.66406 1.625 -0.894531 ... 1.17188 0.621094 1.58594]\n",
            "  [-1.48438 0.234375 -0.925781 ... -0.820312 -0.679688 0.0294189]\n",
            "  [-1.70312 0.320312 -0.59375 ... -0.765625 -0.441406 -0.609375]]\n",
            "\n",
            " [[-1.32812 1.90625 -0.683594 ... -0.0913086 0.419922 0.539062]\n",
            "  [-0.921875 1.6875 -0.90625 ... 0.416016 0.390625 0.511719]\n",
            "  [-1.27344 2.34375 -2.95312 ... -0.859375 0.839844 0.710938]\n",
            "  ...\n",
            "  [1.66406 1.625 -0.894531 ... 1.17188 0.621094 1.58594]\n",
            "  [-1.48438 0.234375 -0.925781 ... -0.820312 -0.679688 0.0294189]\n",
            "  [-1.70312 0.320312 -0.59375 ... -0.765625 -0.441406 -0.609375]]\n",
            "\n",
            " [[-1.32812 1.90625 -0.683594 ... -0.0913086 0.419922 0.539062]\n",
            "  [-0.921875 1.6875 -0.90625 ... 0.416016 0.390625 0.511719]\n",
            "  [-1.27344 2.34375 -2.95312 ... -0.859375 0.839844 0.710938]\n",
            "  ...\n",
            "  [1.66406 1.625 -0.894531 ... 1.17188 0.621094 1.58594]\n",
            "  [-1.48438 0.234375 -0.925781 ... -0.820312 -0.679688 0.0294189]\n",
            "  [-1.70312 0.320312 -0.59375 ... -0.765625 -0.441406 -0.609375]]]\n",
            "next_layer_addition_dropped_out=[[[-5.03125 9.4375 -3.70312 ... -1.64062 3.04688 2.53125]\n",
            "  [-3.85938 7.6875 -4.40625 ... 1.21094 1.89062 2.375]\n",
            "  [-3.73438 9.5625 -13.125 ... -3.90625 3.85938 2.79688]\n",
            "  ...\n",
            "  [6.625 5.46875 -4.1875 ... 4.0625 2.3125 6.4375]\n",
            "  [-8.1875 0.835938 -5.4375 ... -6.5625 -5.4375 -0.224609]\n",
            "  [-8.8125 1.21875 -3.6875 ... -4.46875 -2.9375 -2.26562]]\n",
            "\n",
            " [[-5.03125 9.4375 -3.70312 ... -1.64062 3.04688 2.53125]\n",
            "  [-3.85938 7.6875 -4.40625 ... 1.21094 1.89062 2.375]\n",
            "  [-3.73438 9.5625 -13.125 ... -3.90625 3.85938 2.79688]\n",
            "  ...\n",
            "  [6.625 5.46875 -4.1875 ... 4.0625 2.3125 6.4375]\n",
            "  [-8.1875 0.835938 -5.4375 ... -6.5625 -5.4375 -0.224609]\n",
            "  [-8.8125 1.21875 -3.6875 ... -4.46875 -2.9375 -2.26562]]\n",
            "\n",
            " [[-5.03125 9.4375 -3.70312 ... -1.64062 3.04688 2.53125]\n",
            "  [-3.85938 7.6875 -4.40625 ... 1.21094 1.89062 2.375]\n",
            "  [-3.73438 9.5625 -13.125 ... -3.90625 3.85938 2.79688]\n",
            "  ...\n",
            "  [6.625 5.46875 -4.1875 ... 4.0625 2.3125 6.4375]\n",
            "  [-8.1875 0.835938 -5.4375 ... -6.5625 -5.4375 -0.224609]\n",
            "  [-8.8125 1.21875 -3.6875 ... -4.46875 -2.9375 -2.26562]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-5.03125 9.4375 -3.70312 ... -1.64062 3.04688 2.53125]\n",
            "  [-3.85938 7.6875 -4.40625 ... 1.21094 1.89062 2.375]\n",
            "  [-3.73438 9.5625 -13.125 ... -3.90625 3.85938 2.79688]\n",
            "  ...\n",
            "  [6.625 5.46875 -4.1875 ... 4.0625 2.3125 6.4375]\n",
            "  [-8.1875 0.835938 -5.4375 ... -6.5625 -5.4375 -0.224609]\n",
            "  [-8.8125 1.21875 -3.6875 ... -4.46875 -2.9375 -2.26562]]\n",
            "\n",
            " [[-5.03125 9.4375 -3.70312 ... -1.64062 3.04688 2.53125]\n",
            "  [-3.85938 7.6875 -4.40625 ... 1.21094 1.89062 2.375]\n",
            "  [-3.73438 9.5625 -13.125 ... -3.90625 3.85938 2.79688]\n",
            "  ...\n",
            "  [6.625 5.46875 -4.1875 ... 4.0625 2.3125 6.4375]\n",
            "  [-8.1875 0.835938 -5.4375 ... -6.5625 -5.4375 -0.224609]\n",
            "  [-8.8125 1.21875 -3.6875 ... -4.46875 -2.9375 -2.26562]]\n",
            "\n",
            " [[-5.03125 9.4375 -3.70312 ... -1.64062 3.04688 2.53125]\n",
            "  [-3.85938 7.6875 -4.40625 ... 1.21094 1.89062 2.375]\n",
            "  [-3.73438 9.5625 -13.125 ... -3.90625 3.85938 2.79688]\n",
            "  ...\n",
            "  [6.625 5.46875 -4.1875 ... 4.0625 2.3125 6.4375]\n",
            "  [-8.1875 0.835938 -5.4375 ... -6.5625 -5.4375 -0.224609]\n",
            "  [-8.8125 1.21875 -3.6875 ... -4.46875 -2.9375 -2.26562]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00]]]]]\n",
            "inputs=[[[-5.03125 9.4375 -3.70312 ... -1.64062 3.04688 2.53125]\n",
            "  [-3.85938 7.6875 -4.40625 ... 1.21094 1.89062 2.375]\n",
            "  [-3.73438 9.5625 -13.125 ... -3.90625 3.85938 2.79688]\n",
            "  ...\n",
            "  [6.625 5.46875 -4.1875 ... 4.0625 2.3125 6.4375]\n",
            "  [-8.1875 0.835938 -5.4375 ... -6.5625 -5.4375 -0.224609]\n",
            "  [-8.8125 1.21875 -3.6875 ... -4.46875 -2.9375 -2.26562]]\n",
            "\n",
            " [[-5.03125 9.4375 -3.70312 ... -1.64062 3.04688 2.53125]\n",
            "  [-3.85938 7.6875 -4.40625 ... 1.21094 1.89062 2.375]\n",
            "  [-3.73438 9.5625 -13.125 ... -3.90625 3.85938 2.79688]\n",
            "  ...\n",
            "  [6.625 5.46875 -4.1875 ... 4.0625 2.3125 6.4375]\n",
            "  [-8.1875 0.835938 -5.4375 ... -6.5625 -5.4375 -0.224609]\n",
            "  [-8.8125 1.21875 -3.6875 ... -4.46875 -2.9375 -2.26562]]\n",
            "\n",
            " [[-5.03125 9.4375 -3.70312 ... -1.64062 3.04688 2.53125]\n",
            "  [-3.85938 7.6875 -4.40625 ... 1.21094 1.89062 2.375]\n",
            "  [-3.73438 9.5625 -13.125 ... -3.90625 3.85938 2.79688]\n",
            "  ...\n",
            "  [6.625 5.46875 -4.1875 ... 4.0625 2.3125 6.4375]\n",
            "  [-8.1875 0.835938 -5.4375 ... -6.5625 -5.4375 -0.224609]\n",
            "  [-8.8125 1.21875 -3.6875 ... -4.46875 -2.9375 -2.26562]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-5.03125 9.4375 -3.70312 ... -1.64062 3.04688 2.53125]\n",
            "  [-3.85938 7.6875 -4.40625 ... 1.21094 1.89062 2.375]\n",
            "  [-3.73438 9.5625 -13.125 ... -3.90625 3.85938 2.79688]\n",
            "  ...\n",
            "  [6.625 5.46875 -4.1875 ... 4.0625 2.3125 6.4375]\n",
            "  [-8.1875 0.835938 -5.4375 ... -6.5625 -5.4375 -0.224609]\n",
            "  [-8.8125 1.21875 -3.6875 ... -4.46875 -2.9375 -2.26562]]\n",
            "\n",
            " [[-5.03125 9.4375 -3.70312 ... -1.64062 3.04688 2.53125]\n",
            "  [-3.85938 7.6875 -4.40625 ... 1.21094 1.89062 2.375]\n",
            "  [-3.73438 9.5625 -13.125 ... -3.90625 3.85938 2.79688]\n",
            "  ...\n",
            "  [6.625 5.46875 -4.1875 ... 4.0625 2.3125 6.4375]\n",
            "  [-8.1875 0.835938 -5.4375 ... -6.5625 -5.4375 -0.224609]\n",
            "  [-8.8125 1.21875 -3.6875 ... -4.46875 -2.9375 -2.26562]]\n",
            "\n",
            " [[-5.03125 9.4375 -3.70312 ... -1.64062 3.04688 2.53125]\n",
            "  [-3.85938 7.6875 -4.40625 ... 1.21094 1.89062 2.375]\n",
            "  [-3.73438 9.5625 -13.125 ... -3.90625 3.85938 2.79688]\n",
            "  ...\n",
            "  [6.625 5.46875 -4.1875 ... 4.0625 2.3125 6.4375]\n",
            "  [-8.1875 0.835938 -5.4375 ... -6.5625 -5.4375 -0.224609]\n",
            "  [-8.8125 1.21875 -3.6875 ... -4.46875 -2.9375 -2.26562]]]\n",
            "lnx=[[[-1.01562 1.90625 -0.746094 ... -0.332031 0.613281 0.511719]\n",
            "  [-0.863281 1.71875 -0.984375 ... 0.271484 0.421875 0.53125]\n",
            "  [-0.875 2.25 -3.07812 ... -0.917969 0.90625 0.65625]\n",
            "  ...\n",
            "  [1.78906 1.47656 -1.13281 ... 1.09375 0.625 1.73438]\n",
            "  [-1.3125 0.133789 -0.871094 ... -1.05469 -0.871094 -0.0358887]\n",
            "  [-1.69531 0.234375 -0.710938 ... -0.859375 -0.566406 -0.435547]]\n",
            "\n",
            " [[-1.01562 1.90625 -0.746094 ... -0.332031 0.613281 0.511719]\n",
            "  [-0.863281 1.71875 -0.984375 ... 0.271484 0.421875 0.53125]\n",
            "  [-0.875 2.25 -3.07812 ... -0.917969 0.90625 0.65625]\n",
            "  ...\n",
            "  [1.78906 1.47656 -1.13281 ... 1.09375 0.625 1.73438]\n",
            "  [-1.3125 0.133789 -0.871094 ... -1.05469 -0.871094 -0.0358887]\n",
            "  [-1.69531 0.234375 -0.710938 ... -0.859375 -0.566406 -0.435547]]\n",
            "\n",
            " [[-1.01562 1.90625 -0.746094 ... -0.332031 0.613281 0.511719]\n",
            "  [-0.863281 1.71875 -0.984375 ... 0.271484 0.421875 0.53125]\n",
            "  [-0.875 2.25 -3.07812 ... -0.917969 0.90625 0.65625]\n",
            "  ...\n",
            "  [1.78906 1.47656 -1.13281 ... 1.09375 0.625 1.73438]\n",
            "  [-1.3125 0.133789 -0.871094 ... -1.05469 -0.871094 -0.0358887]\n",
            "  [-1.69531 0.234375 -0.710938 ... -0.859375 -0.566406 -0.435547]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.01562 1.90625 -0.746094 ... -0.332031 0.613281 0.511719]\n",
            "  [-0.863281 1.71875 -0.984375 ... 0.271484 0.421875 0.53125]\n",
            "  [-0.875 2.25 -3.07812 ... -0.917969 0.90625 0.65625]\n",
            "  ...\n",
            "  [1.78906 1.47656 -1.13281 ... 1.09375 0.625 1.73438]\n",
            "  [-1.3125 0.133789 -0.871094 ... -1.05469 -0.871094 -0.0358887]\n",
            "  [-1.69531 0.234375 -0.710938 ... -0.859375 -0.566406 -0.435547]]\n",
            "\n",
            " [[-1.01562 1.90625 -0.746094 ... -0.332031 0.613281 0.511719]\n",
            "  [-0.863281 1.71875 -0.984375 ... 0.271484 0.421875 0.53125]\n",
            "  [-0.875 2.25 -3.07812 ... -0.917969 0.90625 0.65625]\n",
            "  ...\n",
            "  [1.78906 1.47656 -1.13281 ... 1.09375 0.625 1.73438]\n",
            "  [-1.3125 0.133789 -0.871094 ... -1.05469 -0.871094 -0.0358887]\n",
            "  [-1.69531 0.234375 -0.710938 ... -0.859375 -0.566406 -0.435547]]\n",
            "\n",
            " [[-1.01562 1.90625 -0.746094 ... -0.332031 0.613281 0.511719]\n",
            "  [-0.863281 1.71875 -0.984375 ... 0.271484 0.421875 0.53125]\n",
            "  [-0.875 2.25 -3.07812 ... -0.917969 0.90625 0.65625]\n",
            "  ...\n",
            "  [1.78906 1.47656 -1.13281 ... 1.09375 0.625 1.73438]\n",
            "  [-1.3125 0.133789 -0.871094 ... -1.05469 -0.871094 -0.0358887]\n",
            "  [-1.69531 0.234375 -0.710938 ... -0.859375 -0.566406 -0.435547]]]\n",
            "attention_lnx=[[[-1.60156 0.384766 -1.98438 ... -1.02344 -0.328125 0.652344]\n",
            "  [-0.339844 -0.251953 -1.04688 ... -0.667969 0.597656 0.589844]\n",
            "  [-0.890625 -0.546875 -1.5 ... 0.195312 0.503906 0.451172]\n",
            "  ...\n",
            "  [0.105957 -0.394531 -1.28906 ... -0.273438 0.408203 1.11719]\n",
            "  [0.773438 -0.235352 -1.01562 ... -0.320312 -0.53125 1.00781]\n",
            "  [0.339844 -0.427734 -1.00781 ... -0.34375 -0.194336 1.16406]]\n",
            "\n",
            " [[-1.60156 0.384766 -1.98438 ... -1.02344 -0.328125 0.652344]\n",
            "  [-0.339844 -0.251953 -1.04688 ... -0.667969 0.597656 0.589844]\n",
            "  [-0.890625 -0.546875 -1.5 ... 0.195312 0.503906 0.451172]\n",
            "  ...\n",
            "  [0.105957 -0.394531 -1.28906 ... -0.273438 0.408203 1.11719]\n",
            "  [0.773438 -0.235352 -1.01562 ... -0.320312 -0.53125 1.00781]\n",
            "  [0.339844 -0.427734 -1.00781 ... -0.34375 -0.194336 1.16406]]\n",
            "\n",
            " [[-1.60156 0.384766 -1.98438 ... -1.02344 -0.328125 0.652344]\n",
            "  [-0.339844 -0.251953 -1.04688 ... -0.667969 0.597656 0.589844]\n",
            "  [-0.890625 -0.546875 -1.5 ... 0.195312 0.503906 0.451172]\n",
            "  ...\n",
            "  [0.105957 -0.394531 -1.28906 ... -0.273438 0.408203 1.11719]\n",
            "  [0.773438 -0.235352 -1.01562 ... -0.320312 -0.53125 1.00781]\n",
            "  [0.339844 -0.427734 -1.00781 ... -0.34375 -0.194336 1.16406]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.60156 0.384766 -1.98438 ... -1.02344 -0.328125 0.652344]\n",
            "  [-0.339844 -0.251953 -1.04688 ... -0.667969 0.597656 0.589844]\n",
            "  [-0.890625 -0.546875 -1.5 ... 0.195312 0.503906 0.451172]\n",
            "  ...\n",
            "  [0.105957 -0.394531 -1.28906 ... -0.273438 0.408203 1.11719]\n",
            "  [0.773438 -0.235352 -1.01562 ... -0.320312 -0.53125 1.00781]\n",
            "  [0.339844 -0.427734 -1.00781 ... -0.34375 -0.194336 1.16406]]\n",
            "\n",
            " [[-1.60156 0.384766 -1.98438 ... -1.02344 -0.328125 0.652344]\n",
            "  [-0.339844 -0.251953 -1.04688 ... -0.667969 0.597656 0.589844]\n",
            "  [-0.890625 -0.546875 -1.5 ... 0.195312 0.503906 0.451172]\n",
            "  ...\n",
            "  [0.105957 -0.394531 -1.28906 ... -0.273438 0.408203 1.11719]\n",
            "  [0.773438 -0.235352 -1.01562 ... -0.320312 -0.53125 1.00781]\n",
            "  [0.339844 -0.427734 -1.00781 ... -0.34375 -0.194336 1.16406]]\n",
            "\n",
            " [[-1.60156 0.384766 -1.98438 ... -1.02344 -0.328125 0.652344]\n",
            "  [-0.339844 -0.251953 -1.04688 ... -0.667969 0.597656 0.589844]\n",
            "  [-0.890625 -0.546875 -1.5 ... 0.195312 0.503906 0.451172]\n",
            "  ...\n",
            "  [0.105957 -0.394531 -1.28906 ... -0.273438 0.408203 1.11719]\n",
            "  [0.773438 -0.235352 -1.01562 ... -0.320312 -0.53125 1.00781]\n",
            "  [0.339844 -0.427734 -1.00781 ... -0.34375 -0.194336 1.16406]]]\n",
            "attn_output=[[[-1.3125 1.94531 -1.13281 ... -0.527344 0.539062 0.632812]\n",
            "  [-0.921875 1.63281 -1.20312 ... 0.119141 0.546875 0.652344]\n",
            "  [-1.07031 2.07812 -3.375 ... -0.859375 1.00781 0.75]\n",
            "  ...\n",
            "  [1.77344 1.33594 -1.44531 ... 1.00781 0.71875 2]\n",
            "  [-1.15625 0.0942383 -1.01562 ... -1.07812 -0.933594 0.123047]\n",
            "  [-1.59375 0.149414 -0.882812 ... -0.90625 -0.585938 -0.207031]]\n",
            "\n",
            " [[-1.3125 1.94531 -1.13281 ... -0.527344 0.539062 0.632812]\n",
            "  [-0.921875 1.63281 -1.20312 ... 0.119141 0.546875 0.652344]\n",
            "  [-1.07031 2.07812 -3.375 ... -0.859375 1.00781 0.75]\n",
            "  ...\n",
            "  [1.77344 1.33594 -1.44531 ... 1.00781 0.71875 2]\n",
            "  [-1.15625 0.0942383 -1.01562 ... -1.07812 -0.933594 0.123047]\n",
            "  [-1.59375 0.149414 -0.882812 ... -0.90625 -0.585938 -0.207031]]\n",
            "\n",
            " [[-1.3125 1.94531 -1.13281 ... -0.527344 0.539062 0.632812]\n",
            "  [-0.921875 1.63281 -1.20312 ... 0.119141 0.546875 0.652344]\n",
            "  [-1.07031 2.07812 -3.375 ... -0.859375 1.00781 0.75]\n",
            "  ...\n",
            "  [1.77344 1.33594 -1.44531 ... 1.00781 0.71875 2]\n",
            "  [-1.15625 0.0942383 -1.01562 ... -1.07812 -0.933594 0.123047]\n",
            "  [-1.59375 0.149414 -0.882812 ... -0.90625 -0.585938 -0.207031]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.3125 1.94531 -1.13281 ... -0.527344 0.539062 0.632812]\n",
            "  [-0.921875 1.63281 -1.20312 ... 0.119141 0.546875 0.652344]\n",
            "  [-1.07031 2.07812 -3.375 ... -0.859375 1.00781 0.75]\n",
            "  ...\n",
            "  [1.77344 1.33594 -1.44531 ... 1.00781 0.71875 2]\n",
            "  [-1.15625 0.0942383 -1.01562 ... -1.07812 -0.933594 0.123047]\n",
            "  [-1.59375 0.149414 -0.882812 ... -0.90625 -0.585938 -0.207031]]\n",
            "\n",
            " [[-1.3125 1.94531 -1.13281 ... -0.527344 0.539062 0.632812]\n",
            "  [-0.921875 1.63281 -1.20312 ... 0.119141 0.546875 0.652344]\n",
            "  [-1.07031 2.07812 -3.375 ... -0.859375 1.00781 0.75]\n",
            "  ...\n",
            "  [1.77344 1.33594 -1.44531 ... 1.00781 0.71875 2]\n",
            "  [-1.15625 0.0942383 -1.01562 ... -1.07812 -0.933594 0.123047]\n",
            "  [-1.59375 0.149414 -0.882812 ... -0.90625 -0.585938 -0.207031]]\n",
            "\n",
            " [[-1.3125 1.94531 -1.13281 ... -0.527344 0.539062 0.632812]\n",
            "  [-0.921875 1.63281 -1.20312 ... 0.119141 0.546875 0.652344]\n",
            "  [-1.07031 2.07812 -3.375 ... -0.859375 1.00781 0.75]\n",
            "  ...\n",
            "  [1.77344 1.33594 -1.44531 ... 1.00781 0.71875 2]\n",
            "  [-1.15625 0.0942383 -1.01562 ... -1.07812 -0.933594 0.123047]\n",
            "  [-1.59375 0.149414 -0.882812 ... -0.90625 -0.585938 -0.207031]]]\n",
            "next_layer_addition_dropped_out=[[[-6.59375 9.3125 -5.5 ... -2.73438 2.92188 4.65625]\n",
            "  [-3.15625 6.5 -5.21875 ... 1.45312 2.0625 4.21875]\n",
            "  [-5.15625 7.8125 -14.375 ... -4.5 3.625 3.51562]\n",
            "  ...\n",
            "  [6.71875 4.90625 -5.21875 ... 5.34375 1.94531 7.1875]\n",
            "  [-9.5 -0.746094 -7.34375 ... -9.375 -7.71875 0.0305176]\n",
            "  [-10.9375 0.0578613 -5.09375 ... -7.625 -3.96875 -1.66406]]\n",
            "\n",
            " [[-6.59375 9.3125 -5.5 ... -2.73438 2.92188 4.65625]\n",
            "  [-3.15625 6.5 -5.21875 ... 1.45312 2.0625 4.21875]\n",
            "  [-5.15625 7.8125 -14.375 ... -4.5 3.625 3.51562]\n",
            "  ...\n",
            "  [6.71875 4.90625 -5.21875 ... 5.34375 1.94531 7.1875]\n",
            "  [-9.5 -0.746094 -7.34375 ... -9.375 -7.71875 0.0305176]\n",
            "  [-10.9375 0.0578613 -5.09375 ... -7.625 -3.96875 -1.66406]]\n",
            "\n",
            " [[-6.59375 9.3125 -5.5 ... -2.73438 2.92188 4.65625]\n",
            "  [-3.15625 6.5 -5.21875 ... 1.45312 2.0625 4.21875]\n",
            "  [-5.15625 7.8125 -14.375 ... -4.5 3.625 3.51562]\n",
            "  ...\n",
            "  [6.71875 4.90625 -5.21875 ... 5.34375 1.94531 7.1875]\n",
            "  [-9.5 -0.746094 -7.34375 ... -9.375 -7.71875 0.0305176]\n",
            "  [-10.9375 0.0578613 -5.09375 ... -7.625 -3.96875 -1.66406]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-6.59375 9.3125 -5.5 ... -2.73438 2.92188 4.65625]\n",
            "  [-3.15625 6.5 -5.21875 ... 1.45312 2.0625 4.21875]\n",
            "  [-5.15625 7.8125 -14.375 ... -4.5 3.625 3.51562]\n",
            "  ...\n",
            "  [6.71875 4.90625 -5.21875 ... 5.34375 1.94531 7.1875]\n",
            "  [-9.5 -0.746094 -7.34375 ... -9.375 -7.71875 0.0305176]\n",
            "  [-10.9375 0.0578613 -5.09375 ... -7.625 -3.96875 -1.66406]]\n",
            "\n",
            " [[-6.59375 9.3125 -5.5 ... -2.73438 2.92188 4.65625]\n",
            "  [-3.15625 6.5 -5.21875 ... 1.45312 2.0625 4.21875]\n",
            "  [-5.15625 7.8125 -14.375 ... -4.5 3.625 3.51562]\n",
            "  ...\n",
            "  [6.71875 4.90625 -5.21875 ... 5.34375 1.94531 7.1875]\n",
            "  [-9.5 -0.746094 -7.34375 ... -9.375 -7.71875 0.0305176]\n",
            "  [-10.9375 0.0578613 -5.09375 ... -7.625 -3.96875 -1.66406]]\n",
            "\n",
            " [[-6.59375 9.3125 -5.5 ... -2.73438 2.92188 4.65625]\n",
            "  [-3.15625 6.5 -5.21875 ... 1.45312 2.0625 4.21875]\n",
            "  [-5.15625 7.8125 -14.375 ... -4.5 3.625 3.51562]\n",
            "  ...\n",
            "  [6.71875 4.90625 -5.21875 ... 5.34375 1.94531 7.1875]\n",
            "  [-9.5 -0.746094 -7.34375 ... -9.375 -7.71875 0.0305176]\n",
            "  [-10.9375 0.0578613 -5.09375 ... -7.625 -3.96875 -1.66406]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00  0.0000000e+00]]]]]\n",
            "inputs=[[[-6.59375 9.3125 -5.5 ... -2.73438 2.92188 4.65625]\n",
            "  [-3.15625 6.5 -5.21875 ... 1.45312 2.0625 4.21875]\n",
            "  [-5.15625 7.8125 -14.375 ... -4.5 3.625 3.51562]\n",
            "  ...\n",
            "  [6.71875 4.90625 -5.21875 ... 5.34375 1.94531 7.1875]\n",
            "  [-9.5 -0.746094 -7.34375 ... -9.375 -7.71875 0.0305176]\n",
            "  [-10.9375 0.0578613 -5.09375 ... -7.625 -3.96875 -1.66406]]\n",
            "\n",
            " [[-6.59375 9.3125 -5.5 ... -2.73438 2.92188 4.65625]\n",
            "  [-3.15625 6.5 -5.21875 ... 1.45312 2.0625 4.21875]\n",
            "  [-5.15625 7.8125 -14.375 ... -4.5 3.625 3.51562]\n",
            "  ...\n",
            "  [6.71875 4.90625 -5.21875 ... 5.34375 1.94531 7.1875]\n",
            "  [-9.5 -0.746094 -7.34375 ... -9.375 -7.71875 0.0305176]\n",
            "  [-10.9375 0.0578613 -5.09375 ... -7.625 -3.96875 -1.66406]]\n",
            "\n",
            " [[-6.59375 9.3125 -5.5 ... -2.73438 2.92188 4.65625]\n",
            "  [-3.15625 6.5 -5.21875 ... 1.45312 2.0625 4.21875]\n",
            "  [-5.15625 7.8125 -14.375 ... -4.5 3.625 3.51562]\n",
            "  ...\n",
            "  [6.71875 4.90625 -5.21875 ... 5.34375 1.94531 7.1875]\n",
            "  [-9.5 -0.746094 -7.34375 ... -9.375 -7.71875 0.0305176]\n",
            "  [-10.9375 0.0578613 -5.09375 ... -7.625 -3.96875 -1.66406]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-6.59375 9.3125 -5.5 ... -2.73438 2.92188 4.65625]\n",
            "  [-3.15625 6.5 -5.21875 ... 1.45312 2.0625 4.21875]\n",
            "  [-5.15625 7.8125 -14.375 ... -4.5 3.625 3.51562]\n",
            "  ...\n",
            "  [6.71875 4.90625 -5.21875 ... 5.34375 1.94531 7.1875]\n",
            "  [-9.5 -0.746094 -7.34375 ... -9.375 -7.71875 0.0305176]\n",
            "  [-10.9375 0.0578613 -5.09375 ... -7.625 -3.96875 -1.66406]]\n",
            "\n",
            " [[-6.59375 9.3125 -5.5 ... -2.73438 2.92188 4.65625]\n",
            "  [-3.15625 6.5 -5.21875 ... 1.45312 2.0625 4.21875]\n",
            "  [-5.15625 7.8125 -14.375 ... -4.5 3.625 3.51562]\n",
            "  ...\n",
            "  [6.71875 4.90625 -5.21875 ... 5.34375 1.94531 7.1875]\n",
            "  [-9.5 -0.746094 -7.34375 ... -9.375 -7.71875 0.0305176]\n",
            "  [-10.9375 0.0578613 -5.09375 ... -7.625 -3.96875 -1.66406]]\n",
            "\n",
            " [[-6.59375 9.3125 -5.5 ... -2.73438 2.92188 4.65625]\n",
            "  [-3.15625 6.5 -5.21875 ... 1.45312 2.0625 4.21875]\n",
            "  [-5.15625 7.8125 -14.375 ... -4.5 3.625 3.51562]\n",
            "  ...\n",
            "  [6.71875 4.90625 -5.21875 ... 5.34375 1.94531 7.1875]\n",
            "  [-9.5 -0.746094 -7.34375 ... -9.375 -7.71875 0.0305176]\n",
            "  [-10.9375 0.0578613 -5.09375 ... -7.625 -3.96875 -1.66406]]]\n",
            "lnx=[[[-1.29688 1.83594 -1.08594 ... -0.539062 0.578125 0.917969]\n",
            "  [-0.6875 1.41406 -1.13281 ... 0.316406 0.447266 0.917969]\n",
            "  [-1.16406 1.76562 -3.25 ... -1.01562 0.820312 0.792969]\n",
            "  ...\n",
            "  [1.74219 1.27344 -1.35156 ... 1.38281 0.503906 1.86719]\n",
            "  [-1.32031 -0.103516 -1.02344 ... -1.30469 -1.07031 0.00424194]\n",
            "  [-1.83594 0.00970459 -0.855469 ... -1.28125 -0.667969 -0.279297]]\n",
            "\n",
            " [[-1.29688 1.83594 -1.08594 ... -0.539062 0.578125 0.917969]\n",
            "  [-0.6875 1.41406 -1.13281 ... 0.316406 0.447266 0.917969]\n",
            "  [-1.16406 1.76562 -3.25 ... -1.01562 0.820312 0.792969]\n",
            "  ...\n",
            "  [1.74219 1.27344 -1.35156 ... 1.38281 0.503906 1.86719]\n",
            "  [-1.32031 -0.103516 -1.02344 ... -1.30469 -1.07031 0.00424194]\n",
            "  [-1.83594 0.00970459 -0.855469 ... -1.28125 -0.667969 -0.279297]]\n",
            "\n",
            " [[-1.29688 1.83594 -1.08594 ... -0.539062 0.578125 0.917969]\n",
            "  [-0.6875 1.41406 -1.13281 ... 0.316406 0.447266 0.917969]\n",
            "  [-1.16406 1.76562 -3.25 ... -1.01562 0.820312 0.792969]\n",
            "  ...\n",
            "  [1.74219 1.27344 -1.35156 ... 1.38281 0.503906 1.86719]\n",
            "  [-1.32031 -0.103516 -1.02344 ... -1.30469 -1.07031 0.00424194]\n",
            "  [-1.83594 0.00970459 -0.855469 ... -1.28125 -0.667969 -0.279297]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.29688 1.83594 -1.08594 ... -0.539062 0.578125 0.917969]\n",
            "  [-0.6875 1.41406 -1.13281 ... 0.316406 0.447266 0.917969]\n",
            "  [-1.16406 1.76562 -3.25 ... -1.01562 0.820312 0.792969]\n",
            "  ...\n",
            "  [1.74219 1.27344 -1.35156 ... 1.38281 0.503906 1.86719]\n",
            "  [-1.32031 -0.103516 -1.02344 ... -1.30469 -1.07031 0.00424194]\n",
            "  [-1.83594 0.00970459 -0.855469 ... -1.28125 -0.667969 -0.279297]]\n",
            "\n",
            " [[-1.29688 1.83594 -1.08594 ... -0.539062 0.578125 0.917969]\n",
            "  [-0.6875 1.41406 -1.13281 ... 0.316406 0.447266 0.917969]\n",
            "  [-1.16406 1.76562 -3.25 ... -1.01562 0.820312 0.792969]\n",
            "  ...\n",
            "  [1.74219 1.27344 -1.35156 ... 1.38281 0.503906 1.86719]\n",
            "  [-1.32031 -0.103516 -1.02344 ... -1.30469 -1.07031 0.00424194]\n",
            "  [-1.83594 0.00970459 -0.855469 ... -1.28125 -0.667969 -0.279297]]\n",
            "\n",
            " [[-1.29688 1.83594 -1.08594 ... -0.539062 0.578125 0.917969]\n",
            "  [-0.6875 1.41406 -1.13281 ... 0.316406 0.447266 0.917969]\n",
            "  [-1.16406 1.76562 -3.25 ... -1.01562 0.820312 0.792969]\n",
            "  ...\n",
            "  [1.74219 1.27344 -1.35156 ... 1.38281 0.503906 1.86719]\n",
            "  [-1.32031 -0.103516 -1.02344 ... -1.30469 -1.07031 0.00424194]\n",
            "  [-1.83594 0.00970459 -0.855469 ... -1.28125 -0.667969 -0.279297]]]\n",
            "attention_lnx=[[[0.244141 -0.378906 -0.361328 ... -0.988281 0.667969 -1.13281]\n",
            "  [0.349609 0.0629883 -0.0795898 ... 0.0498047 0.457031 -0.427734]\n",
            "  [0.298828 0.265625 -0.427734 ... -0.304688 0.474609 -0.847656]\n",
            "  ...\n",
            "  [0.212891 -0.423828 -0.198242 ... -0.566406 -0.0849609 -1.45312]\n",
            "  [-0.515625 -0.582031 -0.453125 ... -1.375 0.285156 -2.21875]\n",
            "  [-0.470703 -0.496094 -0.298828 ... -1 0.277344 -2.04688]]\n",
            "\n",
            " [[0.244141 -0.378906 -0.361328 ... -0.988281 0.667969 -1.13281]\n",
            "  [0.349609 0.0629883 -0.0795898 ... 0.0498047 0.457031 -0.427734]\n",
            "  [0.298828 0.265625 -0.427734 ... -0.304688 0.474609 -0.847656]\n",
            "  ...\n",
            "  [0.212891 -0.423828 -0.198242 ... -0.566406 -0.0849609 -1.45312]\n",
            "  [-0.515625 -0.582031 -0.453125 ... -1.375 0.285156 -2.21875]\n",
            "  [-0.470703 -0.496094 -0.298828 ... -1 0.277344 -2.04688]]\n",
            "\n",
            " [[0.244141 -0.378906 -0.361328 ... -0.988281 0.667969 -1.13281]\n",
            "  [0.349609 0.0629883 -0.0795898 ... 0.0498047 0.457031 -0.427734]\n",
            "  [0.298828 0.265625 -0.427734 ... -0.304688 0.474609 -0.847656]\n",
            "  ...\n",
            "  [0.212891 -0.423828 -0.198242 ... -0.566406 -0.0849609 -1.45312]\n",
            "  [-0.515625 -0.582031 -0.453125 ... -1.375 0.285156 -2.21875]\n",
            "  [-0.470703 -0.496094 -0.298828 ... -1 0.277344 -2.04688]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.244141 -0.378906 -0.361328 ... -0.988281 0.667969 -1.13281]\n",
            "  [0.349609 0.0629883 -0.0795898 ... 0.0498047 0.457031 -0.427734]\n",
            "  [0.298828 0.265625 -0.427734 ... -0.304688 0.474609 -0.847656]\n",
            "  ...\n",
            "  [0.212891 -0.423828 -0.198242 ... -0.566406 -0.0849609 -1.45312]\n",
            "  [-0.515625 -0.582031 -0.453125 ... -1.375 0.285156 -2.21875]\n",
            "  [-0.470703 -0.496094 -0.298828 ... -1 0.277344 -2.04688]]\n",
            "\n",
            " [[0.244141 -0.378906 -0.361328 ... -0.988281 0.667969 -1.13281]\n",
            "  [0.349609 0.0629883 -0.0795898 ... 0.0498047 0.457031 -0.427734]\n",
            "  [0.298828 0.265625 -0.427734 ... -0.304688 0.474609 -0.847656]\n",
            "  ...\n",
            "  [0.212891 -0.423828 -0.198242 ... -0.566406 -0.0849609 -1.45312]\n",
            "  [-0.515625 -0.582031 -0.453125 ... -1.375 0.285156 -2.21875]\n",
            "  [-0.470703 -0.496094 -0.298828 ... -1 0.277344 -2.04688]]\n",
            "\n",
            " [[0.244141 -0.378906 -0.361328 ... -0.988281 0.667969 -1.13281]\n",
            "  [0.349609 0.0629883 -0.0795898 ... 0.0498047 0.457031 -0.427734]\n",
            "  [0.298828 0.265625 -0.427734 ... -0.304688 0.474609 -0.847656]\n",
            "  ...\n",
            "  [0.212891 -0.423828 -0.198242 ... -0.566406 -0.0849609 -1.45312]\n",
            "  [-0.515625 -0.582031 -0.453125 ... -1.375 0.285156 -2.21875]\n",
            "  [-0.470703 -0.496094 -0.298828 ... -1 0.277344 -2.04688]]]\n",
            "attn_output=[[[-1.22656 1.72656 -1.14062 ... -0.71875 0.695312 0.683594]\n",
            "  [-0.601562 1.39844 -1.13281 ... 0.320312 0.535156 0.8125]\n",
            "  [-1.07812 1.78906 -3.29688 ... -1.07031 0.910156 0.59375]\n",
            "  ...\n",
            "  [1.77344 1.14062 -1.38281 ... 1.21875 0.474609 1.46094]\n",
            "  [-1.35156 -0.179688 -1.05469 ... -1.45312 -1.00781 -0.294922]\n",
            "  [-1.875 -0.0717773 -0.882812 ... -1.41406 -0.601562 -0.609375]]\n",
            "\n",
            " [[-1.22656 1.72656 -1.14062 ... -0.71875 0.695312 0.683594]\n",
            "  [-0.601562 1.39844 -1.13281 ... 0.320312 0.535156 0.8125]\n",
            "  [-1.07812 1.78906 -3.29688 ... -1.07031 0.910156 0.59375]\n",
            "  ...\n",
            "  [1.77344 1.14062 -1.38281 ... 1.21875 0.474609 1.46094]\n",
            "  [-1.35156 -0.179688 -1.05469 ... -1.45312 -1.00781 -0.294922]\n",
            "  [-1.875 -0.0717773 -0.882812 ... -1.41406 -0.601562 -0.609375]]\n",
            "\n",
            " [[-1.22656 1.72656 -1.14062 ... -0.71875 0.695312 0.683594]\n",
            "  [-0.601562 1.39844 -1.13281 ... 0.320312 0.535156 0.8125]\n",
            "  [-1.07812 1.78906 -3.29688 ... -1.07031 0.910156 0.59375]\n",
            "  ...\n",
            "  [1.77344 1.14062 -1.38281 ... 1.21875 0.474609 1.46094]\n",
            "  [-1.35156 -0.179688 -1.05469 ... -1.45312 -1.00781 -0.294922]\n",
            "  [-1.875 -0.0717773 -0.882812 ... -1.41406 -0.601562 -0.609375]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.22656 1.72656 -1.14062 ... -0.71875 0.695312 0.683594]\n",
            "  [-0.601562 1.39844 -1.13281 ... 0.320312 0.535156 0.8125]\n",
            "  [-1.07812 1.78906 -3.29688 ... -1.07031 0.910156 0.59375]\n",
            "  ...\n",
            "  [1.77344 1.14062 -1.38281 ... 1.21875 0.474609 1.46094]\n",
            "  [-1.35156 -0.179688 -1.05469 ... -1.45312 -1.00781 -0.294922]\n",
            "  [-1.875 -0.0717773 -0.882812 ... -1.41406 -0.601562 -0.609375]]\n",
            "\n",
            " [[-1.22656 1.72656 -1.14062 ... -0.71875 0.695312 0.683594]\n",
            "  [-0.601562 1.39844 -1.13281 ... 0.320312 0.535156 0.8125]\n",
            "  [-1.07812 1.78906 -3.29688 ... -1.07031 0.910156 0.59375]\n",
            "  ...\n",
            "  [1.77344 1.14062 -1.38281 ... 1.21875 0.474609 1.46094]\n",
            "  [-1.35156 -0.179688 -1.05469 ... -1.45312 -1.00781 -0.294922]\n",
            "  [-1.875 -0.0717773 -0.882812 ... -1.41406 -0.601562 -0.609375]]\n",
            "\n",
            " [[-1.22656 1.72656 -1.14062 ... -0.71875 0.695312 0.683594]\n",
            "  [-0.601562 1.39844 -1.13281 ... 0.320312 0.535156 0.8125]\n",
            "  [-1.07812 1.78906 -3.29688 ... -1.07031 0.910156 0.59375]\n",
            "  ...\n",
            "  [1.77344 1.14062 -1.38281 ... 1.21875 0.474609 1.46094]\n",
            "  [-1.35156 -0.179688 -1.05469 ... -1.45312 -1.00781 -0.294922]\n",
            "  [-1.875 -0.0717773 -0.882812 ... -1.41406 -0.601562 -0.609375]]]\n",
            "next_layer_addition_dropped_out=[[[-6.65625 9.0625 -6.625 ... -2.6875 3.75 3.8125]\n",
            "  [-3.20312 7.53125 -5.28125 ... 2.625 3.40625 4.28125]\n",
            "  [-5.25 8.1875 -15.8125 ... -6.09375 4.75 2.60938]\n",
            "  ...\n",
            "  [5.875 5.09375 -5.5625 ... 3.34375 3.04688 5.40625]\n",
            "  [-8.375 -2.20312 -9.4375 ... -12.6875 -9.125 -2.85938]\n",
            "  [-11.1875 0.355469 -4.71875 ... -10 -3.57812 -2.89062]]\n",
            "\n",
            " [[-6.65625 9.0625 -6.625 ... -2.6875 3.75 3.8125]\n",
            "  [-3.20312 7.53125 -5.28125 ... 2.625 3.40625 4.28125]\n",
            "  [-5.25 8.1875 -15.8125 ... -6.09375 4.75 2.60938]\n",
            "  ...\n",
            "  [5.875 5.09375 -5.5625 ... 3.34375 3.04688 5.40625]\n",
            "  [-8.375 -2.20312 -9.4375 ... -12.6875 -9.125 -2.85938]\n",
            "  [-11.1875 0.355469 -4.71875 ... -10 -3.57812 -2.89062]]\n",
            "\n",
            " [[-6.65625 9.0625 -6.625 ... -2.6875 3.75 3.8125]\n",
            "  [-3.20312 7.53125 -5.28125 ... 2.625 3.40625 4.28125]\n",
            "  [-5.25 8.1875 -15.8125 ... -6.09375 4.75 2.60938]\n",
            "  ...\n",
            "  [5.875 5.09375 -5.5625 ... 3.34375 3.04688 5.40625]\n",
            "  [-8.375 -2.20312 -9.4375 ... -12.6875 -9.125 -2.85938]\n",
            "  [-11.1875 0.355469 -4.71875 ... -10 -3.57812 -2.89062]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-6.65625 9.0625 -6.625 ... -2.6875 3.75 3.8125]\n",
            "  [-3.20312 7.53125 -5.28125 ... 2.625 3.40625 4.28125]\n",
            "  [-5.25 8.1875 -15.8125 ... -6.09375 4.75 2.60938]\n",
            "  ...\n",
            "  [5.875 5.09375 -5.5625 ... 3.34375 3.04688 5.40625]\n",
            "  [-8.375 -2.20312 -9.4375 ... -12.6875 -9.125 -2.85938]\n",
            "  [-11.1875 0.355469 -4.71875 ... -10 -3.57812 -2.89062]]\n",
            "\n",
            " [[-6.65625 9.0625 -6.625 ... -2.6875 3.75 3.8125]\n",
            "  [-3.20312 7.53125 -5.28125 ... 2.625 3.40625 4.28125]\n",
            "  [-5.25 8.1875 -15.8125 ... -6.09375 4.75 2.60938]\n",
            "  ...\n",
            "  [5.875 5.09375 -5.5625 ... 3.34375 3.04688 5.40625]\n",
            "  [-8.375 -2.20312 -9.4375 ... -12.6875 -9.125 -2.85938]\n",
            "  [-11.1875 0.355469 -4.71875 ... -10 -3.57812 -2.89062]]\n",
            "\n",
            " [[-6.65625 9.0625 -6.625 ... -2.6875 3.75 3.8125]\n",
            "  [-3.20312 7.53125 -5.28125 ... 2.625 3.40625 4.28125]\n",
            "  [-5.25 8.1875 -15.8125 ... -6.09375 4.75 2.60938]\n",
            "  ...\n",
            "  [5.875 5.09375 -5.5625 ... 3.34375 3.04688 5.40625]\n",
            "  [-8.375 -2.20312 -9.4375 ... -12.6875 -9.125 -2.85938]\n",
            "  [-11.1875 0.355469 -4.71875 ... -10 -3.57812 -2.89062]]]\n"
          ]
        }
      ],
      "source": [
        "import jax.numpy as jnp\n",
        "import os\n",
        "# I'm assuming you have these imports in your script.\n",
        "# If not, you'll need to add them.\n",
        "# from tunix.models.gemma import gemma as gemma_lib\n",
        "# from path.to import data_lib\n",
        "\n",
        "TEMP_BATCH_SIZE = 8\n",
        "\n",
        "# Assuming gemma is a pre-loaded instance of gemma_lib.Transformer\n",
        "# and data_lib is available.\n",
        "gemma_tokenizer = data_lib.GemmaTokenizer(\n",
        "    os.path.join(kaggle_ckpt_path, \"tokenizer.model\")\n",
        ")\n",
        "\n",
        "tokens = gemma_tokenizer.encode(\"Translate this into French:\\nHello, my name is Morgane.\\n\")\n",
        "# tokens.append(jnp.repeat(jnp.array([gemma_tokenizer.eos_id()]),2048-len(tokens),axis=0))  \n",
        "# tokens = [gemma_tokenizer.bos_id()]+gemma_tokenizer.encode(\"The color of the sky is blue but\")\n",
        "\n",
        "repeated_tokens = jnp.repeat(jnp.array(tokens)[None, :], TEMP_BATCH_SIZE, axis=0)\n",
        "positions = jnp.repeat(jnp.arange(0, len(tokens))[None, :], TEMP_BATCH_SIZE, axis=0)\n",
        "\n",
        "# --- FIX STARTS HERE ---\n",
        "\n",
        "# The Gemma model requires an attention mask. Passing `None` causes an error.\n",
        "# We need to create a causal attention mask for the prefill step.\n",
        "\n",
        "# 1. Create a boolean mask for the input tokens (True for valid tokens, False for padding).\n",
        "#    Assuming `gemma_tokenizer.pad_id()` exists. If not, and there's no padding,\n",
        "#    `jnp.ones_like(repeated_tokens, dtype=jnp.bool_)` would also work.\n",
        "#    A common pad_id is 0.\n",
        "pad_id = gemma_tokenizer.pad_id()\n",
        "input_mask = (repeated_tokens != pad_id)\n",
        "\n",
        "# 2. Create a causal attention mask from the input mask.\n",
        "#    This prevents the model from attending to future tokens.\n",
        "attention_mask = gemma_lib.make_causal_attn_mask(input_mask)\n",
        "\n",
        "# 3. Call the model with the correct attention mask.\n",
        "gemma_output_logits, _ = gemma(repeated_tokens, positions, cache=None, attention_mask=attention_mask)  # Test the model to ensure it works\n",
        "\n",
        "# --- FIX ENDS HERE ---\n",
        "\n",
        "# The commented out line below would also need a proper attention_mask.\n",
        "# For example:\n",
        "# dummy_tokens = jnp.ones((TEMP_BATCH_SIZE, 16), jnp.int32)\n",
        "# dummy_positions = jnp.repeat(jnp.arange(0,16)[None,:], TEMP_BATCH_SIZE, axis=0)\n",
        "# dummy_mask = gemma_lib.make_causal_attn_mask(jnp.ones_like(dummy_tokens, dtype=jnp.bool_))\n",
        "# gemma_output = gemma(dummy_tokens, dummy_positions, cache=None, attention_mask=dummy_mask)\n",
        "\n",
        "print(\"Successfully ran the model!\")\n",
        "print(\"Output shape:\", gemma_output_logits.shape)\n",
        "\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"positions: {positions}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TunixMaxTextLlama( # Param: 2,506,434,560 (5.0 GB), RngState: 4 (24 B), Total: 2,506,434,564 (5.0 GB)\n",
              "  base=TransformerNNX( # Param: 2,506,434,560 (5.0 GB), RngState: 4 (24 B), Total: 2,506,434,564 (5.0 GB)\n",
              "    config=<MaxText.pyconfig.HyperParameters object at 0x758fa28d97d0>,\n",
              "    decoder=ToNNX( # Param: 1,981,884,416 (4.0 GB), RngState: 4 (24 B), Total: 1,981,884,420 (4.0 GB)\n",
              "      decoder_norm={'scale': Param( # 2,048 (4.1 KB)\n",
              "        value=Array(shape=(2048,), dtype=dtype(bfloat16)),\n",
              "        mesh=None,\n",
              "        sharding=('norm',),\n",
              "        sharding_rules=None,\n",
              "        linen_meta_type=LogicallyPartitioned\n",
              "      )},\n",
              "      layers={'mlp': {'wi_0': {'kernel': Param( # 603,979,776 (1.2 GB)\n",
              "        value=Array(shape=(2048, 18, 16384), dtype=dtype(bfloat16)),\n",
              "        mesh=None,\n",
              "        sharding=('embed', 'layers', 'mlp'),\n",
              "        sharding_rules=None,\n",
              "        linen_meta_type=LogicallyPartitioned\n",
              "      )}, 'wi_1': {'kernel': Param( # 603,979,776 (1.2 GB)\n",
              "        value=Array(shape=(2048, 18, 16384), dtype=dtype(bfloat16)),\n",
              "        mesh=None,\n",
              "        sharding=('embed', 'layers', 'mlp'),\n",
              "        sharding_rules=None,\n",
              "        linen_meta_type=LogicallyPartitioned\n",
              "      )}, 'wo': {'kernel': Param( # 603,979,776 (1.2 GB)\n",
              "        value=Array(shape=(16384, 18, 2048), dtype=dtype(bfloat16)),\n",
              "        mesh=None,\n",
              "        sharding=('mlp', 'layers', 'embed'),\n",
              "        sharding_rules=None,\n",
              "        linen_meta_type=LogicallyPartitioned\n",
              "      )}}, 'pre_ffw_norm': {'scale': Param( # 36,864 (73.7 KB)\n",
              "        value=Array(shape=(2048, 18), dtype=dtype(bfloat16)),\n",
              "        mesh=None,\n",
              "        sharding=('norm', 'layers'),\n",
              "        sharding_rules=None,\n",
              "        linen_meta_type=LogicallyPartitioned\n",
              "      )}, 'pre_self_attention_norm': {'scale': Param( # 36,864 (73.7 KB)\n",
              "        value=Array(shape=(2048, 18), dtype=dtype(bfloat16)),\n",
              "        mesh=None,\n",
              "        sharding=('norm', 'layers'),\n",
              "        sharding_rules=None,\n",
              "        linen_meta_type=LogicallyPartitioned\n",
              "      )}, 'self_attention': {'key': {'kernel': Param( # 9,437,184 (18.9 MB)\n",
              "        value=Array(shape=(2048, 18, 1, 256), dtype=dtype(bfloat16)),\n",
              "        mesh=None,\n",
              "        sharding=('embed', 'layers', 'kv_heads', 'kv_head_dim'),\n",
              "        sharding_rules=None,\n",
              "        linen_meta_type=LogicallyPartitioned\n",
              "      )}, 'out': {'kernel': Param( # 75,497,472 (151.0 MB)\n",
              "        value=Array(shape=(8, 18, 256, 2048), dtype=dtype(bfloat16)),\n",
              "        mesh=None,\n",
              "        sharding=('heads', 'layers', 'kv', 'embed'),\n",
              "        sharding_rules=None,\n",
              "        linen_meta_type=LogicallyPartitioned\n",
              "      )}, 'query': {'kernel': Param( # 75,497,472 (151.0 MB)\n",
              "        value=Array(shape=(2048, 18, 8, 256), dtype=dtype(bfloat16)),\n",
              "        mesh=None,\n",
              "        sharding=('embed', 'layers', 'q_heads', 'kv'),\n",
              "        sharding_rules=None,\n",
              "        linen_meta_type=LogicallyPartitioned\n",
              "      )}, 'value': {'kernel': Param( # 9,437,184 (18.9 MB)\n",
              "        value=Array(shape=(2048, 18, 1, 256), dtype=dtype(bfloat16)),\n",
              "        mesh=None,\n",
              "        sharding=('embed', 'layers', 'kv_heads', 'kv_head_dim'),\n",
              "        sharding_rules=None,\n",
              "        linen_meta_type=LogicallyPartitioned\n",
              "      )}}},\n",
              "      to_nnx__module=Decoder(\n",
              "          # attributes\n",
              "          config = <MaxText.pyconfig.HyperParameters object at 0x758fa28d97d0>\n",
              "          mesh = Mesh(device_ids=array([[[[[[[[[[[[0]]]]]]]]],\n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "                   [[[[[[[[[1]]]]]]]]],\n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "                   [[[[[[[[[2]]]]]]]]],\n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "                   [[[[[[[[[3]]]]]]]]],\n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "                   [[[[[[[[[7]]]]]]]]],\n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "                   [[[[[[[[[6]]]]]]]]],\n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "                   [[[[[[[[[5]]]]]]]]],\n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "          \n",
              "                   [[[[[[[[[4]]]]]]]]]]]]), axis_names=('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'), axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto))\n",
              "          quant = None\n",
              "      ),\n",
              "      to_nnx__rngs=Rngs( # RngState: 4 (24 B)\n",
              "        dropout=RngStream( # RngState: 2 (12 B)\n",
              "          count=RngCount( # 1 (4 B)\n",
              "            value=Array(2, dtype=uint32),\n",
              "            tag='dropout'\n",
              "          ),\n",
              "          key=RngKey( # 1 (8 B)\n",
              "            value=Array((), dtype=key<fry>) overlaying:\n",
              "            [ 507451445 1853169794],\n",
              "            tag='dropout'\n",
              "          ),\n",
              "          tag='dropout'\n",
              "        ),\n",
              "        params=RngStream( # RngState: 2 (12 B)\n",
              "          count=RngCount( # 1 (4 B)\n",
              "            value=Array(2, dtype=uint32),\n",
              "            tag='params'\n",
              "          ),\n",
              "          key=RngKey( # 1 (8 B)\n",
              "            value=Array((), dtype=key<fry>) overlaying:\n",
              "            [ 928981903 3453687069],\n",
              "            tag='params'\n",
              "          ),\n",
              "          tag='params'\n",
              "        )\n",
              "      )\n",
              "    ),\n",
              "    mesh=Mesh(device_ids=array([[[[[[[[[[[[0]]]]]]]]],\n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "             [[[[[[[[[1]]]]]]]]],\n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "             [[[[[[[[[2]]]]]]]]],\n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "             [[[[[[[[[3]]]]]]]]],\n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "             [[[[[[[[[7]]]]]]]]],\n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "             [[[[[[[[[6]]]]]]]]],\n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "             [[[[[[[[[5]]]]]]]]],\n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "    \n",
              "             [[[[[[[[[4]]]]]]]]]]]]), axis_names=('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'), axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)),\n",
              "    quant=None,\n",
              "    token_embedder=Embed( # Param: 524,550,144 (1.0 GB)\n",
              "      attend_dtype=dtype(bfloat16),\n",
              "      cast_input_dtype=None,\n",
              "      config=<MaxText.pyconfig.HyperParameters object at 0x758fa28d97d0>,\n",
              "      dtype=dtype(bfloat16),\n",
              "      embedding=Param( # 524,550,144 (1.0 GB)\n",
              "        value=Array(shape=(256128, 2048), dtype=dtype(bfloat16)),\n",
              "        sharding=('vocab', 'embed')\n",
              "      ),\n",
              "      num_embeddings=256128,\n",
              "      num_features=2048\n",
              "    ),\n",
              "    vision_encoder=None\n",
              "  ),\n",
              "  use_attention_mask=False\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gemma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8, 15, 256128)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Array([[ 49688,    736,   1280,   6987, 235292,    108,   4521, 235269,\n",
              "           970,   1503,    603,  20189, 235249, 235265,    108],\n",
              "       [ 49688,    736,   1280,   6987, 235292,    108,   4521, 235269,\n",
              "           970,   1503,    603,  20189, 235249, 235265,    108],\n",
              "       [ 49688,    736,   1280,   6987, 235292,    108,   4521, 235269,\n",
              "           970,   1503,    603,  20189, 235249, 235265,    108],\n",
              "       [ 49688,    736,   1280,   6987, 235292,    108,   4521, 235269,\n",
              "           970,   1503,    603,  20189, 235249, 235265,    108],\n",
              "       [ 49688,    736,   1280,   6987, 235292,    108,   4521, 235269,\n",
              "           970,   1503,    603,  20189, 235249, 235265,    108],\n",
              "       [ 49688,    736,   1280,   6987, 235292,    108,   4521, 235269,\n",
              "           970,   1503,    603,  20189, 235249, 235265,    108],\n",
              "       [ 49688,    736,   1280,   6987, 235292,    108,   4521, 235269,\n",
              "           970,   1503,    603,  20189, 235249, 235265,    108],\n",
              "       [ 49688,    736,   1280,   6987, 235292,    108,   4521, 235269,\n",
              "           970,   1503,    603,  20189, 235249, 235265,    108]],      dtype=int32)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(gemma_output_logits.shape)\n",
        "jnp.argmax(gemma_output_logits, axis=2)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Predicted next token ID: 108\n",
            "Predicted next token: '\n",
            "'\n"
          ]
        }
      ],
      "source": [
        "last_token_logits = gemma_output_logits[:, -1, :]\n",
        "predicted_token_id = jnp.argmax(last_token_logits, axis=-1)\n",
        "# Decode the token ID to see the predicted word.\n",
        "# Since TEMP_BATCH_SIZE is 1, we can just grab the first element.\n",
        "next_token_id = predicted_token_id[0]\n",
        "predicted_token_text = gemma_tokenizer.decode([int(next_token_id)])\n",
        "\n",
        "print(f\"\\nPredicted next token ID: {next_token_id}\")\n",
        "print(f\"Predicted next token: '{predicted_token_text}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array(0.85546875, dtype=float32)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gemma_output_logits[0][0,7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFKUZT10T3Uv"
      },
      "outputs": [],
      "source": [
        "# def get_base_model(ckpt_path):\n",
        "\n",
        "#   model_config = gemma_lib.TransformerConfig.gemma_2b()\n",
        "#   mesh = jax.make_mesh(*MESH)\n",
        "#   abs_gemma: nnx.Module = nnx.eval_shape(\n",
        "#       lambda: gemma_lib.Transformer(model_config, rngs=nnx.Rngs(params=0))\n",
        "#   )\n",
        "#   abs_state = nnx.state(abs_gemma)\n",
        "#   abs_state = jax.tree.map(\n",
        "#       lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n",
        "#       abs_state,\n",
        "#       nnx.get_named_sharding(abs_state, mesh),\n",
        "#   )\n",
        "#   checkpointer = ocp.StandardCheckpointer()\n",
        "#   restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n",
        "\n",
        "#   graph_def, _ = nnx.split(abs_gemma)\n",
        "#   gemma = nnx.merge(graph_def, restored_params)\n",
        "#   return gemma, mesh, model_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2iDYbnsT3Uv"
      },
      "source": [
        "## Prompt the model\n",
        "\n",
        "Let's see how the model performs on the English-French translation task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH82cHpAT3Uv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs=[[[1.15625 -0.353516 1.4375 ... -1.57812 0.0245361 -0.644531]\n",
            "  [1.15625 -0.353516 1.4375 ... -1.57812 0.0245361 -0.644531]\n",
            "  [1.15625 -0.353516 1.4375 ... -1.57812 0.0245361 -0.644531]\n",
            "  ...\n",
            "  [0.539062 -0.333984 0.123047 ... -0.439453 -0.796875 2.51562]\n",
            "  [-1.03125 -0.621094 -0.292969 ... -0.271484 0.8125 0.785156]\n",
            "  [-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.550781 1.32812 0.304688 ... -0.691406 0.921875 -1.73438]\n",
            "  [0.0245361 1.15625 0.515625 ... 0.472656 1.04688 1.625]\n",
            "  [-1 1.24219 -1.83594 ... 1.19531 -0.824219 -0.132812]\n",
            "  ...\n",
            "  [0.472656 0.261719 -0.0146484 ... -0.171875 2.01562 0.65625]\n",
            "  [0.5625 0.632812 -0.292969 ... 2.51562 -0.333984 0.123047]\n",
            "  [-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[1.15625 -0.353516 1.4375 ... -1.57812 0.0245361 -0.644531]\n",
            "  [1.15625 -0.353516 1.4375 ... -1.57812 0.0245361 -0.644531]\n",
            "  [1.15625 -0.353516 1.4375 ... -1.57812 0.0245361 -0.644531]\n",
            "  ...\n",
            "  [-0.0344238 0.103027 0.494141 ... -1.65625 -0.152344 -0.112305]\n",
            "  [-1.25781 0.201172 0.123047 ... -0.644531 -0.271484 -0.333984]\n",
            "  [-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[1.15625 -0.353516 1.4375 ... -1.57812 0.0245361 -0.644531]\n",
            "  [1.15625 -0.353516 1.4375 ... -1.57812 0.0245361 -0.644531]\n",
            "  [1.15625 -0.353516 1.4375 ... -1.57812 0.0245361 -0.644531]\n",
            "  ...\n",
            "  [1.38281 -1.65625 -0.0737305 ... -1.83594 0.632812 -0.152344]\n",
            "  [0.5625 0.632812 -0.292969 ... 2.51562 -0.333984 0.123047]\n",
            "  [-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[1.15625 -0.353516 1.4375 ... -1.57812 0.0245361 -0.644531]\n",
            "  [-0.550781 1.32812 0.304688 ... -0.691406 0.921875 -1.73438]\n",
            "  [0.0245361 1.15625 0.515625 ... 0.472656 1.04688 1.625]\n",
            "  ...\n",
            "  [-0.597656 0.785156 -1.57812 ... 0.515625 0.386719 2.20312]\n",
            "  [0.5625 -0.769531 0.427734 ... 0.449219 2.51562 0.261719]\n",
            "  [-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]]\n",
            "lnx=[[[1.17969 -0.359375 1.46875 ... -1.60938 0.0250244 -0.65625]\n",
            "  [1.17969 -0.359375 1.46875 ... -1.60938 0.0250244 -0.65625]\n",
            "  [1.17969 -0.359375 1.46875 ... -1.60938 0.0250244 -0.65625]\n",
            "  ...\n",
            "  [0.535156 -0.332031 0.122559 ... -0.4375 -0.792969 2.5]\n",
            "  [-1.04688 -0.632812 -0.296875 ... -0.275391 0.824219 0.796875]\n",
            "  [-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.546875 1.32031 0.302734 ... -0.6875 0.914062 -1.71875]\n",
            "  [0.024292 1.14844 0.511719 ... 0.46875 1.03906 1.60938]\n",
            "  [-1.01562 1.25781 -1.85938 ... 1.21094 -0.835938 -0.134766]\n",
            "  ...\n",
            "  [0.474609 0.263672 -0.0147095 ... -0.172852 2.03125 0.660156]\n",
            "  [0.5625 0.632812 -0.292969 ... 2.51562 -0.333984 0.123047]\n",
            "  [-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[1.17969 -0.359375 1.46875 ... -1.60938 0.0250244 -0.65625]\n",
            "  [1.17969 -0.359375 1.46875 ... -1.60938 0.0250244 -0.65625]\n",
            "  [1.17969 -0.359375 1.46875 ... -1.60938 0.0250244 -0.65625]\n",
            "  ...\n",
            "  [-0.0356445 0.106445 0.511719 ... -1.71094 -0.157227 -0.115723]\n",
            "  [-1.24219 0.199219 0.121582 ... -0.636719 -0.267578 -0.330078]\n",
            "  [-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[1.17969 -0.359375 1.46875 ... -1.60938 0.0250244 -0.65625]\n",
            "  [1.17969 -0.359375 1.46875 ... -1.60938 0.0250244 -0.65625]\n",
            "  [1.17969 -0.359375 1.46875 ... -1.60938 0.0250244 -0.65625]\n",
            "  ...\n",
            "  [1.36719 -1.64062 -0.0727539 ... -1.8125 0.625 -0.150391]\n",
            "  [0.5625 0.632812 -0.292969 ... 2.51562 -0.333984 0.123047]\n",
            "  [-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[1.17969 -0.359375 1.46875 ... -1.60938 0.0250244 -0.65625]\n",
            "  [-0.546875 1.32031 0.302734 ... -0.6875 0.914062 -1.71875]\n",
            "  [0.024292 1.14844 0.511719 ... 0.46875 1.03906 1.60938]\n",
            "  ...\n",
            "  [-0.585938 0.773438 -1.55469 ... 0.507812 0.380859 2.17188]\n",
            "  [0.550781 -0.753906 0.417969 ... 0.439453 2.45312 0.255859]\n",
            "  [-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]]\n",
            "attention_lnx=[[[-0.464844 0.351562 -0.326172 ... 0.375 -0.380859 -0.320312]\n",
            "  [-0.464844 0.351562 -0.326172 ... 0.375 -0.380859 -0.320312]\n",
            "  [-0.466797 0.355469 -0.326172 ... 0.373047 -0.378906 -0.322266]\n",
            "  ...\n",
            "  [-0.330078 0.179688 0.0581055 ... -0.476562 -0.112793 -0.134766]\n",
            "  [0.333984 0.400391 -1.03125 ... -0.257812 -0.230469 -0.205078]\n",
            "  [-0.21582 -0.0344238 0.359375 ... -0.0800781 -0.271484 -0.0559082]]\n",
            "\n",
            " [[-0.124023 1.59375 0.886719 ... -0.15332 -0.251953 0.0473633]\n",
            "  [-0.00860596 1.33594 0.369141 ... -0.691406 -0.462891 -0.566406]\n",
            "  [-0.667969 0.0756836 1.5 ... -0.679688 -1.58594 -0.953125]\n",
            "  ...\n",
            "  [-0.0128174 0.112793 -0.0324707 ... -0.378906 0.371094 -0.458984]\n",
            "  [0.120117 0.0471191 -0.625 ... -0.283203 -0.0639648 -0.460938]\n",
            "  [0.0149536 0.21875 -0.425781 ... -0.224609 -0.123535 -0.427734]]\n",
            "\n",
            " [[-0.464844 0.351562 -0.326172 ... 0.375 -0.380859 -0.320312]\n",
            "  [-0.464844 0.351562 -0.326172 ... 0.375 -0.380859 -0.320312]\n",
            "  [-0.466797 0.355469 -0.326172 ... 0.373047 -0.378906 -0.322266]\n",
            "  ...\n",
            "  [-0.65625 -0.361328 -0.470703 ... -0.474609 -0.683594 -0.081543]\n",
            "  [-0.388672 -0.263672 -0.308594 ... -0.243164 -0.404297 -0.511719]\n",
            "  [-0.0541992 -0.910156 0.108887 ... 0.0732422 -0.601562 -0.585938]]\n",
            "\n",
            " [[-0.464844 0.351562 -0.326172 ... 0.375 -0.380859 -0.320312]\n",
            "  [-0.464844 0.351562 -0.326172 ... 0.375 -0.380859 -0.320312]\n",
            "  [-0.466797 0.355469 -0.326172 ... 0.373047 -0.378906 -0.322266]\n",
            "  ...\n",
            "  [-0.839844 -0.0233154 -0.106445 ... -0.185547 -0.234375 -0.449219]\n",
            "  [-0.371094 0.0415039 -0.078125 ... -0.180664 -0.617188 -0.652344]\n",
            "  [-0.199219 -0.11377 0.390625 ... -0.0830078 -0.601562 -0.507812]]\n",
            "\n",
            " [[-0.464844 0.351562 -0.326172 ... 0.375 -0.380859 -0.320312]\n",
            "  [-0.605469 1.14062 0.194336 ... 0.628906 -0.251953 0.367188]\n",
            "  [-0.132812 1.10156 0.0383301 ... -0.636719 -0.447266 -0.730469]\n",
            "  ...\n",
            "  [-0.365234 -0.213867 0.597656 ... -0.359375 -0.511719 -0.384766]\n",
            "  [-0.198242 -0.141602 0.112793 ... -0.15625 -0.554688 -0.707031]\n",
            "  [0.000379562 -0.640625 0.832031 ... -0.492188 -0.625 -0.566406]]]\n",
            "attn_output=[[[0.494141 -0.00139618 0.792969 ... -0.859375 -0.253906 -0.6875]\n",
            "  [0.494141 -0.00139618 0.792969 ... -0.859375 -0.253906 -0.6875]\n",
            "  [0.492188 0.00139618 0.792969 ... -0.859375 -0.251953 -0.691406]\n",
            "  ...\n",
            "  [0.175781 -0.129883 0.152344 ... -0.769531 -0.765625 2]\n",
            "  [-0.628906 -0.198242 -1.19531 ... -0.476562 0.523438 0.523438]\n",
            "  [-0.304688 -0.90625 0.0581055 ... -0.239258 0.652344 1.07031]]\n",
            "\n",
            " [[-0.472656 2.04688 0.832031 ... -0.589844 0.46875 -1.17969]\n",
            "  [0.0129395 2.01562 0.71875 ... -0.177734 0.474609 0.859375]\n",
            "  [-1.35156 1.07031 -0.273438 ... 0.417969 -1.95312 -0.882812]\n",
            "  ...\n",
            "  [0.419922 0.341797 -0.0432129 ... -0.503906 2.1875 0.180664]\n",
            "  [0.648438 0.644531 -0.871094 ... 2.10938 -0.376953 -0.320312]\n",
            "  [-0.111328 -0.738281 -0.679688 ... -0.392578 0.839844 0.804688]]\n",
            "\n",
            " [[0.494141 -0.00139618 0.792969 ... -0.859375 -0.253906 -0.6875]\n",
            "  [0.494141 -0.00139618 0.792969 ... -0.859375 -0.253906 -0.6875]\n",
            "  [0.492188 0.00139618 0.792969 ... -0.859375 -0.251953 -0.691406]\n",
            "  ...\n",
            "  [-0.65625 -0.245117 0.0222168 ... -2.01562 -0.792969 -0.183594]\n",
            "  [-1.52344 -0.0578613 -0.171875 ... -0.824219 -0.625 -0.785156]\n",
            "  [-0.169922 -1.73438 -0.166992 ... -0.10791 0.375 0.632812]]\n",
            "\n",
            " [[0.494141 -0.00139618 0.792969 ... -0.859375 -0.253906 -0.6875]\n",
            "  [0.494141 -0.00139618 0.792969 ... -0.859375 -0.253906 -0.6875]\n",
            "  [0.492188 0.00139618 0.792969 ... -0.859375 -0.251953 -0.691406]\n",
            "  ...\n",
            "  [0.5 -1.54688 -0.166016 ... -1.85938 0.367188 -0.554688]\n",
            "  [0.178711 0.628906 -0.345703 ... 2.1875 -0.886719 -0.494141]\n",
            "  [-0.304688 -1.02344 0.0898438 ... -0.251953 0.380859 0.710938]]\n",
            "\n",
            " [[0.494141 -0.00139618 0.792969 ... -0.859375 -0.253906 -0.6875]\n",
            "  [-0.886719 1.89062 0.382812 ... -0.0478516 0.515625 -1.04688]\n",
            "  [-0.090332 1.88281 0.462891 ... -0.136719 0.5 0.746094]\n",
            "  ...\n",
            "  [-0.890625 0.527344 -0.90625 ... 0.144531 -0.115234 1.67969]\n",
            "  [0.337891 -0.84375 0.5 ... 0.271484 1.8125 -0.412109]\n",
            "  [-0.124512 -1.54688 0.507812 ... -0.644531 0.367188 0.671875]]]\n",
            "next_layer_addition_dropped_out=[[[1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.304688 0.867188 ... -1.51562 -1.29688 -1.49219]\n",
            "  ...\n",
            "  [0.0576172 -0.664062 0.78125 ... -0.0195312 -0.625 2.03125]\n",
            "  [-0.792969 -0.0654297 -2.01562 ... -0.496094 0.644531 -0.179688]\n",
            "  [-1.42188 -0.957031 0.582031 ... -0.347656 0.96875 0.382812]]\n",
            "\n",
            " [[-1.01562 3.1875 0.980469 ... -0.164062 -0.03125 -1.77344]\n",
            "  [0.255859 2.1875 1.71875 ... -0.441406 0.726562 1.96875]\n",
            "  [-2.875 1.48438 -0.0546875 ... 1.17969 -2.84375 -0.4375]\n",
            "  ...\n",
            "  [1.00781 0.0859375 0.423828 ... 1.6875 1.79688 0.00390625]\n",
            "  [-0.785156 -0.015625 -0.785156 ... 2.3125 0.1875 0.695312]\n",
            "  [-0.957031 -0.380859 -0.0898438 ... 0.714844 1.35938 -0.226562]]\n",
            "\n",
            " [[1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.304688 0.867188 ... -1.51562 -1.29688 -1.49219]\n",
            "  ...\n",
            "  [0.125 -0.100586 -0.369141 ... -1.45312 -1.52344 -0.355469]\n",
            "  [-1.46094 -0.898438 -1.04688 ... -0.203125 -1.28125 0.304688]\n",
            "  [-1.375 -1.85938 -0.0527344 ... 0.277344 0.0488281 -0.414062]]\n",
            "\n",
            " [[1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.304688 0.867188 ... -1.51562 -1.29688 -1.49219]\n",
            "  ...\n",
            "  [0.242188 -0.84375 -0.859375 ... -1.53125 0.365234 -0.808594]\n",
            "  [-0.675781 0.253906 -0.198242 ... 2.53125 -0.640625 0.140625]\n",
            "  [-1.64062 -0.96875 0.644531 ... 0.0917969 0.353516 -0.183594]]\n",
            "\n",
            " [[1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [-1.3125 2.4375 0.167969 ... -0.132812 0.316406 -1.03906]\n",
            "  [-0.453125 2.14062 1.24219 ... -0.609375 0.992188 0.671875]\n",
            "  ...\n",
            "  [-0.558594 -0.429688 -0.664062 ... 0.449219 0.410156 3.09375]\n",
            "  [-0.429688 -1.48438 0.730469 ... 0.161133 2.9375 -0.457031]\n",
            "  [-1.32812 -1.01562 1.21875 ... -0.419922 0.410156 -0.402344]]]\n",
            "inputs=[[[1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.304688 0.867188 ... -1.51562 -1.29688 -1.49219]\n",
            "  ...\n",
            "  [0.0576172 -0.664062 0.78125 ... -0.0195312 -0.625 2.03125]\n",
            "  [-0.792969 -0.0654297 -2.01562 ... -0.496094 0.644531 -0.179688]\n",
            "  [-1.42188 -0.957031 0.582031 ... -0.347656 0.96875 0.382812]]\n",
            "\n",
            " [[-1.01562 3.1875 0.980469 ... -0.164062 -0.03125 -1.77344]\n",
            "  [0.255859 2.1875 1.71875 ... -0.441406 0.726562 1.96875]\n",
            "  [-2.875 1.48438 -0.0546875 ... 1.17969 -2.84375 -0.4375]\n",
            "  ...\n",
            "  [1.00781 0.0859375 0.423828 ... 1.6875 1.79688 0.00390625]\n",
            "  [-0.785156 -0.015625 -0.785156 ... 2.3125 0.1875 0.695312]\n",
            "  [-0.957031 -0.380859 -0.0898438 ... 0.714844 1.35938 -0.226562]]\n",
            "\n",
            " [[1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.304688 0.867188 ... -1.51562 -1.29688 -1.49219]\n",
            "  ...\n",
            "  [0.125 -0.100586 -0.369141 ... -1.45312 -1.52344 -0.355469]\n",
            "  [-1.46094 -0.898438 -1.04688 ... -0.203125 -1.28125 0.304688]\n",
            "  [-1.375 -1.85938 -0.0527344 ... 0.277344 0.0488281 -0.414062]]\n",
            "\n",
            " [[1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.304688 0.867188 ... -1.51562 -1.29688 -1.49219]\n",
            "  ...\n",
            "  [0.242188 -0.84375 -0.859375 ... -1.53125 0.365234 -0.808594]\n",
            "  [-0.675781 0.253906 -0.198242 ... 2.53125 -0.640625 0.140625]\n",
            "  [-1.64062 -0.96875 0.644531 ... 0.0917969 0.353516 -0.183594]]\n",
            "\n",
            " [[1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [-1.3125 2.4375 0.167969 ... -0.132812 0.316406 -1.03906]\n",
            "  [-0.453125 2.14062 1.24219 ... -0.609375 0.992188 0.671875]\n",
            "  ...\n",
            "  [-0.558594 -0.429688 -0.664062 ... 0.449219 0.410156 3.09375]\n",
            "  [-0.429688 -1.48438 0.730469 ... 0.161133 2.9375 -0.457031]\n",
            "  [-1.32812 -1.01562 1.21875 ... -0.419922 0.410156 -0.402344]]]\n",
            "lnx=[[[0.855469 -0.196289 0.558594 ... -0.964844 -0.828125 -0.949219]\n",
            "  [0.855469 -0.196289 0.558594 ... -0.964844 -0.828125 -0.949219]\n",
            "  [0.855469 -0.194336 0.550781 ... -0.964844 -0.828125 -0.949219]\n",
            "  ...\n",
            "  [0.0424805 -0.490234 0.578125 ... -0.0144043 -0.460938 1.5]\n",
            "  [-0.625 -0.0517578 -1.59375 ... -0.392578 0.507812 -0.141602]\n",
            "  [-1.07031 -0.722656 0.439453 ... -0.261719 0.730469 0.289062]]\n",
            "\n",
            " [[-0.640625 2.01562 0.621094 ... -0.104004 -0.0197754 -1.125]\n",
            "  [0.18457 1.57812 1.24219 ... -0.318359 0.523438 1.42188]\n",
            "  [-2.09375 1.07812 -0.0397949 ... 0.859375 -2.0625 -0.318359]\n",
            "  ...\n",
            "  [0.792969 0.0678711 0.333984 ... 1.32812 1.41406 0.00308228]\n",
            "  [-0.621094 -0.0123901 -0.621094 ... 1.83594 0.148438 0.550781]\n",
            "  [-0.773438 -0.306641 -0.0722656 ... 0.578125 1.09375 -0.182617]]\n",
            "\n",
            " [[0.855469 -0.196289 0.558594 ... -0.964844 -0.828125 -0.949219]\n",
            "  [0.855469 -0.196289 0.558594 ... -0.964844 -0.828125 -0.949219]\n",
            "  [0.855469 -0.194336 0.550781 ... -0.964844 -0.828125 -0.949219]\n",
            "  ...\n",
            "  [0.101562 -0.081543 -0.298828 ... -1.17969 -1.23438 -0.289062]\n",
            "  [-1.14062 -0.703125 -0.820312 ... -0.15918 -1 0.238281]\n",
            "  [-1.07031 -1.44531 -0.0410156 ... 0.21582 0.0378418 -0.322266]]\n",
            "\n",
            " [[0.855469 -0.196289 0.558594 ... -0.964844 -0.828125 -0.949219]\n",
            "  [0.855469 -0.196289 0.558594 ... -0.964844 -0.828125 -0.949219]\n",
            "  [0.855469 -0.194336 0.550781 ... -0.964844 -0.828125 -0.949219]\n",
            "  ...\n",
            "  [0.189453 -0.660156 -0.671875 ... -1.19531 0.285156 -0.632812]\n",
            "  [-0.53125 0.200195 -0.15625 ... 1.99219 -0.503906 0.11084]\n",
            "  [-1.28125 -0.753906 0.503906 ... 0.0717773 0.275391 -0.143555]]\n",
            "\n",
            " [[0.855469 -0.196289 0.558594 ... -0.964844 -0.828125 -0.949219]\n",
            "  [-0.898438 1.66406 0.114746 ... -0.0908203 0.216797 -0.710938]\n",
            "  [-0.333984 1.57812 0.914062 ... -0.449219 0.730469 0.494141]\n",
            "  ...\n",
            "  [-0.457031 -0.351562 -0.542969 ... 0.367188 0.335938 2.53125]\n",
            "  [-0.34375 -1.1875 0.582031 ... 0.128906 2.34375 -0.365234]\n",
            "  [-1.07031 -0.816406 0.980469 ... -0.337891 0.330078 -0.322266]]]\n",
            "attention_lnx=[[[2.20312 0.679688 0.71875 ... 0.492188 -0.3125 -0.628906]\n",
            "  [2.20312 0.679688 0.71875 ... 0.492188 -0.3125 -0.628906]\n",
            "  [2.1875 0.679688 0.71875 ... 0.486328 -0.314453 -0.625]\n",
            "  ...\n",
            "  [1.50781 1.10156 0.925781 ... 0.149414 0.0471191 -0.0439453]\n",
            "  [1.14062 0.558594 0.429688 ... -0.298828 0.578125 -0.412109]\n",
            "  [1.63281 1.09375 0.910156 ... 0.0234375 -0.0410156 -0.0393066]]\n",
            "\n",
            " [[0.9375 1.53125 1.10156 ... -0.380859 -1.25781 2.04688]\n",
            "  [0.410156 1.40625 1.55469 ... -0.269531 -0.859375 1.69531]\n",
            "  [1.05469 1.38281 1.16406 ... -0.239258 -0.648438 0.882812]\n",
            "  ...\n",
            "  [0.691406 0.5625 0.8125 ... -0.730469 0.398438 0.164062]\n",
            "  [0.503906 0.601562 0.832031 ... -0.878906 0.188477 0.130859]\n",
            "  [0.0541992 0.523438 0.644531 ... -0.302734 -0.245117 -0.074707]]\n",
            "\n",
            " [[2.20312 0.679688 0.71875 ... 0.492188 -0.3125 -0.628906]\n",
            "  [2.20312 0.679688 0.71875 ... 0.492188 -0.3125 -0.628906]\n",
            "  [2.1875 0.679688 0.71875 ... 0.486328 -0.314453 -0.625]\n",
            "  ...\n",
            "  [0.941406 0.859375 1.09375 ... 0.0461426 0.235352 0.0588379]\n",
            "  [0.984375 0.824219 0.925781 ... -0.170898 0.0220947 -0.251953]\n",
            "  [1.07031 0.761719 0.447266 ... -0.28125 -0.46875 0.158203]]\n",
            "\n",
            " [[2.20312 0.679688 0.71875 ... 0.492188 -0.3125 -0.628906]\n",
            "  [2.20312 0.679688 0.71875 ... 0.492188 -0.3125 -0.628906]\n",
            "  [2.1875 0.679688 0.71875 ... 0.486328 -0.314453 -0.625]\n",
            "  ...\n",
            "  [1.00781 1.04688 0.742188 ... -0.363281 -0.460938 -0.878906]\n",
            "  [0.46875 0.404297 1.16406 ... -0.546875 0.10498 0.0883789]\n",
            "  [1.17969 0.78125 0.910156 ... -0.09375 0.155273 0.0192871]]\n",
            "\n",
            " [[2.20312 0.679688 0.71875 ... 0.492188 -0.3125 -0.628906]\n",
            "  [1.16406 0.707031 1.34375 ... 0.466797 -0.427734 0.671875]\n",
            "  [0.875 1.1875 0.667969 ... 0.574219 -0.761719 0.59375]\n",
            "  ...\n",
            "  [0.130859 0.816406 0.695312 ... -0.376953 -0.114258 -0.419922]\n",
            "  [0.691406 1.25781 0.337891 ... -0.193359 -0.134766 -0.104004]\n",
            "  [0.695312 0.871094 0.283203 ... 0.0174561 0.0620117 0.294922]]]\n",
            "attn_output=[[[1.89062 0.198242 0.847656 ... -0.546875 -0.859375 -1.13281]\n",
            "  [1.89062 0.198242 0.847656 ... -0.546875 -0.859375 -1.13281]\n",
            "  [1.88281 0.200195 0.84375 ... -0.546875 -0.859375 -1.125]\n",
            "  ...\n",
            "  [1.02344 0.285156 1.10938 ... 0.0844727 -0.376953 1.29688]\n",
            "  [0.237305 0.335938 -1.07812 ... -0.542969 0.832031 -0.404297]\n",
            "  [0.143555 0.0927734 1.01562 ... -0.220703 0.628906 0.233398]]\n",
            "\n",
            " [[-0.0427246 2.57812 1.14062 ... -0.298828 -0.707031 0.149414]\n",
            "  [0.412109 2.21875 2.03125 ... -0.439453 -0.0820312 2.26562]\n",
            "  [-1.17188 1.84375 0.714844 ... 0.605469 -2.25 0.287109]\n",
            "  ...\n",
            "  [1.28125 0.488281 0.933594 ... 0.722656 1.65625 0.126953]\n",
            "  [-0.209961 0.4375 0.0349121 ... 1.07031 0.28125 0.617188]\n",
            "  [-0.695312 0.109375 0.425781 ... 0.316406 0.855469 -0.231445]]\n",
            "\n",
            " [[1.89062 0.198242 0.847656 ... -0.546875 -0.859375 -1.13281]\n",
            "  [1.89062 0.198242 0.847656 ... -0.546875 -0.859375 -1.13281]\n",
            "  [1.88281 0.200195 0.84375 ... -0.546875 -0.859375 -1.125]\n",
            "  ...\n",
            "  [0.796875 0.566406 0.542969 ... -1.05469 -0.964844 -0.22168]\n",
            "  [-0.345703 -0.0537109 -0.0874023 ... -0.271484 -0.910156 0.0380859]\n",
            "  [-0.219727 -0.792969 0.285156 ... -0.00282288 -0.302734 -0.18457]]\n",
            "\n",
            " [[1.89062 0.198242 0.847656 ... -0.546875 -0.859375 -1.13281]\n",
            "  [1.89062 0.198242 0.847656 ... -0.546875 -0.859375 -1.13281]\n",
            "  [1.88281 0.200195 0.84375 ... -0.546875 -0.859375 -1.125]\n",
            "  ...\n",
            "  [0.90625 0.147461 -0.0849609 ... -1.375 -0.0693359 -1.21875]\n",
            "  [-0.151367 0.482422 0.707031 ... 1.45312 -0.392578 0.167969]\n",
            "  [-0.339844 -0.137695 1.14844 ... -0.00144196 0.375 -0.121094]]\n",
            "\n",
            " [[1.89062 0.198242 0.847656 ... -0.546875 -0.859375 -1.13281]\n",
            "  [-0.0874023 1.85938 0.890625 ... 0.197266 -0.065918 -0.216797]\n",
            "  [0.267578 2.125 1.21875 ... -0.0223389 0.146484 0.804688]\n",
            "  ...\n",
            "  [-0.330078 0.298828 0.0241699 ... 0.0559082 0.228516 2.0625]\n",
            "  [0.198242 -0.171875 0.808594 ... -0.0244141 2.125 -0.425781]\n",
            "  [-0.482422 -0.109863 1.14062 ... -0.306641 0.359375 -0.0820312]]]\n",
            "next_layer_addition_dropped_out=[[[2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.5625 0.722656 1.5625 ... -1.15625 -1.52344 -2.20312]\n",
            "  ...\n",
            "  [1.47656 0.84375 1.17188 ... -0.00292969 -0.21875 2.53125]\n",
            "  [0.462891 0.152344 -1.875 ... -0.433594 0.488281 -0.539062]\n",
            "  [-0.523438 -0.605469 1.375 ... -1.15625 0.976562 -0.21875]]\n",
            "\n",
            " [[0.135742 5.125 2.48438 ... -0.515625 -1.74219 0.119141]\n",
            "  [1.40625 2.75 2.3125 ... -0.992188 0.173828 3.78125]\n",
            "  [-0.765625 2.07812 1.02344 ... 1.48438 -2.5 0.796875]\n",
            "  ...\n",
            "  [2.0625 0.345703 1.14062 ... 1.10156 2 -0.496094]\n",
            "  [-0.253906 0.398438 0.0957031 ... 1.53906 0.578125 0.964844]\n",
            "  [-0.875 -0.148438 0.503906 ... -0.859375 1.92188 -1.34375]]\n",
            "\n",
            " [[2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.5625 0.722656 1.5625 ... -1.15625 -1.52344 -2.20312]\n",
            "  ...\n",
            "  [1.375 1.5 0.730469 ... -2.10938 -2.15625 -0.816406]\n",
            "  [-1.34375 0.320312 -0.808594 ... -1.17969 -1.75 0.065918]\n",
            "  [-0.996094 -1.17969 -0.0527344 ... -0.722656 0.123047 -0.6875]]\n",
            "\n",
            " [[2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.5625 0.722656 1.5625 ... -1.15625 -1.52344 -2.20312]\n",
            "  ...\n",
            "  [0.679688 0.792969 -0.523438 ... -2.42188 -0.382812 -1.77344]\n",
            "  [-0.235352 0.332031 0.851562 ... 2.20312 -0.371094 -0.416016]\n",
            "  [-1.17188 -0.648438 1.39844 ... -1.17969 1.08594 -1.09375]]\n",
            "\n",
            " [[2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [0.617188 3.14062 1.34375 ... 0.40625 -0.84375 -0.820312]\n",
            "  [0.789062 2.26562 1.60156 ... -0.507812 -0.0234375 1.95312]\n",
            "  ...\n",
            "  [-0.328125 1.73438 0.365234 ... -0.632812 0.78125 3.23438]\n",
            "  [0.269531 -0.219727 1.1875 ... -0.71875 2.875 -0.890625]\n",
            "  [-1.5 -0.0620117 0.304688 ... -1.15625 1.33594 -1.19531]]]\n",
            "inputs=[[[2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.5625 0.722656 1.5625 ... -1.15625 -1.52344 -2.20312]\n",
            "  ...\n",
            "  [1.47656 0.84375 1.17188 ... -0.00292969 -0.21875 2.53125]\n",
            "  [0.462891 0.152344 -1.875 ... -0.433594 0.488281 -0.539062]\n",
            "  [-0.523438 -0.605469 1.375 ... -1.15625 0.976562 -0.21875]]\n",
            "\n",
            " [[0.135742 5.125 2.48438 ... -0.515625 -1.74219 0.119141]\n",
            "  [1.40625 2.75 2.3125 ... -0.992188 0.173828 3.78125]\n",
            "  [-0.765625 2.07812 1.02344 ... 1.48438 -2.5 0.796875]\n",
            "  ...\n",
            "  [2.0625 0.345703 1.14062 ... 1.10156 2 -0.496094]\n",
            "  [-0.253906 0.398438 0.0957031 ... 1.53906 0.578125 0.964844]\n",
            "  [-0.875 -0.148438 0.503906 ... -0.859375 1.92188 -1.34375]]\n",
            "\n",
            " [[2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.5625 0.722656 1.5625 ... -1.15625 -1.52344 -2.20312]\n",
            "  ...\n",
            "  [1.375 1.5 0.730469 ... -2.10938 -2.15625 -0.816406]\n",
            "  [-1.34375 0.320312 -0.808594 ... -1.17969 -1.75 0.065918]\n",
            "  [-0.996094 -1.17969 -0.0527344 ... -0.722656 0.123047 -0.6875]]\n",
            "\n",
            " [[2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.5625 0.722656 1.5625 ... -1.15625 -1.52344 -2.20312]\n",
            "  ...\n",
            "  [0.679688 0.792969 -0.523438 ... -2.42188 -0.382812 -1.77344]\n",
            "  [-0.235352 0.332031 0.851562 ... 2.20312 -0.371094 -0.416016]\n",
            "  [-1.17188 -0.648438 1.39844 ... -1.17969 1.08594 -1.09375]]\n",
            "\n",
            " [[2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [0.617188 3.14062 1.34375 ... 0.40625 -0.84375 -0.820312]\n",
            "  [0.789062 2.26562 1.60156 ... -0.507812 -0.0234375 1.95312]\n",
            "  ...\n",
            "  [-0.328125 1.73438 0.365234 ... -0.632812 0.78125 3.23438]\n",
            "  [0.269531 -0.219727 1.1875 ... -0.71875 2.875 -0.890625]\n",
            "  [-1.5 -0.0620117 0.304688 ... -1.15625 1.33594 -1.19531]]]\n",
            "lnx=[[[1.29688 0.363281 0.785156 ... -0.582031 -0.761719 -1.10156]\n",
            "  [1.29688 0.363281 0.785156 ... -0.582031 -0.761719 -1.10156]\n",
            "  [1.28125 0.361328 0.78125 ... -0.578125 -0.761719 -1.10156]\n",
            "  ...\n",
            "  [0.875 0.5 0.695312 ... -0.00173187 -0.129883 1.5]\n",
            "  [0.291016 0.0957031 -1.17969 ... -0.273438 0.306641 -0.339844]\n",
            "  [-0.322266 -0.371094 0.84375 ... -0.710938 0.601562 -0.134766]]\n",
            "\n",
            " [[0.0703125 2.64062 1.28125 ... -0.265625 -0.898438 0.0615234]\n",
            "  [0.804688 1.57031 1.32031 ... -0.566406 0.0991211 2.15625]\n",
            "  [-0.453125 1.23438 0.605469 ... 0.878906 -1.48438 0.472656]\n",
            "  ...\n",
            "  [1.41406 0.236328 0.78125 ... 0.753906 1.36719 -0.339844]\n",
            "  [-0.167969 0.263672 0.0634766 ... 1.02344 0.382812 0.640625]\n",
            "  [-0.605469 -0.103027 0.349609 ... -0.597656 1.33594 -0.929688]]\n",
            "\n",
            " [[1.29688 0.363281 0.785156 ... -0.582031 -0.761719 -1.10156]\n",
            "  [1.29688 0.363281 0.785156 ... -0.582031 -0.761719 -1.10156]\n",
            "  [1.28125 0.361328 0.78125 ... -0.578125 -0.761719 -1.10156]\n",
            "  ...\n",
            "  [0.941406 1.02344 0.5 ... -1.44531 -1.47656 -0.558594]\n",
            "  [-0.882812 0.210938 -0.53125 ... -0.773438 -1.14844 0.0432129]\n",
            "  [-0.652344 -0.773438 -0.034668 ... -0.474609 0.0805664 -0.451172]]\n",
            "\n",
            " [[1.29688 0.363281 0.785156 ... -0.582031 -0.761719 -1.10156]\n",
            "  [1.29688 0.363281 0.785156 ... -0.582031 -0.761719 -1.10156]\n",
            "  [1.28125 0.361328 0.78125 ... -0.578125 -0.761719 -1.10156]\n",
            "  ...\n",
            "  [0.449219 0.523438 -0.345703 ... -1.60156 -0.253906 -1.17188]\n",
            "  [-0.155273 0.21875 0.5625 ... 1.45312 -0.245117 -0.273438]\n",
            "  [-0.773438 -0.427734 0.921875 ... -0.777344 0.714844 -0.722656]]\n",
            "\n",
            " [[1.29688 0.363281 0.785156 ... -0.582031 -0.761719 -1.10156]\n",
            "  [0.341797 1.73438 0.742188 ... 0.224609 -0.466797 -0.453125]\n",
            "  [0.462891 1.32812 0.941406 ... -0.298828 -0.0137939 1.14844]\n",
            "  ...\n",
            "  [-0.228516 1.21094 0.253906 ... -0.441406 0.542969 2.25]\n",
            "  [0.180664 -0.146484 0.792969 ... -0.480469 1.92188 -0.59375]\n",
            "  [-1.03125 -0.0424805 0.208984 ... -0.792969 0.917969 -0.820312]]]\n",
            "attention_lnx=[[[-1.27344 -1.07812 1.3125 ... -0.410156 0.984375 -0.294922]\n",
            "  [-1.27344 -1.07812 1.3125 ... -0.410156 0.984375 -0.294922]\n",
            "  [-1.27344 -1.08594 1.30469 ... -0.412109 0.984375 -0.298828]\n",
            "  ...\n",
            "  [-0.734375 -1.01562 1.1875 ... 0.245117 0.330078 0.0263672]\n",
            "  [-0.0717773 -1.03125 1.01562 ... 0.699219 0.203125 0.257812]\n",
            "  [-0.738281 -0.613281 1.01562 ... 0.414062 0.0368652 0.0849609]]\n",
            "\n",
            " [[0.769531 -0.609375 0.96875 ... -1.74219 0.273438 0.00454712]\n",
            "  [0.632812 0.439453 1.42188 ... -0.71875 0.351562 0.277344]\n",
            "  [0.267578 0.388672 0.279297 ... -0.166992 0.480469 0.695312]\n",
            "  ...\n",
            "  [0.032959 0.539062 0.308594 ... -0.609375 0.208008 -0.267578]\n",
            "  [0.511719 0.3125 0.601562 ... -0.523438 0.226562 -0.460938]\n",
            "  [0.230469 0.636719 0.283203 ... -0.511719 0.0961914 0.154297]]\n",
            "\n",
            " [[-1.27344 -1.07812 1.3125 ... -0.410156 0.984375 -0.294922]\n",
            "  [-1.27344 -1.07812 1.3125 ... -0.410156 0.984375 -0.294922]\n",
            "  [-1.27344 -1.08594 1.30469 ... -0.412109 0.984375 -0.298828]\n",
            "  ...\n",
            "  [-0.394531 -0.0161133 0.217773 ... 0.193359 0.404297 0.173828]\n",
            "  [-0.386719 0.0825195 0.648438 ... -0.257812 0.753906 0.412109]\n",
            "  [0.326172 -0.105957 0.324219 ... -0.261719 0.542969 0.492188]]\n",
            "\n",
            " [[-1.27344 -1.07812 1.3125 ... -0.410156 0.984375 -0.294922]\n",
            "  [-1.27344 -1.07812 1.3125 ... -0.410156 0.984375 -0.294922]\n",
            "  [-1.27344 -1.08594 1.30469 ... -0.412109 0.984375 -0.298828]\n",
            "  ...\n",
            "  [-0.349609 -0.0495605 0.210938 ... 0.09375 0.213867 0.0319824]\n",
            "  [-0.316406 0.160156 0.251953 ... 0.136719 -0.0888672 0.570312]\n",
            "  [-0.0412598 0.0942383 0.102051 ... -0.0541992 0.149414 0.613281]]\n",
            "\n",
            " [[-1.27344 -1.07812 1.3125 ... -0.410156 0.984375 -0.294922]\n",
            "  [-0.585938 -1.14062 1.23438 ... -0.337891 0.660156 0.28125]\n",
            "  [-0.605469 0.239258 1.32812 ... -0.0742188 0.515625 0.414062]\n",
            "  ...\n",
            "  [-0.435547 0.902344 0.617188 ... -0.474609 0.310547 0.839844]\n",
            "  [-0.134766 0.5 0.414062 ... 0.128906 0.589844 0.304688]\n",
            "  [-0.253906 0.6875 0.0893555 ... -0.0126343 0.380859 0.371094]]]\n",
            "attn_output=[[[0.582031 -0.154297 1.26562 ... -0.691406 -0.237305 -1.09375]\n",
            "  [0.582031 -0.154297 1.26562 ... -0.691406 -0.237305 -1.09375]\n",
            "  [0.566406 -0.15918 1.25781 ... -0.6875 -0.237305 -1.10156]\n",
            "  ...\n",
            "  [0.394531 -0.0913086 1.25 ... 0.128906 0.059082 1.35938]\n",
            "  [0.223633 -0.503906 -0.492188 ... 0.152344 0.396484 -0.161133]\n",
            "  [-0.699219 -0.675781 1.32812 ... -0.412109 0.5625 -0.0742188]]\n",
            "\n",
            " [[0.417969 2.07812 1.59375 ... -1.03906 -0.675781 0.0571289]\n",
            "  [1.05469 1.64844 1.92969 ... -0.882812 0.271484 2.09375]\n",
            "  [-0.275391 1.36719 0.71875 ... 0.726562 -1.11719 0.824219]\n",
            "  ...\n",
            "  [1.375 0.578125 0.949219 ... 0.322266 1.44531 -0.5]\n",
            "  [0.163086 0.451172 0.441406 ... 0.644531 0.511719 0.320312]\n",
            "  [-0.423828 0.322266 0.519531 ... -0.902344 1.32812 -0.78125]]\n",
            "\n",
            " [[0.582031 -0.154297 1.26562 ... -0.691406 -0.237305 -1.09375]\n",
            "  [0.582031 -0.154297 1.26562 ... -0.691406 -0.237305 -1.09375]\n",
            "  [0.566406 -0.15918 1.25781 ... -0.6875 -0.237305 -1.10156]\n",
            "  ...\n",
            "  [0.625 0.945312 0.605469 ... -1.21875 -1.11719 -0.410156]\n",
            "  [-1.05469 0.245117 -0.0976562 ... -0.875 -0.605469 0.291016]\n",
            "  [-0.410156 -0.789062 0.166016 ... -0.601562 0.408203 -0.119629]]\n",
            "\n",
            " [[0.582031 -0.154297 1.26562 ... -0.691406 -0.237305 -1.09375]\n",
            "  [0.582031 -0.154297 1.26562 ... -0.691406 -0.237305 -1.09375]\n",
            "  [0.566406 -0.15918 1.25781 ... -0.6875 -0.237305 -1.10156]\n",
            "  ...\n",
            "  [0.203125 0.457031 -0.192383 ... -1.42969 -0.104004 -1.07031]\n",
            "  [-0.339844 0.302734 0.679688 ... 1.44531 -0.283203 0.0952148]\n",
            "  [-0.75 -0.34375 0.929688 ... -0.761719 0.765625 -0.296875]]\n",
            "\n",
            " [[0.582031 -0.154297 1.26562 ... -0.691406 -0.237305 -1.09375]\n",
            "  [0.015564 0.996094 1.28125 ... 0.0339355 -0.0913086 -0.267578]\n",
            "  [0.0981445 1.33594 1.5625 ... -0.310547 0.263672 1.26562]\n",
            "  ...\n",
            "  [-0.498047 1.71875 0.640625 ... -0.722656 0.710938 2.65625]\n",
            "  [0.0859375 0.178711 1.02344 ... -0.376953 2.20312 -0.373047]\n",
            "  [-1.13281 0.402344 0.253906 ... -0.753906 1.10938 -0.53125]]]\n",
            "next_layer_addition_dropped_out=[[[1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.77344 -0.0683594 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  ...\n",
            "  [0.349609 -0.0283203 2.71875 ... -0.193359 0.34375 2.23438]\n",
            "  [0.578125 -1.07812 0.859375 ... 0.59375 0.386719 -0.382812]\n",
            "  [-1.96875 -1.23438 2.64062 ... -0.710938 0.730469 -1.26562]]\n",
            "\n",
            " [[1.20312 4.03125 2.92188 ... -1.57812 -1.0625 0.0336914]\n",
            "  [0.820312 3 3.21875 ... -1.07812 1.75 3.46875]\n",
            "  [-0.867188 1.96875 1.4375 ... 1.20312 -1.85938 1.78906]\n",
            "  ...\n",
            "  [3.21875 0.152344 1.60156 ... 0.363281 2.0625 -1.02344]\n",
            "  [-0.0332031 0.703125 0.972656 ... 0.570312 0.746094 -0.09375]\n",
            "  [-1.64062 -0.253906 -0.382812 ... -1.94531 1.91406 -2.34375]]\n",
            "\n",
            " [[1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.77344 -0.0683594 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  ...\n",
            "  [0.882812 1.36719 1.28125 ... -2.15625 -2.75 -1.03125]\n",
            "  [-2.03125 0.427734 0.585938 ... -1.57031 -1.89844 -0.0839844]\n",
            "  [-1.07031 -2.40625 0.166016 ... -1.4375 0.0273438 -1.35156]]\n",
            "\n",
            " [[1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.77344 -0.0683594 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  ...\n",
            "  [0.765625 1.29688 -0.433594 ... -1.41406 -0.726562 -1.71094]\n",
            "  [-0.6875 -0.210938 0.53125 ... 2.53125 -0.236328 -0.392578]\n",
            "  [-2.125 -1.32812 0.777344 ... -1.35156 0.992188 -1.92188]]\n",
            "\n",
            " [[1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.21094 1.82031 2.25 ... -0.101562 -0.871094 -0.878906]\n",
            "  [-0.21875 2.96875 2.35938 ... 0.0976562 1.21875 0.976562]\n",
            "  ...\n",
            "  [-0.738281 3.28125 0.800781 ... -1.0625 1.28125 3.96875]\n",
            "  [0.6875 0.116211 1.4375 ... -0.75 4.125 -1.46875]\n",
            "  [-2.5625 0.498047 -0.34375 ... -1.46875 1.40625 -2.20312]]]\n",
            "inputs=[[[1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.77344 -0.0683594 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  ...\n",
            "  [0.349609 -0.0283203 2.71875 ... -0.193359 0.34375 2.23438]\n",
            "  [0.578125 -1.07812 0.859375 ... 0.59375 0.386719 -0.382812]\n",
            "  [-1.96875 -1.23438 2.64062 ... -0.710938 0.730469 -1.26562]]\n",
            "\n",
            " [[1.20312 4.03125 2.92188 ... -1.57812 -1.0625 0.0336914]\n",
            "  [0.820312 3 3.21875 ... -1.07812 1.75 3.46875]\n",
            "  [-0.867188 1.96875 1.4375 ... 1.20312 -1.85938 1.78906]\n",
            "  ...\n",
            "  [3.21875 0.152344 1.60156 ... 0.363281 2.0625 -1.02344]\n",
            "  [-0.0332031 0.703125 0.972656 ... 0.570312 0.746094 -0.09375]\n",
            "  [-1.64062 -0.253906 -0.382812 ... -1.94531 1.91406 -2.34375]]\n",
            "\n",
            " [[1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.77344 -0.0683594 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  ...\n",
            "  [0.882812 1.36719 1.28125 ... -2.15625 -2.75 -1.03125]\n",
            "  [-2.03125 0.427734 0.585938 ... -1.57031 -1.89844 -0.0839844]\n",
            "  [-1.07031 -2.40625 0.166016 ... -1.4375 0.0273438 -1.35156]]\n",
            "\n",
            " [[1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.77344 -0.0683594 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  ...\n",
            "  [0.765625 1.29688 -0.433594 ... -1.41406 -0.726562 -1.71094]\n",
            "  [-0.6875 -0.210938 0.53125 ... 2.53125 -0.236328 -0.392578]\n",
            "  [-2.125 -1.32812 0.777344 ... -1.35156 0.992188 -1.92188]]\n",
            "\n",
            " [[1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.21094 1.82031 2.25 ... -0.101562 -0.871094 -0.878906]\n",
            "  [-0.21875 2.96875 2.35938 ... 0.0976562 1.21875 0.976562]\n",
            "  ...\n",
            "  [-0.738281 3.28125 0.800781 ... -1.0625 1.28125 3.96875]\n",
            "  [0.6875 0.116211 1.4375 ... -0.75 4.125 -1.46875]\n",
            "  [-2.5625 0.498047 -0.34375 ... -1.46875 1.40625 -2.20312]]]\n",
            "lnx=[[[0.769531 -0.0240479 1.42969 ... -1.09375 -0.726562 -1.24219]\n",
            "  [0.769531 -0.0240479 1.42969 ... -1.09375 -0.726562 -1.24219]\n",
            "  [0.753906 -0.0290527 1.42969 ... -1.09375 -0.726562 -1.24219]\n",
            "  ...\n",
            "  [0.174805 -0.0141602 1.35938 ... -0.0966797 0.171875 1.11719]\n",
            "  [0.3125 -0.582031 0.464844 ... 0.320312 0.208984 -0.207031]\n",
            "  [-1.01562 -0.636719 1.36719 ... -0.367188 0.376953 -0.652344]]\n",
            "\n",
            " [[0.535156 1.78906 1.29688 ... -0.699219 -0.470703 0.0149536]\n",
            "  [0.40625 1.48438 1.59375 ... -0.535156 0.867188 1.71875]\n",
            "  [-0.455078 1.03125 0.753906 ... 0.632812 -0.976562 0.9375]\n",
            "  ...\n",
            "  [1.9375 0.0917969 0.964844 ... 0.21875 1.24219 -0.617188]\n",
            "  [-0.019043 0.402344 0.558594 ... 0.326172 0.427734 -0.0537109]\n",
            "  [-0.980469 -0.152344 -0.229492 ... -1.16406 1.14844 -1.40625]]\n",
            "\n",
            " [[0.769531 -0.0240479 1.42969 ... -1.09375 -0.726562 -1.24219]\n",
            "  [0.769531 -0.0240479 1.42969 ... -1.09375 -0.726562 -1.24219]\n",
            "  [0.753906 -0.0290527 1.42969 ... -1.09375 -0.726562 -1.24219]\n",
            "  ...\n",
            "  [0.519531 0.804688 0.753906 ... -1.27344 -1.625 -0.609375]\n",
            "  [-1.17188 0.246094 0.337891 ... -0.902344 -1.09375 -0.0483398]\n",
            "  [-0.601562 -1.35156 0.0932617 ... -0.808594 0.0153809 -0.757812]]\n",
            "\n",
            " [[0.769531 -0.0240479 1.42969 ... -1.09375 -0.726562 -1.24219]\n",
            "  [0.769531 -0.0240479 1.42969 ... -1.09375 -0.726562 -1.24219]\n",
            "  [0.753906 -0.0290527 1.42969 ... -1.09375 -0.726562 -1.24219]\n",
            "  ...\n",
            "  [0.439453 0.746094 -0.249023 ... -0.8125 -0.417969 -0.984375]\n",
            "  [-0.388672 -0.119141 0.300781 ... 1.42969 -0.133789 -0.22168]\n",
            "  [-1.19531 -0.746094 0.435547 ... -0.757812 0.558594 -1.07812]]\n",
            "\n",
            " [[0.769531 -0.0240479 1.42969 ... -1.09375 -0.726562 -1.24219]\n",
            "  [0.574219 0.867188 1.07031 ... -0.0483398 -0.414062 -0.417969]\n",
            "  [-0.111328 1.50781 1.19531 ... 0.0495605 0.621094 0.496094]\n",
            "  ...\n",
            "  [-0.441406 1.96094 0.478516 ... -0.636719 0.765625 2.375]\n",
            "  [0.40625 0.0683594 0.847656 ... -0.441406 2.4375 -0.867188]\n",
            "  [-1.49219 0.289062 -0.200195 ... -0.855469 0.816406 -1.28125]]]\n",
            "attention_lnx=[[[0.244141 -0.337891 -0.0585938 ... -0.433594 1.28125 0.232422]\n",
            "  [0.244141 -0.337891 -0.0585938 ... -0.433594 1.28125 0.232422]\n",
            "  [0.242188 -0.341797 -0.0612793 ... -0.435547 1.27344 0.229492]\n",
            "  ...\n",
            "  [-0.0825195 -0.824219 0.00854492 ... 0.267578 1.1875 0.0612793]\n",
            "  [-0.00289917 -0.914062 -0.138672 ... 0.482422 1.04688 0.105957]\n",
            "  [-0.0175781 -0.710938 -0.216797 ... 0.335938 1.03125 -0.0512695]]\n",
            "\n",
            " [[0.511719 -0.933594 0.976562 ... 0.417969 -0.734375 0.0820312]\n",
            "  [0.333984 -0.318359 0.613281 ... 0.0766602 -0.640625 -0.020874]\n",
            "  [0.388672 -0.0634766 0.400391 ... -0.0722656 -0.492188 0.179688]\n",
            "  ...\n",
            "  [0.207031 0.675781 0.138672 ... -0.730469 -0.347656 -0.285156]\n",
            "  [0.285156 0.703125 0.539062 ... -0.128906 -0.188477 -0.353516]\n",
            "  [0.269531 0.679688 0.597656 ... -0.507812 -0.333984 -0.410156]]\n",
            "\n",
            " [[0.244141 -0.337891 -0.0585938 ... -0.433594 1.28125 0.232422]\n",
            "  [0.244141 -0.337891 -0.0585938 ... -0.433594 1.28125 0.232422]\n",
            "  [0.242188 -0.341797 -0.0612793 ... -0.435547 1.27344 0.229492]\n",
            "  ...\n",
            "  [-0.302734 0.0407715 0.308594 ... 0.283203 0.957031 0.425781]\n",
            "  [-0.478516 0.100586 0.4375 ... 0.398438 0.992188 0.165039]\n",
            "  [-0.0800781 0.0245361 0.000249863 ... 0.640625 0.765625 -0.0864258]]\n",
            "\n",
            " [[0.244141 -0.337891 -0.0585938 ... -0.433594 1.28125 0.232422]\n",
            "  [0.244141 -0.337891 -0.0585938 ... -0.433594 1.28125 0.232422]\n",
            "  [0.242188 -0.341797 -0.0612793 ... -0.435547 1.27344 0.229492]\n",
            "  ...\n",
            "  [-0.474609 0.423828 -0.116699 ... 0.462891 0.59375 -0.269531]\n",
            "  [-0.53125 0.277344 -0.251953 ... 0.777344 0.304688 -0.597656]\n",
            "  [-0.020874 0.200195 -0.546875 ... 0.660156 0.636719 -0.699219]]\n",
            "\n",
            " [[0.244141 -0.337891 -0.0585938 ... -0.433594 1.28125 0.232422]\n",
            "  [-0.0478516 -0.445312 0.511719 ... -0.582031 0.898438 -0.10791]\n",
            "  [-0.179688 -0.363281 0.191406 ... -0.255859 0.421875 -0.219727]\n",
            "  ...\n",
            "  [-0.347656 0.585938 0.0961914 ... 0.273438 0.21875 -0.121094]\n",
            "  [-0.241211 0.652344 0.341797 ... 0.314453 0.28125 -0.398438]\n",
            "  [-0.0324707 0.628906 -0.155273 ... 0.287109 0.12793 -0.496094]]]\n",
            "attn_output=[[[0.800781 -0.15332 1.28906 ... -1.17188 -0.166992 -1.05469]\n",
            "  [0.800781 -0.15332 1.28906 ... -1.17188 -0.166992 -1.05469]\n",
            "  [0.785156 -0.15918 1.28906 ... -1.17188 -0.169922 -1.05469]\n",
            "  ...\n",
            "  [0.124023 -0.396484 1.26562 ... 0.0344238 0.710938 1.0625]\n",
            "  [0.289062 -1 0.363281 ... 0.542969 0.722656 -0.139648]\n",
            "  [-0.960938 -0.941406 1.17188 ... -0.181641 0.851562 -0.636719]]\n",
            "\n",
            " [[0.695312 1.25781 1.58594 ... -0.470703 -0.730469 0.046875]\n",
            "  [0.53125 1.23438 1.76562 ... -0.462891 0.511719 1.59375]\n",
            "  [-0.231445 0.921875 0.890625 ... 0.546875 -1.14062 0.953125]\n",
            "  ...\n",
            "  [1.95312 0.472656 0.992188 ... -0.209961 0.980469 -0.746094]\n",
            "  [0.137695 0.765625 0.824219 ... 0.241211 0.304688 -0.244141]\n",
            "  [-0.789062 0.245117 0.123535 ... -1.41406 0.910156 -1.58594]]\n",
            "\n",
            " [[0.800781 -0.15332 1.28906 ... -1.17188 -0.166992 -1.05469]\n",
            "  [0.800781 -0.15332 1.28906 ... -1.17188 -0.166992 -1.05469]\n",
            "  [0.785156 -0.15918 1.28906 ... -1.17188 -0.169922 -1.05469]\n",
            "  ...\n",
            "  [0.320312 0.777344 0.878906 ... -1.03906 -0.992188 -0.333984]\n",
            "  [-1.36719 0.289062 0.558594 ... -0.640625 -0.494141 0.0441895]\n",
            "  [-0.613281 -1.26562 0.0883789 ... -0.423828 0.421875 -0.765625]]\n",
            "\n",
            " [[0.800781 -0.15332 1.28906 ... -1.17188 -0.166992 -1.05469]\n",
            "  [0.800781 -0.15332 1.28906 ... -1.17188 -0.166992 -1.05469]\n",
            "  [0.785156 -0.15918 1.28906 ... -1.17188 -0.169922 -1.05469]\n",
            "  ...\n",
            "  [0.15625 0.925781 -0.294922 ... -0.511719 -0.0712891 -1.0625]\n",
            "  [-0.65625 0.0356445 0.150391 ... 1.78125 0.0368652 -0.53125]\n",
            "  [-1.13281 -0.597656 0.12207 ... -0.365234 0.859375 -1.38281]]\n",
            "\n",
            " [[0.800781 -0.15332 1.28906 ... -1.17188 -0.166992 -1.05469]\n",
            "  [0.515625 0.609375 1.22656 ... -0.302734 0.012146 -0.4375]\n",
            "  [-0.189453 1.23438 1.21094 ... -0.0751953 0.777344 0.359375]\n",
            "  ...\n",
            "  [-0.621094 2.21875 0.511719 ... -0.451172 0.859375 2.20312]\n",
            "  [0.251953 0.433594 1.00781 ... -0.246094 2.48438 -1.05469]\n",
            "  [-1.44531 0.628906 -0.279297 ... -0.660156 0.855469 -1.50781]]]\n",
            "next_layer_addition_dropped_out=[[[1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.92188 0.109375 3.39062 ... -3.25 0.546875 -2.4375]\n",
            "  ...\n",
            "  [0.0625 -0.585938 2.79688 ... 0.21582 1.95312 1.5625]\n",
            "  [0.402344 -2.42188 0.90625 ... 0.539062 2.65625 0.148438]\n",
            "  [-2.53125 -2.89062 3.125 ... -0.0527344 1.41406 -3.09375]]\n",
            "\n",
            " [[1.57812 4.28125 3.0625 ... -0.148438 -2.375 0.484375]\n",
            "  [1.51562 3.46875 4.25 ... -1.125 1.0625 1.92188]\n",
            "  [0.765625 1.48438 1.77344 ... 0.753906 -1.79688 0.773438]\n",
            "  ...\n",
            "  [4.0625 1.40625 1.44531 ... -0.453125 2.29688 -0.789062]\n",
            "  [-0.00976562 1.4375 0.695312 ... 1.20312 0.205078 -0.357422]\n",
            "  [-1.84375 1.26562 0.6875 ... -2.21875 1.07031 -4.5]]\n",
            "\n",
            " [[1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.92188 0.109375 3.39062 ... -3.25 0.546875 -2.4375]\n",
            "  ...\n",
            "  [1.75 1.21094 -0.109375 ... -2.6875 -2.875 0.511719]\n",
            "  [-2.73438 1.39062 1.52344 ... -1.77344 -0.800781 -0.4375]\n",
            "  [-0.964844 -2.5625 -0.158203 ... -0.113281 -0.441406 -2.34375]]\n",
            "\n",
            " [[1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.92188 0.109375 3.39062 ... -3.25 0.546875 -2.4375]\n",
            "  ...\n",
            "  [0.742188 1.5625 -0.625 ... -0.59375 0.0908203 -3.25]\n",
            "  [-1.35156 -0.113281 -0.0253906 ... 3.79688 -0.283203 -2.15625]\n",
            "  [-1.625 -0.957031 0.480469 ... -0.00390625 1.28125 -4.09375]]\n",
            "\n",
            " [[1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [0.460938 2.84375 2.64062 ... -1.14062 0.53125 -0.925781]\n",
            "  [-0.202148 3.65625 2.125 ... -1.60156 1.72656 -0.515625]\n",
            "  ...\n",
            "  [-2.4375 3.875 1.09375 ... -0.8125 1.08594 3.78125]\n",
            "  [1.16406 1.875 0.757812 ... -0.208984 3.25 -2.84375]\n",
            "  [-2.125 1.07812 -0.742188 ... -0.171875 0.992188 -3.48438]]]\n",
            "inputs=[[[1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.92188 0.109375 3.39062 ... -3.25 0.546875 -2.4375]\n",
            "  ...\n",
            "  [0.0625 -0.585938 2.79688 ... 0.21582 1.95312 1.5625]\n",
            "  [0.402344 -2.42188 0.90625 ... 0.539062 2.65625 0.148438]\n",
            "  [-2.53125 -2.89062 3.125 ... -0.0527344 1.41406 -3.09375]]\n",
            "\n",
            " [[1.57812 4.28125 3.0625 ... -0.148438 -2.375 0.484375]\n",
            "  [1.51562 3.46875 4.25 ... -1.125 1.0625 1.92188]\n",
            "  [0.765625 1.48438 1.77344 ... 0.753906 -1.79688 0.773438]\n",
            "  ...\n",
            "  [4.0625 1.40625 1.44531 ... -0.453125 2.29688 -0.789062]\n",
            "  [-0.00976562 1.4375 0.695312 ... 1.20312 0.205078 -0.357422]\n",
            "  [-1.84375 1.26562 0.6875 ... -2.21875 1.07031 -4.5]]\n",
            "\n",
            " [[1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.92188 0.109375 3.39062 ... -3.25 0.546875 -2.4375]\n",
            "  ...\n",
            "  [1.75 1.21094 -0.109375 ... -2.6875 -2.875 0.511719]\n",
            "  [-2.73438 1.39062 1.52344 ... -1.77344 -0.800781 -0.4375]\n",
            "  [-0.964844 -2.5625 -0.158203 ... -0.113281 -0.441406 -2.34375]]\n",
            "\n",
            " [[1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.92188 0.109375 3.39062 ... -3.25 0.546875 -2.4375]\n",
            "  ...\n",
            "  [0.742188 1.5625 -0.625 ... -0.59375 0.0908203 -3.25]\n",
            "  [-1.35156 -0.113281 -0.0253906 ... 3.79688 -0.283203 -2.15625]\n",
            "  [-1.625 -0.957031 0.480469 ... -0.00390625 1.28125 -4.09375]]\n",
            "\n",
            " [[1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [0.460938 2.84375 2.64062 ... -1.14062 0.53125 -0.925781]\n",
            "  [-0.202148 3.65625 2.125 ... -1.60156 1.72656 -0.515625]\n",
            "  ...\n",
            "  [-2.4375 3.875 1.09375 ... -0.8125 1.08594 3.78125]\n",
            "  [1.16406 1.875 0.757812 ... -0.208984 3.25 -2.84375]\n",
            "  [-2.125 1.07812 -0.742188 ... -0.171875 0.992188 -3.48438]]]\n",
            "lnx=[[[0.738281 0.0454102 1.27344 ... -1.21875 0.202148 -0.910156]\n",
            "  [0.738281 0.0454102 1.27344 ... -1.21875 0.202148 -0.910156]\n",
            "  [0.722656 0.0410156 1.27344 ... -1.21875 0.205078 -0.914062]\n",
            "  ...\n",
            "  [0.027832 -0.259766 1.24219 ... 0.0957031 0.867188 0.695312]\n",
            "  [0.193359 -1.16406 0.435547 ... 0.259766 1.28125 0.0712891]\n",
            "  [-1.14844 -1.3125 1.41406 ... -0.0239258 0.640625 -1.40625]]\n",
            "\n",
            " [[0.621094 1.6875 1.20312 ... -0.0583496 -0.933594 0.19043]\n",
            "  [0.667969 1.53125 1.875 ... -0.496094 0.46875 0.847656]\n",
            "  [0.359375 0.699219 0.832031 ... 0.353516 -0.84375 0.363281]\n",
            "  ...\n",
            "  [2.14062 0.742188 0.761719 ... -0.239258 1.21094 -0.416016]\n",
            "  [-0.00500488 0.738281 0.357422 ... 0.617188 0.105469 -0.183594]\n",
            "  [-0.988281 0.679688 0.369141 ... -1.1875 0.574219 -2.40625]]\n",
            "\n",
            " [[0.738281 0.0454102 1.27344 ... -1.21875 0.202148 -0.910156]\n",
            "  [0.738281 0.0454102 1.27344 ... -1.21875 0.202148 -0.910156]\n",
            "  [0.722656 0.0410156 1.27344 ... -1.21875 0.205078 -0.914062]\n",
            "  ...\n",
            "  [0.914062 0.632812 -0.0571289 ... -1.40625 -1.5 0.267578]\n",
            "  [-1.41406 0.71875 0.789062 ... -0.917969 -0.414062 -0.226562]\n",
            "  [-0.480469 -1.28125 -0.0791016 ... -0.0563965 -0.219727 -1.17188]]\n",
            "\n",
            " [[0.738281 0.0454102 1.27344 ... -1.21875 0.202148 -0.910156]\n",
            "  [0.738281 0.0454102 1.27344 ... -1.21875 0.202148 -0.910156]\n",
            "  [0.722656 0.0410156 1.27344 ... -1.21875 0.205078 -0.914062]\n",
            "  ...\n",
            "  [0.375 0.789062 -0.316406 ... -0.300781 0.0458984 -1.64062]\n",
            "  [-0.679688 -0.0571289 -0.0127563 ... 1.91406 -0.142578 -1.08594]\n",
            "  [-0.800781 -0.472656 0.237305 ... -0.00193024 0.632812 -2.01562]]\n",
            "\n",
            " [[0.738281 0.0454102 1.27344 ... -1.21875 0.202148 -0.910156]\n",
            "  [0.196289 1.21094 1.125 ... -0.484375 0.225586 -0.392578]\n",
            "  [-0.0908203 1.64062 0.953125 ... -0.71875 0.777344 -0.231445]\n",
            "  ...\n",
            "  [-1.3125 2.07812 0.585938 ... -0.435547 0.582031 2.03125]\n",
            "  [0.621094 1 0.404297 ... -0.111328 1.73438 -1.51562]\n",
            "  [-1.09375 0.554688 -0.382812 ... -0.0883789 0.511719 -1.79688]]]\n",
            "attention_lnx=[[[1.60156 -1.25781 1.13281 ... 1.33594 -0.808594 0.433594]\n",
            "  [1.60156 -1.25781 1.13281 ... 1.33594 -0.808594 0.433594]\n",
            "  [1.59375 -1.25781 1.125 ... 1.33594 -0.808594 0.429688]\n",
            "  ...\n",
            "  [1.19531 -1.46875 0.847656 ... 1.05469 -0.882812 0.234375]\n",
            "  [1.05469 -1.13281 0.773438 ... 1.125 -0.789062 0.382812]\n",
            "  [0.882812 -1.17969 0.730469 ... 0.910156 -1.05469 0.225586]]\n",
            "\n",
            " [[-1.26562 -0.890625 -1.19531 ... -0.585938 -0.988281 0.255859]\n",
            "  [-1.5 -1.04688 -0.769531 ... 0.929688 -1.08594 0.367188]\n",
            "  [-1.10938 -1.25781 -0.339844 ... -0.328125 -0.464844 0.119629]\n",
            "  ...\n",
            "  [-0.605469 0.129883 -0.554688 ... 0.503906 -0.0537109 -0.402344]\n",
            "  [-0.691406 -0.410156 -0.435547 ... -0.209961 0.000471115 -0.75]\n",
            "  [-0.589844 0.0878906 -0.394531 ... -0.0253906 -0.110352 -0.196289]]\n",
            "\n",
            " [[1.60156 -1.25781 1.13281 ... 1.33594 -0.808594 0.433594]\n",
            "  [1.60156 -1.25781 1.13281 ... 1.33594 -0.808594 0.433594]\n",
            "  [1.59375 -1.25781 1.125 ... 1.33594 -0.808594 0.429688]\n",
            "  ...\n",
            "  [0.400391 -1.20312 0.0585938 ... 1.03125 -0.761719 -0.0035553]\n",
            "  [0.539062 -1.25781 0.183594 ... 0.785156 -0.851562 -0.205078]\n",
            "  [0.734375 -1.22656 0.523438 ... 0.447266 -0.734375 0.175781]]\n",
            "\n",
            " [[1.60156 -1.25781 1.13281 ... 1.33594 -0.808594 0.433594]\n",
            "  [1.60156 -1.25781 1.13281 ... 1.33594 -0.808594 0.433594]\n",
            "  [1.59375 -1.25781 1.125 ... 1.33594 -0.808594 0.429688]\n",
            "  ...\n",
            "  [0.597656 -0.574219 0.345703 ... 0.53125 -0.523438 -0.429688]\n",
            "  [0.648438 -0.476562 0.225586 ... 0.777344 -0.304688 -0.796875]\n",
            "  [0.617188 -0.890625 0.304688 ... 0.671875 -0.496094 -0.128906]]\n",
            "\n",
            " [[1.60156 -1.25781 1.13281 ... 1.33594 -0.808594 0.433594]\n",
            "  [1.30469 -1.57031 0.726562 ... -0.212891 -1.24219 -0.0927734]\n",
            "  [0.5 -1.23438 0.227539 ... 1.03906 -1.17188 -0.554688]\n",
            "  ...\n",
            "  [-0.186523 -0.792969 -0.10498 ... 0.0419922 -0.585938 -0.0737305]\n",
            "  [0.11377 -1.02344 0.119141 ... 0.043457 -0.386719 -0.554688]\n",
            "  [-0.0299072 -1.05469 0.416016 ... 0.306641 -0.769531 0.353516]]]\n",
            "attn_output=[[[1.25781 -0.400391 1.59375 ... -0.675781 -0.0952148 -0.699219]\n",
            "  [1.25781 -0.400391 1.59375 ... -0.675781 -0.0952148 -0.699219]\n",
            "  [1.24219 -0.404297 1.59375 ... -0.675781 -0.0922852 -0.707031]\n",
            "  ...\n",
            "  [0.53125 -0.867188 1.53906 ... 0.535156 0.451172 0.757812]\n",
            "  [0.65625 -1.60156 0.753906 ... 0.746094 0.839844 0.239258]\n",
            "  [-0.703125 -1.74219 1.64844 ... 0.367188 0.15332 -1.22656]]\n",
            "\n",
            " [[0.114258 1.24219 0.683594 ... -0.269531 -1.23438 0.271484]\n",
            "  [0.00640869 0.996094 1.42969 ... -0.0800781 -0.00964355 0.941406]\n",
            "  [-0.151367 0.100098 0.632812 ... 0.188477 -1 0.394531]\n",
            "  ...\n",
            "  [1.75781 0.78125 0.451172 ... 0.0257568 1.14062 -0.605469]\n",
            "  [-0.345703 0.503906 0.12793 ... 0.488281 0.101074 -0.542969]\n",
            "  [-1.25781 0.699219 0.151367 ... -1.15625 0.496094 -2.42188]]\n",
            "\n",
            " [[1.25781 -0.400391 1.59375 ... -0.675781 -0.0952148 -0.699219]\n",
            "  [1.25781 -0.400391 1.59375 ... -0.675781 -0.0952148 -0.699219]\n",
            "  [1.24219 -0.404297 1.59375 ... -0.675781 -0.0922852 -0.707031]\n",
            "  ...\n",
            "  [1.04688 0.0038147 -0.0247803 ... -0.808594 -1.77344 0.248047]\n",
            "  [-1.07031 0.0644531 0.832031 ... -0.480469 -0.804688 -0.3125]\n",
            "  [-0.108887 -1.78906 0.172852 ... 0.158203 -0.554688 -1.02344]]\n",
            "\n",
            " [[1.25781 -0.400391 1.59375 ... -0.675781 -0.0952148 -0.699219]\n",
            "  [1.25781 -0.400391 1.59375 ... -0.675781 -0.0952148 -0.699219]\n",
            "  [1.24219 -0.404297 1.59375 ... -0.675781 -0.0922852 -0.707031]\n",
            "  ...\n",
            "  [0.636719 0.46875 -0.132812 ... -0.0296631 -0.205078 -1.75]\n",
            "  [-0.335938 -0.283203 0.0957031 ... 2.1875 -0.28125 -1.41406]\n",
            "  [-0.474609 -0.867188 0.369141 ... 0.314453 0.369141 -1.98438]]\n",
            "\n",
            " [[1.25781 -0.400391 1.59375 ... -0.675781 -0.0952148 -0.699219]\n",
            "  [0.703125 0.507812 1.33594 ... -0.539062 -0.283203 -0.404297]\n",
            "  [0.125977 1.02344 0.996094 ... -0.238281 0.235352 -0.453125]\n",
            "  ...\n",
            "  [-1.35938 1.59375 0.511719 ... -0.398438 0.257812 1.91406]\n",
            "  [0.652344 0.435547 0.447266 ... -0.0844727 1.46094 -1.73438]\n",
            "  [-1.07031 0.0116577 -0.162109 ... 0.0668945 0.11084 -1.55469]]]\n",
            "next_layer_addition_dropped_out=[[[3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3 -2.6875 4.03125 ... -1.375 -0.253906 -2.0625]\n",
            "  ...\n",
            "  [0.753906 -2.53125 3.17188 ... 0.867188 1.11719 2.1875]\n",
            "  [1.94531 -3.78125 2.25 ... 2.17188 1.98438 1.14062]\n",
            "  [-3.09375 -4.40625 4.5625 ... 0.613281 1.14062 -3.28125]]\n",
            "\n",
            " [[-0.773438 2.32812 1.20312 ... -0.353516 -3.60938 1.75781]\n",
            "  [-0.345703 1.90625 3.60938 ... -0.371094 -0.484375 2.79688]\n",
            "  [-0.488281 0.464844 1.27344 ... 0.652344 -1.39062 0.144531]\n",
            "  ...\n",
            "  [3.35938 1.89844 0.0078125 ... 0.314453 1.44531 -1.42969]\n",
            "  [-0.617188 0.195312 0.929688 ... 1.82031 -0.310547 -1.5]\n",
            "  [-3.17188 2.21875 0.953125 ... -0.84375 0.390625 -4]]\n",
            "\n",
            " [[3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3 -2.6875 4.03125 ... -1.375 -0.253906 -2.0625]\n",
            "  ...\n",
            "  [2.10938 -1.44531 -0.90625 ... -0.660156 -3.67188 -0.671875]\n",
            "  [-3.5625 -0.839844 1.25781 ... -1.35156 -1.47656 -1.39844]\n",
            "  [-1.76562 -4 0.632812 ... -0.193359 -1.28906 -1.5625]]\n",
            "\n",
            " [[3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3 -2.6875 4.03125 ... -1.375 -0.253906 -2.0625]\n",
            "  ...\n",
            "  [-0.46875 -0.152344 0.287109 ... -0.208008 -1.6875 -3.48438]\n",
            "  [-1.35156 -2.21875 0.0244141 ... 4.5 -1.10938 -4]\n",
            "  [-2.59375 -2.07812 0.945312 ... 0.109375 1.09375 -3.84375]]\n",
            "\n",
            " [[3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [1.21094 -0.117188 2.78125 ... -1.74219 -0.546875 -0.703125]\n",
            "  [-0.132812 2.09375 1.95312 ... -0.820312 0.765625 -1.49219]\n",
            "  ...\n",
            "  [-2.89062 2.59375 1.40625 ... -0.988281 0.367188 3.79688]\n",
            "  [1.40625 0.457031 0.671875 ... 0.6875 3.09375 -3.5]\n",
            "  [-3.40625 0.0476074 -0.291016 ... 1.02344 0.332031 -3.1875]]]\n",
            "inputs=[[[3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3 -2.6875 4.03125 ... -1.375 -0.253906 -2.0625]\n",
            "  ...\n",
            "  [0.753906 -2.53125 3.17188 ... 0.867188 1.11719 2.1875]\n",
            "  [1.94531 -3.78125 2.25 ... 2.17188 1.98438 1.14062]\n",
            "  [-3.09375 -4.40625 4.5625 ... 0.613281 1.14062 -3.28125]]\n",
            "\n",
            " [[-0.773438 2.32812 1.20312 ... -0.353516 -3.60938 1.75781]\n",
            "  [-0.345703 1.90625 3.60938 ... -0.371094 -0.484375 2.79688]\n",
            "  [-0.488281 0.464844 1.27344 ... 0.652344 -1.39062 0.144531]\n",
            "  ...\n",
            "  [3.35938 1.89844 0.0078125 ... 0.314453 1.44531 -1.42969]\n",
            "  [-0.617188 0.195312 0.929688 ... 1.82031 -0.310547 -1.5]\n",
            "  [-3.17188 2.21875 0.953125 ... -0.84375 0.390625 -4]]\n",
            "\n",
            " [[3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3 -2.6875 4.03125 ... -1.375 -0.253906 -2.0625]\n",
            "  ...\n",
            "  [2.10938 -1.44531 -0.90625 ... -0.660156 -3.67188 -0.671875]\n",
            "  [-3.5625 -0.839844 1.25781 ... -1.35156 -1.47656 -1.39844]\n",
            "  [-1.76562 -4 0.632812 ... -0.193359 -1.28906 -1.5625]]\n",
            "\n",
            " [[3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3 -2.6875 4.03125 ... -1.375 -0.253906 -2.0625]\n",
            "  ...\n",
            "  [-0.46875 -0.152344 0.287109 ... -0.208008 -1.6875 -3.48438]\n",
            "  [-1.35156 -2.21875 0.0244141 ... 4.5 -1.10938 -4]\n",
            "  [-2.59375 -2.07812 0.945312 ... 0.109375 1.09375 -3.84375]]\n",
            "\n",
            " [[3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [1.21094 -0.117188 2.78125 ... -1.74219 -0.546875 -0.703125]\n",
            "  [-0.132812 2.09375 1.95312 ... -0.820312 0.765625 -1.49219]\n",
            "  ...\n",
            "  [-2.89062 2.59375 1.40625 ... -0.988281 0.367188 3.79688]\n",
            "  [1.40625 0.457031 0.671875 ... 0.6875 3.09375 -3.5]\n",
            "  [-3.40625 0.0476074 -0.291016 ... 1.02344 0.332031 -3.1875]]]\n",
            "lnx=[[[1.04688 -0.921875 1.39062 ... -0.474609 -0.090332 -0.703125]\n",
            "  [1.04688 -0.921875 1.39062 ... -0.474609 -0.090332 -0.703125]\n",
            "  [1.03125 -0.921875 1.38281 ... -0.470703 -0.0869141 -0.707031]\n",
            "  ...\n",
            "  [0.306641 -1.03125 1.28906 ... 0.351562 0.453125 0.886719]\n",
            "  [0.84375 -1.64062 0.976562 ... 0.941406 0.859375 0.494141]\n",
            "  [-1.24219 -1.77344 1.83594 ... 0.24707 0.458984 -1.32031]]\n",
            "\n",
            " [[-0.275391 0.828125 0.427734 ... -0.125977 -1.28125 0.625]\n",
            "  [-0.136719 0.753906 1.42969 ... -0.146484 -0.191406 1.10938]\n",
            "  [-0.205078 0.195312 0.535156 ... 0.273438 -0.582031 0.060791]\n",
            "  ...\n",
            "  [1.625 0.917969 0.00378418 ... 0.152344 0.699219 -0.691406]\n",
            "  [-0.285156 0.0898438 0.427734 ... 0.839844 -0.143555 -0.691406]\n",
            "  [-1.52344 1.07031 0.458984 ... -0.40625 0.1875 -1.92188]]\n",
            "\n",
            " [[1.04688 -0.921875 1.39062 ... -0.474609 -0.090332 -0.703125]\n",
            "  [1.04688 -0.921875 1.39062 ... -0.474609 -0.090332 -0.703125]\n",
            "  [1.03125 -0.921875 1.38281 ... -0.470703 -0.0869141 -0.707031]\n",
            "  ...\n",
            "  [0.992188 -0.679688 -0.425781 ... -0.310547 -1.72656 -0.316406]\n",
            "  [-1.66406 -0.390625 0.585938 ... -0.628906 -0.6875 -0.652344]\n",
            "  [-0.78125 -1.77344 0.279297 ... -0.0854492 -0.570312 -0.691406]]\n",
            "\n",
            " [[1.04688 -0.921875 1.39062 ... -0.474609 -0.090332 -0.703125]\n",
            "  [1.04688 -0.921875 1.39062 ... -0.474609 -0.090332 -0.703125]\n",
            "  [1.03125 -0.921875 1.38281 ... -0.470703 -0.0869141 -0.707031]\n",
            "  ...\n",
            "  [-0.213867 -0.0693359 0.130859 ... -0.0947266 -0.769531 -1.58594]\n",
            "  [-0.613281 -1.00781 0.0111084 ... 2.04688 -0.503906 -1.82031]\n",
            "  [-1.14844 -0.917969 0.417969 ... 0.0483398 0.482422 -1.69531]]\n",
            "\n",
            " [[1.04688 -0.921875 1.39062 ... -0.474609 -0.090332 -0.703125]\n",
            "  [0.46875 -0.0454102 1.07812 ... -0.675781 -0.211914 -0.271484]\n",
            "  [-0.0541992 0.855469 0.796875 ... -0.333984 0.3125 -0.609375]\n",
            "  ...\n",
            "  [-1.42969 1.28125 0.695312 ... -0.490234 0.181641 1.88281]\n",
            "  [0.683594 0.22168 0.326172 ... 0.333984 1.5 -1.70312]\n",
            "  [-1.59375 0.0222168 -0.135742 ... 0.478516 0.155273 -1.49219]]]\n",
            "attention_lnx=[[[0.523438 0.149414 0.0262451 ... 0.945312 -0.894531 0.494141]\n",
            "  [0.523438 0.149414 0.0262451 ... 0.945312 -0.894531 0.494141]\n",
            "  [0.527344 0.150391 0.0273438 ... 0.941406 -0.894531 0.492188]\n",
            "  ...\n",
            "  [0.988281 0.0141602 -0.314453 ... 0.15332 -0.816406 0.582031]\n",
            "  [0.890625 -0.169922 -0.470703 ... 0.0888672 -0.730469 0.392578]\n",
            "  [1.02344 0.111816 -0.230469 ... 0.259766 -0.703125 0.333984]]\n",
            "\n",
            " [[0.147461 -2.10938 -0.3125 ... -0.460938 -0.894531 0.65625]\n",
            "  [-0.0563965 -1.34375 -0.474609 ... -0.636719 -2.03125 0.9375]\n",
            "  [-0.306641 -0.828125 -0.753906 ... -0.605469 -1.29688 0.201172]\n",
            "  ...\n",
            "  [0.243164 0.322266 -0.910156 ... 0.03125 -0.566406 0.482422]\n",
            "  [-0.365234 0.789062 -0.078125 ... -0.296875 -0.726562 0.832031]\n",
            "  [-0.298828 0.601562 -0.245117 ... -0.257812 -0.5625 0.371094]]\n",
            "\n",
            " [[0.523438 0.149414 0.0262451 ... 0.945312 -0.894531 0.494141]\n",
            "  [0.523438 0.149414 0.0262451 ... 0.945312 -0.894531 0.494141]\n",
            "  [0.527344 0.150391 0.0273438 ... 0.941406 -0.894531 0.492188]\n",
            "  ...\n",
            "  [1.10938 0.220703 -0.318359 ... 0.322266 -0.683594 0.441406]\n",
            "  [1.14062 0.060791 -0.255859 ... -0.157227 -0.96875 0.660156]\n",
            "  [1.11719 0.349609 0.181641 ... 0.102539 -0.699219 0.462891]]\n",
            "\n",
            " [[0.523438 0.149414 0.0262451 ... 0.945312 -0.894531 0.494141]\n",
            "  [0.523438 0.149414 0.0262451 ... 0.945312 -0.894531 0.494141]\n",
            "  [0.527344 0.150391 0.0273438 ... 0.941406 -0.894531 0.492188]\n",
            "  ...\n",
            "  [0.898438 -0.304688 -0.133789 ... -0.131836 -0.890625 0.609375]\n",
            "  [0.773438 0.228516 -0.0473633 ... -0.216797 -1.28125 0.613281]\n",
            "  [1.02344 0.419922 0.11084 ... -0.105469 -1.08594 0.476562]]\n",
            "\n",
            " [[0.523438 0.149414 0.0262451 ... 0.945312 -0.894531 0.494141]\n",
            "  [1.91406 -0.765625 -0.228516 ... 0.114746 -0.855469 0.496094]\n",
            "  [1.48438 -0.675781 0.201172 ... 0.245117 -1.19531 0.5]\n",
            "  ...\n",
            "  [0.808594 0.621094 0.0371094 ... -0.585938 -1.19531 0.304688]\n",
            "  [0.875 0.332031 -0.0332031 ... -0.392578 -1.02344 0.19043]\n",
            "  [0.953125 0.792969 0.0449219 ... -0.263672 -0.632812 -0.146484]]]\n",
            "attn_output=[[[1.17188 -0.828125 1.33594 ... -0.142578 -0.378906 -0.507812]\n",
            "  [1.17188 -0.828125 1.33594 ... -0.142578 -0.378906 -0.507812]\n",
            "  [1.15625 -0.828125 1.32812 ... -0.141602 -0.375 -0.515625]\n",
            "  ...\n",
            "  [0.679688 -0.980469 1.10938 ... 0.396484 0.117188 1.07812]\n",
            "  [1.17969 -1.64062 0.738281 ... 0.9375 0.519531 0.636719]\n",
            "  [-0.800781 -1.66406 1.67969 ... 0.337891 0.169922 -1.14062]]\n",
            "\n",
            " [[-0.207031 0.0727539 0.294922 ... -0.269531 -1.49219 0.800781]\n",
            "  [-0.148438 0.207031 1.15625 ... -0.371094 -0.925781 1.375]\n",
            "  [-0.314453 -0.143555 0.205078 ... 0.0185547 -1.0625 0.136719]\n",
            "  ...\n",
            "  [1.66406 1.02344 -0.416016 ... 0.15918 0.40625 -0.4375]\n",
            "  [-0.427734 0.429688 0.371094 ... 0.664062 -0.453125 -0.291016]\n",
            "  [-1.59375 1.29688 0.326172 ... -0.507812 -0.0791016 -1.67188]]\n",
            "\n",
            " [[1.17188 -0.828125 1.33594 ... -0.142578 -0.378906 -0.507812]\n",
            "  [1.17188 -0.828125 1.33594 ... -0.142578 -0.378906 -0.507812]\n",
            "  [1.15625 -0.828125 1.32812 ... -0.141602 -0.375 -0.515625]\n",
            "  ...\n",
            "  [1.42969 -0.542969 -0.542969 ... -0.150391 -1.9375 -0.102539]\n",
            "  [-1.07812 -0.345703 0.445312 ... -0.671875 -1.08594 -0.328125]\n",
            "  [-0.271484 -1.53125 0.341797 ... -0.0380859 -0.832031 -0.460938]]\n",
            "\n",
            " [[1.17188 -0.828125 1.33594 ... -0.142578 -0.378906 -0.507812]\n",
            "  [1.17188 -0.828125 1.33594 ... -0.142578 -0.378906 -0.507812]\n",
            "  [1.15625 -0.828125 1.32812 ... -0.141602 -0.375 -0.515625]\n",
            "  ...\n",
            "  [0.1875 -0.199219 0.0668945 ... -0.148438 -1.125 -1.25]\n",
            "  [-0.251953 -0.867188 -0.0100098 ... 1.86719 -1.04688 -1.47656]\n",
            "  [-0.664062 -0.699219 0.445312 ... 0.00164795 0.0032959 -1.42188]]\n",
            "\n",
            " [[1.17188 -0.828125 1.33594 ... -0.142578 -0.378906 -0.507812]\n",
            "  [1.15625 -0.326172 0.945312 ... -0.601562 -0.519531 -0.0766602]\n",
            "  [0.519531 0.546875 0.832031 ... -0.22168 -0.166016 -0.382812]\n",
            "  ...\n",
            "  [-0.980469 1.51562 0.679688 ... -0.742188 -0.390625 1.92969]\n",
            "  [1.07031 0.371094 0.300781 ... 0.138672 0.972656 -1.55469]\n",
            "  [-1.10156 0.378906 -0.11084 ... 0.341797 -0.135742 -1.5]]]\n",
            "next_layer_addition_dropped_out=[[[3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.6875 -2.8125 4.5625 ... -1.125 -1.85938 -1.91406]\n",
            "  ...\n",
            "  [1.82031 -2.09375 3.34375 ... -0.523438 -0.0371094 3.28125]\n",
            "  [2.79688 -4.625 1.4375 ... 1.64062 1.88281 1.75]\n",
            "  [-2.14062 -3.89062 4.1875 ... -0.0234375 1.3125 -2.35938]]\n",
            "\n",
            " [[-0.6875 0.824219 0.933594 ... -0.554688 -5.3125 3.39062]\n",
            "  [-0.808594 0.769531 2.59375 ... -1.53125 -2.73438 4.03125]\n",
            "  [-0.328125 0.273438 0.539062 ... 0.441406 -1.47656 -1]\n",
            "  ...\n",
            "  [3.71875 3.84375 -1.17188 ... 0.804688 1.69531 -0.447266]\n",
            "  [-1.32812 1.27344 0.652344 ... 0.683594 -1.625 -0.132812]\n",
            "  [-2.75 2.65625 2.42188 ... -1.5 -0.249023 -4.09375]]\n",
            "\n",
            " [[3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.6875 -2.8125 4.5625 ... -1.125 -1.85938 -1.91406]\n",
            "  ...\n",
            "  [2.9375 -1.39844 -2.625 ... -0.106445 -4.75 -0.00488281]\n",
            "  [-2.45312 -1.11719 0.710938 ... -2.14062 -3.59375 -1.26562]\n",
            "  [-0.8125 -3.45312 1.10938 ... -1.625 -1.35156 -1.07031]]\n",
            "\n",
            " [[3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.6875 -2.8125 4.5625 ... -1.125 -1.85938 -1.91406]\n",
            "  ...\n",
            "  [0.875 -0.00976562 0.925781 ... 0.15625 -2.57812 -2.10938]\n",
            "  [-0.660156 -2.04688 0.703125 ... 2.42188 -1.97656 -2.0625]\n",
            "  [-2.15625 -0.945312 1.48438 ... -1.09375 0.457031 -3.51562]]\n",
            "\n",
            " [[3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [2.28125 -0.300781 3.10938 ... -2.35938 -0.695312 0.222656]\n",
            "  [1 1.10156 2.35938 ... -0.177734 -1.35938 -1.28125]\n",
            "  ...\n",
            "  [-2.14062 3.14062 1.13281 ... -1.67969 -0.539062 4.46875]\n",
            "  [1.57812 1.10156 0.523438 ... 0.00976562 2.375 -3.625]\n",
            "  [-2.8125 1.57812 1.25 ... -0.515625 -0.431641 -2.67188]]]\n",
            "inputs=[[[3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.6875 -2.8125 4.5625 ... -1.125 -1.85938 -1.91406]\n",
            "  ...\n",
            "  [1.82031 -2.09375 3.34375 ... -0.523438 -0.0371094 3.28125]\n",
            "  [2.79688 -4.625 1.4375 ... 1.64062 1.88281 1.75]\n",
            "  [-2.14062 -3.89062 4.1875 ... -0.0234375 1.3125 -2.35938]]\n",
            "\n",
            " [[-0.6875 0.824219 0.933594 ... -0.554688 -5.3125 3.39062]\n",
            "  [-0.808594 0.769531 2.59375 ... -1.53125 -2.73438 4.03125]\n",
            "  [-0.328125 0.273438 0.539062 ... 0.441406 -1.47656 -1]\n",
            "  ...\n",
            "  [3.71875 3.84375 -1.17188 ... 0.804688 1.69531 -0.447266]\n",
            "  [-1.32812 1.27344 0.652344 ... 0.683594 -1.625 -0.132812]\n",
            "  [-2.75 2.65625 2.42188 ... -1.5 -0.249023 -4.09375]]\n",
            "\n",
            " [[3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.6875 -2.8125 4.5625 ... -1.125 -1.85938 -1.91406]\n",
            "  ...\n",
            "  [2.9375 -1.39844 -2.625 ... -0.106445 -4.75 -0.00488281]\n",
            "  [-2.45312 -1.11719 0.710938 ... -2.14062 -3.59375 -1.26562]\n",
            "  [-0.8125 -3.45312 1.10938 ... -1.625 -1.35156 -1.07031]]\n",
            "\n",
            " [[3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.6875 -2.8125 4.5625 ... -1.125 -1.85938 -1.91406]\n",
            "  ...\n",
            "  [0.875 -0.00976562 0.925781 ... 0.15625 -2.57812 -2.10938]\n",
            "  [-0.660156 -2.04688 0.703125 ... 2.42188 -1.97656 -2.0625]\n",
            "  [-2.15625 -0.945312 1.48438 ... -1.09375 0.457031 -3.51562]]\n",
            "\n",
            " [[3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [2.28125 -0.300781 3.10938 ... -2.35938 -0.695312 0.222656]\n",
            "  [1 1.10156 2.35938 ... -0.177734 -1.35938 -1.28125]\n",
            "  ...\n",
            "  [-2.14062 3.14062 1.13281 ... -1.67969 -0.539062 4.46875]\n",
            "  [1.57812 1.10156 0.523438 ... 0.00976562 2.375 -3.625]\n",
            "  [-2.8125 1.57812 1.25 ... -0.515625 -0.431641 -2.67188]]]\n",
            "lnx=[[[1.19531 -0.894531 1.45312 ... -0.353516 -0.597656 -0.601562]\n",
            "  [1.19531 -0.894531 1.45312 ... -0.353516 -0.597656 -0.601562]\n",
            "  [1.17188 -0.898438 1.45312 ... -0.359375 -0.59375 -0.609375]\n",
            "  ...\n",
            "  [0.6875 -0.789062 1.26562 ... -0.197266 -0.0140381 1.24219]\n",
            "  [1.11719 -1.85156 0.574219 ... 0.65625 0.753906 0.699219]\n",
            "  [-0.792969 -1.44531 1.55469 ... -0.00866699 0.486328 -0.875]]\n",
            "\n",
            " [[-0.222656 0.267578 0.302734 ... -0.179688 -1.71875 1.10156]\n",
            "  [-0.291016 0.277344 0.9375 ... -0.554688 -0.988281 1.45312]\n",
            "  [-0.125 0.104492 0.206055 ... 0.167969 -0.5625 -0.380859]\n",
            "  ...\n",
            "  [1.64062 1.69531 -0.515625 ... 0.355469 0.746094 -0.197266]\n",
            "  [-0.550781 0.527344 0.271484 ... 0.283203 -0.675781 -0.0551758]\n",
            "  [-1.1875 1.14844 1.04688 ... -0.648438 -0.107422 -1.76562]]\n",
            "\n",
            " [[1.19531 -0.894531 1.45312 ... -0.353516 -0.597656 -0.601562]\n",
            "  [1.19531 -0.894531 1.45312 ... -0.353516 -0.597656 -0.601562]\n",
            "  [1.17188 -0.898438 1.45312 ... -0.359375 -0.59375 -0.609375]\n",
            "  ...\n",
            "  [1.25 -0.597656 -1.11719 ... -0.0454102 -2.03125 -0.00209045]\n",
            "  [-1.05469 -0.478516 0.304688 ... -0.917969 -1.53906 -0.542969]\n",
            "  [-0.324219 -1.38281 0.443359 ... -0.648438 -0.539062 -0.427734]]\n",
            "\n",
            " [[1.19531 -0.894531 1.45312 ... -0.353516 -0.597656 -0.601562]\n",
            "  [1.19531 -0.894531 1.45312 ... -0.353516 -0.597656 -0.601562]\n",
            "  [1.17188 -0.898438 1.45312 ... -0.359375 -0.59375 -0.609375]\n",
            "  ...\n",
            "  [0.369141 -0.00411987 0.390625 ... 0.065918 -1.08594 -0.886719]\n",
            "  [-0.28125 -0.871094 0.298828 ... 1.03125 -0.839844 -0.878906]\n",
            "  [-0.867188 -0.380859 0.597656 ... -0.439453 0.183594 -1.41406]]\n",
            "\n",
            " [[1.19531 -0.894531 1.45312 ... -0.353516 -0.597656 -0.601562]\n",
            "  [0.824219 -0.108887 1.125 ... -0.855469 -0.251953 0.0805664]\n",
            "  [0.373047 0.412109 0.882812 ... -0.0664062 -0.507812 -0.478516]\n",
            "  ...\n",
            "  [-0.953125 1.39844 0.503906 ... -0.746094 -0.239258 1.98438]\n",
            "  [0.703125 0.490234 0.232422 ... 0.0043335 1.05469 -1.60938]\n",
            "  [-1.20312 0.675781 0.535156 ... -0.220703 -0.18457 -1.14062]]]\n",
            "attention_lnx=[[[-1.14844 0.226562 -0.53125 ... -0.314453 0.0366211 -0.746094]\n",
            "  [-1.14844 0.226562 -0.53125 ... -0.314453 0.0366211 -0.746094]\n",
            "  [-1.14844 0.217773 -0.53125 ... -0.322266 0.0415039 -0.746094]\n",
            "  ...\n",
            "  [-1.19531 0.507812 -0.394531 ... -0.275391 -0.00485229 -0.511719]\n",
            "  [-1.32031 0.429688 -0.457031 ... 0.0256348 0.431641 -0.347656]\n",
            "  [-1.25 0.392578 -0.519531 ... -0.107422 0.0529785 -0.589844]]\n",
            "\n",
            " [[1.07031 -0.621094 -0.0150757 ... -0.208984 0.996094 -0.130859]\n",
            "  [-0.197266 -0.566406 -0.78125 ... -0.480469 1.21875 -0.275391]\n",
            "  [-0.261719 -0.878906 -0.427734 ... -0.941406 1.32812 -0.00787354]\n",
            "  ...\n",
            "  [-1.34375 0.363281 -0.78125 ... -1.10938 1.46094 0.237305]\n",
            "  [-0.90625 0.125977 -0.542969 ... -1.19531 0.945312 0.0810547]\n",
            "  [-0.933594 0.163086 -0.59375 ... -0.914062 0.9375 0.601562]]\n",
            "\n",
            " [[-1.14844 0.226562 -0.53125 ... -0.314453 0.0366211 -0.746094]\n",
            "  [-1.14844 0.226562 -0.53125 ... -0.314453 0.0366211 -0.746094]\n",
            "  [-1.14844 0.217773 -0.53125 ... -0.322266 0.0415039 -0.746094]\n",
            "  ...\n",
            "  [-0.90625 0.570312 -0.119141 ... -0.660156 -0.457031 -0.196289]\n",
            "  [-1.38281 0.335938 -0.259766 ... -0.703125 -0.535156 0.0615234]\n",
            "  [-1.26562 0.267578 -0.373047 ... -0.636719 -0.597656 0.251953]]\n",
            "\n",
            " [[-1.14844 0.226562 -0.53125 ... -0.314453 0.0366211 -0.746094]\n",
            "  [-1.14844 0.226562 -0.53125 ... -0.314453 0.0366211 -0.746094]\n",
            "  [-1.14844 0.217773 -0.53125 ... -0.322266 0.0415039 -0.746094]\n",
            "  ...\n",
            "  [-0.957031 0.0864258 -0.458984 ... -0.972656 -0.601562 -0.503906]\n",
            "  [-1.17188 0.365234 -0.328125 ... -0.984375 -0.652344 -0.145508]\n",
            "  [-1.0625 0.240234 -0.441406 ... -1.02344 -0.992188 0.0839844]]\n",
            "\n",
            " [[-1.14844 0.226562 -0.53125 ... -0.314453 0.0366211 -0.746094]\n",
            "  [-0.984375 0.660156 -0.4375 ... -0.213867 -0.279297 -0.328125]\n",
            "  [-1.59375 0.414062 -0.410156 ... -0.205078 0.0893555 -0.243164]\n",
            "  ...\n",
            "  [-1.32031 0.0422363 -0.275391 ... -1.25781 -0.429688 -0.476562]\n",
            "  [-1.17188 -0.140625 -0.0402832 ... -1.53906 -0.324219 -0.349609]\n",
            "  [-1.35938 0.00582886 0.0888672 ... -1.28906 -0.6875 -0.261719]]]\n",
            "attn_output=[[[0.792969 -0.789062 1.22656 ... -0.433594 -0.558594 -0.804688]\n",
            "  [0.792969 -0.789062 1.22656 ... -0.433594 -0.558594 -0.804688]\n",
            "  [0.773438 -0.789062 1.22656 ... -0.441406 -0.554688 -0.8125]\n",
            "  ...\n",
            "  [0.225586 -0.574219 1.0625 ... -0.289062 -0.0151367 1]\n",
            "  [0.558594 -1.58594 0.371094 ... 0.628906 0.875 0.53125]\n",
            "  [-1.20312 -1.23438 1.29688 ... -0.0463867 0.482422 -1.04688]]\n",
            "\n",
            " [[0.119629 0.0634766 0.287109 ... -0.239258 -1.35156 1.02344]\n",
            "  [-0.347656 0.0703125 0.625 ... -0.695312 -0.523438 1.29688]\n",
            "  [-0.216797 -0.22168 0.0407715 ... -0.183594 -0.0544434 -0.369141]\n",
            "  ...\n",
            "  [1.02344 1.80469 -0.839844 ... -0.130859 1.35938 -0.090332]\n",
            "  [-0.90625 0.566406 0.0444336 ... -0.208008 -0.275391 -0.0209961]\n",
            "  [-1.54688 1.17969 0.765625 ... -1.00781 0.289062 -1.46094]]\n",
            "\n",
            " [[0.792969 -0.789062 1.22656 ... -0.433594 -0.558594 -0.804688]\n",
            "  [0.792969 -0.789062 1.22656 ... -0.433594 -0.558594 -0.804688]\n",
            "  [0.773438 -0.789062 1.22656 ... -0.441406 -0.554688 -0.8125]\n",
            "  ...\n",
            "  [0.824219 -0.335938 -1.11719 ... -0.310547 -2.10938 -0.081543]\n",
            "  [-1.5625 -0.318359 0.183594 ... -1.15625 -1.6875 -0.490234]\n",
            "  [-0.792969 -1.21875 0.28125 ... -0.863281 -0.742188 -0.3125]]\n",
            "\n",
            " [[0.792969 -0.789062 1.22656 ... -0.433594 -0.558594 -0.804688]\n",
            "  [0.792969 -0.789062 1.22656 ... -0.433594 -0.558594 -0.804688]\n",
            "  [0.773438 -0.789062 1.22656 ... -0.441406 -0.554688 -0.8125]\n",
            "  ...\n",
            "  [-0.0327148 0.0306396 0.186523 ... -0.326172 -1.27344 -1.04688]\n",
            "  [-0.742188 -0.683594 0.152344 ... 0.582031 -1.0625 -0.894531]\n",
            "  [-1.23438 -0.271484 0.400391 ... -0.8125 -0.206055 -1.32031]]\n",
            "\n",
            " [[0.792969 -0.789062 1.22656 ... -0.433594 -0.558594 -0.804688]\n",
            "  [0.449219 0.124023 0.921875 ... -0.890625 -0.337891 -0.036377]\n",
            "  [-0.213867 0.546875 0.703125 ... -0.137695 -0.457031 -0.546875]\n",
            "  ...\n",
            "  [-1.48438 1.35938 0.367188 ... -1.25781 -0.414062 1.71094]\n",
            "  [0.173828 0.410156 0.207031 ... -0.652344 0.878906 -1.70312]\n",
            "  [-1.71875 0.652344 0.550781 ... -0.742188 -0.460938 -1.21094]]]\n",
            "next_layer_addition_dropped_out=[[[1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.84375 -3.0625 2.85938 ... -1.29688 -1.14062 -2.45312]\n",
            "  ...\n",
            "  [0.972656 -1.20312 2.3125 ... -1.32812 1.30469 3.3125]\n",
            "  [0.625 -5.25 1.89844 ... 1.76562 3.5625 1.07812]\n",
            "  [-3.53125 -3.51562 3.57812 ... -1.13281 1.72656 -3.28125]]\n",
            "\n",
            " [[-0.0566406 0.152344 0.1875 ... 0.03125 -4.1875 3.125]\n",
            "  [-0.4375 -0.0878906 1.76562 ... -1.21875 -0.648438 4.15625]\n",
            "  [-0.804688 -1.125 -0.376953 ... -0.373047 -0.554688 -0.710938]\n",
            "  ...\n",
            "  [3.53125 3.53125 -2.96875 ... -0.24707 2.34375 -0.671875]\n",
            "  [-1.64062 2.4375 -0.105469 ... 0.164062 -2.03125 -1.92188]\n",
            "  [-3.4375 4.125 2.29688 ... -3.21875 0.757812 -2.9375]]\n",
            "\n",
            " [[1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.84375 -3.0625 2.85938 ... -1.29688 -1.14062 -2.45312]\n",
            "  ...\n",
            "  [2.01562 -1.5625 -2.78125 ... -0.539062 -4.6875 -0.328125]\n",
            "  [-3.54688 -1.36719 0.632812 ... -3.21875 -3.04688 -1.39062]\n",
            "  [-2.14062 -2.25 1.26562 ... -3.20312 -1.39844 -1.90625]]\n",
            "\n",
            " [[1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.84375 -3.0625 2.85938 ... -1.29688 -1.14062 -2.45312]\n",
            "  ...\n",
            "  [0.308594 0.773438 0.628906 ... 0.105469 -2.5 -3.84375]\n",
            "  [-2.8125 -1.33594 1.17969 ... 1.40625 -1.03906 -3.40625]\n",
            "  [-3.26562 0.4375 1.59375 ... -2.53125 -0.179688 -4.25]]\n",
            "\n",
            " [[1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.90625 -0.351562 1.57031 ... -1.65625 -0.167969 -0.0351562]\n",
            "  [0.421875 0.96875 2.20312 ... 0.214844 -1.01562 -2.6875]\n",
            "  ...\n",
            "  [-4.03125 3.8125 1.03125 ... -2.40625 -0.535156 4]\n",
            "  [0.597656 0.421875 1.17188 ... -0.550781 2.17188 -4.59375]\n",
            "  [-4.90625 3.125 2.28125 ... -2.34375 -1.74219 -3.39062]]]\n",
            "inputs=[[[1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.84375 -3.0625 2.85938 ... -1.29688 -1.14062 -2.45312]\n",
            "  ...\n",
            "  [0.972656 -1.20312 2.3125 ... -1.32812 1.30469 3.3125]\n",
            "  [0.625 -5.25 1.89844 ... 1.76562 3.5625 1.07812]\n",
            "  [-3.53125 -3.51562 3.57812 ... -1.13281 1.72656 -3.28125]]\n",
            "\n",
            " [[-0.0566406 0.152344 0.1875 ... 0.03125 -4.1875 3.125]\n",
            "  [-0.4375 -0.0878906 1.76562 ... -1.21875 -0.648438 4.15625]\n",
            "  [-0.804688 -1.125 -0.376953 ... -0.373047 -0.554688 -0.710938]\n",
            "  ...\n",
            "  [3.53125 3.53125 -2.96875 ... -0.24707 2.34375 -0.671875]\n",
            "  [-1.64062 2.4375 -0.105469 ... 0.164062 -2.03125 -1.92188]\n",
            "  [-3.4375 4.125 2.29688 ... -3.21875 0.757812 -2.9375]]\n",
            "\n",
            " [[1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.84375 -3.0625 2.85938 ... -1.29688 -1.14062 -2.45312]\n",
            "  ...\n",
            "  [2.01562 -1.5625 -2.78125 ... -0.539062 -4.6875 -0.328125]\n",
            "  [-3.54688 -1.36719 0.632812 ... -3.21875 -3.04688 -1.39062]\n",
            "  [-2.14062 -2.25 1.26562 ... -3.20312 -1.39844 -1.90625]]\n",
            "\n",
            " [[1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.84375 -3.0625 2.85938 ... -1.29688 -1.14062 -2.45312]\n",
            "  ...\n",
            "  [0.308594 0.773438 0.628906 ... 0.105469 -2.5 -3.84375]\n",
            "  [-2.8125 -1.33594 1.17969 ... 1.40625 -1.03906 -3.40625]\n",
            "  [-3.26562 0.4375 1.59375 ... -2.53125 -0.179688 -4.25]]\n",
            "\n",
            " [[1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.90625 -0.351562 1.57031 ... -1.65625 -0.167969 -0.0351562]\n",
            "  [0.421875 0.96875 2.20312 ... 0.214844 -1.01562 -2.6875]\n",
            "  ...\n",
            "  [-4.03125 3.8125 1.03125 ... -2.40625 -0.535156 4]\n",
            "  [0.597656 0.421875 1.17188 ... -0.550781 2.17188 -4.59375]\n",
            "  [-4.90625 3.125 2.28125 ... -2.34375 -1.74219 -3.39062]]]\n",
            "lnx=[[[0.574219 -0.910156 0.855469 ... -0.378906 -0.34375 -0.730469]\n",
            "  [0.574219 -0.910156 0.855469 ... -0.378906 -0.34375 -0.730469]\n",
            "  [0.546875 -0.910156 0.851562 ... -0.384766 -0.339844 -0.730469]\n",
            "  ...\n",
            "  [0.339844 -0.419922 0.808594 ... -0.464844 0.455078 1.15625]\n",
            "  [0.230469 -1.9375 0.699219 ... 0.652344 1.3125 0.398438]\n",
            "  [-1.21094 -1.21094 1.22656 ... -0.388672 0.59375 -1.125]]\n",
            "\n",
            " [[-0.017334 0.0466309 0.057373 ... 0.00958252 -1.28125 0.957031]\n",
            "  [-0.147461 -0.0296631 0.597656 ... -0.412109 -0.21875 1.40625]\n",
            "  [-0.285156 -0.398438 -0.133789 ... -0.131836 -0.196289 -0.251953]\n",
            "  ...\n",
            "  [1.46094 1.46094 -1.22656 ... -0.102051 0.96875 -0.277344]\n",
            "  [-0.625 0.929688 -0.0402832 ... 0.0625 -0.777344 -0.734375]\n",
            "  [-1.39062 1.67188 0.929688 ... -1.30469 0.306641 -1.1875]]\n",
            "\n",
            " [[0.574219 -0.910156 0.855469 ... -0.378906 -0.34375 -0.730469]\n",
            "  [0.574219 -0.910156 0.855469 ... -0.378906 -0.34375 -0.730469]\n",
            "  [0.546875 -0.910156 0.851562 ... -0.384766 -0.339844 -0.730469]\n",
            "  ...\n",
            "  [0.785156 -0.609375 -1.08594 ... -0.209961 -1.82812 -0.12793]\n",
            "  [-1.38281 -0.535156 0.24707 ... -1.25781 -1.1875 -0.542969]\n",
            "  [-0.773438 -0.8125 0.457031 ... -1.15625 -0.503906 -0.6875]]\n",
            "\n",
            " [[0.574219 -0.910156 0.855469 ... -0.378906 -0.34375 -0.730469]\n",
            "  [0.574219 -0.910156 0.855469 ... -0.378906 -0.34375 -0.730469]\n",
            "  [0.546875 -0.910156 0.851562 ... -0.384766 -0.339844 -0.730469]\n",
            "  ...\n",
            "  [0.118652 0.296875 0.241211 ... 0.0405273 -0.960938 -1.47656]\n",
            "  [-1.10156 -0.523438 0.462891 ... 0.550781 -0.40625 -1.33594]\n",
            "  [-1.19531 0.160156 0.582031 ... -0.925781 -0.0654297 -1.55469]]\n",
            "\n",
            " [[0.574219 -0.910156 0.855469 ... -0.378906 -0.34375 -0.730469]\n",
            "  [0.640625 -0.118164 0.527344 ... -0.558594 -0.0566406 -0.0118408]\n",
            "  [0.146484 0.335938 0.765625 ... 0.074707 -0.351562 -0.933594]\n",
            "  ...\n",
            "  [-1.66406 1.57812 0.425781 ... -0.992188 -0.220703 1.64844]\n",
            "  [0.244141 0.171875 0.478516 ... -0.224609 0.886719 -1.875]\n",
            "  [-1.92969 1.22656 0.894531 ... -0.921875 -0.683594 -1.32812]]]\n",
            "attention_lnx=[[[-0.0167236 0.451172 -0.53125 ... 0.0473633 -0.414062 -0.511719]\n",
            "  [-0.0167236 0.451172 -0.53125 ... 0.0473633 -0.414062 -0.511719]\n",
            "  [-0.0133057 0.457031 -0.535156 ... 0.0478516 -0.414062 -0.507812]\n",
            "  ...\n",
            "  [0.103027 0.289062 -0.773438 ... 0.267578 -0.714844 -0.427734]\n",
            "  [-0.0306396 0.205078 -0.648438 ... 0.137695 -0.757812 -0.173828]\n",
            "  [0.0732422 0.384766 -0.949219 ... 0.239258 -0.632812 -0.214844]]\n",
            "\n",
            " [[0.131836 1.36719 -0.265625 ... -1 0.257812 0.0957031]\n",
            "  [-0.246094 1.58594 -0.628906 ... -1.00781 0.373047 -0.394531]\n",
            "  [-0.131836 1.21094 -0.640625 ... -0.636719 1.39844 -0.160156]\n",
            "  ...\n",
            "  [0.0849609 1.72656 -1.57812 ... -1.10156 0.84375 0.53125]\n",
            "  [-0.0878906 1.75 -1.65625 ... -1.20312 0.652344 0.707031]\n",
            "  [-0.134766 1.59375 -1.32031 ... -0.988281 0.566406 0.417969]]\n",
            "\n",
            " [[-0.0167236 0.451172 -0.53125 ... 0.0473633 -0.414062 -0.511719]\n",
            "  [-0.0167236 0.451172 -0.53125 ... 0.0473633 -0.414062 -0.511719]\n",
            "  [-0.0133057 0.457031 -0.535156 ... 0.0478516 -0.414062 -0.507812]\n",
            "  ...\n",
            "  [-0.046875 0.0859375 -1.67969 ... 0.0825195 -0.147461 0.237305]\n",
            "  [0.298828 0.324219 -1.64844 ... 0.162109 -0.220703 0.0732422]\n",
            "  [-0.00582886 0.271484 -1.86719 ... 0.0458984 -0.421875 0.0454102]]\n",
            "\n",
            " [[-0.0167236 0.451172 -0.53125 ... 0.0473633 -0.414062 -0.511719]\n",
            "  [-0.0167236 0.451172 -0.53125 ... 0.0473633 -0.414062 -0.511719]\n",
            "  [-0.0133057 0.457031 -0.535156 ... 0.0478516 -0.414062 -0.507812]\n",
            "  ...\n",
            "  [0.503906 0.617188 -1.85156 ... 0.355469 -0.197266 0.119141]\n",
            "  [0.777344 0.628906 -2.26562 ... 0.189453 -0.152344 0.421875]\n",
            "  [0.365234 0.363281 -1.82031 ... 0.134766 -0.546875 0.118164]]\n",
            "\n",
            " [[-0.0167236 0.451172 -0.53125 ... 0.0473633 -0.414062 -0.511719]\n",
            "  [-0.0458984 0.472656 -0.90625 ... -0.0429688 0.118164 -0.390625]\n",
            "  [-0.316406 0.828125 -0.730469 ... 0.0402832 0.131836 -0.486328]\n",
            "  ...\n",
            "  [0.285156 0.726562 -1.96094 ... -0.0383301 -0.0441895 -0.335938]\n",
            "  [0.178711 0.765625 -1.99219 ... 0.0795898 0.435547 -0.104492]\n",
            "  [0.251953 0.255859 -2.15625 ... -0.155273 0.124023 0.0240479]]]\n",
            "attn_output=[[[0.546875 -0.746094 0.667969 ... -0.349609 -0.447266 -0.84375]\n",
            "  [0.546875 -0.746094 0.667969 ... -0.349609 -0.447266 -0.84375]\n",
            "  [0.523438 -0.742188 0.664062 ... -0.355469 -0.443359 -0.84375]\n",
            "  ...\n",
            "  [0.363281 -0.308594 0.519531 ... -0.357422 0.198242 0.972656]\n",
            "  [0.208984 -1.77344 0.439453 ... 0.667969 0.984375 0.318359]\n",
            "  [-1.14062 -1.03125 0.863281 ... -0.292969 0.359375 -1.14844]]\n",
            "\n",
            " [[0.0219727 0.443359 -0.0228271 ... -0.283203 -1.14844 0.9375]\n",
            "  [-0.22168 0.486328 0.369141 ... -0.722656 -0.0893555 1.21875]\n",
            "  [-0.318359 0.0291748 -0.345703 ... -0.341797 0.285156 -0.294922]\n",
            "  ...\n",
            "  [1.42969 2.07812 -1.79688 ... -0.535156 1.25781 -0.0556641]\n",
            "  [-0.636719 1.54688 -0.648438 ... -0.382812 -0.507812 -0.447266]\n",
            "  [-1.39844 2.23438 0.380859 ... -1.64062 0.515625 -0.984375]]\n",
            "\n",
            " [[0.546875 -0.746094 0.667969 ... -0.349609 -0.447266 -0.84375]\n",
            "  [0.546875 -0.746094 0.667969 ... -0.349609 -0.447266 -0.84375]\n",
            "  [0.523438 -0.742188 0.664062 ... -0.355469 -0.443359 -0.84375]\n",
            "  ...\n",
            "  [0.730469 -0.546875 -1.65625 ... -0.169922 -1.79688 -0.0336914]\n",
            "  [-1.21875 -0.392578 -0.380859 ... -1.14844 -1.22656 -0.494141]\n",
            "  [-0.746094 -0.6875 -0.208984 ... -1.09375 -0.632812 -0.648438]]\n",
            "\n",
            " [[0.546875 -0.746094 0.667969 ... -0.349609 -0.447266 -0.84375]\n",
            "  [0.546875 -0.746094 0.667969 ... -0.349609 -0.447266 -0.84375]\n",
            "  [0.523438 -0.742188 0.664062 ... -0.355469 -0.443359 -0.84375]\n",
            "  ...\n",
            "  [0.298828 0.511719 -0.449219 ... 0.169922 -0.992188 -1.36719]\n",
            "  [-0.769531 -0.267578 -0.410156 ... 0.601562 -0.449219 -1.125]\n",
            "  [-1.02344 0.283203 -0.0800781 ... -0.84375 -0.255859 -1.45312]]\n",
            "\n",
            " [[0.546875 -0.746094 0.667969 ... -0.349609 -0.447266 -0.84375]\n",
            "  [0.597656 0.0390625 0.213867 ... -0.546875 -0.0159912 -0.136719]\n",
            "  [0.0351562 0.597656 0.490234 ... 0.0849609 -0.292969 -1.05469]\n",
            "  ...\n",
            "  [-1.48438 1.79688 -0.367188 ... -0.96875 -0.229492 1.45312]\n",
            "  [0.304688 0.466797 -0.322266 ... -0.185547 1.02344 -1.84375]\n",
            "  [-1.77344 1.28906 0.0476074 ... -0.953125 -0.617188 -1.28125]]]\n",
            "next_layer_addition_dropped_out=[[[1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1 -2.5625 2.54688 ... -1.48438 -0.742188 -3.21875]\n",
            "  ...\n",
            "  [0.46875 -1 1.53125 ... -1.08594 0.230469 2.98438]\n",
            "  [0.306641 -5.90625 -0.421875 ... 1.95312 3.21875 0.796875]\n",
            "  [-3.46875 -3.1875 2.32812 ... -1.39062 0.914062 -3.59375]]\n",
            "\n",
            " [[-0.78125 1.09375 0.667969 ... -0.601562 -4.90625 3.0625]\n",
            "  [0.0976562 1.44531 1.75 ... -1.99219 0.462891 3.90625]\n",
            "  [-0.914062 -1.00781 -0.8125 ... -0.742188 1.13281 -0.566406]\n",
            "  ...\n",
            "  [3.53125 4.875 -4.78125 ... -1.55469 3.04688 -0.988281]\n",
            "  [-2.54688 3.29688 -1.375 ... -1.54688 -1.04688 -0.890625]\n",
            "  [-4.5 5.53125 1.3125 ... -5 0.796875 -4.125]]\n",
            "\n",
            " [[1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1 -2.5625 2.54688 ... -1.48438 -0.742188 -3.21875]\n",
            "  ...\n",
            "  [2.21875 -3.10938 -3.78125 ... 0.539062 -4.4375 -0.921875]\n",
            "  [-3.29688 -2.03125 -0.394531 ... -2.90625 -3.82812 -1.60938]\n",
            "  [-3.09375 -3.0625 -0.984375 ... -3.40625 -2.6875 -2.4375]]\n",
            "\n",
            " [[1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1 -2.5625 2.54688 ... -1.48438 -0.742188 -3.21875]\n",
            "  ...\n",
            "  [0.416016 1.49219 -1.03125 ... -0.371094 -3.03125 -3.6875]\n",
            "  [-1.35156 -0.703125 -2.39062 ... 0.917969 -1.875 -3.23438]\n",
            "  [-3.70312 -0.511719 -0.236328 ... -3.40625 -2.0625 -5.3125]]\n",
            "\n",
            " [[1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1.625 0.292969 0.71875 ... -1.85156 -0.277344 -0.152344]\n",
            "  [0.24707 1.55469 1.41406 ... 0.211914 -0.458984 -3]\n",
            "  ...\n",
            "  [-3.85938 4.875 -1.85156 ... -1.125 -0.253906 3.125]\n",
            "  [-0.753906 1.38281 -1.48438 ... -0.197266 3.20312 -4.59375]\n",
            "  [-6.0625 2.65625 0.546875 ... -2.34375 -3.1875 -4.15625]]]\n",
            "inputs=[[[1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1 -2.5625 2.54688 ... -1.48438 -0.742188 -3.21875]\n",
            "  ...\n",
            "  [0.46875 -1 1.53125 ... -1.08594 0.230469 2.98438]\n",
            "  [0.306641 -5.90625 -0.421875 ... 1.95312 3.21875 0.796875]\n",
            "  [-3.46875 -3.1875 2.32812 ... -1.39062 0.914062 -3.59375]]\n",
            "\n",
            " [[-0.78125 1.09375 0.667969 ... -0.601562 -4.90625 3.0625]\n",
            "  [0.0976562 1.44531 1.75 ... -1.99219 0.462891 3.90625]\n",
            "  [-0.914062 -1.00781 -0.8125 ... -0.742188 1.13281 -0.566406]\n",
            "  ...\n",
            "  [3.53125 4.875 -4.78125 ... -1.55469 3.04688 -0.988281]\n",
            "  [-2.54688 3.29688 -1.375 ... -1.54688 -1.04688 -0.890625]\n",
            "  [-4.5 5.53125 1.3125 ... -5 0.796875 -4.125]]\n",
            "\n",
            " [[1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1 -2.5625 2.54688 ... -1.48438 -0.742188 -3.21875]\n",
            "  ...\n",
            "  [2.21875 -3.10938 -3.78125 ... 0.539062 -4.4375 -0.921875]\n",
            "  [-3.29688 -2.03125 -0.394531 ... -2.90625 -3.82812 -1.60938]\n",
            "  [-3.09375 -3.0625 -0.984375 ... -3.40625 -2.6875 -2.4375]]\n",
            "\n",
            " [[1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1 -2.5625 2.54688 ... -1.48438 -0.742188 -3.21875]\n",
            "  ...\n",
            "  [0.416016 1.49219 -1.03125 ... -0.371094 -3.03125 -3.6875]\n",
            "  [-1.35156 -0.703125 -2.39062 ... 0.917969 -1.875 -3.23438]\n",
            "  [-3.70312 -0.511719 -0.236328 ... -3.40625 -2.0625 -5.3125]]\n",
            "\n",
            " [[1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1.625 0.292969 0.71875 ... -1.85156 -0.277344 -0.152344]\n",
            "  [0.24707 1.55469 1.41406 ... 0.211914 -0.458984 -3]\n",
            "  ...\n",
            "  [-3.85938 4.875 -1.85156 ... -1.125 -0.253906 3.125]\n",
            "  [-0.753906 1.38281 -1.48438 ... -0.197266 3.20312 -4.59375]\n",
            "  [-6.0625 2.65625 0.546875 ... -2.34375 -3.1875 -4.15625]]]\n",
            "lnx=[[[0.304688 -0.710938 0.710938 ... -0.402344 -0.205078 -0.894531]\n",
            "  [0.304688 -0.710938 0.710938 ... -0.402344 -0.205078 -0.894531]\n",
            "  [0.277344 -0.710938 0.707031 ... -0.412109 -0.206055 -0.894531]\n",
            "  ...\n",
            "  [0.155273 -0.330078 0.507812 ... -0.359375 0.0761719 0.988281]\n",
            "  [0.10498 -2.01562 -0.144531 ... 0.667969 1.10156 0.271484]\n",
            "  [-1.10156 -1.00781 0.738281 ... -0.441406 0.289062 -1.14062]]\n",
            "\n",
            " [[-0.222656 0.3125 0.19043 ... -0.171875 -1.39844 0.875]\n",
            "  [0.03125 0.462891 0.558594 ... -0.636719 0.148438 1.25]\n",
            "  [-0.302734 -0.333984 -0.269531 ... -0.246094 0.375 -0.1875]\n",
            "  ...\n",
            "  [1.34375 1.85938 -1.82031 ... -0.59375 1.16406 -0.376953]\n",
            "  [-0.894531 1.15625 -0.484375 ... -0.542969 -0.367188 -0.3125]\n",
            "  [-1.67188 2.0625 0.488281 ... -1.85938 0.296875 -1.53906]]\n",
            "\n",
            " [[0.304688 -0.710938 0.710938 ... -0.402344 -0.205078 -0.894531]\n",
            "  [0.304688 -0.710938 0.710938 ... -0.402344 -0.205078 -0.894531]\n",
            "  [0.277344 -0.710938 0.707031 ... -0.412109 -0.206055 -0.894531]\n",
            "  ...\n",
            "  [0.789062 -1.10156 -1.34375 ... 0.191406 -1.57812 -0.328125]\n",
            "  [-1.19531 -0.734375 -0.142578 ... -1.04688 -1.38281 -0.582031]\n",
            "  [-1.02344 -1.00781 -0.324219 ... -1.125 -0.886719 -0.804688]]\n",
            "\n",
            " [[0.304688 -0.710938 0.710938 ... -0.402344 -0.205078 -0.894531]\n",
            "  [0.304688 -0.710938 0.710938 ... -0.402344 -0.205078 -0.894531]\n",
            "  [0.277344 -0.710938 0.707031 ... -0.412109 -0.206055 -0.894531]\n",
            "  ...\n",
            "  [0.147461 0.53125 -0.365234 ... -0.131836 -1.07812 -1.3125]\n",
            "  [-0.494141 -0.255859 -0.871094 ... 0.335938 -0.683594 -1.17969]\n",
            "  [-1.22656 -0.169922 -0.078125 ... -1.125 -0.683594 -1.75781]]\n",
            "\n",
            " [[0.304688 -0.710938 0.710938 ... -0.402344 -0.205078 -0.894531]\n",
            "  [0.511719 0.0922852 0.225586 ... -0.582031 -0.0874023 -0.0478516]\n",
            "  [0.0795898 0.5 0.455078 ... 0.0678711 -0.147461 -0.964844]\n",
            "  ...\n",
            "  [-1.46875 1.85938 -0.707031 ... -0.427734 -0.0966797 1.1875]\n",
            "  [-0.287109 0.527344 -0.566406 ... -0.0751953 1.22656 -1.75781]\n",
            "  [-2.20312 0.964844 0.199219 ... -0.851562 -1.15625 -1.50781]]]\n",
            "attention_lnx=[[[0.347656 -0.162109 -0.0844727 ... -0.142578 -0.996094 0.703125]\n",
            "  [0.347656 -0.162109 -0.0844727 ... -0.142578 -0.996094 0.703125]\n",
            "  [0.34375 -0.160156 -0.0834961 ... -0.139648 -0.996094 0.707031]\n",
            "  ...\n",
            "  [0.447266 0.052002 -0.443359 ... -0.371094 -0.683594 1.08594]\n",
            "  [0.322266 0.0120239 -0.431641 ... -0.324219 -0.664062 0.960938]\n",
            "  [0.523438 0.0722656 -0.431641 ... -0.570312 -0.671875 0.847656]]\n",
            "\n",
            " [[0.0103149 1.80469 -0.211914 ... -2.03125 -1.88281 -0.96875]\n",
            "  [0.193359 0.792969 -0.472656 ... -1.20312 -1.69531 -0.302734]\n",
            "  [0.394531 0.648438 -0.0678711 ... -0.980469 -1.21875 -0.0107422]\n",
            "  ...\n",
            "  [0.330078 0.0722656 -0.419922 ... -0.0437012 -1.10938 -0.135742]\n",
            "  [-0.429688 -0.265625 -0.714844 ... -0.341797 -1.25781 0.125]\n",
            "  [-0.330078 -0.0134888 -0.349609 ... -0.316406 -1.58594 0.121582]]\n",
            "\n",
            " [[0.347656 -0.162109 -0.0844727 ... -0.142578 -0.996094 0.703125]\n",
            "  [0.347656 -0.162109 -0.0844727 ... -0.142578 -0.996094 0.703125]\n",
            "  [0.34375 -0.160156 -0.0834961 ... -0.139648 -0.996094 0.707031]\n",
            "  ...\n",
            "  [0.757812 -0.0299072 -0.404297 ... -0.738281 -0.244141 0.96875]\n",
            "  [0.90625 -0.048584 -0.302734 ... -0.546875 -0.0071106 0.71875]\n",
            "  [0.714844 -0.0932617 -0.660156 ... -0.527344 -0.0732422 0.574219]]\n",
            "\n",
            " [[0.347656 -0.162109 -0.0844727 ... -0.142578 -0.996094 0.703125]\n",
            "  [0.347656 -0.162109 -0.0844727 ... -0.142578 -0.996094 0.703125]\n",
            "  [0.34375 -0.160156 -0.0834961 ... -0.139648 -0.996094 0.707031]\n",
            "  ...\n",
            "  [0.765625 -0.296875 -0.601562 ... -0.570312 -0.455078 1.11719]\n",
            "  [0.835938 -0.164062 -0.792969 ... -0.539062 -0.498047 1.10938]\n",
            "  [0.65625 -0.128906 -0.65625 ... -0.457031 -0.480469 0.847656]]\n",
            "\n",
            " [[0.347656 -0.162109 -0.0844727 ... -0.142578 -0.996094 0.703125]\n",
            "  [0.824219 0.746094 -0.347656 ... -0.695312 -0.917969 0.275391]\n",
            "  [0.796875 0.617188 -0.535156 ... -0.507812 -0.785156 0.886719]\n",
            "  ...\n",
            "  [0.392578 0.0688477 -0.511719 ... -0.219727 -0.388672 0.714844]\n",
            "  [0.65625 0.111816 -0.269531 ... -0.310547 -0.589844 0.267578]\n",
            "  [0.554688 -0.255859 -0.158203 ... -0.271484 -0.369141 0.367188]]]\n",
            "attn_output=[[[0.388672 -0.734375 0.667969 ... -0.427734 -0.46875 -0.679688]\n",
            "  [0.388672 -0.734375 0.667969 ... -0.427734 -0.46875 -0.679688]\n",
            "  [0.363281 -0.734375 0.664062 ... -0.439453 -0.46875 -0.679688]\n",
            "  ...\n",
            "  [0.287109 -0.298828 0.341797 ... -0.457031 -0.142578 1.28125]\n",
            "  [0.206055 -1.92969 -0.279297 ... 0.535156 0.835938 0.574219]\n",
            "  [-0.902344 -0.957031 0.582031 ... -0.601562 0.0742188 -0.84375]]\n",
            "\n",
            " [[-0.208984 0.785156 0.123535 ... -0.710938 -1.83594 0.566406]\n",
            "  [0.0883789 0.679688 0.388672 ... -0.972656 -0.375 1.09375]\n",
            "  [-0.163086 -0.112793 -0.277344 ... -0.542969 -0.0269775 -0.181641]\n",
            "  ...\n",
            "  [1.4375 1.84375 -1.9375 ... -0.59375 0.722656 -0.417969]\n",
            "  [-1.01562 1.03125 -0.710938 ... -0.644531 -0.785156 -0.261719]\n",
            "  [-1.75 2 0.349609 ... -1.92969 -0.287109 -1.45312]]\n",
            "\n",
            " [[0.388672 -0.734375 0.667969 ... -0.427734 -0.46875 -0.679688]\n",
            "  [0.388672 -0.734375 0.667969 ... -0.427734 -0.46875 -0.679688]\n",
            "  [0.363281 -0.734375 0.664062 ... -0.439453 -0.46875 -0.679688]\n",
            "  ...\n",
            "  [1.02344 -1.07812 -1.4375 ... -0.0683594 -1.60938 0.0161133]\n",
            "  [-0.828125 -0.71875 -0.241211 ... -1.19531 -1.32812 -0.308594]\n",
            "  [-0.761719 -1.00781 -0.527344 ... -1.25781 -0.882812 -0.59375]]\n",
            "\n",
            " [[0.388672 -0.734375 0.667969 ... -0.427734 -0.46875 -0.679688]\n",
            "  [0.388672 -0.734375 0.667969 ... -0.427734 -0.46875 -0.679688]\n",
            "  [0.363281 -0.734375 0.664062 ... -0.439453 -0.46875 -0.679688]\n",
            "  ...\n",
            "  [0.40625 0.410156 -0.558594 ... -0.322266 -1.19531 -0.882812]\n",
            "  [-0.179688 -0.302734 -1.10938 ... 0.132812 -0.828125 -0.742188]\n",
            "  [-0.980469 -0.206055 -0.287109 ... -1.24219 -0.820312 -1.4375]]\n",
            "\n",
            " [[0.388672 -0.734375 0.667969 ... -0.427734 -0.46875 -0.679688]\n",
            "  [0.738281 0.3125 0.111816 ... -0.769531 -0.361328 0.0371094]\n",
            "  [0.322266 0.671875 0.271484 ... -0.0913086 -0.384766 -0.652344]\n",
            "  ...\n",
            "  [-1.28125 1.82812 -0.875 ... -0.498047 -0.237305 1.42188]\n",
            "  [-0.0361328 0.550781 -0.648438 ... -0.1875 0.964844 -1.60156]\n",
            "  [-1.95312 0.851562 0.137695 ... -0.925781 -1.25781 -1.34375]]]\n",
            "next_layer_addition_dropped_out=[[[0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.296875 -3.34375 1.70312 ... -1.79688 -2.29688 -1.6875]\n",
            "  ...\n",
            "  [-0.351562 -1.28906 1.23438 ... -0.8125 -0.435547 4.6875]\n",
            "  [0.386719 -6.0625 -2.28125 ... 2.35938 2.90625 1.23438]\n",
            "  [-5.6875 -2.9375 1.89844 ... -1.40625 -0.984375 -3.1875]]\n",
            "\n",
            " [[-0.0976562 3.42188 0.242188 ... -2.57812 -5.9375 2.04688]\n",
            "  [1.67188 2.01562 0.941406 ... -2.75 -1.16406 4.03125]\n",
            "  [-0.341797 -0.320312 -1.39062 ... -0.90625 -0.476562 -0.882812]\n",
            "  ...\n",
            "  [4.78125 5.75 -5.125 ... -1.64844 2.07812 0]\n",
            "  [-2.9375 4 -2.76562 ... -2.35938 -2.01562 -0.835938]\n",
            "  [-5.34375 6.15625 0.878906 ... -5.15625 0.0078125 -3.96875]]\n",
            "\n",
            " [[0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.296875 -3.34375 1.70312 ... -1.79688 -2.29688 -1.6875]\n",
            "  ...\n",
            "  [2.21875 -3.3125 -4.65625 ... 0.242188 -5.75 0.233398]\n",
            "  [-2.54688 -0.921875 -2.0625 ... -3.53125 -3.9375 -0.976562]\n",
            "  [-5 -2.32812 -2.90625 ... -4.125 -4.59375 -1.75]]\n",
            "\n",
            " [[0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.296875 -3.34375 1.70312 ... -1.79688 -2.29688 -1.6875]\n",
            "  ...\n",
            "  [0.929688 1.19531 -1.59375 ... -0.738281 -3.5 -2.71875]\n",
            "  [-0.984375 0.121094 -3.48438 ... 1.125 -2.98438 -1.52344]\n",
            "  [-6.09375 -0.59375 -2.1875 ... -3.8125 -3.59375 -4.84375]]\n",
            "\n",
            " [[0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [2.1875 1.65625 0.296875 ... -2.51562 -0.75 0.149414]\n",
            "  [1.14062 1.75781 0.474609 ... 0.371094 -2.32812 -2.39062]\n",
            "  ...\n",
            "  [-3.67188 5.75 -3.09375 ... -1.36719 -1.65625 4.4375]\n",
            "  [-0.285156 1.35938 -2.78125 ... 0.378906 1.23438 -3.17188]\n",
            "  [-7.34375 2.75 -0.953125 ... -3.46875 -3.75 -4.4375]]]\n",
            "inputs=[[[0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.296875 -3.34375 1.70312 ... -1.79688 -2.29688 -1.6875]\n",
            "  ...\n",
            "  [-0.351562 -1.28906 1.23438 ... -0.8125 -0.435547 4.6875]\n",
            "  [0.386719 -6.0625 -2.28125 ... 2.35938 2.90625 1.23438]\n",
            "  [-5.6875 -2.9375 1.89844 ... -1.40625 -0.984375 -3.1875]]\n",
            "\n",
            " [[-0.0976562 3.42188 0.242188 ... -2.57812 -5.9375 2.04688]\n",
            "  [1.67188 2.01562 0.941406 ... -2.75 -1.16406 4.03125]\n",
            "  [-0.341797 -0.320312 -1.39062 ... -0.90625 -0.476562 -0.882812]\n",
            "  ...\n",
            "  [4.78125 5.75 -5.125 ... -1.64844 2.07812 0]\n",
            "  [-2.9375 4 -2.76562 ... -2.35938 -2.01562 -0.835938]\n",
            "  [-5.34375 6.15625 0.878906 ... -5.15625 0.0078125 -3.96875]]\n",
            "\n",
            " [[0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.296875 -3.34375 1.70312 ... -1.79688 -2.29688 -1.6875]\n",
            "  ...\n",
            "  [2.21875 -3.3125 -4.65625 ... 0.242188 -5.75 0.233398]\n",
            "  [-2.54688 -0.921875 -2.0625 ... -3.53125 -3.9375 -0.976562]\n",
            "  [-5 -2.32812 -2.90625 ... -4.125 -4.59375 -1.75]]\n",
            "\n",
            " [[0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.296875 -3.34375 1.70312 ... -1.79688 -2.29688 -1.6875]\n",
            "  ...\n",
            "  [0.929688 1.19531 -1.59375 ... -0.738281 -3.5 -2.71875]\n",
            "  [-0.984375 0.121094 -3.48438 ... 1.125 -2.98438 -1.52344]\n",
            "  [-6.09375 -0.59375 -2.1875 ... -3.8125 -3.59375 -4.84375]]\n",
            "\n",
            " [[0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [2.1875 1.65625 0.296875 ... -2.51562 -0.75 0.149414]\n",
            "  [1.14062 1.75781 0.474609 ... 0.371094 -2.32812 -2.39062]\n",
            "  ...\n",
            "  [-3.67188 5.75 -3.09375 ... -1.36719 -1.65625 4.4375]\n",
            "  [-0.285156 1.35938 -2.78125 ... 0.378906 1.23438 -3.17188]\n",
            "  [-7.34375 2.75 -0.953125 ... -3.46875 -3.75 -4.4375]]]\n",
            "lnx=[[[0.101562 -0.886719 0.457031 ... -0.466797 -0.613281 -0.443359]\n",
            "  [0.101562 -0.886719 0.457031 ... -0.466797 -0.613281 -0.443359]\n",
            "  [0.0786133 -0.886719 0.451172 ... -0.476562 -0.609375 -0.447266]\n",
            "  ...\n",
            "  [-0.109375 -0.400391 0.384766 ... -0.251953 -0.135742 1.46094]\n",
            "  [0.124512 -1.95312 -0.734375 ... 0.761719 0.9375 0.398438]\n",
            "  [-1.69531 -0.875 0.566406 ... -0.417969 -0.292969 -0.949219]]\n",
            "\n",
            " [[-0.026001 0.910156 0.0644531 ... -0.6875 -1.57812 0.542969]\n",
            "  [0.5 0.601562 0.28125 ... -0.820312 -0.347656 1.20312]\n",
            "  [-0.104492 -0.0981445 -0.425781 ... -0.277344 -0.145508 -0.269531]\n",
            "  ...\n",
            "  [1.71875 2.07812 -1.84375 ... -0.59375 0.75 0]\n",
            "  [-0.953125 1.29688 -0.898438 ... -0.765625 -0.65625 -0.271484]\n",
            "  [-1.83594 2.10938 0.300781 ... -1.77344 0.00268555 -1.35938]]\n",
            "\n",
            " [[0.101562 -0.886719 0.457031 ... -0.466797 -0.613281 -0.443359]\n",
            "  [0.101562 -0.886719 0.457031 ... -0.466797 -0.613281 -0.443359]\n",
            "  [0.0786133 -0.886719 0.451172 ... -0.476562 -0.609375 -0.447266]\n",
            "  ...\n",
            "  [0.734375 -1.09375 -1.53906 ... 0.0800781 -1.89844 0.0771484]\n",
            "  [-0.851562 -0.308594 -0.6875 ... -1.17969 -1.3125 -0.326172]\n",
            "  [-1.5 -0.699219 -0.875 ... -1.24219 -1.38281 -0.527344]]\n",
            "\n",
            " [[0.101562 -0.886719 0.457031 ... -0.466797 -0.613281 -0.443359]\n",
            "  [0.101562 -0.886719 0.457031 ... -0.466797 -0.613281 -0.443359]\n",
            "  [0.0786133 -0.886719 0.451172 ... -0.476562 -0.609375 -0.447266]\n",
            "  ...\n",
            "  [0.310547 0.400391 -0.53125 ... -0.24707 -1.17188 -0.910156]\n",
            "  [-0.333984 0.0410156 -1.17969 ... 0.380859 -1.00781 -0.515625]\n",
            "  [-1.85938 -0.180664 -0.667969 ... -1.16406 -1.09375 -1.47656]]\n",
            "\n",
            " [[0.101562 -0.886719 0.457031 ... -0.466797 -0.613281 -0.443359]\n",
            "  [0.648438 0.492188 0.0883789 ... -0.746094 -0.222656 0.0444336]\n",
            "  [0.34375 0.527344 0.142578 ... 0.111816 -0.699219 -0.71875]\n",
            "  ...\n",
            "  [-1.3125 2.0625 -1.10938 ... -0.490234 -0.59375 1.59375]\n",
            "  [-0.103516 0.492188 -1.00781 ... 0.137695 0.447266 -1.14844]\n",
            "  [-2.45312 0.917969 -0.318359 ... -1.15625 -1.25 -1.48438]]]\n",
            "attention_lnx=[[[-0.065918 0.773438 -3.0625 ... 0.34375 -0.382812 0.369141]\n",
            "  [-0.065918 0.773438 -3.0625 ... 0.34375 -0.382812 0.369141]\n",
            "  [-0.0678711 0.769531 -3.0625 ... 0.347656 -0.382812 0.367188]\n",
            "  ...\n",
            "  [0.00457764 0.402344 -2.8125 ... 0.333984 0.0490723 0.396484]\n",
            "  [-0.0214844 0.3125 -2.8125 ... 0.683594 0.135742 0.398438]\n",
            "  [-0.0529785 0.378906 -2.70312 ... 0.535156 0.134766 0.404297]]\n",
            "\n",
            " [[-0.365234 0.601562 -2.32812 ... -0.929688 0.957031 0.722656]\n",
            "  [-0.378906 1.44531 -1.96094 ... -0.75 0.902344 0.652344]\n",
            "  [-0.671875 1.55469 -2.20312 ... -0.212891 0.945312 0.294922]\n",
            "  ...\n",
            "  [-0.208008 1.64844 -1.70312 ... -0.486328 0.275391 -0.917969]\n",
            "  [-0.318359 1.28125 -1.55469 ... -0.632812 0.394531 -0.065918]\n",
            "  [-0.349609 1.21094 -1.74219 ... -0.28125 0.294922 -0.15332]]\n",
            "\n",
            " [[-0.065918 0.773438 -3.0625 ... 0.34375 -0.382812 0.369141]\n",
            "  [-0.065918 0.773438 -3.0625 ... 0.34375 -0.382812 0.369141]\n",
            "  [-0.0678711 0.769531 -3.0625 ... 0.347656 -0.382812 0.367188]\n",
            "  ...\n",
            "  [-0.431641 0.398438 -2.1875 ... 0.5 0.10498 0.265625]\n",
            "  [-0.464844 0.255859 -2.20312 ... 0.628906 0.550781 -0.00110626]\n",
            "  [-0.59375 0.157227 -2.15625 ... 0.542969 0.367188 0.3125]]\n",
            "\n",
            " [[-0.065918 0.773438 -3.0625 ... 0.34375 -0.382812 0.369141]\n",
            "  [-0.065918 0.773438 -3.0625 ... 0.34375 -0.382812 0.369141]\n",
            "  [-0.0678711 0.769531 -3.0625 ... 0.347656 -0.382812 0.367188]\n",
            "  ...\n",
            "  [-0.267578 0.333984 -2.0625 ... 0.421875 0.102051 0.242188]\n",
            "  [-0.394531 0.490234 -1.82031 ... 0.404297 -0.00616455 0.449219]\n",
            "  [-0.287109 0.207031 -1.96875 ... 0.378906 0.200195 0.0913086]]\n",
            "\n",
            " [[-0.065918 0.773438 -3.0625 ... 0.34375 -0.382812 0.369141]\n",
            "  [0.117188 0.472656 -2.70312 ... 0.314453 0.108887 0.726562]\n",
            "  [-0.107422 0.167969 -2.1875 ... 0.0507812 0.3125 0.359375]\n",
            "  ...\n",
            "  [-0.578125 0.349609 -1.91406 ... 0.0678711 0.558594 0.0825195]\n",
            "  [-0.388672 0.145508 -2.09375 ... 0.316406 0.412109 0.181641]\n",
            "  [-0.277344 -0.0761719 -2.29688 ... 0.11377 0.359375 -0.0742188]]]\n",
            "attn_output=[[[0.081543 -0.660156 -0.34375 ... -0.365234 -0.691406 -0.333984]\n",
            "  [0.081543 -0.660156 -0.34375 ... -0.365234 -0.691406 -0.333984]\n",
            "  [0.0588379 -0.660156 -0.349609 ... -0.373047 -0.6875 -0.339844]\n",
            "  ...\n",
            "  [-0.104004 -0.265625 -0.474609 ... -0.143555 -0.116211 1.52344]\n",
            "  [0.114746 -1.80469 -1.60156 ... 0.953125 0.953125 0.511719]\n",
            "  [-1.66406 -0.742188 -0.233398 ... -0.251953 -0.246094 -0.808594]]\n",
            "\n",
            " [[-0.119141 1.03906 -0.539062 ... -0.90625 -1.28125 0.714844]\n",
            "  [0.373047 1 -0.294922 ... -1.00781 -0.0756836 1.35156]\n",
            "  [-0.296875 0.361328 -1.05469 ... -0.328125 0.137695 -0.172852]\n",
            "  ...\n",
            "  [1.59375 2.57812 -2.375 ... -0.742188 0.820312 -0.320312]\n",
            "  [-1.03125 1.67969 -1.375 ... -0.953125 -0.515625 -0.287109]\n",
            "  [-1.89844 2.45312 -0.289062 ... -1.8125 0.101074 -1.375]]\n",
            "\n",
            " [[0.081543 -0.660156 -0.34375 ... -0.365234 -0.691406 -0.333984]\n",
            "  [0.081543 -0.660156 -0.34375 ... -0.365234 -0.691406 -0.333984]\n",
            "  [0.0588379 -0.660156 -0.349609 ... -0.373047 -0.6875 -0.339844]\n",
            "  ...\n",
            "  [0.574219 -0.933594 -2.1875 ... 0.238281 -1.8125 0.160156]\n",
            "  [-0.976562 -0.21582 -1.38281 ... -0.941406 -1.09375 -0.316406]\n",
            "  [-1.64062 -0.636719 -1.48438 ... -1.04688 -1.23438 -0.421875]]\n",
            "\n",
            " [[0.081543 -0.660156 -0.34375 ... -0.365234 -0.691406 -0.333984]\n",
            "  [0.081543 -0.660156 -0.34375 ... -0.365234 -0.691406 -0.333984]\n",
            "  [0.0588379 -0.660156 -0.349609 ... -0.373047 -0.6875 -0.339844]\n",
            "  ...\n",
            "  [0.217773 0.503906 -1.20312 ... -0.104004 -1.11719 -0.816406]\n",
            "  [-0.458984 0.203125 -1.76562 ... 0.507812 -0.992188 -0.357422]\n",
            "  [-1.90625 -0.115234 -1.24219 ... -1.02344 -1.01562 -1.41406]]\n",
            "\n",
            " [[0.081543 -0.660156 -0.34375 ... -0.365234 -0.691406 -0.333984]\n",
            "  [0.667969 0.617188 -0.699219 ... -0.636719 -0.185547 0.253906]\n",
            "  [0.300781 0.5625 -0.5 ... 0.123047 -0.589844 -0.59375]\n",
            "  ...\n",
            "  [-1.47656 2.125 -1.74219 ... -0.453125 -0.382812 1.57031]\n",
            "  [-0.239258 0.535156 -1.72656 ... 0.24707 0.585938 -1.0625]\n",
            "  [-2.46875 0.863281 -1.05469 ... -1.08594 -1.09375 -1.46094]]]\n",
            "next_layer_addition_dropped_out=[[[-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.175781 -2.875 -1.13281 ... -1.125 -3.01562 -0.808594]\n",
            "  ...\n",
            "  [-0.765625 -0.964844 -1.26562 ... -0.0527344 -0.410156 5.4375]\n",
            "  [0.859375 -5.625 -5.03125 ... 3.5625 2.5 0.507812]\n",
            "  [-5 -2.875 -1.92188 ... 0.410156 -0.470703 -3.23438]]\n",
            "\n",
            " [[0.248047 3.53125 -3.125 ... -4.0625 -3.45312 2.0625]\n",
            "  [2.5625 2.32812 -1.52344 ... -3.95312 1.09375 3.95312]\n",
            "  [-0.5 1.10938 -4.4375 ... -1.03125 1.33594 -2.04688]\n",
            "  ...\n",
            "  [5.375 6.34375 -7.53125 ... -2.53125 2.875 -0.410156]\n",
            "  [-3.34375 4.59375 -3.6875 ... -4.96875 -0.894531 0.0976562]\n",
            "  [-6.0625 5.5 -1.24219 ... -4.59375 0.632812 -4.21875]]\n",
            "\n",
            " [[-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.175781 -2.875 -1.13281 ... -1.125 -3.01562 -0.808594]\n",
            "  ...\n",
            "  [3.375 -2.45312 -7.625 ... -0.0273438 -6.96875 0.621094]\n",
            "  [-1.96875 -2.15625 -3.70312 ... -2.5625 -2.64062 -1.3125]\n",
            "  [-4.71875 -2.75 -5.65625 ... -3.04688 -3.73438 -1.5]]\n",
            "\n",
            " [[-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.175781 -2.875 -1.13281 ... -1.125 -3.01562 -0.808594]\n",
            "  ...\n",
            "  [1.48438 1.45312 -3 ... -0.203125 -3.5625 -2.89062]\n",
            "  [-0.296875 -0.132812 -6.09375 ... 1.22656 -1.95312 -1.05469]\n",
            "  [-5.3125 -0.792969 -5.09375 ... -3.20312 -2.84375 -5.75]]\n",
            "\n",
            " [[-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [2.73438 2.29688 -2.25 ... -1.78125 -0.980469 -0.28125]\n",
            "  [1.3125 0.820312 -1.60156 ... 0.0175781 -2.21875 -2.46875]\n",
            "  ...\n",
            "  [-4.3125 5.9375 -4.59375 ... -2.32812 0.882812 4.25]\n",
            "  [-0.410156 1.69531 -5.09375 ... 0.554688 2.5625 -2.90625]\n",
            "  [-7.25 1.53906 -3.96875 ... -3.76562 -2.53125 -5.09375]]]\n",
            "inputs=[[[-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.175781 -2.875 -1.13281 ... -1.125 -3.01562 -0.808594]\n",
            "  ...\n",
            "  [-0.765625 -0.964844 -1.26562 ... -0.0527344 -0.410156 5.4375]\n",
            "  [0.859375 -5.625 -5.03125 ... 3.5625 2.5 0.507812]\n",
            "  [-5 -2.875 -1.92188 ... 0.410156 -0.470703 -3.23438]]\n",
            "\n",
            " [[0.248047 3.53125 -3.125 ... -4.0625 -3.45312 2.0625]\n",
            "  [2.5625 2.32812 -1.52344 ... -3.95312 1.09375 3.95312]\n",
            "  [-0.5 1.10938 -4.4375 ... -1.03125 1.33594 -2.04688]\n",
            "  ...\n",
            "  [5.375 6.34375 -7.53125 ... -2.53125 2.875 -0.410156]\n",
            "  [-3.34375 4.59375 -3.6875 ... -4.96875 -0.894531 0.0976562]\n",
            "  [-6.0625 5.5 -1.24219 ... -4.59375 0.632812 -4.21875]]\n",
            "\n",
            " [[-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.175781 -2.875 -1.13281 ... -1.125 -3.01562 -0.808594]\n",
            "  ...\n",
            "  [3.375 -2.45312 -7.625 ... -0.0273438 -6.96875 0.621094]\n",
            "  [-1.96875 -2.15625 -3.70312 ... -2.5625 -2.64062 -1.3125]\n",
            "  [-4.71875 -2.75 -5.65625 ... -3.04688 -3.73438 -1.5]]\n",
            "\n",
            " [[-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.175781 -2.875 -1.13281 ... -1.125 -3.01562 -0.808594]\n",
            "  ...\n",
            "  [1.48438 1.45312 -3 ... -0.203125 -3.5625 -2.89062]\n",
            "  [-0.296875 -0.132812 -6.09375 ... 1.22656 -1.95312 -1.05469]\n",
            "  [-5.3125 -0.792969 -5.09375 ... -3.20312 -2.84375 -5.75]]\n",
            "\n",
            " [[-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [2.73438 2.29688 -2.25 ... -1.78125 -0.980469 -0.28125]\n",
            "  [1.3125 0.820312 -1.60156 ... 0.0175781 -2.21875 -2.46875]\n",
            "  ...\n",
            "  [-4.3125 5.9375 -4.59375 ... -2.32812 0.882812 4.25]\n",
            "  [-0.410156 1.69531 -5.09375 ... 0.554688 2.5625 -2.90625]\n",
            "  [-7.25 1.53906 -3.96875 ... -3.76562 -2.53125 -5.09375]]]\n",
            "lnx=[[[-0.0240479 -0.71875 -0.277344 ... -0.275391 -0.761719 -0.202148]\n",
            "  [-0.0240479 -0.71875 -0.277344 ... -0.275391 -0.761719 -0.202148]\n",
            "  [-0.0441895 -0.722656 -0.285156 ... -0.283203 -0.757812 -0.203125]\n",
            "  ...\n",
            "  [-0.225586 -0.283203 -0.373047 ... -0.0155029 -0.120605 1.60156]\n",
            "  [0.263672 -1.72656 -1.53906 ... 1.09375 0.765625 0.155273]\n",
            "  [-1.39844 -0.804688 -0.539062 ... 0.114746 -0.131836 -0.90625]]\n",
            "\n",
            " [[0.0629883 0.894531 -0.792969 ... -1.03125 -0.875 0.523438]\n",
            "  [0.726562 0.660156 -0.433594 ... -1.125 0.310547 1.125]\n",
            "  [-0.144531 0.320312 -1.28125 ... -0.298828 0.386719 -0.59375]\n",
            "  ...\n",
            "  [1.82031 2.15625 -2.54688 ... -0.859375 0.976562 -0.138672]\n",
            "  [-1.01562 1.39062 -1.11719 ... -1.50781 -0.271484 0.029541]\n",
            "  [-1.9375 1.75781 -0.396484 ... -1.46875 0.202148 -1.34375]]\n",
            "\n",
            " [[-0.0240479 -0.71875 -0.277344 ... -0.275391 -0.761719 -0.202148]\n",
            "  [-0.0240479 -0.71875 -0.277344 ... -0.275391 -0.761719 -0.202148]\n",
            "  [-0.0441895 -0.722656 -0.285156 ... -0.283203 -0.757812 -0.203125]\n",
            "  ...\n",
            "  [1.04688 -0.757812 -2.35938 ... -0.00848389 -2.15625 0.192383]\n",
            "  [-0.613281 -0.671875 -1.15625 ... -0.800781 -0.824219 -0.410156]\n",
            "  [-1.29688 -0.757812 -1.55469 ... -0.839844 -1.02344 -0.412109]]\n",
            "\n",
            " [[-0.0240479 -0.71875 -0.277344 ... -0.275391 -0.761719 -0.202148]\n",
            "  [-0.0240479 -0.71875 -0.277344 ... -0.275391 -0.761719 -0.202148]\n",
            "  [-0.0441895 -0.722656 -0.285156 ... -0.283203 -0.757812 -0.203125]\n",
            "  ...\n",
            "  [0.478516 0.46875 -0.96875 ... -0.0654297 -1.14844 -0.933594]\n",
            "  [-0.0966797 -0.0432129 -1.98438 ... 0.400391 -0.636719 -0.34375]\n",
            "  [-1.5 -0.223633 -1.4375 ... -0.902344 -0.800781 -1.625]]\n",
            "\n",
            " [[-0.0240479 -0.71875 -0.277344 ... -0.275391 -0.761719 -0.202148]\n",
            "  [0.777344 0.65625 -0.640625 ... -0.507812 -0.279297 -0.0800781]\n",
            "  [0.376953 0.235352 -0.458984 ... 0.0050354 -0.636719 -0.707031]\n",
            "  ...\n",
            "  [-1.46875 2.03125 -1.57031 ... -0.792969 0.300781 1.45312]\n",
            "  [-0.141602 0.585938 -1.75781 ... 0.191406 0.886719 -1.00781]\n",
            "  [-2.23438 0.474609 -1.21875 ... -1.15625 -0.777344 -1.57031]]]\n",
            "attention_lnx=[[[-1.59375 0.847656 -1.61719 ... -0.734375 1.64062 1.32031]\n",
            "  [-1.59375 0.847656 -1.61719 ... -0.734375 1.64062 1.32031]\n",
            "  [-1.60156 0.847656 -1.60938 ... -0.738281 1.64062 1.32031]\n",
            "  ...\n",
            "  [-1.33594 0.984375 -1.74219 ... -0.710938 1.15625 1.40625]\n",
            "  [-1.28906 1.01562 -1.63281 ... -0.765625 1.30469 1.10156]\n",
            "  [-1.36719 1.02344 -1.35938 ... -0.539062 0.882812 1.48438]]\n",
            "\n",
            " [[-1.07812 0.388672 0.539062 ... 0.152344 -0.0712891 -0.431641]\n",
            "  [-0.22168 0.511719 0.104004 ... 0.535156 -0.116211 -0.691406]\n",
            "  [-0.451172 0.546875 0.589844 ... 0.164062 -0.186523 -0.392578]\n",
            "  ...\n",
            "  [-1.03125 -0.0571289 0.0240479 ... -0.106445 -0.180664 0.472656]\n",
            "  [-1.28906 -0.00622559 0.0927734 ... 0.283203 -0.314453 0.691406]\n",
            "  [-1.23438 0.125 -0.0140991 ... -0.0424805 -0.585938 0.78125]]\n",
            "\n",
            " [[-1.59375 0.847656 -1.61719 ... -0.734375 1.64062 1.32031]\n",
            "  [-1.59375 0.847656 -1.61719 ... -0.734375 1.64062 1.32031]\n",
            "  [-1.60156 0.847656 -1.60938 ... -0.738281 1.64062 1.32031]\n",
            "  ...\n",
            "  [-1.35156 0.578125 -1.28906 ... 0.480469 1.19531 1.35938]\n",
            "  [-1.625 0.628906 -1.03906 ... 0.0510254 1.14062 1.36719]\n",
            "  [-1.16406 0.722656 -1.21094 ... 0.166016 1.01562 1.29688]]\n",
            "\n",
            " [[-1.59375 0.847656 -1.61719 ... -0.734375 1.64062 1.32031]\n",
            "  [-1.59375 0.847656 -1.61719 ... -0.734375 1.64062 1.32031]\n",
            "  [-1.60156 0.847656 -1.60938 ... -0.738281 1.64062 1.32031]\n",
            "  ...\n",
            "  [-1.28125 0.757812 -1.19531 ... 0.277344 0.792969 1.55469]\n",
            "  [-1.21094 0.539062 -1.32812 ... -0.0175781 0.875 1.53906]\n",
            "  [-1.07031 0.761719 -1.09375 ... 0.0302734 0.355469 1.67969]]\n",
            "\n",
            " [[-1.59375 0.847656 -1.61719 ... -0.734375 1.64062 1.32031]\n",
            "  [-1.5 0.945312 -1.5 ... -0.28125 1.15625 0.792969]\n",
            "  [-1.25 0.730469 -0.808594 ... -0.0429688 1.95312 0.773438]\n",
            "  ...\n",
            "  [-1.46094 0.124512 -1.50781 ... 0.204102 1.11719 0.886719]\n",
            "  [-1.41406 0.15918 -1.04688 ... 0.3125 1.44531 0.847656]\n",
            "  [-1.21875 0.421875 -0.96875 ... 0.145508 0.753906 1.11719]]]\n",
            "attn_output=[[[-0.414062 -0.494141 -0.667969 ... -0.449219 -0.341797 0.126953]\n",
            "  [-0.414062 -0.494141 -0.667969 ... -0.449219 -0.341797 0.126953]\n",
            "  [-0.435547 -0.498047 -0.671875 ... -0.457031 -0.337891 0.125977]\n",
            "  ...\n",
            "  [-0.601562 0.00558472 -0.863281 ... -0.21875 0.213867 1.96094]\n",
            "  [-0.128906 -1.38281 -2 ... 0.839844 1.14062 0.482422]\n",
            "  [-1.73438 -0.503906 -0.894531 ... -0.0351562 0.112305 -0.476562]]\n",
            "\n",
            " [[-0.205078 0.96875 -0.640625 ... -0.964844 -0.871094 0.402344]\n",
            "  [0.648438 0.785156 -0.392578 ... -0.945312 0.269531 0.902344]\n",
            "  [-0.269531 0.466797 -1.08594 ... -0.245117 0.324219 -0.6875]\n",
            "  ...\n",
            "  [1.42969 2.07812 -2.46875 ... -0.871094 0.886719 0.0206299]\n",
            "  [-1.375 1.35938 -1.07031 ... -1.39062 -0.359375 0.234375]\n",
            "  [-2.26562 1.75 -0.390625 ... -1.44531 0.0145874 -1.07031]]\n",
            "\n",
            " [[-0.414062 -0.494141 -0.667969 ... -0.449219 -0.341797 0.126953]\n",
            "  [-0.414062 -0.494141 -0.667969 ... -0.449219 -0.341797 0.126953]\n",
            "  [-0.435547 -0.498047 -0.671875 ... -0.457031 -0.337891 0.125977]\n",
            "  ...\n",
            "  [0.609375 -0.5625 -2.67188 ... 0.135742 -1.73438 0.59375]\n",
            "  [-1.08594 -0.460938 -1.42969 ... -0.757812 -0.453125 0.0164795]\n",
            "  [-1.57031 -0.539062 -1.82812 ... -0.769531 -0.726562 -0.0541992]]\n",
            "\n",
            " [[-0.414062 -0.494141 -0.667969 ... -0.449219 -0.341797 0.126953]\n",
            "  [-0.414062 -0.494141 -0.667969 ... -0.449219 -0.341797 0.126953]\n",
            "  [-0.435547 -0.498047 -0.671875 ... -0.457031 -0.337891 0.125977]\n",
            "  ...\n",
            "  [0.0634766 0.691406 -1.3125 ... 0.0231934 -0.867188 -0.417969]\n",
            "  [-0.476562 0.128906 -2.34375 ... 0.382812 -0.341797 0.15332]\n",
            "  [-1.75 -0.00854492 -1.69531 ... -0.871094 -0.683594 -1.11719]]\n",
            "\n",
            " [[-0.414062 -0.494141 -0.667969 ... -0.449219 -0.341797 0.126953]\n",
            "  [0.339844 0.890625 -1.03125 ... -0.566406 0.0483398 0.140625]\n",
            "  [0.0174561 0.431641 -0.671875 ... -0.00708008 -0.0737305 -0.472656]\n",
            "  ...\n",
            "  [-1.91406 2.01562 -2.01562 ... -0.703125 0.664062 1.70312]\n",
            "  [-0.613281 0.621094 -2.0625 ... 0.291016 1.34375 -0.691406]\n",
            "  [-2.53125 0.585938 -1.47656 ... -1.07812 -0.53125 -1.1875]]]\n",
            "next_layer_addition_dropped_out=[[[-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.6875 -3.0625 -3.65625 ... -0.757812 -1.64062 -0.535156]\n",
            "  ...\n",
            "  [-2.85938 -0.427734 -3.9375 ... 0.679688 1.35938 6.90625]\n",
            "  [-0.746094 -5.09375 -6 ... 2.79688 3.40625 1.28125]\n",
            "  [-7.21875 -2.9375 -2.76562 ... -0.5 0.373047 -3.96875]]\n",
            "\n",
            " [[-0.597656 3.53125 -2.14062 ... -3.625 -4.34375 0.960938]\n",
            "  [2.59375 2.35938 -2.3125 ... -3.04688 0.910156 3.39062]\n",
            "  [-1.33594 1.70312 -3.28125 ... -1.30469 0.976562 -3.26562]\n",
            "  ...\n",
            "  [5 6.9375 -7.1875 ... -2.71875 2.32812 0.574219]\n",
            "  [-3.96875 3.3125 -4.3125 ... -4.875 -2.0625 -0.0429688]\n",
            "  [-7.84375 4.90625 -0.597656 ... -5.1875 0.205078 -4.40625]]\n",
            "\n",
            " [[-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.6875 -3.0625 -3.65625 ... -0.757812 -1.64062 -0.535156]\n",
            "  ...\n",
            "  [1.94531 -2.78125 -8.9375 ... 1.03125 -6.875 0.992188]\n",
            "  [-5.40625 -3.0625 -4.03125 ... -2.5625 -2 -0.960938]\n",
            "  [-6.09375 -3.57812 -6.1875 ... -4.5 -2.75 -1.46094]]\n",
            "\n",
            " [[-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.6875 -3.0625 -3.65625 ... -0.757812 -1.64062 -0.535156]\n",
            "  ...\n",
            "  [-0.992188 2.3125 -4.3125 ... 0.847656 -2.95312 -0.902344]\n",
            "  [-1.21094 0.675781 -7.15625 ... 0.914062 -0.847656 -0.460938]\n",
            "  [-6.09375 -1.59375 -5.5625 ... -4.0625 -2.54688 -5.6875]]\n",
            "\n",
            " [[-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [0.652344 2.60938 -3.29688 ... -1.09375 -0.679688 -0.113281]\n",
            "  [0.828125 0.1875 -3.09375 ... 0.609375 -1.65625 -2.73438]\n",
            "  ...\n",
            "  [-5.125 5.78125 -6.0625 ... -1.05469 1.46094 3.9375]\n",
            "  [-1.21875 2.14062 -6.625 ... 2.17188 3.4375 -2.59375]\n",
            "  [-7.0625 0.257812 -4.875 ... -4.40625 -1.75 -5.09375]]]\n",
            "inputs=[[[-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.6875 -3.0625 -3.65625 ... -0.757812 -1.64062 -0.535156]\n",
            "  ...\n",
            "  [-2.85938 -0.427734 -3.9375 ... 0.679688 1.35938 6.90625]\n",
            "  [-0.746094 -5.09375 -6 ... 2.79688 3.40625 1.28125]\n",
            "  [-7.21875 -2.9375 -2.76562 ... -0.5 0.373047 -3.96875]]\n",
            "\n",
            " [[-0.597656 3.53125 -2.14062 ... -3.625 -4.34375 0.960938]\n",
            "  [2.59375 2.35938 -2.3125 ... -3.04688 0.910156 3.39062]\n",
            "  [-1.33594 1.70312 -3.28125 ... -1.30469 0.976562 -3.26562]\n",
            "  ...\n",
            "  [5 6.9375 -7.1875 ... -2.71875 2.32812 0.574219]\n",
            "  [-3.96875 3.3125 -4.3125 ... -4.875 -2.0625 -0.0429688]\n",
            "  [-7.84375 4.90625 -0.597656 ... -5.1875 0.205078 -4.40625]]\n",
            "\n",
            " [[-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.6875 -3.0625 -3.65625 ... -0.757812 -1.64062 -0.535156]\n",
            "  ...\n",
            "  [1.94531 -2.78125 -8.9375 ... 1.03125 -6.875 0.992188]\n",
            "  [-5.40625 -3.0625 -4.03125 ... -2.5625 -2 -0.960938]\n",
            "  [-6.09375 -3.57812 -6.1875 ... -4.5 -2.75 -1.46094]]\n",
            "\n",
            " [[-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.6875 -3.0625 -3.65625 ... -0.757812 -1.64062 -0.535156]\n",
            "  ...\n",
            "  [-0.992188 2.3125 -4.3125 ... 0.847656 -2.95312 -0.902344]\n",
            "  [-1.21094 0.675781 -7.15625 ... 0.914062 -0.847656 -0.460938]\n",
            "  [-6.09375 -1.59375 -5.5625 ... -4.0625 -2.54688 -5.6875]]\n",
            "\n",
            " [[-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [0.652344 2.60938 -3.29688 ... -1.09375 -0.679688 -0.113281]\n",
            "  [0.828125 0.1875 -3.09375 ... 0.609375 -1.65625 -2.73438]\n",
            "  ...\n",
            "  [-5.125 5.78125 -6.0625 ... -1.05469 1.46094 3.9375]\n",
            "  [-1.21875 2.14062 -6.625 ... 2.17188 3.4375 -2.59375]\n",
            "  [-7.0625 0.257812 -4.875 ... -4.40625 -1.75 -5.09375]]]\n",
            "lnx=[[[-0.621094 -0.738281 -0.867188 ... -0.175781 -0.394531 -0.128906]\n",
            "  [-0.621094 -0.738281 -0.867188 ... -0.175781 -0.394531 -0.128906]\n",
            "  [-0.644531 -0.734375 -0.875 ... -0.181641 -0.392578 -0.12793]\n",
            "  ...\n",
            "  [-0.804688 -0.120117 -1.10938 ... 0.191406 0.382812 1.94531]\n",
            "  [-0.219727 -1.5 -1.77344 ... 0.824219 1.00781 0.378906]\n",
            "  [-1.89062 -0.769531 -0.722656 ... -0.130859 0.0976562 -1.03906]]\n",
            "\n",
            " [[-0.145508 0.859375 -0.519531 ... -0.882812 -1.05469 0.233398]\n",
            "  [0.707031 0.640625 -0.628906 ... -0.828125 0.248047 0.921875]\n",
            "  [-0.371094 0.474609 -0.914062 ... -0.363281 0.271484 -0.910156]\n",
            "  ...\n",
            "  [1.60938 2.23438 -2.3125 ... -0.875 0.75 0.185547]\n",
            "  [-1.10938 0.925781 -1.20312 ... -1.36719 -0.578125 -0.0120239]\n",
            "  [-2.35938 1.47656 -0.179688 ... -1.5625 0.0617676 -1.32812]]\n",
            "\n",
            " [[-0.621094 -0.738281 -0.867188 ... -0.175781 -0.394531 -0.128906]\n",
            "  [-0.621094 -0.738281 -0.867188 ... -0.175781 -0.394531 -0.128906]\n",
            "  [-0.644531 -0.734375 -0.875 ... -0.181641 -0.392578 -0.12793]\n",
            "  ...\n",
            "  [0.566406 -0.808594 -2.59375 ... 0.300781 -2 0.289062]\n",
            "  [-1.57031 -0.890625 -1.17188 ... -0.746094 -0.582031 -0.279297]\n",
            "  [-1.52344 -0.894531 -1.54688 ... -1.125 -0.6875 -0.365234]]\n",
            "\n",
            " [[-0.621094 -0.738281 -0.867188 ... -0.175781 -0.394531 -0.128906]\n",
            "  [-0.621094 -0.738281 -0.867188 ... -0.175781 -0.394531 -0.128906]\n",
            "  [-0.644531 -0.734375 -0.875 ... -0.181641 -0.392578 -0.12793]\n",
            "  ...\n",
            "  [-0.304688 0.710938 -1.32812 ... 0.259766 -0.90625 -0.277344]\n",
            "  [-0.376953 0.209961 -2.21875 ... 0.283203 -0.263672 -0.143555]\n",
            "  [-1.57031 -0.410156 -1.42969 ... -1.04688 -0.65625 -1.46094]]\n",
            "\n",
            " [[-0.621094 -0.738281 -0.867188 ... -0.175781 -0.394531 -0.128906]\n",
            "  [0.176758 0.707031 -0.894531 ... -0.296875 -0.18457 -0.0306396]\n",
            "  [0.226562 0.0512695 -0.84375 ... 0.166992 -0.453125 -0.746094]\n",
            "  ...\n",
            "  [-1.66406 1.875 -1.96875 ... -0.341797 0.474609 1.28125]\n",
            "  [-0.400391 0.703125 -2.1875 ... 0.714844 1.13281 -0.855469]\n",
            "  [-1.98438 0.0722656 -1.36719 ... -1.23438 -0.492188 -1.42969]]]\n",
            "attention_lnx=[[[-0.204102 -0.202148 -0.878906 ... -0.53125 -0.285156 -1.23438]\n",
            "  [-0.204102 -0.202148 -0.878906 ... -0.53125 -0.285156 -1.23438]\n",
            "  [-0.199219 -0.198242 -0.882812 ... -0.527344 -0.291016 -1.23438]\n",
            "  ...\n",
            "  [-0.259766 -0.773438 -0.730469 ... -0.322266 0.220703 -0.839844]\n",
            "  [-0.277344 -0.667969 -0.769531 ... -0.511719 0.157227 -0.796875]\n",
            "  [-0.241211 -0.699219 -0.753906 ... -0.816406 0.143555 -0.675781]]\n",
            "\n",
            " [[-0.421875 -1.47656 0.0078125 ... 0.921875 0.714844 0.847656]\n",
            "  [-0.675781 -0.953125 0.832031 ... 1.19531 0.542969 0.15918]\n",
            "  [-0.400391 -1.05469 0.330078 ... 1.07031 0.78125 0.0864258]\n",
            "  ...\n",
            "  [0.109863 -0.00265503 -1.35156 ... 0.917969 0.443359 -0.419922]\n",
            "  [-0.0178223 -0.386719 -1.10938 ... 0.75 0.992188 -0.24707]\n",
            "  [-0.148438 0.0292969 -1.03906 ... 0.851562 0.722656 -0.144531]]\n",
            "\n",
            " [[-0.204102 -0.202148 -0.878906 ... -0.53125 -0.285156 -1.23438]\n",
            "  [-0.204102 -0.202148 -0.878906 ... -0.53125 -0.285156 -1.23438]\n",
            "  [-0.199219 -0.198242 -0.882812 ... -0.527344 -0.291016 -1.23438]\n",
            "  ...\n",
            "  [0.102539 -0.949219 -0.761719 ... -0.363281 -0.257812 -0.271484]\n",
            "  [0.0412598 -0.867188 -0.824219 ... -0.613281 -0.300781 -0.248047]\n",
            "  [0.0927734 -0.664062 -1.07031 ... -0.820312 -0.168945 -0.425781]]\n",
            "\n",
            " [[-0.204102 -0.202148 -0.878906 ... -0.53125 -0.285156 -1.23438]\n",
            "  [-0.204102 -0.202148 -0.878906 ... -0.53125 -0.285156 -1.23438]\n",
            "  [-0.199219 -0.198242 -0.882812 ... -0.527344 -0.291016 -1.23438]\n",
            "  ...\n",
            "  [0.0170898 -0.847656 -0.902344 ... -0.0294189 -0.138672 -0.353516]\n",
            "  [0.041748 -0.472656 -0.441406 ... -0.605469 -0.0116577 -0.695312]\n",
            "  [-0.0888672 -0.185547 -0.84375 ... -0.984375 -0.0471191 -0.345703]]\n",
            "\n",
            " [[-0.204102 -0.202148 -0.878906 ... -0.53125 -0.285156 -1.23438]\n",
            "  [-0.128906 -1.25781 -0.992188 ... 0.515625 0.558594 -0.0201416]\n",
            "  [0.300781 -1.03906 -0.652344 ... 0.792969 0.660156 -0.0167236]\n",
            "  ...\n",
            "  [0.376953 -1.25 -1.53125 ... 0.423828 0.114746 -0.000289917]\n",
            "  [0.238281 -1.28125 -1.46875 ... 0.209961 -0.118164 -0.210938]\n",
            "  [-0.347656 -1.25781 -1.58594 ... -0.0869141 -0.0378418 -0.292969]]]\n",
            "attn_output=[[[-0.652344 -0.761719 -1.04688 ... -0.294922 -0.449219 -0.412109]\n",
            "  [-0.652344 -0.761719 -1.04688 ... -0.294922 -0.449219 -0.412109]\n",
            "  [-0.671875 -0.757812 -1.05469 ... -0.298828 -0.449219 -0.412109]\n",
            "  ...\n",
            "  [-0.847656 -0.326172 -1.26562 ... 0.097168 0.429688 1.64844]\n",
            "  [-0.291016 -1.64062 -1.92188 ... 0.648438 1.01562 0.137695]\n",
            "  [-1.88281 -0.917969 -0.886719 ... -0.332031 0.129883 -1.17188]]\n",
            "\n",
            " [[-0.240234 0.484375 -0.503906 ... -0.636719 -0.855469 0.425781]\n",
            "  [0.503906 0.369141 -0.388672 ... -0.486328 0.382812 0.933594]\n",
            "  [-0.46875 0.174805 -0.796875 ... -0.0634766 0.474609 -0.859375]\n",
            "  ...\n",
            "  [1.60156 2.17188 -2.67188 ... -0.5625 0.867188 0.0483398]\n",
            "  [-1.09375 0.800781 -1.48438 ... -1.13281 -0.292969 -0.0795898]\n",
            "  [-2.34375 1.45312 -0.480469 ... -1.27344 0.271484 -1.33594]]\n",
            "\n",
            " [[-0.652344 -0.761719 -1.04688 ... -0.294922 -0.449219 -0.412109]\n",
            "  [-0.652344 -0.761719 -1.04688 ... -0.294922 -0.449219 -0.412109]\n",
            "  [-0.671875 -0.757812 -1.05469 ... -0.298828 -0.449219 -0.412109]\n",
            "  ...\n",
            "  [0.582031 -1.05469 -2.75 ... 0.189453 -2.01562 0.204102]\n",
            "  [-1.51562 -1.10938 -1.36719 ... -0.894531 -0.648438 -0.341797]\n",
            "  [-1.45312 -1.02344 -1.75781 ... -1.28906 -0.707031 -0.457031]]\n",
            "\n",
            " [[-0.652344 -0.761719 -1.04688 ... -0.294922 -0.449219 -0.412109]\n",
            "  [-0.652344 -0.761719 -1.04688 ... -0.294922 -0.449219 -0.412109]\n",
            "  [-0.671875 -0.757812 -1.05469 ... -0.298828 -0.449219 -0.412109]\n",
            "  ...\n",
            "  [-0.291016 0.4375 -1.55469 ... 0.244141 -0.921875 -0.375]\n",
            "  [-0.351562 0.0610352 -2.28125 ... 0.0927734 -0.257812 -0.347656]\n",
            "  [-1.53906 -0.443359 -1.60156 ... -1.25781 -0.648438 -1.50781]]\n",
            "\n",
            " [[-0.652344 -0.761719 -1.04688 ... -0.294922 -0.449219 -0.412109]\n",
            "  [0.138672 0.357422 -1.13281 ... -0.152344 -0.0319824 -0.0351562]\n",
            "  [0.302734 -0.227539 -1 ... 0.375 -0.265625 -0.734375]\n",
            "  ...\n",
            "  [-1.49219 1.42969 -2.39062 ... -0.199219 0.496094 1.24219]\n",
            "  [-0.314453 0.275391 -2.59375 ... 0.765625 1.0625 -0.902344]\n",
            "  [-2.03125 -0.273438 -1.76562 ... -1.22656 -0.488281 -1.47656]]]\n",
            "next_layer_addition_dropped_out=[[[-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.92188 -3.10938 -4.59375 ... -1.875 -0.570312 -3.5625]\n",
            "  ...\n",
            "  [-2.64062 -1.01562 -4.59375 ... 0.0546875 2.3125 5.59375]\n",
            "  [-1.07812 -5.25 -6.3125 ... 2.04688 4.09375 0.828125]\n",
            "  [-7.59375 -4.03125 -3.25 ... -3.45312 0.291016 -6.75]]\n",
            "\n",
            " [[-2.0625 1.85156 -1.97656 ... -3.35938 -3.75 2.29688]\n",
            "  [2.39062 1.25781 -1.32031 ... -1.94531 1.75 3.75]\n",
            "  [-2.51562 0.71875 -2.65625 ... 0.0107422 1.625 -3.65625]\n",
            "  ...\n",
            "  [5.9375 7.4375 -9.125 ... -2.20312 1.83594 0.804688]\n",
            "  [-5.125 3.57812 -5.84375 ... -3.23438 -2.21875 -0.589844]\n",
            "  [-7 5.03125 -2.96875 ... -4.53125 0.523438 -3.45312]]\n",
            "\n",
            " [[-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.92188 -3.10938 -4.59375 ... -1.875 -0.570312 -3.5625]\n",
            "  ...\n",
            "  [2.25 -5.125 -10.625 ... 0.296875 -6.59375 -0.265625]\n",
            "  [-5.25 -4.28125 -3.875 ... -4.375 -2.29688 -2.51562]\n",
            "  [-6.53125 -5.03125 -7.84375 ... -6.75 -2.78125 -3.25]]\n",
            "\n",
            " [[-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.92188 -3.10938 -4.59375 ... -1.875 -0.570312 -3.5625]\n",
            "  ...\n",
            "  [-1.28125 1.42188 -3.84375 ... 0.144531 -2.15625 -1.84375]\n",
            "  [-2.25 0.535156 -6.6875 ... 0.4375 -0.203125 -2.09375]\n",
            "  [-6.09375 -2.8125 -7.1875 ... -6.3125 -2.21875 -7.0625]]\n",
            "\n",
            " [[-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [1.26562 1.70312 -4.0625 ... -1.32812 0.644531 -0.402344]\n",
            "  [1.42188 -0.3125 -3.54688 ... 0.640625 -0.367188 -4.59375]\n",
            "  ...\n",
            "  [-4.375 3.89062 -7.875 ... -0.585938 2.15625 3.65625]\n",
            "  [-0.265625 0.140625 -8.25 ... 2.875 3.42188 -3.34375]\n",
            "  [-6.5625 -1.48438 -6.6875 ... -5.15625 -1.72656 -6.4375]]]\n",
            "inputs=[[[-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.92188 -3.10938 -4.59375 ... -1.875 -0.570312 -3.5625]\n",
            "  ...\n",
            "  [-2.64062 -1.01562 -4.59375 ... 0.0546875 2.3125 5.59375]\n",
            "  [-1.07812 -5.25 -6.3125 ... 2.04688 4.09375 0.828125]\n",
            "  [-7.59375 -4.03125 -3.25 ... -3.45312 0.291016 -6.75]]\n",
            "\n",
            " [[-2.0625 1.85156 -1.97656 ... -3.35938 -3.75 2.29688]\n",
            "  [2.39062 1.25781 -1.32031 ... -1.94531 1.75 3.75]\n",
            "  [-2.51562 0.71875 -2.65625 ... 0.0107422 1.625 -3.65625]\n",
            "  ...\n",
            "  [5.9375 7.4375 -9.125 ... -2.20312 1.83594 0.804688]\n",
            "  [-5.125 3.57812 -5.84375 ... -3.23438 -2.21875 -0.589844]\n",
            "  [-7 5.03125 -2.96875 ... -4.53125 0.523438 -3.45312]]\n",
            "\n",
            " [[-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.92188 -3.10938 -4.59375 ... -1.875 -0.570312 -3.5625]\n",
            "  ...\n",
            "  [2.25 -5.125 -10.625 ... 0.296875 -6.59375 -0.265625]\n",
            "  [-5.25 -4.28125 -3.875 ... -4.375 -2.29688 -2.51562]\n",
            "  [-6.53125 -5.03125 -7.84375 ... -6.75 -2.78125 -3.25]]\n",
            "\n",
            " [[-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.92188 -3.10938 -4.59375 ... -1.875 -0.570312 -3.5625]\n",
            "  ...\n",
            "  [-1.28125 1.42188 -3.84375 ... 0.144531 -2.15625 -1.84375]\n",
            "  [-2.25 0.535156 -6.6875 ... 0.4375 -0.203125 -2.09375]\n",
            "  [-6.09375 -2.8125 -7.1875 ... -6.3125 -2.21875 -7.0625]]\n",
            "\n",
            " [[-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [1.26562 1.70312 -4.0625 ... -1.32812 0.644531 -0.402344]\n",
            "  [1.42188 -0.3125 -3.54688 ... 0.640625 -0.367188 -4.59375]\n",
            "  ...\n",
            "  [-4.375 3.89062 -7.875 ... -0.585938 2.15625 3.65625]\n",
            "  [-0.265625 0.140625 -8.25 ... 2.875 3.42188 -3.34375]\n",
            "  [-6.5625 -1.48438 -6.6875 ... -5.15625 -1.72656 -6.4375]]]\n",
            "lnx=[[[-0.412109 -0.710938 -1.04688 ... -0.423828 -0.129883 -0.808594]\n",
            "  [-0.412109 -0.710938 -1.04688 ... -0.423828 -0.129883 -0.808594]\n",
            "  [-0.4375 -0.707031 -1.04688 ... -0.425781 -0.129883 -0.808594]\n",
            "  ...\n",
            "  [-0.707031 -0.271484 -1.22656 ... 0.0146484 0.617188 1.5]\n",
            "  [-0.302734 -1.46875 -1.76562 ... 0.574219 1.14844 0.231445]\n",
            "  [-1.8125 -0.960938 -0.773438 ... -0.824219 0.0693359 -1.60938]]\n",
            "\n",
            " [[-0.476562 0.427734 -0.457031 ... -0.777344 -0.867188 0.53125]\n",
            "  [0.621094 0.326172 -0.341797 ... -0.503906 0.453125 0.972656]\n",
            "  [-0.667969 0.19043 -0.703125 ... 0.00285339 0.429688 -0.96875]\n",
            "  ...\n",
            "  [1.83594 2.29688 -2.8125 ... -0.679688 0.566406 0.248047]\n",
            "  [-1.32031 0.921875 -1.50781 ... -0.835938 -0.570312 -0.152344]\n",
            "  [-1.96094 1.40625 -0.832031 ... -1.27344 0.146484 -0.96875]]\n",
            "\n",
            " [[-0.412109 -0.710938 -1.04688 ... -0.423828 -0.129883 -0.808594]\n",
            "  [-0.412109 -0.710938 -1.04688 ... -0.423828 -0.129883 -0.808594]\n",
            "  [-0.4375 -0.707031 -1.04688 ... -0.425781 -0.129883 -0.808594]\n",
            "  ...\n",
            "  [0.613281 -1.39844 -2.89062 ... 0.0810547 -1.79688 -0.0722656]\n",
            "  [-1.41406 -1.14844 -1.03906 ... -1.17969 -0.617188 -0.675781]\n",
            "  [-1.46875 -1.13281 -1.76562 ... -1.51562 -0.625 -0.730469]]\n",
            "\n",
            " [[-0.412109 -0.710938 -1.04688 ... -0.423828 -0.129883 -0.808594]\n",
            "  [-0.412109 -0.710938 -1.04688 ... -0.423828 -0.129883 -0.808594]\n",
            "  [-0.4375 -0.707031 -1.04688 ... -0.425781 -0.129883 -0.808594]\n",
            "  ...\n",
            "  [-0.371094 0.412109 -1.11719 ... 0.0419922 -0.625 -0.535156]\n",
            "  [-0.65625 0.15625 -1.95312 ... 0.12793 -0.0593262 -0.609375]\n",
            "  [-1.41406 -0.652344 -1.66406 ... -1.46094 -0.515625 -1.64062]]\n",
            "\n",
            " [[-0.412109 -0.710938 -1.04688 ... -0.423828 -0.129883 -0.808594]\n",
            "  [0.328125 0.441406 -1.05469 ... -0.34375 0.166992 -0.104004]\n",
            "  [0.373047 -0.0820312 -0.933594 ... 0.167969 -0.0966797 -1.20312]\n",
            "  ...\n",
            "  [-1.35156 1.20312 -2.4375 ... -0.180664 0.664062 1.13281]\n",
            "  [-0.0834961 0.0441895 -2.59375 ... 0.902344 1.07031 -1.04688]\n",
            "  [-1.67969 -0.380859 -1.71094 ... -1.32031 -0.441406 -1.64844]]]\n",
            "attention_lnx=[[[-0.197266 -0.296875 0.330078 ... -0.0131836 0.605469 -1.46094]\n",
            "  [-0.197266 -0.296875 0.330078 ... -0.0131836 0.605469 -1.46094]\n",
            "  [-0.202148 -0.294922 0.333984 ... -0.0186768 0.597656 -1.46094]\n",
            "  ...\n",
            "  [-0.492188 0.0371094 0.585938 ... 0.306641 0.816406 -1.34375]\n",
            "  [-0.5 -0.0957031 0.539062 ... 0.145508 0.738281 -1.25781]\n",
            "  [-0.488281 -0.0480957 0.347656 ... 0.337891 0.585938 -1.15625]]\n",
            "\n",
            " [[0.355469 1.41406 1.21875 ... 0.566406 0.0622559 1.5625]\n",
            "  [0.363281 0.910156 0.636719 ... 0.507812 -0.11084 1.4375]\n",
            "  [0.484375 1.36719 0.898438 ... 0.410156 -0.396484 1.49219]\n",
            "  ...\n",
            "  [0.238281 0.523438 1.03906 ... 0.972656 -0.412109 0.851562]\n",
            "  [-0.259766 1.10156 0.621094 ... 1.28125 -0.237305 0.306641]\n",
            "  [-0.285156 0.761719 0.65625 ... 1.44531 -0.233398 0.675781]]\n",
            "\n",
            " [[-0.197266 -0.296875 0.330078 ... -0.0131836 0.605469 -1.46094]\n",
            "  [-0.197266 -0.296875 0.330078 ... -0.0131836 0.605469 -1.46094]\n",
            "  [-0.202148 -0.294922 0.333984 ... -0.0186768 0.597656 -1.46094]\n",
            "  ...\n",
            "  [-0.3125 0.0366211 0.101074 ... 0.980469 0.554688 -0.498047]\n",
            "  [-0.523438 0.026001 -0.125 ... 1.0625 0.625 -0.546875]\n",
            "  [-0.181641 -0.0393066 -0.474609 ... 1.35938 0.388672 -1.29688]]\n",
            "\n",
            " [[-0.197266 -0.296875 0.330078 ... -0.0131836 0.605469 -1.46094]\n",
            "  [-0.197266 -0.296875 0.330078 ... -0.0131836 0.605469 -1.46094]\n",
            "  [-0.202148 -0.294922 0.333984 ... -0.0186768 0.597656 -1.46094]\n",
            "  ...\n",
            "  [-0.239258 0.351562 -0.0795898 ... 0.660156 0.722656 -0.628906]\n",
            "  [-0.546875 0.249023 0.0917969 ... 0.511719 0.980469 -0.498047]\n",
            "  [-0.326172 0.200195 -0.324219 ... 0.882812 0.585938 -1.01562]]\n",
            "\n",
            " [[-0.197266 -0.296875 0.330078 ... -0.0131836 0.605469 -1.46094]\n",
            "  [0.144531 0.8125 1.36719 ... 0.181641 0.695312 -0.558594]\n",
            "  [-0.158203 0.455078 0.628906 ... -0.00588989 0.287109 -0.535156]\n",
            "  ...\n",
            "  [-0.217773 -0.0148315 -0.178711 ... 0.949219 0.0375977 0.0742188]\n",
            "  [0.236328 0.235352 -0.105957 ... 0.90625 0.0966797 0.285156]\n",
            "  [-0.494141 -0.167969 -0.279297 ... 1.19531 0.0517578 -0.605469]]]\n",
            "attn_output=[[[-0.441406 -0.75 -0.933594 ... -0.412109 0.00772095 -1.10156]\n",
            "  [-0.441406 -0.75 -0.933594 ... -0.412109 0.00772095 -1.10156]\n",
            "  [-0.464844 -0.746094 -0.933594 ... -0.416016 0.00598145 -1.10156]\n",
            "  ...\n",
            "  [-0.8125 -0.253906 -1.03906 ... 0.09375 0.8125 1.10156]\n",
            "  [-0.427734 -1.44531 -1.5625 ... 0.59375 1.30469 -0.116211]\n",
            "  [-1.85938 -0.9375 -0.667969 ... -0.714844 0.202148 -1.82031]]\n",
            "\n",
            " [[-0.384766 0.734375 -0.170898 ... -0.628906 -0.832031 0.871094]\n",
            "  [0.703125 0.550781 -0.173828 ... -0.367188 0.417969 1.32031]\n",
            "  [-0.527344 0.542969 -0.457031 ... 0.109863 0.320312 -0.5625]\n",
            "  ...\n",
            "  [1.86719 2.40625 -2.45312 ... -0.373047 0.431641 0.5]\n",
            "  [-1.35156 1.17188 -1.3125 ... -0.490234 -0.617188 -0.0712891]\n",
            "  [-1.99219 1.58594 -0.632812 ... -0.84375 0.0791016 -0.757812]]\n",
            "\n",
            " [[-0.441406 -0.75 -0.933594 ... -0.412109 0.00772095 -1.10156]\n",
            "  [-0.441406 -0.75 -0.933594 ... -0.412109 0.00772095 -1.10156]\n",
            "  [-0.464844 -0.746094 -0.933594 ... -0.416016 0.00598145 -1.10156]\n",
            "  ...\n",
            "  [0.511719 -1.34375 -2.76562 ... 0.335938 -1.59375 -0.201172]\n",
            "  [-1.50781 -1.10938 -1.04688 ... -0.863281 -0.4375 -0.800781]\n",
            "  [-1.45312 -1.10156 -1.80469 ... -1.17188 -0.519531 -0.984375]]\n",
            "\n",
            " [[-0.441406 -0.75 -0.933594 ... -0.412109 0.00772095 -1.10156]\n",
            "  [-0.441406 -0.75 -0.933594 ... -0.412109 0.00772095 -1.10156]\n",
            "  [-0.464844 -0.746094 -0.933594 ... -0.416016 0.00598145 -1.10156]\n",
            "  ...\n",
            "  [-0.427734 0.498047 -1.10156 ... 0.225586 -0.402344 -0.695312]\n",
            "  [-0.78125 0.219727 -1.84375 ... 0.265625 0.217773 -0.726562]\n",
            "  [-1.4375 -0.585938 -1.67969 ... -1.21094 -0.365234 -1.80469]]\n",
            "\n",
            " [[-0.441406 -0.75 -0.933594 ... -0.412109 0.00772095 -1.10156]\n",
            "  [0.353516 0.628906 -0.675781 ... -0.287109 0.335938 -0.240234]\n",
            "  [0.322266 0.036377 -0.746094 ... 0.162109 -0.0203857 -1.3125]\n",
            "  ...\n",
            "  [-1.375 1.16406 -2.42188 ... 0.108887 0.65625 1.11719]\n",
            "  [-0.00891113 0.114746 -2.54688 ... 1.15625 1.07031 -0.933594]\n",
            "  [-1.75 -0.410156 -1.72656 ... -0.984375 -0.416016 -1.75]]]\n",
            "next_layer_addition_dropped_out=[[[-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.57812 -1.69531 -4.46875 ... -3.23438 0.632812 -4.25]\n",
            "  ...\n",
            "  [-2.67188 -0.429688 -4.3125 ... -0.279297 3.40625 3.9375]\n",
            "  [-0.59375 -4.40625 -5.375 ... 2.40625 5.125 0.679688]\n",
            "  [-7.4375 -4.59375 -2.90625 ... -4.125 0.5 -7.28125]]\n",
            "\n",
            " [[-1.15625 3.42188 -1.32812 ... -3.5625 -4.15625 3.57812]\n",
            "  [3.23438 3.125 0.25 ... -1.50781 0.941406 4.53125]\n",
            "  [-0.820312 2.17188 -1.42969 ... 0.333984 0.835938 -1.82812]\n",
            "  ...\n",
            "  [6.34375 9.3125 -8.875 ... -1.5 3.125 2.78125]\n",
            "  [-6.375 4.625 -6.53125 ... -4.875 -2.32812 -1.10156]\n",
            "  [-8.5 5.59375 -2.48438 ... -4.09375 0.476562 -4.0625]]\n",
            "\n",
            " [[-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.57812 -1.69531 -4.46875 ... -3.23438 0.632812 -4.25]\n",
            "  ...\n",
            "  [1.80469 -3.78125 -10.25 ... 0.8125 -5.6875 -0.964844]\n",
            "  [-5.8125 -3.34375 -4.34375 ... -3.0625 -2.5 -3.15625]\n",
            "  [-7 -6 -7.3125 ... -6.5 -4 -4.53125]]\n",
            "\n",
            " [[-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.57812 -1.69531 -4.46875 ... -3.23438 0.632812 -4.25]\n",
            "  ...\n",
            "  [-1.82031 2.40625 -3.92188 ... -0.00390625 -1.89062 -1.39062]\n",
            "  [-2.92188 1.27344 -7.125 ... 0.261719 0.482422 -3.28125]\n",
            "  [-5.59375 -4.03125 -6.65625 ... -6.40625 -2.90625 -7.1875]]\n",
            "\n",
            " [[-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [1.60938 2.51562 -2.89062 ... -0.691406 2.10938 -0.371094]\n",
            "  [0.53125 1.17969 -2.78125 ... 0.167969 0.507812 -5]\n",
            "  ...\n",
            "  [-4.09375 3.95312 -8.125 ... 0.248047 1.39062 3.125]\n",
            "  [0.294922 -0.265625 -9.625 ... 3.65625 4.09375 -1.46875]\n",
            "  [-6.4375 -2.10938 -6.90625 ... -5 -2.39062 -6.90625]]]\n",
            "inputs=[[[-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.57812 -1.69531 -4.46875 ... -3.23438 0.632812 -4.25]\n",
            "  ...\n",
            "  [-2.67188 -0.429688 -4.3125 ... -0.279297 3.40625 3.9375]\n",
            "  [-0.59375 -4.40625 -5.375 ... 2.40625 5.125 0.679688]\n",
            "  [-7.4375 -4.59375 -2.90625 ... -4.125 0.5 -7.28125]]\n",
            "\n",
            " [[-1.15625 3.42188 -1.32812 ... -3.5625 -4.15625 3.57812]\n",
            "  [3.23438 3.125 0.25 ... -1.50781 0.941406 4.53125]\n",
            "  [-0.820312 2.17188 -1.42969 ... 0.333984 0.835938 -1.82812]\n",
            "  ...\n",
            "  [6.34375 9.3125 -8.875 ... -1.5 3.125 2.78125]\n",
            "  [-6.375 4.625 -6.53125 ... -4.875 -2.32812 -1.10156]\n",
            "  [-8.5 5.59375 -2.48438 ... -4.09375 0.476562 -4.0625]]\n",
            "\n",
            " [[-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.57812 -1.69531 -4.46875 ... -3.23438 0.632812 -4.25]\n",
            "  ...\n",
            "  [1.80469 -3.78125 -10.25 ... 0.8125 -5.6875 -0.964844]\n",
            "  [-5.8125 -3.34375 -4.34375 ... -3.0625 -2.5 -3.15625]\n",
            "  [-7 -6 -7.3125 ... -6.5 -4 -4.53125]]\n",
            "\n",
            " [[-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.57812 -1.69531 -4.46875 ... -3.23438 0.632812 -4.25]\n",
            "  ...\n",
            "  [-1.82031 2.40625 -3.92188 ... -0.00390625 -1.89062 -1.39062]\n",
            "  [-2.92188 1.27344 -7.125 ... 0.261719 0.482422 -3.28125]\n",
            "  [-5.59375 -4.03125 -6.65625 ... -6.40625 -2.90625 -7.1875]]\n",
            "\n",
            " [[-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [1.60938 2.51562 -2.89062 ... -0.691406 2.10938 -0.371094]\n",
            "  [0.53125 1.17969 -2.78125 ... 0.167969 0.507812 -5]\n",
            "  ...\n",
            "  [-4.09375 3.95312 -8.125 ... 0.248047 1.39062 3.125]\n",
            "  [0.294922 -0.265625 -9.625 ... 3.65625 4.09375 -1.46875]\n",
            "  [-6.4375 -2.10938 -6.90625 ... -5 -2.39062 -6.90625]]]\n",
            "lnx=[[[-0.3125 -0.367188 -0.960938 ... -0.6875 0.136719 -0.910156]\n",
            "  [-0.3125 -0.367188 -0.960938 ... -0.6875 0.136719 -0.910156]\n",
            "  [-0.337891 -0.363281 -0.957031 ... -0.691406 0.135742 -0.910156]\n",
            "  ...\n",
            "  [-0.683594 -0.109863 -1.10156 ... -0.0712891 0.871094 1.00781]\n",
            "  [-0.157227 -1.17188 -1.42969 ... 0.640625 1.35938 0.180664]\n",
            "  [-1.60938 -0.996094 -0.628906 ... -0.894531 0.108398 -1.57812]]\n",
            "\n",
            " [[-0.257812 0.761719 -0.294922 ... -0.792969 -0.925781 0.796875]\n",
            "  [0.8125 0.785156 0.0629883 ... -0.378906 0.237305 1.14062]\n",
            "  [-0.211914 0.558594 -0.369141 ... 0.0859375 0.21582 -0.470703]\n",
            "  ...\n",
            "  [1.86719 2.75 -2.60938 ... -0.441406 0.921875 0.820312]\n",
            "  [-1.5 1.08594 -1.53906 ... -1.14844 -0.546875 -0.259766]\n",
            "  [-2.20312 1.45312 -0.644531 ... -1.0625 0.123535 -1.05469]]\n",
            "\n",
            " [[-0.3125 -0.367188 -0.960938 ... -0.6875 0.136719 -0.910156]\n",
            "  [-0.3125 -0.367188 -0.960938 ... -0.6875 0.136719 -0.910156]\n",
            "  [-0.337891 -0.363281 -0.957031 ... -0.691406 0.135742 -0.910156]\n",
            "  ...\n",
            "  [0.453125 -0.949219 -2.57812 ... 0.204102 -1.42969 -0.242188]\n",
            "  [-1.42969 -0.824219 -1.07031 ... -0.753906 -0.617188 -0.777344]\n",
            "  [-1.375 -1.17969 -1.4375 ... -1.28125 -0.785156 -0.890625]]\n",
            "\n",
            " [[-0.3125 -0.367188 -0.960938 ... -0.6875 0.136719 -0.910156]\n",
            "  [-0.3125 -0.367188 -0.960938 ... -0.6875 0.136719 -0.910156]\n",
            "  [-0.337891 -0.363281 -0.957031 ... -0.691406 0.135742 -0.910156]\n",
            "  ...\n",
            "  [-0.498047 0.660156 -1.07812 ... -0.00106812 -0.519531 -0.380859]\n",
            "  [-0.796875 0.347656 -1.94531 ... 0.0712891 0.131836 -0.894531]\n",
            "  [-1.13281 -0.816406 -1.34375 ... -1.29688 -0.589844 -1.45312]]\n",
            "\n",
            " [[-0.3125 -0.367188 -0.960938 ... -0.6875 0.136719 -0.910156]\n",
            "  [0.394531 0.617188 -0.707031 ... -0.168945 0.515625 -0.0908203]\n",
            "  [0.132812 0.294922 -0.699219 ... 0.0422363 0.126953 -1.25]\n",
            "  ...\n",
            "  [-1.19531 1.15625 -2.375 ... 0.0722656 0.40625 0.910156]\n",
            "  [0.0878906 -0.0795898 -2.875 ... 1.09375 1.22656 -0.439453]\n",
            "  [-1.46094 -0.478516 -1.5625 ... -1.13281 -0.542969 -1.5625]]]\n",
            "attention_lnx=[[[-0.5 0.0218506 -1.44531 ... -0.0722656 -1.25 -0.644531]\n",
            "  [-0.5 0.0218506 -1.44531 ... -0.0722656 -1.25 -0.644531]\n",
            "  [-0.503906 0.0233154 -1.45312 ... -0.0703125 -1.25 -0.640625]\n",
            "  ...\n",
            "  [-0.361328 0.515625 -1.375 ... -0.22168 -1.4375 -0.427734]\n",
            "  [-0.398438 0.515625 -1.39062 ... -0.207031 -1.30469 -0.574219]\n",
            "  [-0.609375 0.330078 -1.35156 ... -0.234375 -1.21875 -0.582031]]\n",
            "\n",
            " [[-0.386719 -0.0751953 -0.097168 ... -0.957031 -0.679688 -0.925781]\n",
            "  [-0.722656 0.151367 -0.722656 ... -0.726562 0.00674438 -0.404297]\n",
            "  [-0.71875 0.0554199 -1.17188 ... -0.255859 0.227539 -0.558594]\n",
            "  ...\n",
            "  [-0.291016 -0.263672 -0.824219 ... 0.15625 0.130859 -0.796875]\n",
            "  [-0.21582 -0.585938 -0.208984 ... 0.251953 0.107422 -0.679688]\n",
            "  [-0.53125 -0.597656 -0.141602 ... 0.191406 -0.0228271 -0.574219]]\n",
            "\n",
            " [[-0.5 0.0218506 -1.44531 ... -0.0722656 -1.25 -0.644531]\n",
            "  [-0.5 0.0218506 -1.44531 ... -0.0722656 -1.25 -0.644531]\n",
            "  [-0.503906 0.0233154 -1.45312 ... -0.0703125 -1.25 -0.640625]\n",
            "  ...\n",
            "  [-0.208984 0.291016 -1.32031 ... 0.00576782 -0.660156 -0.269531]\n",
            "  [-0.239258 0.396484 -1.10938 ... -0.0578613 -0.652344 -0.318359]\n",
            "  [-0.392578 0.710938 -1.17188 ... -0.00473022 -0.5625 -0.365234]]\n",
            "\n",
            " [[-0.5 0.0218506 -1.44531 ... -0.0722656 -1.25 -0.644531]\n",
            "  [-0.5 0.0218506 -1.44531 ... -0.0722656 -1.25 -0.644531]\n",
            "  [-0.503906 0.0233154 -1.45312 ... -0.0703125 -1.25 -0.640625]\n",
            "  ...\n",
            "  [-0.226562 0.367188 -1.41406 ... -0.182617 -0.738281 -0.376953]\n",
            "  [-0.419922 0.414062 -1.5 ... 0.0500488 -0.835938 -0.40625]\n",
            "  [-0.5 0.46875 -1.53125 ... -0.0169678 -0.839844 -0.453125]]\n",
            "\n",
            " [[-0.5 0.0218506 -1.44531 ... -0.0722656 -1.25 -0.644531]\n",
            "  [-0.386719 0.470703 -1.5 ... -0.421875 -1.21875 -0.445312]\n",
            "  [-0.570312 0.65625 -1.63281 ... -0.527344 -1.17969 -0.316406]\n",
            "  ...\n",
            "  [-0.84375 0.482422 -1.63281 ... 0.474609 -0.804688 -0.613281]\n",
            "  [-0.871094 0.722656 -1.55469 ... 0.78125 -0.992188 -0.617188]\n",
            "  [-0.828125 0.439453 -1.32812 ... 0.318359 -0.738281 -0.648438]]]\n",
            "attn_output=[[[-0.410156 -0.355469 -1.24219 ... -0.6875 -0.12793 -1.02344]\n",
            "  [-0.410156 -0.355469 -1.24219 ... -0.6875 -0.12793 -1.02344]\n",
            "  [-0.435547 -0.349609 -1.24219 ... -0.691406 -0.128906 -1.02344]\n",
            "  ...\n",
            "  [-0.757812 0.0214844 -1.42188 ... -0.125 0.492188 0.875]\n",
            "  [-0.257812 -1.00781 -1.75781 ... 0.570312 0.992188 0.0274658]\n",
            "  [-1.70312 -0.902344 -0.898438 ... -0.921875 -0.152344 -1.66406]]\n",
            "\n",
            " [[-0.335938 0.726562 -0.308594 ... -0.980469 -1.04688 0.574219]\n",
            "  [0.621094 0.808594 -0.116699 ... -0.550781 0.234375 1.01562]\n",
            "  [-0.392578 0.570312 -0.664062 ... 0.0198975 0.271484 -0.609375]\n",
            "  ...\n",
            "  [1.75 2.60938 -2.79688 ... -0.388672 0.941406 0.574219]\n",
            "  [-1.50781 0.925781 -1.53906 ... -1.05469 -0.507812 -0.408203]\n",
            "  [-2.29688 1.27344 -0.667969 ... -0.992188 0.115234 -1.17969]]\n",
            "\n",
            " [[-0.410156 -0.355469 -1.24219 ... -0.6875 -0.12793 -1.02344]\n",
            "  [-0.410156 -0.355469 -1.24219 ... -0.6875 -0.12793 -1.02344]\n",
            "  [-0.435547 -0.349609 -1.24219 ... -0.691406 -0.128906 -1.02344]\n",
            "  ...\n",
            "  [0.392578 -0.859375 -2.85938 ... 0.202148 -1.5625 -0.304688]\n",
            "  [-1.45312 -0.707031 -1.3125 ... -0.75 -0.757812 -0.835938]\n",
            "  [-1.42969 -1.02344 -1.64062 ... -1.25781 -0.878906 -0.945312]]\n",
            "\n",
            " [[-0.410156 -0.355469 -1.24219 ... -0.6875 -0.12793 -1.02344]\n",
            "  [-0.410156 -0.355469 -1.24219 ... -0.6875 -0.12793 -1.02344]\n",
            "  [-0.435547 -0.349609 -1.24219 ... -0.691406 -0.128906 -1.02344]\n",
            "  ...\n",
            "  [-0.546875 0.742188 -1.42969 ... -0.0500488 -0.703125 -0.474609]\n",
            "  [-0.886719 0.449219 -2.29688 ... 0.0830078 -0.09375 -0.980469]\n",
            "  [-1.21094 -0.707031 -1.625 ... -1.27344 -0.742188 -1.51562]]\n",
            "\n",
            " [[-0.410156 -0.355469 -1.24219 ... -0.6875 -0.12793 -1.02344]\n",
            "  [0.292969 0.714844 -1.05469 ... -0.265625 0.212891 -0.195312]\n",
            "  [-0.00958252 0.451172 -1.08594 ... -0.0883789 -0.165039 -1.30469]\n",
            "  ...\n",
            "  [-1.40625 1.26562 -2.78125 ... 0.206055 0.166992 0.714844]\n",
            "  [-0.167969 0.132812 -3.26562 ... 1.29688 0.90625 -0.609375]\n",
            "  [-1.60938 -0.369141 -1.82031 ... -1.03906 -0.691406 -1.67188]]]\n",
            "next_layer_addition_dropped_out=[[[-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.5 -1.66406 -5.78125 ... -2.0625 -1.21875 -3.32812]\n",
            "  ...\n",
            "  [-3.15625 0.226562 -6.09375 ... 0.578125 2.71875 3.8125]\n",
            "  [-0.328125 -3.96875 -6.875 ... 2.875 3.1875 0.722656]\n",
            "  [-7.40625 -4.5625 -6.59375 ... -2.125 -1.03125 -6.9375]]\n",
            "\n",
            " [[-1.48438 3.34375 -1.9375 ... -4.875 -4.5625 2.6875]\n",
            "  [2.6875 2.59375 -1.34375 ... -2.21875 1.17969 4.03125]\n",
            "  [-1.9375 1.76562 -3.23438 ... -1.14062 0.449219 -2.95312]\n",
            "  ...\n",
            "  [4.9375 8.5 -10.5625 ... -0.671875 3.07812 3.15625]\n",
            "  [-7.71875 3.07812 -6.875 ... -4.875 -2.79688 -2.6875]\n",
            "  [-8.9375 4.375 -2.59375 ... -5.0625 0.0371094 -4.25]]\n",
            "\n",
            " [[-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.5 -1.66406 -5.78125 ... -2.0625 -1.21875 -3.32812]\n",
            "  ...\n",
            "  [0.773438 -3.90625 -11.0625 ... 2.53125 -7.625 0.757812]\n",
            "  [-6.8125 -3.9375 -7.3125 ... -0.046875 -2.9375 -3.1875]\n",
            "  [-6.53125 -6.6875 -10.4375 ... -3.84375 -4.4375 -4.09375]]\n",
            "\n",
            " [[-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.5 -1.66406 -5.78125 ... -2.0625 -1.21875 -3.32812]\n",
            "  ...\n",
            "  [-3.03125 3.03125 -5.8125 ... -0.378906 -3.125 -1.74219]\n",
            "  [-3.82812 1.10938 -9.125 ... 1.78906 -1 -3.26562]\n",
            "  [-6.03125 -5.25 -10.5625 ... -3.92188 -3.75 -5.9375]]\n",
            "\n",
            " [[-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [1.42188 3.5625 -4.78125 ... -0.851562 0.4375 0.574219]\n",
            "  [-0.640625 2.10938 -5.21875 ... 0.328125 0.117188 -4.125]\n",
            "  ...\n",
            "  [-4.375 3.98438 -9.375 ... 0.894531 -0.421875 2.6875]\n",
            "  [-1.32812 0.462891 -12.4375 ... 5.03125 3.625 -1.3125]\n",
            "  [-7.25 -2.60938 -9.75 ... -3.71875 -4 -7.3125]]]\n",
            "inputs=[[[-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.5 -1.66406 -5.78125 ... -2.0625 -1.21875 -3.32812]\n",
            "  ...\n",
            "  [-3.15625 0.226562 -6.09375 ... 0.578125 2.71875 3.8125]\n",
            "  [-0.328125 -3.96875 -6.875 ... 2.875 3.1875 0.722656]\n",
            "  [-7.40625 -4.5625 -6.59375 ... -2.125 -1.03125 -6.9375]]\n",
            "\n",
            " [[-1.48438 3.34375 -1.9375 ... -4.875 -4.5625 2.6875]\n",
            "  [2.6875 2.59375 -1.34375 ... -2.21875 1.17969 4.03125]\n",
            "  [-1.9375 1.76562 -3.23438 ... -1.14062 0.449219 -2.95312]\n",
            "  ...\n",
            "  [4.9375 8.5 -10.5625 ... -0.671875 3.07812 3.15625]\n",
            "  [-7.71875 3.07812 -6.875 ... -4.875 -2.79688 -2.6875]\n",
            "  [-8.9375 4.375 -2.59375 ... -5.0625 0.0371094 -4.25]]\n",
            "\n",
            " [[-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.5 -1.66406 -5.78125 ... -2.0625 -1.21875 -3.32812]\n",
            "  ...\n",
            "  [0.773438 -3.90625 -11.0625 ... 2.53125 -7.625 0.757812]\n",
            "  [-6.8125 -3.9375 -7.3125 ... -0.046875 -2.9375 -3.1875]\n",
            "  [-6.53125 -6.6875 -10.4375 ... -3.84375 -4.4375 -4.09375]]\n",
            "\n",
            " [[-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.5 -1.66406 -5.78125 ... -2.0625 -1.21875 -3.32812]\n",
            "  ...\n",
            "  [-3.03125 3.03125 -5.8125 ... -0.378906 -3.125 -1.74219]\n",
            "  [-3.82812 1.10938 -9.125 ... 1.78906 -1 -3.26562]\n",
            "  [-6.03125 -5.25 -10.5625 ... -3.92188 -3.75 -5.9375]]\n",
            "\n",
            " [[-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [1.42188 3.5625 -4.78125 ... -0.851562 0.4375 0.574219]\n",
            "  [-0.640625 2.10938 -5.21875 ... 0.328125 0.117188 -4.125]\n",
            "  ...\n",
            "  [-4.375 3.98438 -9.375 ... 0.894531 -0.421875 2.6875]\n",
            "  [-1.32812 0.462891 -12.4375 ... 5.03125 3.625 -1.3125]\n",
            "  [-7.25 -2.60938 -9.75 ... -3.71875 -4 -7.3125]]]\n",
            "lnx=[[[-0.28125 -0.345703 -1.17969 ... -0.417969 -0.24707 -0.675781]\n",
            "  [-0.28125 -0.345703 -1.17969 ... -0.417969 -0.24707 -0.675781]\n",
            "  [-0.304688 -0.337891 -1.17188 ... -0.417969 -0.24707 -0.675781]\n",
            "  ...\n",
            "  [-0.773438 0.0556641 -1.5 ... 0.141602 0.667969 0.9375]\n",
            "  [-0.0830078 -1 -1.74219 ... 0.726562 0.804688 0.182617]\n",
            "  [-1.46094 -0.902344 -1.30469 ... -0.419922 -0.204102 -1.36719]]\n",
            "\n",
            " [[-0.318359 0.714844 -0.414062 ... -1.04688 -0.976562 0.574219]\n",
            "  [0.65625 0.632812 -0.328125 ... -0.542969 0.289062 0.984375]\n",
            "  [-0.484375 0.441406 -0.808594 ... -0.285156 0.112305 -0.738281]\n",
            "  ...\n",
            "  [1.40625 2.42188 -3 ... -0.191406 0.875 0.898438]\n",
            "  [-1.625 0.648438 -1.44531 ... -1.02344 -0.585938 -0.566406]\n",
            "  [-2.125 1.03906 -0.617188 ... -1.20312 0.0088501 -1.00781]]\n",
            "\n",
            " [[-0.28125 -0.345703 -1.17969 ... -0.417969 -0.24707 -0.675781]\n",
            "  [-0.28125 -0.345703 -1.17969 ... -0.417969 -0.24707 -0.675781]\n",
            "  [-0.304688 -0.337891 -1.17188 ... -0.417969 -0.24707 -0.675781]\n",
            "  ...\n",
            "  [0.179688 -0.910156 -2.57812 ... 0.589844 -1.77344 0.176758]\n",
            "  [-1.53125 -0.886719 -1.64062 ... -0.0105591 -0.660156 -0.714844]\n",
            "  [-1.13281 -1.16406 -1.8125 ... -0.667969 -0.773438 -0.710938]]\n",
            "\n",
            " [[-0.28125 -0.345703 -1.17969 ... -0.417969 -0.24707 -0.675781]\n",
            "  [-0.28125 -0.345703 -1.17969 ... -0.417969 -0.24707 -0.675781]\n",
            "  [-0.304688 -0.337891 -1.17188 ... -0.417969 -0.24707 -0.675781]\n",
            "  ...\n",
            "  [-0.792969 0.792969 -1.51562 ... -0.0991211 -0.816406 -0.455078]\n",
            "  [-0.976562 0.283203 -2.32812 ... 0.457031 -0.255859 -0.832031]\n",
            "  [-1.08594 -0.945312 -1.89844 ... -0.707031 -0.675781 -1.07031]]\n",
            "\n",
            " [[-0.28125 -0.345703 -1.17969 ... -0.417969 -0.24707 -0.675781]\n",
            "  [0.332031 0.832031 -1.11719 ... -0.199219 0.102539 0.134766]\n",
            "  [-0.154297 0.507812 -1.25781 ... 0.0791016 0.0281982 -0.996094]\n",
            "  ...\n",
            "  [-1.21875 1.10938 -2.625 ... 0.25 -0.117676 0.75]\n",
            "  [-0.378906 0.131836 -3.54688 ... 1.4375 1.03125 -0.375]\n",
            "  [-1.46875 -0.527344 -1.96875 ... -0.753906 -0.808594 -1.47656]]]\n",
            "attention_lnx=[[[0.722656 -0.871094 0.417969 ... 1.26562 -0.0578613 -1.30469]\n",
            "  [0.722656 -0.871094 0.417969 ... 1.26562 -0.0578613 -1.30469]\n",
            "  [0.714844 -0.863281 0.410156 ... 1.26562 -0.0578613 -1.29688]\n",
            "  ...\n",
            "  [0.863281 -0.683594 0.503906 ... 1.13281 -0.194336 -0.890625]\n",
            "  [0.976562 -0.554688 0.449219 ... 0.886719 -0.0795898 -0.9375]\n",
            "  [0.671875 -0.863281 0.542969 ... 0.988281 0.0932617 -1.0625]]\n",
            "\n",
            " [[0.808594 0.417969 -1.05469 ... -1.90625 1.26562 -0.929688]\n",
            "  [0.333984 0.652344 -1.875 ... -1.22656 0.458984 -0.367188]\n",
            "  [0.0537109 0.765625 -1.61719 ... -1.46094 0.170898 -0.142578]\n",
            "  ...\n",
            "  [-0.361328 -0.217773 -1.59375 ... -0.244141 -0.695312 -0.761719]\n",
            "  [-0.546875 -1.22656 -0.871094 ... 0.328125 -0.439453 -0.78125]\n",
            "  [-0.582031 -1.10156 -1 ... 0.447266 -0.503906 -0.498047]]\n",
            "\n",
            " [[0.722656 -0.871094 0.417969 ... 1.26562 -0.0578613 -1.30469]\n",
            "  [0.722656 -0.871094 0.417969 ... 1.26562 -0.0578613 -1.30469]\n",
            "  [0.714844 -0.863281 0.410156 ... 1.26562 -0.0578613 -1.29688]\n",
            "  ...\n",
            "  [0.5625 -0.898438 0.298828 ... 0.535156 0.226562 -0.255859]\n",
            "  [0.648438 -1 0.353516 ... 0.8125 0.177734 -0.431641]\n",
            "  [0.367188 -1.875 0.425781 ... 0.488281 0.310547 -0.472656]]\n",
            "\n",
            " [[0.722656 -0.871094 0.417969 ... 1.26562 -0.0578613 -1.30469]\n",
            "  [0.722656 -0.871094 0.417969 ... 1.26562 -0.0578613 -1.30469]\n",
            "  [0.714844 -0.863281 0.410156 ... 1.26562 -0.0578613 -1.29688]\n",
            "  ...\n",
            "  [0.886719 -0.578125 0.494141 ... 0.980469 -0.191406 -0.283203]\n",
            "  [0.683594 -0.859375 0.402344 ... 0.679688 -0.0512695 -0.609375]\n",
            "  [0.285156 -1.36719 0.542969 ... 0.71875 0.180664 -0.476562]]\n",
            "\n",
            " [[0.722656 -0.871094 0.417969 ... 1.26562 -0.0578613 -1.30469]\n",
            "  [1.13281 -0.921875 0.126953 ... 0.439453 0.353516 -1.28125]\n",
            "  [0.59375 -0.867188 -0.0776367 ... 0.433594 -0.179688 -0.878906]\n",
            "  ...\n",
            "  [0.382812 -1.03906 0.109863 ... 0.773438 -0.0717773 -0.194336]\n",
            "  [0.0698242 -0.714844 -0.245117 ... 0.582031 -0.00732422 -0.207031]\n",
            "  [-0.0559082 -1.42188 0.0683594 ... 0.726562 0.116699 0.0388184]]]\n",
            "attn_output=[[[-0.131836 -0.507812 -1.0625 ... -0.157227 -0.251953 -0.917969]\n",
            "  [-0.131836 -0.507812 -1.0625 ... -0.157227 -0.251953 -0.917969]\n",
            "  [-0.155273 -0.498047 -1.0625 ... -0.157227 -0.251953 -0.910156]\n",
            "  ...\n",
            "  [-0.546875 -0.109375 -1.33594 ... 0.408203 0.601562 0.699219]\n",
            "  [0.15918 -1.10938 -1.57812 ... 0.921875 0.761719 -0.0527344]\n",
            "  [-1.29688 -1.04688 -1.16406 ... -0.21875 -0.180664 -1.53906]]\n",
            "\n",
            " [[-0.140625 0.785156 -0.625 ... -1.41406 -0.6875 0.367188]\n",
            "  [0.722656 0.777344 -0.769531 ... -0.824219 0.390625 0.875]\n",
            "  [-0.462891 0.621094 -1.19531 ... -0.640625 0.152344 -0.761719]\n",
            "  ...\n",
            "  [1.27344 2.3125 -3.39062 ... -0.255859 0.664062 0.667969]\n",
            "  [-1.69531 0.380859 -1.59375 ... -0.933594 -0.664062 -0.710938]\n",
            "  [-2.20312 0.757812 -0.832031 ... -1.07031 -0.108398 -1.10156]]\n",
            "\n",
            " [[-0.131836 -0.507812 -1.0625 ... -0.157227 -0.251953 -0.917969]\n",
            "  [-0.131836 -0.507812 -1.0625 ... -0.157227 -0.251953 -0.917969]\n",
            "  [-0.155273 -0.498047 -1.0625 ... -0.157227 -0.251953 -0.910156]\n",
            "  ...\n",
            "  [0.300781 -1.07812 -2.42188 ... 0.691406 -1.66406 0.112793]\n",
            "  [-1.34375 -1.07812 -1.52344 ... 0.166992 -0.601562 -0.789062]\n",
            "  [-1.03906 -1.44531 -1.69531 ... -0.566406 -0.699219 -0.773438]]\n",
            "\n",
            " [[-0.131836 -0.507812 -1.0625 ... -0.157227 -0.251953 -0.917969]\n",
            "  [-0.131836 -0.507812 -1.0625 ... -0.157227 -0.251953 -0.917969]\n",
            "  [-0.155273 -0.498047 -1.0625 ... -0.157227 -0.251953 -0.910156]\n",
            "  ...\n",
            "  [-0.546875 0.625 -1.35156 ... 0.15332 -0.84375 -0.515625]\n",
            "  [-0.78125 0.0622559 -2.17188 ... 0.613281 -0.261719 -0.964844]\n",
            "  [-1 -1.15625 -1.75 ... -0.558594 -0.625 -1.11719]]\n",
            "\n",
            " [[-0.131836 -0.507812 -1.0625 ... -0.157227 -0.251953 -0.917969]\n",
            "  [0.582031 0.601562 -1.05469 ... -0.09375 0.179688 -0.160156]\n",
            "  [-0.0109863 0.291016 -1.24219 ... 0.178711 -0.0146484 -1.17188]\n",
            "  ...\n",
            "  [-1.08594 0.800781 -2.51562 ... 0.453125 -0.133789 0.679688]\n",
            "  [-0.349609 -0.0703125 -3.53125 ... 1.5625 1.00781 -0.423828]\n",
            "  [-1.4375 -0.796875 -1.90625 ... -0.589844 -0.765625 -1.4375]]]\n",
            "next_layer_addition_dropped_out=[[[-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.92188 -1.98438 -5.5625 ... -0.808594 -0.117188 -4.96875]\n",
            "  ...\n",
            "  [-3.07812 0.34375 -6.5625 ... 1.49219 2.65625 3.35938]\n",
            "  [0.158203 -4.09375 -7.25 ... 3.73438 3.71875 0.0722656]\n",
            "  [-8.1875 -4.75 -8.0625 ... -0.859375 -1.35156 -6.8125]]\n",
            "\n",
            " [[0.113281 4.53125 -2.73438 ... -4.40625 -4.15625 1.60156]\n",
            "  [3.54688 3.8125 -3 ... -2.59375 1.78125 4.6875]\n",
            "  [-0.492188 2.34375 -4.625 ... -1.8125 0.396484 -3.3125]\n",
            "  ...\n",
            "  [5.125 7.5625 -11.6875 ... -0.90625 0.945312 2.40625]\n",
            "  [-8.5 1.21875 -10.625 ... -3.8125 -2.96875 -2.90625]\n",
            "  [-9.4375 1.36719 -6.53125 ... -4.5 0.451172 -3.71875]]\n",
            "\n",
            " [[-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.92188 -1.98438 -5.5625 ... -0.808594 -0.117188 -4.96875]\n",
            "  ...\n",
            "  [0.460938 -5.28125 -13.375 ... 3.28125 -8.9375 1.375]\n",
            "  [-7.5625 -5.875 -8.6875 ... -0.0351562 -3.8125 -3.67188]\n",
            "  [-7.03125 -9.8125 -13.1875 ... -3.4375 -3.84375 -3.40625]]\n",
            "\n",
            " [[-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.92188 -1.98438 -5.5625 ... -0.808594 -0.117188 -4.96875]\n",
            "  ...\n",
            "  [-2 2.85938 -5.40625 ... 1.32812 -2.625 -2.34375]\n",
            "  [-2.26562 0.761719 -10.0625 ... 1.8125 -1.88281 -3.875]\n",
            "  [-6.6875 -8.375 -13 ... -2.79688 -3.125 -4.90625]]\n",
            "\n",
            " [[-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [2.17188 2.78125 -5.65625 ... -0.298828 1.00781 -0.207031]\n",
            "  [-0.00292969 0.992188 -6.40625 ... -0.128906 0.0273438 -4.09375]\n",
            "  ...\n",
            "  [-2.65625 2.84375 -9.5625 ... 3.375 0.158203 2.23438]\n",
            "  [-0.851562 -0.929688 -12.5 ... 5.34375 2.54688 -1.34375]\n",
            "  [-6.8125 -5.375 -12.1875 ... -2.46875 -3.59375 -6.1875]]]\n",
            "inputs=[[[-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.92188 -1.98438 -5.5625 ... -0.808594 -0.117188 -4.96875]\n",
            "  ...\n",
            "  [-3.07812 0.34375 -6.5625 ... 1.49219 2.65625 3.35938]\n",
            "  [0.158203 -4.09375 -7.25 ... 3.73438 3.71875 0.0722656]\n",
            "  [-8.1875 -4.75 -8.0625 ... -0.859375 -1.35156 -6.8125]]\n",
            "\n",
            " [[0.113281 4.53125 -2.73438 ... -4.40625 -4.15625 1.60156]\n",
            "  [3.54688 3.8125 -3 ... -2.59375 1.78125 4.6875]\n",
            "  [-0.492188 2.34375 -4.625 ... -1.8125 0.396484 -3.3125]\n",
            "  ...\n",
            "  [5.125 7.5625 -11.6875 ... -0.90625 0.945312 2.40625]\n",
            "  [-8.5 1.21875 -10.625 ... -3.8125 -2.96875 -2.90625]\n",
            "  [-9.4375 1.36719 -6.53125 ... -4.5 0.451172 -3.71875]]\n",
            "\n",
            " [[-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.92188 -1.98438 -5.5625 ... -0.808594 -0.117188 -4.96875]\n",
            "  ...\n",
            "  [0.460938 -5.28125 -13.375 ... 3.28125 -8.9375 1.375]\n",
            "  [-7.5625 -5.875 -8.6875 ... -0.0351562 -3.8125 -3.67188]\n",
            "  [-7.03125 -9.8125 -13.1875 ... -3.4375 -3.84375 -3.40625]]\n",
            "\n",
            " [[-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.92188 -1.98438 -5.5625 ... -0.808594 -0.117188 -4.96875]\n",
            "  ...\n",
            "  [-2 2.85938 -5.40625 ... 1.32812 -2.625 -2.34375]\n",
            "  [-2.26562 0.761719 -10.0625 ... 1.8125 -1.88281 -3.875]\n",
            "  [-6.6875 -8.375 -13 ... -2.79688 -3.125 -4.90625]]\n",
            "\n",
            " [[-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [2.17188 2.78125 -5.65625 ... -0.298828 1.00781 -0.207031]\n",
            "  [-0.00292969 0.992188 -6.40625 ... -0.128906 0.0273438 -4.09375]\n",
            "  ...\n",
            "  [-2.65625 2.84375 -9.5625 ... 3.375 0.158203 2.23438]\n",
            "  [-0.851562 -0.929688 -12.5 ... 5.34375 2.54688 -1.34375]\n",
            "  [-6.8125 -5.375 -12.1875 ... -2.46875 -3.59375 -6.1875]]]\n",
            "lnx=[[[-0.341797 -0.386719 -1.0625 ... -0.150391 -0.0178223 -0.945312]\n",
            "  [-0.341797 -0.386719 -1.0625 ... -0.150391 -0.0178223 -0.945312]\n",
            "  [-0.365234 -0.376953 -1.0625 ... -0.154297 -0.0223389 -0.945312]\n",
            "  ...\n",
            "  [-0.714844 0.0800781 -1.52344 ... 0.347656 0.617188 0.78125]\n",
            "  [0.0378418 -0.980469 -1.73438 ... 0.894531 0.890625 0.017334]\n",
            "  [-1.44531 -0.839844 -1.42188 ... -0.151367 -0.238281 -1.20312]]\n",
            "\n",
            " [[0.0231934 0.929688 -0.558594 ... -0.902344 -0.851562 0.328125]\n",
            "  [0.839844 0.902344 -0.710938 ... -0.613281 0.421875 1.10938]\n",
            "  [-0.119629 0.570312 -1.125 ... -0.439453 0.0961914 -0.804688]\n",
            "  ...\n",
            "  [1.39844 2.0625 -3.1875 ... -0.24707 0.257812 0.65625]\n",
            "  [-1.58594 0.227539 -1.98438 ... -0.710938 -0.554688 -0.542969]\n",
            "  [-2.01562 0.291016 -1.39062 ... -0.957031 0.0961914 -0.792969]]\n",
            "\n",
            " [[-0.341797 -0.386719 -1.0625 ... -0.150391 -0.0178223 -0.945312]\n",
            "  [-0.341797 -0.386719 -1.0625 ... -0.150391 -0.0178223 -0.945312]\n",
            "  [-0.365234 -0.376953 -1.0625 ... -0.154297 -0.0223389 -0.945312]\n",
            "  ...\n",
            "  [0.0976562 -1.11719 -2.82812 ... 0.695312 -1.89062 0.291016]\n",
            "  [-1.52344 -1.1875 -1.75 ... -0.00708008 -0.769531 -0.742188]\n",
            "  [-1.05469 -1.47656 -1.97656 ... -0.515625 -0.578125 -0.511719]]\n",
            "\n",
            " [[-0.341797 -0.386719 -1.0625 ... -0.150391 -0.0178223 -0.945312]\n",
            "  [-0.341797 -0.386719 -1.0625 ... -0.150391 -0.0178223 -0.945312]\n",
            "  [-0.365234 -0.376953 -1.0625 ... -0.154297 -0.0223389 -0.945312]\n",
            "  ...\n",
            "  [-0.494141 0.707031 -1.33594 ... 0.328125 -0.648438 -0.578125]\n",
            "  [-0.539062 0.181641 -2.40625 ... 0.431641 -0.449219 -0.925781]\n",
            "  [-1.03906 -1.30469 -2.01562 ... -0.435547 -0.486328 -0.761719]]\n",
            "\n",
            " [[-0.341797 -0.386719 -1.0625 ... -0.150391 -0.0178223 -0.945312]\n",
            "  [0.480469 0.617188 -1.25 ... -0.065918 0.222656 -0.0458984]\n",
            "  [-0.000671387 0.227539 -1.46875 ... -0.029541 0.0062561 -0.9375]\n",
            "  ...\n",
            "  [-0.703125 0.753906 -2.53125 ... 0.894531 0.041748 0.589844]\n",
            "  [-0.232422 -0.253906 -3.40625 ... 1.46094 0.695312 -0.367188]\n",
            "  [-1.20312 -0.949219 -2.15625 ... -0.4375 -0.636719 -1.09375]]]\n",
            "attention_lnx=[[[1.03125 -0.143555 -0.542969 ... -0.804688 0.480469 -0.277344]\n",
            "  [1.03125 -0.143555 -0.542969 ... -0.804688 0.480469 -0.277344]\n",
            "  [1.03125 -0.134766 -0.558594 ... -0.804688 0.482422 -0.275391]\n",
            "  ...\n",
            "  [0.703125 -0.0917969 -0.3125 ... -1.125 0.78125 -0.458984]\n",
            "  [0.660156 -0.152344 -0.199219 ... -1.28125 0.734375 -0.390625]\n",
            "  [0.417969 0.00515747 -0.101074 ... -1.23438 0.542969 -0.283203]]\n",
            "\n",
            " [[-0.753906 0.185547 -0.241211 ... 2.51562 -0.753906 -1.98438]\n",
            "  [-0.255859 -0.11084 -0.984375 ... 1.35938 -0.449219 -1.48438]\n",
            "  [-0.181641 0.0737305 -0.984375 ... 1.25 -0.753906 -1.71094]\n",
            "  ...\n",
            "  [0.102539 0.824219 -0.298828 ... 0.0090332 -0.640625 -1.19531]\n",
            "  [-0.0947266 1.20312 0.149414 ... -0.15332 -1.15625 -1.04688]\n",
            "  [-0.012085 1.13281 0.0178223 ... -0.251953 -1.15625 -1.03125]]\n",
            "\n",
            " [[1.03125 -0.143555 -0.542969 ... -0.804688 0.480469 -0.277344]\n",
            "  [1.03125 -0.143555 -0.542969 ... -0.804688 0.480469 -0.277344]\n",
            "  [1.03125 -0.134766 -0.558594 ... -0.804688 0.482422 -0.275391]\n",
            "  ...\n",
            "  [0.12793 -0.402344 0.149414 ... -0.945312 -0.108887 -0.769531]\n",
            "  [0.0500488 -0.546875 0.347656 ... -1.00781 -0.335938 -0.867188]\n",
            "  [-0.132812 -0.328125 0.671875 ... -0.953125 -0.886719 -0.570312]]\n",
            "\n",
            " [[1.03125 -0.143555 -0.542969 ... -0.804688 0.480469 -0.277344]\n",
            "  [1.03125 -0.143555 -0.542969 ... -0.804688 0.480469 -0.277344]\n",
            "  [1.03125 -0.134766 -0.558594 ... -0.804688 0.482422 -0.275391]\n",
            "  ...\n",
            "  [-0.166992 -0.601562 0.134766 ... -0.902344 -0.148438 -0.734375]\n",
            "  [0.0212402 -0.507812 -0.147461 ... -1.14844 -0.0463867 -0.636719]\n",
            "  [-0.169922 -0.304688 0.464844 ... -1.30469 -0.636719 -0.285156]]\n",
            "\n",
            " [[1.03125 -0.143555 -0.542969 ... -0.804688 0.480469 -0.277344]\n",
            "  [0.714844 -0.179688 -0.722656 ... -0.628906 1.17188 -0.828125]\n",
            "  [0.566406 -0.890625 -0.597656 ... -0.546875 0.554688 -1.28906]\n",
            "  ...\n",
            "  [-0.287109 -0.574219 -0.0605469 ... -1.02344 -0.367188 -1]\n",
            "  [-0.503906 -0.644531 -0.412109 ... -1.04688 -0.135742 -0.546875]\n",
            "  [-0.251953 -0.257812 0.196289 ... -1.17188 -0.714844 -0.566406]]]\n",
            "attn_output=[[[-0.141602 -0.402344 -1.13281 ... -0.294922 0.0717773 -0.972656]\n",
            "  [-0.141602 -0.402344 -1.13281 ... -0.294922 0.0717773 -0.972656]\n",
            "  [-0.165039 -0.392578 -1.13281 ... -0.298828 0.0673828 -0.96875]\n",
            "  ...\n",
            "  [-0.539062 0.0571289 -1.55469 ... 0.0830078 0.777344 0.65625]\n",
            "  [0.192383 -1 -1.75781 ... 0.578125 1.04688 -0.0751953]\n",
            "  [-1.33594 -0.816406 -1.40625 ... -0.359375 -0.138672 -1.21875]]\n",
            "\n",
            " [[-0.12793 0.941406 -0.59375 ... -0.376953 -0.980469 -0.0761719]\n",
            "  [0.761719 0.859375 -0.921875 ... -0.285156 0.308594 0.742188]\n",
            "  [-0.160156 0.574219 -1.32812 ... -0.133789 -0.0849609 -1.1875]\n",
            "  ...\n",
            "  [1.39062 2.23438 -3.20312 ... -0.239258 0.0810547 0.322266]\n",
            "  [-1.55469 0.4375 -1.89844 ... -0.71875 -0.746094 -0.714844]\n",
            "  [-1.95312 0.515625 -1.34375 ... -0.980469 -0.145508 -0.980469]]\n",
            "\n",
            " [[-0.141602 -0.402344 -1.13281 ... -0.294922 0.0717773 -0.972656]\n",
            "  [-0.141602 -0.402344 -1.13281 ... -0.294922 0.0717773 -0.972656]\n",
            "  [-0.165039 -0.392578 -1.13281 ... -0.298828 0.0673828 -0.96875]\n",
            "  ...\n",
            "  [0.121582 -1.17188 -2.73438 ... 0.482422 -1.86719 0.125]\n",
            "  [-1.46875 -1.25781 -1.63281 ... -0.204102 -0.8125 -0.890625]\n",
            "  [-1.04688 -1.48438 -1.83594 ... -0.644531 -0.691406 -0.582031]]\n",
            "\n",
            " [[-0.141602 -0.402344 -1.13281 ... -0.294922 0.0717773 -0.972656]\n",
            "  [-0.141602 -0.402344 -1.13281 ... -0.294922 0.0717773 -0.972656]\n",
            "  [-0.165039 -0.392578 -1.13281 ... -0.298828 0.0673828 -0.96875]\n",
            "  ...\n",
            "  [-0.523438 0.546875 -1.27344 ... 0.103027 -0.671875 -0.746094]\n",
            "  [-0.523438 0.059082 -2.375 ... 0.154297 -0.449219 -1.04688]\n",
            "  [-1.03906 -1.32031 -1.89844 ... -0.621094 -0.570312 -0.789062]]\n",
            "\n",
            " [[-0.141602 -0.402344 -1.13281 ... -0.294922 0.0717773 -0.972656]\n",
            "  [0.625 0.5625 -1.38281 ... -0.201172 0.472656 -0.224609]\n",
            "  [0.125977 0.0227051 -1.57031 ... -0.151367 0.130859 -1.20312]\n",
            "  ...\n",
            "  [-0.757812 0.585938 -2.48438 ... 0.605469 -0.0539551 0.318359]\n",
            "  [-0.361328 -0.419922 -3.4375 ... 1.14844 0.640625 -0.503906]\n",
            "  [-1.21875 -0.96875 -2.0625 ... -0.625 -0.742188 -1.16406]]]\n",
            "next_layer_addition_dropped_out=[[[-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.949219 -3.48438 -6.59375 ... -2.65625 1.52344 -5.46875]\n",
            "  ...\n",
            "  [-2.42188 -0.546875 -7.5625 ... -0.363281 3.71875 3]\n",
            "  [0.574219 -5.03125 -7.4375 ... 1.35156 4.59375 -0.820312]\n",
            "  [-8.75 -5.96875 -9.1875 ... -2.21875 -2.42188 -8.5625]]\n",
            "\n",
            " [[-1.78125 5.3125 -3.96875 ... -2.1875 -4.1875 -0.217773]\n",
            "  [3.48438 4.3125 -4.59375 ... -1.35938 2.46875 3.20312]\n",
            "  [-0.546875 2.625 -6 ... -0.117188 -0.597656 -4.6875]\n",
            "  ...\n",
            "  [4.40625 7.625 -14 ... -0.683594 1.1875 2.1875]\n",
            "  [-9.75 0.796875 -12.1875 ... -6.0625 -5.09375 -3.9375]\n",
            "  [-11.75 1.80469 -8.8125 ... -4.75 -1.39844 -4.125]]\n",
            "\n",
            " [[-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.949219 -3.48438 -6.59375 ... -2.65625 1.52344 -5.46875]\n",
            "  ...\n",
            "  [-0.621094 -6.46875 -12.0625 ... 1.76562 -10.375 -0.449219]\n",
            "  [-8.375 -6.71875 -8.75 ... -2.10938 -5.09375 -5.46875]\n",
            "  [-8.125 -10.875 -13.0625 ... -4.5 -7.03125 -4.96875]]\n",
            "\n",
            " [[-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.949219 -3.48438 -6.59375 ... -2.65625 1.52344 -5.46875]\n",
            "  ...\n",
            "  [-1.38281 2.35938 -4.5 ... 0.253906 -3.0625 -2.28125]\n",
            "  [-2.65625 0.957031 -10.0625 ... -0.242188 -2.17188 -5]\n",
            "  [-8.5 -9.125 -13.9375 ... -4.25 -6.375 -6.625]]\n",
            "\n",
            " [[-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [3.03125 1.89062 -6.21875 ... -0.738281 2.39062 -1.42969]\n",
            "  [0.550781 0.216797 -6.78125 ... -0.804688 0.177734 -6.1875]\n",
            "  ...\n",
            "  [-3.67188 2.75 -9.875 ... 3.03125 -0.859375 1.21875]\n",
            "  [-1.69531 -1.38281 -13.25 ... 4.28125 2.78125 -1.35938]\n",
            "  [-9.3125 -6.0625 -13.0625 ... -4.3125 -7.125 -8.1875]]]\n",
            "inputs=[[[-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.949219 -3.48438 -6.59375 ... -2.65625 1.52344 -5.46875]\n",
            "  ...\n",
            "  [-2.42188 -0.546875 -7.5625 ... -0.363281 3.71875 3]\n",
            "  [0.574219 -5.03125 -7.4375 ... 1.35156 4.59375 -0.820312]\n",
            "  [-8.75 -5.96875 -9.1875 ... -2.21875 -2.42188 -8.5625]]\n",
            "\n",
            " [[-1.78125 5.3125 -3.96875 ... -2.1875 -4.1875 -0.217773]\n",
            "  [3.48438 4.3125 -4.59375 ... -1.35938 2.46875 3.20312]\n",
            "  [-0.546875 2.625 -6 ... -0.117188 -0.597656 -4.6875]\n",
            "  ...\n",
            "  [4.40625 7.625 -14 ... -0.683594 1.1875 2.1875]\n",
            "  [-9.75 0.796875 -12.1875 ... -6.0625 -5.09375 -3.9375]\n",
            "  [-11.75 1.80469 -8.8125 ... -4.75 -1.39844 -4.125]]\n",
            "\n",
            " [[-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.949219 -3.48438 -6.59375 ... -2.65625 1.52344 -5.46875]\n",
            "  ...\n",
            "  [-0.621094 -6.46875 -12.0625 ... 1.76562 -10.375 -0.449219]\n",
            "  [-8.375 -6.71875 -8.75 ... -2.10938 -5.09375 -5.46875]\n",
            "  [-8.125 -10.875 -13.0625 ... -4.5 -7.03125 -4.96875]]\n",
            "\n",
            " [[-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.949219 -3.48438 -6.59375 ... -2.65625 1.52344 -5.46875]\n",
            "  ...\n",
            "  [-1.38281 2.35938 -4.5 ... 0.253906 -3.0625 -2.28125]\n",
            "  [-2.65625 0.957031 -10.0625 ... -0.242188 -2.17188 -5]\n",
            "  [-8.5 -9.125 -13.9375 ... -4.25 -6.375 -6.625]]\n",
            "\n",
            " [[-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [3.03125 1.89062 -6.21875 ... -0.738281 2.39062 -1.42969]\n",
            "  [0.550781 0.216797 -6.78125 ... -0.804688 0.177734 -6.1875]\n",
            "  ...\n",
            "  [-3.67188 2.75 -9.875 ... 3.03125 -0.859375 1.21875]\n",
            "  [-1.69531 -1.38281 -13.25 ... 4.28125 2.78125 -1.35938]\n",
            "  [-9.3125 -6.0625 -13.0625 ... -4.3125 -7.125 -8.1875]]]\n",
            "lnx=[[[-0.142578 -0.628906 -1.17188 ... -0.472656 0.273438 -0.972656]\n",
            "  [-0.142578 -0.628906 -1.17188 ... -0.472656 0.273438 -0.972656]\n",
            "  [-0.168945 -0.621094 -1.17188 ... -0.472656 0.271484 -0.972656]\n",
            "  ...\n",
            "  [-0.535156 -0.121094 -1.67188 ... -0.0805664 0.824219 0.664062]\n",
            "  [0.132812 -1.16406 -1.71875 ... 0.3125 1.0625 -0.189453]\n",
            "  [-1.38281 -0.941406 -1.45312 ... -0.349609 -0.382812 -1.35156]]\n",
            "\n",
            " [[-0.349609 1.03906 -0.777344 ... -0.427734 -0.820312 -0.0427246]\n",
            "  [0.792969 0.984375 -1.04688 ... -0.310547 0.5625 0.730469]\n",
            "  [-0.12793 0.613281 -1.40625 ... -0.0274658 -0.139648 -1.10156]\n",
            "  ...\n",
            "  [1.15625 1.99219 -3.65625 ... -0.178711 0.310547 0.574219]\n",
            "  [-1.57031 0.128906 -1.96875 ... -0.976562 -0.820312 -0.636719]\n",
            "  [-2.20312 0.337891 -1.64844 ... -0.890625 -0.261719 -0.773438]]\n",
            "\n",
            " [[-0.142578 -0.628906 -1.17188 ... -0.472656 0.273438 -0.972656]\n",
            "  [-0.142578 -0.628906 -1.17188 ... -0.472656 0.273438 -0.972656]\n",
            "  [-0.168945 -0.621094 -1.17188 ... -0.472656 0.271484 -0.972656]\n",
            "  ...\n",
            "  [-0.121094 -1.26562 -2.35938 ... 0.34375 -2.03125 -0.0874023]\n",
            "  [-1.52344 -1.21875 -1.58594 ... -0.382812 -0.925781 -0.992188]\n",
            "  [-1.07031 -1.42969 -1.71875 ... -0.589844 -0.925781 -0.652344]]\n",
            "\n",
            " [[-0.142578 -0.628906 -1.17188 ... -0.472656 0.273438 -0.972656]\n",
            "  [-0.142578 -0.628906 -1.17188 ... -0.472656 0.273438 -0.972656]\n",
            "  [-0.168945 -0.621094 -1.17188 ... -0.472656 0.271484 -0.972656]\n",
            "  ...\n",
            "  [-0.326172 0.558594 -1.0625 ... 0.0600586 -0.722656 -0.539062]\n",
            "  [-0.589844 0.212891 -2.23438 ... -0.0537109 -0.482422 -1.10938]\n",
            "  [-1.15625 -1.24219 -1.89844 ... -0.578125 -0.867188 -0.902344]]\n",
            "\n",
            " [[-0.142578 -0.628906 -1.17188 ... -0.472656 0.273438 -0.972656]\n",
            "  [0.636719 0.396484 -1.30469 ... -0.155273 0.503906 -0.300781]\n",
            "  [0.121094 0.0476074 -1.49219 ... -0.176758 0.0390625 -1.35938]\n",
            "  ...\n",
            "  [-0.925781 0.695312 -2.48438 ... 0.765625 -0.216797 0.306641]\n",
            "  [-0.443359 -0.361328 -3.46875 ... 1.125 0.726562 -0.355469]\n",
            "  [-1.4375 -0.933594 -2.01562 ... -0.664062 -1.10156 -1.25781]]]\n",
            "attention_lnx=[[[0.648438 -1.96094 -0.408203 ... 0.386719 -2.42188 0.960938]\n",
            "  [0.648438 -1.96094 -0.408203 ... 0.386719 -2.42188 0.960938]\n",
            "  [0.644531 -1.96094 -0.408203 ... 0.378906 -2.42188 0.96875]\n",
            "  ...\n",
            "  [0.0269775 -1.84375 -0.408203 ... -0.097168 -2.40625 1.53125]\n",
            "  [0.00811768 -1.89062 -0.365234 ... -0.100586 -2.35938 1.54688]\n",
            "  [-0.0284424 -2.10938 -0.359375 ... 0.00613403 -2.42188 1.39844]]\n",
            "\n",
            " [[0.109863 -1.42188 0.523438 ... -0.116699 -0.165039 0.181641]\n",
            "  [-0.84375 -0.910156 -0.0957031 ... -0.214844 -0.291016 1.01562]\n",
            "  [-1.25781 -1.23438 -0.0571289 ... -0.34375 0.146484 0.890625]\n",
            "  ...\n",
            "  [-0.40625 -0.820312 -0.664062 ... -0.222656 -0.625 1.85156]\n",
            "  [0.304688 -1.07812 -0.933594 ... -0.320312 -0.855469 1.82812]\n",
            "  [0.230469 -0.925781 -0.820312 ... -0.119629 -0.984375 1.89844]]\n",
            "\n",
            " [[0.648438 -1.96094 -0.408203 ... 0.386719 -2.42188 0.960938]\n",
            "  [0.648438 -1.96094 -0.408203 ... 0.386719 -2.42188 0.960938]\n",
            "  [0.644531 -1.96094 -0.408203 ... 0.378906 -2.42188 0.96875]\n",
            "  ...\n",
            "  [0.0581055 -1.9375 -0.211914 ... 0.298828 -1.64062 1.6875]\n",
            "  [-0.207031 -2.125 -0.175781 ... 0.289062 -1.625 1.92969]\n",
            "  [-0.138672 -1.86719 -0.24707 ... 0.199219 -1.76562 1.75]]\n",
            "\n",
            " [[0.648438 -1.96094 -0.408203 ... 0.386719 -2.42188 0.960938]\n",
            "  [0.648438 -1.96094 -0.408203 ... 0.386719 -2.42188 0.960938]\n",
            "  [0.644531 -1.96094 -0.408203 ... 0.378906 -2.42188 0.96875]\n",
            "  ...\n",
            "  [-0.057373 -1.64844 0.0534668 ... 0.129883 -1.49219 1.58594]\n",
            "  [-0.155273 -1.70312 -0.00156403 ... 0.24707 -1.50781 1.61719]\n",
            "  [-0.318359 -1.64062 -0.185547 ... 0.335938 -1.5625 1.70312]]\n",
            "\n",
            " [[0.648438 -1.96094 -0.408203 ... 0.386719 -2.42188 0.960938]\n",
            "  [0.617188 -1.77344 -0.138672 ... 0.306641 -2.09375 1.34375]\n",
            "  [0.316406 -2.15625 -0.0339355 ... 0.119629 -1.67969 0.867188]\n",
            "  ...\n",
            "  [-0.75 -1.71875 0.0708008 ... 0.259766 -1.16406 2.14062]\n",
            "  [-0.808594 -1.74219 0.171875 ... 0.0927734 -1.39062 2.39062]\n",
            "  [-0.679688 -1.4375 -0.18457 ... 0.129883 -1.34375 2.01562]]]\n",
            "attn_output=[[[-0.0262451 -0.945312 -1.21094 ... -0.390625 -0.15332 -0.777344]\n",
            "  [-0.0262451 -0.945312 -1.21094 ... -0.390625 -0.15332 -0.777344]\n",
            "  [-0.0524902 -0.9375 -1.21094 ... -0.392578 -0.155273 -0.777344]\n",
            "  ...\n",
            "  [-0.515625 -0.515625 -1.71094 ... -0.0991211 0.283203 0.972656]\n",
            "  [0.130859 -1.54688 -1.75 ... 0.279297 0.5 0.163086]\n",
            "  [-1.34375 -1.24219 -1.46094 ... -0.339844 -0.742188 -1.10156]]\n",
            "\n",
            " [[-0.322266 0.75 -0.664062 ... -0.445312 -0.839844 -0.00698853]\n",
            "  [0.589844 0.761719 -1.04688 ... -0.351562 0.486328 0.941406]\n",
            "  [-0.416016 0.320312 -1.39844 ... -0.106445 -0.104004 -0.875]\n",
            "  ...\n",
            "  [1.02344 1.73438 -3.73438 ... -0.231445 0.143555 1.03125]\n",
            "  [-1.49219 -0.0444336 -2.0625 ... -1.00781 -0.9375 -0.332031]\n",
            "  [-2.09375 0.160156 -1.75781 ... -0.886719 -0.433594 -0.40625]]\n",
            "\n",
            " [[-0.0262451 -0.945312 -1.21094 ... -0.390625 -0.15332 -0.777344]\n",
            "  [-0.0262451 -0.945312 -1.21094 ... -0.390625 -0.15332 -0.777344]\n",
            "  [-0.0524902 -0.9375 -1.21094 ... -0.392578 -0.155273 -0.777344]\n",
            "  ...\n",
            "  [-0.107422 -1.60156 -2.34375 ... 0.392578 -2.28125 0.236328]\n",
            "  [-1.51562 -1.5625 -1.57812 ... -0.322266 -1.1875 -0.625]\n",
            "  [-1.0625 -1.63281 -1.71094 ... -0.550781 -1.13281 -0.414062]]\n",
            "\n",
            " [[-0.0262451 -0.945312 -1.21094 ... -0.390625 -0.15332 -0.777344]\n",
            "  [-0.0262451 -0.945312 -1.21094 ... -0.390625 -0.15332 -0.777344]\n",
            "  [-0.0524902 -0.9375 -1.21094 ... -0.392578 -0.155273 -0.777344]\n",
            "  ...\n",
            "  [-0.333984 0.164062 -1.03125 ... 0.0888672 -1.05469 -0.161133]\n",
            "  [-0.609375 -0.161133 -2.17188 ... 0.00105286 -0.796875 -0.730469]\n",
            "  [-1.17188 -1.42969 -1.88281 ... -0.519531 -1.05469 -0.65625]]\n",
            "\n",
            " [[-0.0262451 -0.945312 -1.21094 ... -0.390625 -0.15332 -0.777344]\n",
            "  [0.742188 0.0239258 -1.29688 ... -0.0878906 0.0605469 -0.0174561]\n",
            "  [0.185547 -0.416016 -1.46094 ... -0.146484 -0.322266 -1.14062]\n",
            "  ...\n",
            "  [-1.08594 0.253906 -2.40625 ... 0.808594 -0.496094 0.824219]\n",
            "  [-0.640625 -0.800781 -3.34375 ... 1.11719 0.355469 0.263672]\n",
            "  [-1.5 -1.125 -1.99219 ... -0.628906 -1.27344 -0.925781]]]\n",
            "next_layer_addition_dropped_out=[[[0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.109375 -7.53125 -7.875 ... -4.125 -0.660156 -4.5625]\n",
            "  ...\n",
            "  [-2.17188 -4.25 -7.8125 ... -1.04688 1.60938 5.15625]\n",
            "  [1.10938 -8.375 -8.0625 ... 0.800781 3.03125 -0.0703125]\n",
            "  [-9.8125 -9.8125 -10.375 ... -4.75 -5.75 -7.34375]]\n",
            "\n",
            " [[-2.5625 3.21875 -4.75 ... -3.90625 -4.25 -1.8125]\n",
            "  [2.1875 2.60938 -4.375 ... -1.27344 1.96875 4.375]\n",
            "  [-1.40625 0.488281 -6.21875 ... -0.726562 -0.625 -3.75]\n",
            "  ...\n",
            "  [4.03125 6.84375 -14.625 ... -0.515625 0.177734 3.15625]\n",
            "  [-10.4375 -1.03125 -13.5625 ... -8.625 -6.03125 -3.32812]\n",
            "  [-14.0625 0.0625 -9.75 ... -6.71875 -2.875 -3.65625]]\n",
            "\n",
            " [[0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.109375 -7.53125 -7.875 ... -4.125 -0.660156 -4.5625]\n",
            "  ...\n",
            "  [0.398438 -9.25 -11.6875 ... 0.171875 -11.9375 0.171875]\n",
            "  [-10.3125 -11.375 -9.1875 ... -4.96875 -6.84375 -4.1875]\n",
            "  [-9.625 -14.125 -13.8125 ... -7.75 -10.375 -3.40625]]\n",
            "\n",
            " [[0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.109375 -7.53125 -7.875 ... -4.125 -0.660156 -4.5625]\n",
            "  ...\n",
            "  [-1.73438 -0.875 -4.5625 ... 0.19043 -5.1875 -0.929688]\n",
            "  [-2.09375 -1.875 -9.9375 ... -0.847656 -4.3125 -3.26562]\n",
            "  [-10.4375 -12.375 -14.75 ... -7.1875 -9.4375 -5.6875]]\n",
            "\n",
            " [[0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [2.8125 -1.60156 -7.0625 ... -2.57812 1.40625 0.263672]\n",
            "  [0.9375 -2.8125 -6.375 ... -2.09375 -1.33594 -5]\n",
            "  ...\n",
            "  [-4.25 0.359375 -8.875 ... 3.76562 -0.679688 2.125]\n",
            "  [-2.625 -4.25 -11.3125 ... 3.8125 1.73438 -0.132812]\n",
            "  [-11.25 -9.125 -12.125 ... -6.625 -10.1875 -7.15625]]]\n",
            "inputs=[[[0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.109375 -7.53125 -7.875 ... -4.125 -0.660156 -4.5625]\n",
            "  ...\n",
            "  [-2.17188 -4.25 -7.8125 ... -1.04688 1.60938 5.15625]\n",
            "  [1.10938 -8.375 -8.0625 ... 0.800781 3.03125 -0.0703125]\n",
            "  [-9.8125 -9.8125 -10.375 ... -4.75 -5.75 -7.34375]]\n",
            "\n",
            " [[-2.5625 3.21875 -4.75 ... -3.90625 -4.25 -1.8125]\n",
            "  [2.1875 2.60938 -4.375 ... -1.27344 1.96875 4.375]\n",
            "  [-1.40625 0.488281 -6.21875 ... -0.726562 -0.625 -3.75]\n",
            "  ...\n",
            "  [4.03125 6.84375 -14.625 ... -0.515625 0.177734 3.15625]\n",
            "  [-10.4375 -1.03125 -13.5625 ... -8.625 -6.03125 -3.32812]\n",
            "  [-14.0625 0.0625 -9.75 ... -6.71875 -2.875 -3.65625]]\n",
            "\n",
            " [[0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.109375 -7.53125 -7.875 ... -4.125 -0.660156 -4.5625]\n",
            "  ...\n",
            "  [0.398438 -9.25 -11.6875 ... 0.171875 -11.9375 0.171875]\n",
            "  [-10.3125 -11.375 -9.1875 ... -4.96875 -6.84375 -4.1875]\n",
            "  [-9.625 -14.125 -13.8125 ... -7.75 -10.375 -3.40625]]\n",
            "\n",
            " [[0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.109375 -7.53125 -7.875 ... -4.125 -0.660156 -4.5625]\n",
            "  ...\n",
            "  [-1.73438 -0.875 -4.5625 ... 0.19043 -5.1875 -0.929688]\n",
            "  [-2.09375 -1.875 -9.9375 ... -0.847656 -4.3125 -3.26562]\n",
            "  [-10.4375 -12.375 -14.75 ... -7.1875 -9.4375 -5.6875]]\n",
            "\n",
            " [[0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [2.8125 -1.60156 -7.0625 ... -2.57812 1.40625 0.263672]\n",
            "  [0.9375 -2.8125 -6.375 ... -2.09375 -1.33594 -5]\n",
            "  ...\n",
            "  [-4.25 0.359375 -8.875 ... 3.76562 -0.679688 2.125]\n",
            "  [-2.625 -4.25 -11.3125 ... 3.8125 1.73438 -0.132812]\n",
            "  [-11.25 -9.125 -12.125 ... -6.625 -10.1875 -7.15625]]]\n",
            "lnx=[[[0.0407715 -1.25 -1.29688 ... -0.671875 -0.108398 -0.75]\n",
            "  [0.0407715 -1.25 -1.29688 ... -0.671875 -0.108398 -0.75]\n",
            "  [0.0179443 -1.23438 -1.29688 ... -0.675781 -0.108398 -0.75]\n",
            "  ...\n",
            "  [-0.455078 -0.890625 -1.64062 ... -0.219727 0.337891 1.07812]\n",
            "  [0.241211 -1.82031 -1.75 ... 0.173828 0.660156 -0.0152588]\n",
            "  [-1.36719 -1.36719 -1.44531 ... -0.660156 -0.800781 -1.02344]]\n",
            "\n",
            " [[-0.484375 0.609375 -0.898438 ... -0.738281 -0.804688 -0.341797]\n",
            "  [0.484375 0.578125 -0.96875 ... -0.28125 0.435547 0.96875]\n",
            "  [-0.322266 0.111816 -1.42188 ... -0.166016 -0.142578 -0.859375]\n",
            "  ...\n",
            "  [1 1.70312 -3.64062 ... -0.12793 0.0441895 0.785156]\n",
            "  [-1.44531 -0.142578 -1.88281 ... -1.19531 -0.835938 -0.460938]\n",
            "  [-2.28125 0.0101318 -1.58594 ... -1.09375 -0.466797 -0.59375]]\n",
            "\n",
            " [[0.0407715 -1.25 -1.29688 ... -0.671875 -0.108398 -0.75]\n",
            "  [0.0407715 -1.25 -1.29688 ... -0.671875 -0.108398 -0.75]\n",
            "  [0.0179443 -1.23438 -1.29688 ... -0.675781 -0.108398 -0.75]\n",
            "  ...\n",
            "  [0.0703125 -1.625 -2.0625 ... 0.0302734 -2.09375 0.0302734]\n",
            "  [-1.64062 -1.80469 -1.46094 ... -0.789062 -1.08594 -0.664062]\n",
            "  [-1.10156 -1.60938 -1.57812 ... -0.886719 -1.1875 -0.388672]]\n",
            "\n",
            " [[0.0407715 -1.25 -1.29688 ... -0.671875 -0.108398 -0.75]\n",
            "  [0.0407715 -1.25 -1.29688 ... -0.671875 -0.108398 -0.75]\n",
            "  [0.0179443 -1.23438 -1.29688 ... -0.675781 -0.108398 -0.75]\n",
            "  ...\n",
            "  [-0.388672 -0.196289 -1.02344 ... 0.0427246 -1.16406 -0.208984]\n",
            "  [-0.427734 -0.382812 -2.03125 ... -0.172852 -0.878906 -0.667969]\n",
            "  [-1.23438 -1.46094 -1.74219 ... -0.851562 -1.11719 -0.671875]]\n",
            "\n",
            " [[0.0407715 -1.25 -1.29688 ... -0.671875 -0.108398 -0.75]\n",
            "  [0.554688 -0.314453 -1.39062 ... -0.507812 0.277344 0.0517578]\n",
            "  [0.195312 -0.585938 -1.32812 ... -0.435547 -0.277344 -1.03906]\n",
            "  ...\n",
            "  [-1.00781 0.0854492 -2.10938 ... 0.894531 -0.161133 0.503906]\n",
            "  [-0.660156 -1.0625 -2.84375 ... 0.957031 0.435547 -0.0332031]\n",
            "  [-1.5 -1.21094 -1.61719 ... -0.882812 -1.35938 -0.953125]]]\n",
            "attention_lnx=[[[-1.35156 -0.710938 -0.490234 ... 0.065918 -1.99219 -1.60938]\n",
            "  [-1.35156 -0.710938 -0.490234 ... 0.065918 -1.99219 -1.60938]\n",
            "  [-1.35156 -0.714844 -0.490234 ... 0.0644531 -1.99219 -1.60938]\n",
            "  ...\n",
            "  [-1.25 -0.65625 -0.460938 ... -0.304688 -1.74219 -1.42969]\n",
            "  [-1.11719 -0.730469 -0.332031 ... -0.296875 -1.76562 -1.54688]\n",
            "  [-1.14062 -0.617188 -0.34375 ... -0.722656 -1.55469 -1.65625]]\n",
            "\n",
            " [[-1.07812 -2.60938 0.570312 ... 1.17188 0.0854492 -2.0625]\n",
            "  [-0.761719 -1.40625 0.4375 ... 0.808594 0.601562 -1.55469]\n",
            "  [-0.917969 -1.02344 0.671875 ... 0.902344 0.129883 -1.51562]\n",
            "  ...\n",
            "  [-0.163086 -1.28125 0.287109 ... -0.425781 -0.271484 -2.23438]\n",
            "  [-0.25 -1.38281 0.116699 ... -0.921875 0.0825195 -2.46875]\n",
            "  [-0.259766 -1.17188 0.230469 ... -0.65625 0.164062 -1.91406]]\n",
            "\n",
            " [[-1.35156 -0.710938 -0.490234 ... 0.065918 -1.99219 -1.60938]\n",
            "  [-1.35156 -0.710938 -0.490234 ... 0.065918 -1.99219 -1.60938]\n",
            "  [-1.35156 -0.714844 -0.490234 ... 0.0644531 -1.99219 -1.60938]\n",
            "  ...\n",
            "  [-0.875 -0.960938 -0.458984 ... -0.871094 -1.29688 -1.92188]\n",
            "  [-0.597656 -1.14844 -0.695312 ... -1.10938 -1.20312 -1.99219]\n",
            "  [-0.933594 -1.14844 -0.773438 ... -1.38281 -1.15625 -1.89844]]\n",
            "\n",
            " [[-1.35156 -0.710938 -0.490234 ... 0.065918 -1.99219 -1.60938]\n",
            "  [-1.35156 -0.710938 -0.490234 ... 0.065918 -1.99219 -1.60938]\n",
            "  [-1.35156 -0.714844 -0.490234 ... 0.0644531 -1.99219 -1.60938]\n",
            "  ...\n",
            "  [-0.464844 -0.667969 -0.246094 ... -0.941406 -1.0625 -1.5625]\n",
            "  [-0.625 -0.863281 -0.349609 ... -0.960938 -1.28125 -1.80469]\n",
            "  [-0.65625 -0.925781 -0.523438 ... -1.28125 -1.1875 -1.85156]]\n",
            "\n",
            " [[-1.35156 -0.710938 -0.490234 ... 0.065918 -1.99219 -1.60938]\n",
            "  [-1.28125 -1.40625 -0.144531 ... 0.527344 -1.6875 -2.3125]\n",
            "  [-1.25 -1.24219 -0.324219 ... 0.304688 -1.60938 -2.15625]\n",
            "  ...\n",
            "  [0.00909424 -0.753906 -0.675781 ... -1.04688 -1.02344 -2.17188]\n",
            "  [-0.0786133 -0.90625 -0.734375 ... -1.08594 -0.886719 -2.01562]\n",
            "  [-0.5625 -1.07031 -0.910156 ... -1.32812 -0.78125 -1.75781]]]\n",
            "attn_output=[[[-0.177734 -1.34375 -1.35156 ... -0.648438 -0.427734 -0.996094]\n",
            "  [-0.177734 -1.34375 -1.35156 ... -0.648438 -0.427734 -0.996094]\n",
            "  [-0.200195 -1.32812 -1.35156 ... -0.65625 -0.427734 -0.996094]\n",
            "  ...\n",
            "  [-0.703125 -1.00781 -1.70312 ... -0.277344 -0.0273438 0.765625]\n",
            "  [-0.00167084 -1.94531 -1.78906 ... 0.107422 0.269531 -0.345703]\n",
            "  [-1.49219 -1.42188 -1.46094 ... -0.746094 -0.996094 -1.22656]]\n",
            "\n",
            " [[-0.667969 0.112305 -0.769531 ... -0.503906 -0.765625 -0.710938]\n",
            "  [0.306641 0.259766 -0.847656 ... -0.100098 0.554688 0.605469]\n",
            "  [-0.519531 -0.119629 -1.24219 ... 0.0393066 -0.11084 -1.17969]\n",
            "  ...\n",
            "  [0.945312 1.35938 -3.51562 ... -0.230469 -0.0229492 0.225586]\n",
            "  [-1.44531 -0.326172 -1.8125 ... -1.28906 -0.804688 -0.78125]\n",
            "  [-2.26562 -0.174805 -1.5 ... -1.16406 -0.427734 -0.878906]]\n",
            "\n",
            " [[-0.177734 -1.34375 -1.35156 ... -0.648438 -0.427734 -0.996094]\n",
            "  [-0.177734 -1.34375 -1.35156 ... -0.648438 -0.427734 -0.996094]\n",
            "  [-0.200195 -1.32812 -1.35156 ... -0.65625 -0.427734 -0.996094]\n",
            "  ...\n",
            "  [-0.081543 -1.75 -2.07812 ... -0.120117 -2.26562 -0.300781]\n",
            "  [-1.6875 -1.9375 -1.53125 ... -0.941406 -1.24219 -0.957031]\n",
            "  [-1.17969 -1.70312 -1.625 ... -1.01562 -1.28906 -0.59375]]\n",
            "\n",
            " [[-0.177734 -1.34375 -1.35156 ... -0.648438 -0.427734 -0.996094]\n",
            "  [-0.177734 -1.34375 -1.35156 ... -0.648438 -0.427734 -0.996094]\n",
            "  [-0.200195 -1.32812 -1.35156 ... -0.65625 -0.427734 -0.996094]\n",
            "  ...\n",
            "  [-0.484375 -0.339844 -1.0625 ... -0.165039 -1.375 -0.550781]\n",
            "  [-0.539062 -0.542969 -2.04688 ... -0.359375 -1.10938 -1.00781]\n",
            "  [-1.28125 -1.53906 -1.76562 ... -0.980469 -1.22656 -0.871094]]\n",
            "\n",
            " [[-0.177734 -1.34375 -1.35156 ... -0.648438 -0.427734 -0.996094]\n",
            "  [0.294922 -0.582031 -1.39062 ... -0.396484 -0.0541992 -0.396484]\n",
            "  [-0.0639648 -0.828125 -1.36719 ... -0.365234 -0.601562 -1.46094]\n",
            "  ...\n",
            "  [-0.984375 -0.0913086 -2.21875 ... 0.628906 -0.394531 -0.0108643]\n",
            "  [-0.667969 -1.27344 -2.96875 ... 0.671875 0.208984 -0.53125]\n",
            "  [-1.53906 -1.32812 -1.69531 ... -1.03125 -1.42969 -1.15625]]]\n",
            "next_layer_addition_dropped_out=[[[-1.41406 -8.1875 -8.8125 ... -3.90625 -2.3125 -6.84375]\n",
            "  [-1.41406 -8.1875 -8.8125 ... -3.90625 -2.3125 -6.84375]\n",
            "  [-1.54688 -8.125 -8.875 ... -3.95312 -2.28125 -6.84375]\n",
            "  ...\n",
            "  [-2.46875 -5.3125 -8.6875 ... -0.425781 -0.289062 3.28125]\n",
            "  [0.0776367 -9.125 -8.4375 ... 1.75 2.40625 -2.3125]\n",
            "  [-9.5625 -10.1875 -12.0625 ... -6.25 -6.5 -9.875]]\n",
            "\n",
            " [[-3.04688 0.9375 -4.3125 ... -4 -3.64062 -4.875]\n",
            "  [1.08594 1.73438 -4.1875 ... -0.617188 2.78125 2.54688]\n",
            "  [-2.39062 -0.625 -5.28125 ... -0.464844 -0.138672 -5.34375]\n",
            "  ...\n",
            "  [2.51562 6.34375 -13.4375 ... -2.10938 -0.103027 1.17969]\n",
            "  [-9.5625 -2.3125 -15.375 ... -11.375 -7.09375 -6.6875]\n",
            "  [-14.625 -0.5625 -9.0625 ... -9.375 -2.89062 -4.625]]\n",
            "\n",
            " [[-1.41406 -8.1875 -8.8125 ... -3.90625 -2.3125 -6.84375]\n",
            "  [-1.41406 -8.1875 -8.8125 ... -3.90625 -2.3125 -6.84375]\n",
            "  [-1.54688 -8.125 -8.875 ... -3.95312 -2.28125 -6.84375]\n",
            "  ...\n",
            "  [-0.152344 -10.0625 -12.8125 ... -0.828125 -12.5625 -3.46875]\n",
            "  [-9.8125 -11.3125 -10.25 ... -6.96875 -9 -7.75]\n",
            "  [-9.375 -14.75 -15.75 ... -10.875 -11.4375 -6.53125]]\n",
            "\n",
            " [[-1.41406 -8.1875 -8.8125 ... -3.90625 -2.3125 -6.84375]\n",
            "  [-1.41406 -8.1875 -8.8125 ... -3.90625 -2.3125 -6.84375]\n",
            "  [-1.54688 -8.125 -8.875 ... -3.95312 -2.28125 -6.84375]\n",
            "  ...\n",
            "  [-1.83594 -1.48438 -4.78125 ... -0.953125 -5.8125 -3.65625]\n",
            "  [-1.67188 -3.4375 -11.375 ... -1.98438 -5.3125 -4.34375]\n",
            "  [-10.125 -12.75 -16 ... -9.875 -10.1875 -8.375]]\n",
            "\n",
            " [[-1.41406 -8.1875 -8.8125 ... -3.90625 -2.3125 -6.84375]\n",
            "  [2.5625 -2.15625 -7.65625 ... -1.25 0 -3.28125]\n",
            "  [-0.460938 -4.09375 -7.46875 ... -0.28125 -2.98438 -6.625]\n",
            "  ...\n",
            "  [-4.875 -0.482422 -10.1875 ... 2.85938 -2.57812 -0.357422]\n",
            "  [-4.0625 -5.9375 -12.1875 ... 1.61719 0.585938 -1.94531]\n",
            "  [-11.5 -9.5 -13.375 ... -10.0625 -11.0625 -8.9375]]]\n",
            "inputs=[[[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]]\n",
            "lnx=[[[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]]\n",
            "attention_lnx=[[[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]]\n",
            "attn_output=[[[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]]\n",
            "next_layer_addition_dropped_out=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "inputs=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "lnx=[[[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]]\n",
            "attention_lnx=[[[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]]\n",
            "attn_output=[[[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]]\n",
            "next_layer_addition_dropped_out=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "inputs=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "lnx=[[[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]]\n",
            "attention_lnx=[[[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]]\n",
            "attn_output=[[[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]]\n",
            "next_layer_addition_dropped_out=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "inputs=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "lnx=[[[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]]\n",
            "attention_lnx=[[[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]]\n",
            "attn_output=[[[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]]\n",
            "next_layer_addition_dropped_out=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "inputs=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "lnx=[[[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]]\n",
            "attention_lnx=[[[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]]\n",
            "attn_output=[[[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]]\n",
            "next_layer_addition_dropped_out=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "inputs=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "lnx=[[[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]]\n",
            "attention_lnx=[[[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]]\n",
            "attn_output=[[[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]]\n",
            "next_layer_addition_dropped_out=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "inputs=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "lnx=[[[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]]\n",
            "attention_lnx=[[[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]]\n",
            "attn_output=[[[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]]\n",
            "next_layer_addition_dropped_out=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "inputs=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "lnx=[[[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]]\n",
            "attention_lnx=[[[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]]\n",
            "attn_output=[[[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]]\n",
            "next_layer_addition_dropped_out=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "inputs=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "lnx=[[[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]]\n",
            "attention_lnx=[[[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]]\n",
            "attn_output=[[[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]]\n",
            "next_layer_addition_dropped_out=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "inputs=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "lnx=[[[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]]\n",
            "attention_lnx=[[[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]]\n",
            "attn_output=[[[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]]\n",
            "next_layer_addition_dropped_out=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "inputs=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "lnx=[[[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]]\n",
            "attention_lnx=[[[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]]\n",
            "attn_output=[[[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]]\n",
            "next_layer_addition_dropped_out=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "inputs=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "lnx=[[[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]]\n",
            "attention_lnx=[[[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]]\n",
            "attn_output=[[[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]]\n",
            "next_layer_addition_dropped_out=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "inputs=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "lnx=[[[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]]\n",
            "attention_lnx=[[[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]]\n",
            "attn_output=[[[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]]\n",
            "next_layer_addition_dropped_out=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "inputs=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "lnx=[[[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]]\n",
            "attention_lnx=[[[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]]\n",
            "attn_output=[[[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]]\n",
            "next_layer_addition_dropped_out=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "inputs=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "lnx=[[[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]]\n",
            "attention_lnx=[[[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]]\n",
            "attn_output=[[[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]]\n",
            "next_layer_addition_dropped_out=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "inputs=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "lnx=[[[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]]\n",
            "attention_lnx=[[[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]]\n",
            "attn_output=[[[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]]\n",
            "next_layer_addition_dropped_out=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "inputs=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "lnx=[[[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]]\n",
            "attention_lnx=[[[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]]\n",
            "attn_output=[[[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]]\n",
            "next_layer_addition_dropped_out=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "inputs=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "lnx=[[[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]]\n",
            "attention_lnx=[[[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]]\n",
            "attn_output=[[[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]]\n",
            "next_layer_addition_dropped_out=[[[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]]\n",
            "inputs=[[[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]]\n",
            "lnx=[[[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]]\n",
            "attention_lnx=[[[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]]\n",
            "attn_output=[[[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]]\n",
            "next_layer_addition_dropped_out=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "inputs=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "lnx=[[[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]]\n",
            "attention_lnx=[[[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]]\n",
            "attn_output=[[[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]]\n",
            "next_layer_addition_dropped_out=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "inputs=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "lnx=[[[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]]\n",
            "attention_lnx=[[[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]]\n",
            "attn_output=[[[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]]\n",
            "next_layer_addition_dropped_out=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "inputs=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "lnx=[[[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]]\n",
            "attention_lnx=[[[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]]\n",
            "attn_output=[[[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]]\n",
            "next_layer_addition_dropped_out=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "inputs=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "lnx=[[[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]]\n",
            "attention_lnx=[[[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]]\n",
            "attn_output=[[[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]]\n",
            "next_layer_addition_dropped_out=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "inputs=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "lnx=[[[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]]\n",
            "attention_lnx=[[[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]]\n",
            "attn_output=[[[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]]\n",
            "next_layer_addition_dropped_out=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "inputs=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "lnx=[[[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]]\n",
            "attention_lnx=[[[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]]\n",
            "attn_output=[[[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]]\n",
            "next_layer_addition_dropped_out=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "inputs=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "lnx=[[[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]]\n",
            "attention_lnx=[[[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]]\n",
            "attn_output=[[[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]]\n",
            "next_layer_addition_dropped_out=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "inputs=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "lnx=[[[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]]\n",
            "attention_lnx=[[[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]]\n",
            "attn_output=[[[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]]\n",
            "next_layer_addition_dropped_out=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "inputs=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "lnx=[[[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]]\n",
            "attention_lnx=[[[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]]\n",
            "attn_output=[[[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]]\n",
            "next_layer_addition_dropped_out=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "inputs=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "lnx=[[[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]]\n",
            "attention_lnx=[[[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]]\n",
            "attn_output=[[[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]]\n",
            "next_layer_addition_dropped_out=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "inputs=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "lnx=[[[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]]\n",
            "attention_lnx=[[[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]]\n",
            "attn_output=[[[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]]\n",
            "next_layer_addition_dropped_out=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "inputs=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "lnx=[[[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]]\n",
            "attention_lnx=[[[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]]\n",
            "attn_output=[[[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]]\n",
            "next_layer_addition_dropped_out=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "inputs=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "lnx=[[[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]]\n",
            "attention_lnx=[[[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]]\n",
            "attn_output=[[[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]]\n",
            "next_layer_addition_dropped_out=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "inputs=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "lnx=[[[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]]\n",
            "attention_lnx=[[[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]]\n",
            "attn_output=[[[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]]\n",
            "next_layer_addition_dropped_out=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "inputs=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "lnx=[[[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]]\n",
            "attention_lnx=[[[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]]\n",
            "attn_output=[[[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]]\n",
            "next_layer_addition_dropped_out=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "inputs=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "lnx=[[[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]]\n",
            "attention_lnx=[[[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]]\n",
            "attn_output=[[[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]]\n",
            "next_layer_addition_dropped_out=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "inputs=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "lnx=[[[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]]\n",
            "attention_lnx=[[[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]]\n",
            "attn_output=[[[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]]\n",
            "next_layer_addition_dropped_out=[[[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]]\n",
            "inputs=[[[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]]\n",
            "lnx=[[[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]]\n",
            "attention_lnx=[[[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]]\n",
            "attn_output=[[[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]]\n",
            "next_layer_addition_dropped_out=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "inputs=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "lnx=[[[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]]\n",
            "attention_lnx=[[[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]]\n",
            "attn_output=[[[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]]\n",
            "next_layer_addition_dropped_out=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "inputs=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "lnx=[[[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]]\n",
            "attention_lnx=[[[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]]\n",
            "attn_output=[[[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]]\n",
            "next_layer_addition_dropped_out=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "inputs=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "lnx=[[[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]]\n",
            "attention_lnx=[[[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]]\n",
            "attn_output=[[[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]]\n",
            "next_layer_addition_dropped_out=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "inputs=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "lnx=[[[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]]\n",
            "attention_lnx=[[[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]]\n",
            "attn_output=[[[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]]\n",
            "next_layer_addition_dropped_out=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "inputs=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "lnx=[[[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]]\n",
            "attention_lnx=[[[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]]\n",
            "attn_output=[[[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]]\n",
            "next_layer_addition_dropped_out=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "inputs=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "lnx=[[[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]]\n",
            "attention_lnx=[[[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]]\n",
            "attn_output=[[[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]]\n",
            "next_layer_addition_dropped_out=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "inputs=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "lnx=[[[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]]\n",
            "attention_lnx=[[[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]]\n",
            "attn_output=[[[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]]\n",
            "next_layer_addition_dropped_out=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "inputs=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "lnx=[[[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]]\n",
            "attention_lnx=[[[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]]\n",
            "attn_output=[[[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]]\n",
            "next_layer_addition_dropped_out=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "inputs=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "lnx=[[[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]]\n",
            "attention_lnx=[[[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]]\n",
            "attn_output=[[[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]]\n",
            "next_layer_addition_dropped_out=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "inputs=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "lnx=[[[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]]\n",
            "attention_lnx=[[[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]]\n",
            "attn_output=[[[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]]\n",
            "next_layer_addition_dropped_out=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "inputs=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "lnx=[[[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]]\n",
            "attention_lnx=[[[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]]\n",
            "attn_output=[[[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]]\n",
            "next_layer_addition_dropped_out=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "inputs=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "lnx=[[[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]]\n",
            "attention_lnx=[[[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]]\n",
            "attn_output=[[[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]]\n",
            "next_layer_addition_dropped_out=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "inputs=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "lnx=[[[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]]\n",
            "attention_lnx=[[[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]]\n",
            "attn_output=[[[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]]\n",
            "next_layer_addition_dropped_out=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "inputs=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "lnx=[[[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]]\n",
            "attention_lnx=[[[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]]\n",
            "attn_output=[[[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]]\n",
            "next_layer_addition_dropped_out=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "inputs=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "lnx=[[[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]]\n",
            "attention_lnx=[[[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]]\n",
            "attn_output=[[[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]]\n",
            "next_layer_addition_dropped_out=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "inputs=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "lnx=[[[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]]\n",
            "attention_lnx=[[[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]]\n",
            "attn_output=[[[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]]\n",
            "next_layer_addition_dropped_out=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "inputs=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "lnx=[[[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]]\n",
            "attention_lnx=[[[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]]\n",
            "attn_output=[[[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]]\n",
            "next_layer_addition_dropped_out=[[[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]]\n",
            "inputs=[[[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]]\n",
            "lnx=[[[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]]\n",
            "attention_lnx=[[[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]]\n",
            "attn_output=[[[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]]\n",
            "next_layer_addition_dropped_out=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "inputs=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "lnx=[[[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]]\n",
            "attention_lnx=[[[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]]\n",
            "attn_output=[[[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]]\n",
            "next_layer_addition_dropped_out=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "inputs=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "lnx=[[[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]]\n",
            "attention_lnx=[[[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]]\n",
            "attn_output=[[[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]]\n",
            "next_layer_addition_dropped_out=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "inputs=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "lnx=[[[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]]\n",
            "attention_lnx=[[[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]]\n",
            "attn_output=[[[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]]\n",
            "next_layer_addition_dropped_out=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "inputs=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "lnx=[[[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]]\n",
            "attention_lnx=[[[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]]\n",
            "attn_output=[[[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]]\n",
            "next_layer_addition_dropped_out=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "inputs=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "lnx=[[[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]]\n",
            "attention_lnx=[[[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]]\n",
            "attn_output=[[[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]]\n",
            "next_layer_addition_dropped_out=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "inputs=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "lnx=[[[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]]\n",
            "attention_lnx=[[[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]]\n",
            "attn_output=[[[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]]\n",
            "next_layer_addition_dropped_out=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "inputs=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "lnx=[[[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]]\n",
            "attention_lnx=[[[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]]\n",
            "attn_output=[[[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]]\n",
            "next_layer_addition_dropped_out=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "inputs=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "lnx=[[[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]]\n",
            "attention_lnx=[[[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]]\n",
            "attn_output=[[[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]]\n",
            "next_layer_addition_dropped_out=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "inputs=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "lnx=[[[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]]\n",
            "attention_lnx=[[[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]]\n",
            "attn_output=[[[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]]\n",
            "next_layer_addition_dropped_out=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "inputs=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "lnx=[[[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]]\n",
            "attention_lnx=[[[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]]\n",
            "attn_output=[[[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]]\n",
            "next_layer_addition_dropped_out=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "inputs=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "lnx=[[[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]]\n",
            "attention_lnx=[[[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]]\n",
            "attn_output=[[[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]]\n",
            "next_layer_addition_dropped_out=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "inputs=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "lnx=[[[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]]\n",
            "attention_lnx=[[[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]]\n",
            "attn_output=[[[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]]\n",
            "next_layer_addition_dropped_out=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "inputs=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "lnx=[[[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]]\n",
            "attention_lnx=[[[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]]\n",
            "attn_output=[[[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]]\n",
            "next_layer_addition_dropped_out=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "inputs=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "lnx=[[[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]]\n",
            "attention_lnx=[[[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]]\n",
            "attn_output=[[[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]]\n",
            "next_layer_addition_dropped_out=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "inputs=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "lnx=[[[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]]\n",
            "attention_lnx=[[[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]]\n",
            "attn_output=[[[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]]\n",
            "next_layer_addition_dropped_out=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "inputs=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "lnx=[[[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]]\n",
            "attention_lnx=[[[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]]\n",
            "attn_output=[[[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]]\n",
            "next_layer_addition_dropped_out=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "inputs=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "lnx=[[[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]]\n",
            "attention_lnx=[[[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]]\n",
            "attn_output=[[[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]]\n",
            "next_layer_addition_dropped_out=[[[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]]\n",
            "inputs=[[[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]]\n",
            "lnx=[[[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]]\n",
            "attention_lnx=[[[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]]\n",
            "attn_output=[[[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]]\n",
            "next_layer_addition_dropped_out=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "inputs=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "lnx=[[[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]]\n",
            "attention_lnx=[[[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]]\n",
            "attn_output=[[[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]]\n",
            "next_layer_addition_dropped_out=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "inputs=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "lnx=[[[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]]\n",
            "attention_lnx=[[[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]]\n",
            "attn_output=[[[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]]\n",
            "next_layer_addition_dropped_out=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "inputs=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "lnx=[[[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]]\n",
            "attention_lnx=[[[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]]\n",
            "attn_output=[[[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]]\n",
            "next_layer_addition_dropped_out=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "inputs=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "lnx=[[[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]]\n",
            "attention_lnx=[[[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]]\n",
            "attn_output=[[[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]]\n",
            "next_layer_addition_dropped_out=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "inputs=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "lnx=[[[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]]\n",
            "attention_lnx=[[[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]]\n",
            "attn_output=[[[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]]\n",
            "next_layer_addition_dropped_out=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "inputs=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "lnx=[[[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]]\n",
            "attention_lnx=[[[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]]\n",
            "attn_output=[[[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]]\n",
            "next_layer_addition_dropped_out=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "inputs=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "lnx=[[[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]]\n",
            "attention_lnx=[[[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]]\n",
            "attn_output=[[[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]]\n",
            "next_layer_addition_dropped_out=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "inputs=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "lnx=[[[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]]\n",
            "attention_lnx=[[[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]]\n",
            "attn_output=[[[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]]\n",
            "next_layer_addition_dropped_out=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "inputs=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "lnx=[[[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]]\n",
            "attention_lnx=[[[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]]\n",
            "attn_output=[[[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]]\n",
            "next_layer_addition_dropped_out=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "inputs=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "lnx=[[[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]]\n",
            "attention_lnx=[[[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]]\n",
            "attn_output=[[[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]]\n",
            "next_layer_addition_dropped_out=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "inputs=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "lnx=[[[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]]\n",
            "attention_lnx=[[[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]]\n",
            "attn_output=[[[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]]\n",
            "next_layer_addition_dropped_out=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "inputs=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "lnx=[[[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]]\n",
            "attention_lnx=[[[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]]\n",
            "attn_output=[[[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]]\n",
            "next_layer_addition_dropped_out=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "inputs=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "lnx=[[[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]]\n",
            "attention_lnx=[[[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]]\n",
            "attn_output=[[[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]]\n",
            "next_layer_addition_dropped_out=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "inputs=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "lnx=[[[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]]\n",
            "attention_lnx=[[[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]]\n",
            "attn_output=[[[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]]\n",
            "next_layer_addition_dropped_out=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "inputs=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "lnx=[[[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]]\n",
            "attention_lnx=[[[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]]\n",
            "attn_output=[[[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]]\n",
            "next_layer_addition_dropped_out=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "inputs=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "lnx=[[[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]]\n",
            "attention_lnx=[[[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]]\n",
            "attn_output=[[[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]]\n",
            "next_layer_addition_dropped_out=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "inputs=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "lnx=[[[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]]\n",
            "attention_lnx=[[[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]]\n",
            "attn_output=[[[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]]\n",
            "next_layer_addition_dropped_out=[[[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]]\n",
            "inputs=[[[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]]\n",
            "lnx=[[[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]]\n",
            "attention_lnx=[[[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]]\n",
            "attn_output=[[[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]]\n",
            "next_layer_addition_dropped_out=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "inputs=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "lnx=[[[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]]\n",
            "attention_lnx=[[[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]]\n",
            "attn_output=[[[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]]\n",
            "next_layer_addition_dropped_out=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "inputs=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "lnx=[[[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]]\n",
            "attention_lnx=[[[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]]\n",
            "attn_output=[[[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]]\n",
            "next_layer_addition_dropped_out=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "inputs=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "lnx=[[[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]]\n",
            "attention_lnx=[[[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]]\n",
            "attn_output=[[[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]]\n",
            "next_layer_addition_dropped_out=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "inputs=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "lnx=[[[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]]\n",
            "attention_lnx=[[[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]]\n",
            "attn_output=[[[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]]\n",
            "next_layer_addition_dropped_out=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "inputs=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "lnx=[[[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]]\n",
            "attention_lnx=[[[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]]\n",
            "attn_output=[[[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]]\n",
            "next_layer_addition_dropped_out=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "inputs=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "lnx=[[[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]]\n",
            "attention_lnx=[[[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]]\n",
            "attn_output=[[[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]]\n",
            "next_layer_addition_dropped_out=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "inputs=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "lnx=[[[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]]\n",
            "attention_lnx=[[[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]]\n",
            "attn_output=[[[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]]\n",
            "next_layer_addition_dropped_out=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "inputs=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "lnx=[[[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]]\n",
            "attention_lnx=[[[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]]\n",
            "attn_output=[[[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]]\n",
            "next_layer_addition_dropped_out=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "inputs=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "lnx=[[[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]]\n",
            "attention_lnx=[[[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]]\n",
            "attn_output=[[[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]]\n",
            "next_layer_addition_dropped_out=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "inputs=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "lnx=[[[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]]\n",
            "attention_lnx=[[[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]]\n",
            "attn_output=[[[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]]\n",
            "next_layer_addition_dropped_out=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "inputs=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "lnx=[[[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]]\n",
            "attention_lnx=[[[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]]\n",
            "attn_output=[[[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]]\n",
            "next_layer_addition_dropped_out=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "inputs=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "lnx=[[[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]]\n",
            "attention_lnx=[[[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]]\n",
            "attn_output=[[[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]]\n",
            "next_layer_addition_dropped_out=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "inputs=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "lnx=[[[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]]\n",
            "attention_lnx=[[[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]]\n",
            "attn_output=[[[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]]\n",
            "next_layer_addition_dropped_out=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "inputs=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "lnx=[[[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]]\n",
            "attention_lnx=[[[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]]\n",
            "attn_output=[[[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]]\n",
            "next_layer_addition_dropped_out=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "inputs=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "lnx=[[[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]]\n",
            "attention_lnx=[[[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]]\n",
            "attn_output=[[[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]]\n",
            "next_layer_addition_dropped_out=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "inputs=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "lnx=[[[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]]\n",
            "attention_lnx=[[[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]]\n",
            "attn_output=[[[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]]\n",
            "next_layer_addition_dropped_out=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "inputs=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "lnx=[[[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]]\n",
            "attention_lnx=[[[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]]\n",
            "attn_output=[[[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]]\n",
            "next_layer_addition_dropped_out=[[[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]]\n",
            "inputs=[[[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]]\n",
            "lnx=[[[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]]\n",
            "attention_lnx=[[[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]]\n",
            "attn_output=[[[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]]\n",
            "next_layer_addition_dropped_out=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "inputs=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "lnx=[[[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]]\n",
            "attention_lnx=[[[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]]\n",
            "attn_output=[[[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]]\n",
            "next_layer_addition_dropped_out=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "inputs=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "lnx=[[[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]]\n",
            "attention_lnx=[[[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]]\n",
            "attn_output=[[[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]]\n",
            "next_layer_addition_dropped_out=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "inputs=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "lnx=[[[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]]\n",
            "attention_lnx=[[[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]]\n",
            "attn_output=[[[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]]\n",
            "next_layer_addition_dropped_out=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "inputs=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "lnx=[[[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]]\n",
            "attention_lnx=[[[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]]\n",
            "attn_output=[[[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]]\n",
            "next_layer_addition_dropped_out=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "inputs=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "lnx=[[[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]]\n",
            "attention_lnx=[[[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]]\n",
            "attn_output=[[[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]]\n",
            "next_layer_addition_dropped_out=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "inputs=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "lnx=[[[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]]\n",
            "attention_lnx=[[[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]]\n",
            "attn_output=[[[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]]\n",
            "next_layer_addition_dropped_out=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "inputs=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "lnx=[[[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]]\n",
            "attention_lnx=[[[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]]\n",
            "attn_output=[[[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]]\n",
            "next_layer_addition_dropped_out=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "inputs=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "lnx=[[[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]]\n",
            "attention_lnx=[[[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]]\n",
            "attn_output=[[[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]]\n",
            "next_layer_addition_dropped_out=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "inputs=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "lnx=[[[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]]\n",
            "attention_lnx=[[[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]]\n",
            "attn_output=[[[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]]\n",
            "next_layer_addition_dropped_out=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "inputs=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "lnx=[[[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]]\n",
            "attention_lnx=[[[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]]\n",
            "attn_output=[[[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]]\n",
            "next_layer_addition_dropped_out=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "inputs=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "lnx=[[[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]]\n",
            "attention_lnx=[[[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]]\n",
            "attn_output=[[[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]]\n",
            "next_layer_addition_dropped_out=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "inputs=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "lnx=[[[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]]\n",
            "attention_lnx=[[[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]]\n",
            "attn_output=[[[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]]\n",
            "next_layer_addition_dropped_out=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "inputs=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "lnx=[[[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]]\n",
            "attention_lnx=[[[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]]\n",
            "attn_output=[[[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]]\n",
            "next_layer_addition_dropped_out=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "inputs=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "lnx=[[[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]]\n",
            "attention_lnx=[[[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]]\n",
            "attn_output=[[[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]]\n",
            "next_layer_addition_dropped_out=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "inputs=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "lnx=[[[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]]\n",
            "attention_lnx=[[[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]]\n",
            "attn_output=[[[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]]\n",
            "next_layer_addition_dropped_out=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "inputs=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "lnx=[[[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]]\n",
            "attention_lnx=[[[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]]\n",
            "attn_output=[[[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]]\n",
            "next_layer_addition_dropped_out=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "inputs=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "lnx=[[[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]]\n",
            "attention_lnx=[[[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]]\n",
            "attn_output=[[[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]]\n",
            "next_layer_addition_dropped_out=[[[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]]\n",
            "inputs=[[[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]]\n",
            "lnx=[[[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]]\n",
            "attention_lnx=[[[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]]\n",
            "attn_output=[[[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]]\n",
            "next_layer_addition_dropped_out=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "inputs=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "lnx=[[[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]]\n",
            "attention_lnx=[[[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]]\n",
            "attn_output=[[[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]]\n",
            "next_layer_addition_dropped_out=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "inputs=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "lnx=[[[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]]\n",
            "attention_lnx=[[[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]]\n",
            "attn_output=[[[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]]\n",
            "next_layer_addition_dropped_out=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "inputs=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "lnx=[[[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]]\n",
            "attention_lnx=[[[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]]\n",
            "attn_output=[[[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]]\n",
            "next_layer_addition_dropped_out=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "inputs=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "lnx=[[[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]]\n",
            "attention_lnx=[[[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]]\n",
            "attn_output=[[[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]]\n",
            "next_layer_addition_dropped_out=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "inputs=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "lnx=[[[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]]\n",
            "attention_lnx=[[[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]]\n",
            "attn_output=[[[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]]\n",
            "next_layer_addition_dropped_out=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "inputs=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "lnx=[[[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]]\n",
            "attention_lnx=[[[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]]\n",
            "attn_output=[[[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]]\n",
            "next_layer_addition_dropped_out=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "inputs=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "lnx=[[[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]]\n",
            "attention_lnx=[[[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]]\n",
            "attn_output=[[[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]]\n",
            "next_layer_addition_dropped_out=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "inputs=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "lnx=[[[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]]\n",
            "attention_lnx=[[[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]]\n",
            "attn_output=[[[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]]\n",
            "next_layer_addition_dropped_out=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "inputs=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "lnx=[[[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]]\n",
            "attention_lnx=[[[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]]\n",
            "attn_output=[[[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]]\n",
            "next_layer_addition_dropped_out=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "inputs=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "lnx=[[[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]]\n",
            "attention_lnx=[[[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]]\n",
            "attn_output=[[[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]]\n",
            "next_layer_addition_dropped_out=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "inputs=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "lnx=[[[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]]\n",
            "attention_lnx=[[[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]]\n",
            "attn_output=[[[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]]\n",
            "next_layer_addition_dropped_out=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "inputs=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "lnx=[[[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]]\n",
            "attention_lnx=[[[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]]\n",
            "attn_output=[[[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]]\n",
            "next_layer_addition_dropped_out=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "inputs=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "lnx=[[[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]]\n",
            "attention_lnx=[[[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]]\n",
            "attn_output=[[[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]]\n",
            "next_layer_addition_dropped_out=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "inputs=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "lnx=[[[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]]\n",
            "attention_lnx=[[[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]]\n",
            "attn_output=[[[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]]\n",
            "next_layer_addition_dropped_out=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "inputs=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "lnx=[[[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]]\n",
            "attention_lnx=[[[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]]\n",
            "attn_output=[[[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]]\n",
            "next_layer_addition_dropped_out=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "inputs=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "lnx=[[[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]]\n",
            "attention_lnx=[[[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]]\n",
            "attn_output=[[[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]]\n",
            "next_layer_addition_dropped_out=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "inputs=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "lnx=[[[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]]\n",
            "attention_lnx=[[[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]]\n",
            "attn_output=[[[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]]\n",
            "next_layer_addition_dropped_out=[[[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]]\n",
            "inputs=[[[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]]\n",
            "lnx=[[[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]]\n",
            "attention_lnx=[[[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]]\n",
            "attn_output=[[[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]]\n",
            "next_layer_addition_dropped_out=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "inputs=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "lnx=[[[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]]\n",
            "attention_lnx=[[[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]]\n",
            "attn_output=[[[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]]\n",
            "next_layer_addition_dropped_out=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "inputs=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "lnx=[[[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]]\n",
            "attention_lnx=[[[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]]\n",
            "attn_output=[[[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]]\n",
            "next_layer_addition_dropped_out=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "inputs=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "lnx=[[[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]]\n",
            "attention_lnx=[[[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]]\n",
            "attn_output=[[[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]]\n",
            "next_layer_addition_dropped_out=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "inputs=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "lnx=[[[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]]\n",
            "attention_lnx=[[[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]]\n",
            "attn_output=[[[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]]\n",
            "next_layer_addition_dropped_out=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "inputs=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "lnx=[[[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]]\n",
            "attention_lnx=[[[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]]\n",
            "attn_output=[[[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]]\n",
            "next_layer_addition_dropped_out=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "inputs=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "lnx=[[[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]]\n",
            "attention_lnx=[[[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]]\n",
            "attn_output=[[[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]]\n",
            "next_layer_addition_dropped_out=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "inputs=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "lnx=[[[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]]\n",
            "attention_lnx=[[[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]]\n",
            "attn_output=[[[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]]\n",
            "next_layer_addition_dropped_out=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "inputs=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "lnx=[[[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]]\n",
            "attention_lnx=[[[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]]\n",
            "attn_output=[[[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]]\n",
            "next_layer_addition_dropped_out=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "inputs=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "lnx=[[[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]]\n",
            "attention_lnx=[[[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]]\n",
            "attn_output=[[[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]]\n",
            "next_layer_addition_dropped_out=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "inputs=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "lnx=[[[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]]\n",
            "attention_lnx=[[[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]]\n",
            "attn_output=[[[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]]\n",
            "next_layer_addition_dropped_out=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "inputs=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "lnx=[[[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]]\n",
            "attention_lnx=[[[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]]\n",
            "attn_output=[[[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]]\n",
            "next_layer_addition_dropped_out=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "inputs=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "lnx=[[[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]]\n",
            "attention_lnx=[[[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]]\n",
            "attn_output=[[[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]]\n",
            "next_layer_addition_dropped_out=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "inputs=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "lnx=[[[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]]\n",
            "attention_lnx=[[[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]]\n",
            "attn_output=[[[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]]\n",
            "next_layer_addition_dropped_out=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "inputs=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "lnx=[[[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]]\n",
            "attention_lnx=[[[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]]\n",
            "attn_output=[[[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]]\n",
            "next_layer_addition_dropped_out=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "inputs=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "lnx=[[[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]]\n",
            "attention_lnx=[[[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]]\n",
            "attn_output=[[[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]]\n",
            "next_layer_addition_dropped_out=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "inputs=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "lnx=[[[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]]\n",
            "attention_lnx=[[[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]]\n",
            "attn_output=[[[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]]\n",
            "next_layer_addition_dropped_out=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "inputs=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "lnx=[[[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]]\n",
            "attention_lnx=[[[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]]\n",
            "attn_output=[[[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]]\n",
            "next_layer_addition_dropped_out=[[[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]]\n",
            "inputs=[[[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]]\n",
            "lnx=[[[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]]\n",
            "attention_lnx=[[[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]]\n",
            "attn_output=[[[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]]\n",
            "next_layer_addition_dropped_out=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "inputs=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "lnx=[[[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]]\n",
            "attention_lnx=[[[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]]\n",
            "attn_output=[[[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]]\n",
            "next_layer_addition_dropped_out=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "inputs=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "lnx=[[[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]]\n",
            "attention_lnx=[[[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]]\n",
            "attn_output=[[[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]]\n",
            "next_layer_addition_dropped_out=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "inputs=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "lnx=[[[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]]\n",
            "attention_lnx=[[[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]]\n",
            "attn_output=[[[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]]\n",
            "next_layer_addition_dropped_out=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "inputs=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "lnx=[[[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]]\n",
            "attention_lnx=[[[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]]\n",
            "attn_output=[[[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]]\n",
            "next_layer_addition_dropped_out=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "inputs=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "lnx=[[[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]]\n",
            "attention_lnx=[[[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]]\n",
            "attn_output=[[[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]]\n",
            "next_layer_addition_dropped_out=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "inputs=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "lnx=[[[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]]\n",
            "attention_lnx=[[[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]]\n",
            "attn_output=[[[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]]\n",
            "next_layer_addition_dropped_out=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "inputs=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "lnx=[[[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]]\n",
            "attention_lnx=[[[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]]\n",
            "attn_output=[[[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]]\n",
            "next_layer_addition_dropped_out=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "inputs=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "lnx=[[[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]]\n",
            "attention_lnx=[[[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]]\n",
            "attn_output=[[[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]]\n",
            "next_layer_addition_dropped_out=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "inputs=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "lnx=[[[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]]\n",
            "attention_lnx=[[[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]]\n",
            "attn_output=[[[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]]\n",
            "next_layer_addition_dropped_out=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "inputs=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "lnx=[[[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]]\n",
            "attention_lnx=[[[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]]\n",
            "attn_output=[[[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]]\n",
            "next_layer_addition_dropped_out=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "inputs=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "lnx=[[[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]]\n",
            "attention_lnx=[[[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]]\n",
            "attn_output=[[[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]]\n",
            "next_layer_addition_dropped_out=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "inputs=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "lnx=[[[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]]\n",
            "attention_lnx=[[[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]]\n",
            "attn_output=[[[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]]\n",
            "next_layer_addition_dropped_out=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "inputs=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "lnx=[[[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]]\n",
            "attention_lnx=[[[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]]\n",
            "attn_output=[[[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]]\n",
            "next_layer_addition_dropped_out=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "inputs=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "lnx=[[[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]]\n",
            "attention_lnx=[[[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]]\n",
            "attn_output=[[[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]]\n",
            "next_layer_addition_dropped_out=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "inputs=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "lnx=[[[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]]\n",
            "attention_lnx=[[[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]]\n",
            "attn_output=[[[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]]\n",
            "next_layer_addition_dropped_out=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "inputs=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "lnx=[[[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]]\n",
            "attention_lnx=[[[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]]\n",
            "attn_output=[[[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]]\n",
            "next_layer_addition_dropped_out=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "inputs=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "lnx=[[[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]]\n",
            "attention_lnx=[[[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]]\n",
            "attn_output=[[[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]]\n",
            "next_layer_addition_dropped_out=[[[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]]\n",
            "token_buffers={token_buffers}\n",
            "----------------------\n",
            "Prompt:\n",
            "The color of the sky is \n",
            "\n",
            "Output:\n",
            "The color of the sky is \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Tokens:\n",
            "[     2    651   2881    576    573   8203    603 235248    108    108\n",
            "    108    108    108    108    108    108    108    108    108]\n",
            "----------------------\n",
            "Prompt:\n",
            "Translate this into French:\n",
            "Hello, my name is Morgane.\n",
            "\n",
            "Output:\n",
            "Translate this into French:\n",
            "Hello, my name is Morgane.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Tokens:\n",
            "[     2  49688    736   1280   6987 235292    108   4521 235269    970\n",
            "   1503    603  20189 235249 235265    108    108    108    108    108\n",
            "    108    108    108    108    108    108]\n",
            "----------------------\n",
            "Prompt:\n",
            "Translate this into French:\n",
            "This dish is delicious!\n",
            "\n",
            "Output:\n",
            "Translate this into French:\n",
            "This dish is delicious!\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Tokens:\n",
            "[     2  49688    736   1280   6987 235292    108   1596  15862    603\n",
            "  15855 235341    108    108    108    108    108    108    108    108\n",
            "    108    108    108]\n",
            "----------------------\n",
            "Prompt:\n",
            "Translate this into French:\n",
            "I am a student.\n",
            "\n",
            "Output:\n",
            "Translate this into French:\n",
            "I am a student.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Tokens:\n",
            "[     2  49688    736   1280   6987 235292    108 235285   1144    476\n",
            "   5913 235265    108    108    108    108    108    108    108    108\n",
            "    108    108    108]\n",
            "----------------------\n",
            "Prompt:\n",
            "Translate this into French:\n",
            "How's the weather today?\n",
            "\n",
            "Output:\n",
            "Translate this into French:\n",
            "How's the weather today?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Tokens:\n",
            "[     2  49688    736   1280   6987 235292    108   2299 235303 235256\n",
            "    573   8957   3646 235336    108    108    108    108    108    108\n",
            "    108    108    108    108    108]\n"
          ]
        }
      ],
      "source": [
        "gemma_tokenizer = data_lib.GemmaTokenizer(\n",
        "    os.path.join(kaggle_ckpt_path, \"tokenizer.model\")\n",
        ")\n",
        "\n",
        "from MaxText.input_pipeline import _input_pipeline_utils\n",
        "from MaxText.globals import PKG_DIR\n",
        "\n",
        "# gemma_tokenizer = _input_pipeline_utils.get_tokenizer(\n",
        "#         os.path.join(os.path.dirname(PKG_DIR), \"assets\", \"tokenizer_llama3.tiktoken\"),\n",
        "#         \"tiktoken\",\n",
        "#         add_bos=True,\n",
        "#         add_eos=False,\n",
        "#     )\n",
        "#     )\n",
        "# gemma_tokenizer = data_lib.GemmaTokenizer(\n",
        "# )\n",
        "\n",
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=gemma,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")\n",
        "\n",
        "input_batch = [\n",
        "    \"The color of the sky is \\n\",\n",
        "    \"Translate this into French:\\nHello, my name is Morgane.\\n\",\n",
        "    \"Translate this into French:\\nThis dish is delicious!\\n\",\n",
        "    \"Translate this into French:\\nI am a student.\\n\",\n",
        "    \"Translate this into French:\\nHow's the weather today?\\n\",\n",
        "]\n",
        "\n",
        "out_data = sampler(\n",
        "    input_strings=input_batch,\n",
        "    total_generation_steps=10,  # The number of steps performed when generating a response.\n",
        "    echo=True,  # Whether to echo the input string in the output.\n",
        ")\n",
        "\n",
        "for input_string, out_string, tokens in zip(input_batch, out_data.text, out_data.tokens):\n",
        "  print(f\"----------------------\")\n",
        "  print(f\"Prompt:\\n{input_string}\")\n",
        "  print(f\"Output:\\n{out_string}\")\n",
        "  print(f\"Tokens:\\n{tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'?\\n'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gemma_tokenizer.encode(\"The color\\n\")\n",
        "gemma_tokenizer.decode([235336    , 108 ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "State({\n",
              "  'base': {\n",
              "    'decoder': {\n",
              "      'decoder_norm': {\n",
              "        'scale': Param( # 2,048 (4.1 KB)\n",
              "          value=Array([1, 1, 1, ..., 1, 1, 1], dtype=bfloat16),\n",
              "          mesh=None,\n",
              "          sharding=('norm',),\n",
              "          sharding_rules=None,\n",
              "          linen_meta_type=LogicallyPartitioned\n",
              "        )\n",
              "      },\n",
              "      'layers': {\n",
              "        'mlp': {\n",
              "          'wi_0': {\n",
              "            'kernel': Param( # 603,979,776 (1.2 GB)\n",
              "              value=Array([[[-0.0349121, 0.019165, 0.0178223, ..., -0.00421143, 0.0117188,\n",
              "                       -0.00521851],\n",
              "                      [-0.0375977, 0.0284424, -0.00469971, ..., 0.0314941, -0.017334,\n",
              "                       0],\n",
              "                      [0.0032196, 0.0111694, 0.00750732, ..., 0.0314941, -0.0148315,\n",
              "                       -0.0197754],\n",
              "                      ...,\n",
              "                      [0.00723267, -0.046875, 0.0303955, ..., 0.0441895, 0.00370789,\n",
              "                       -0.0090332],\n",
              "                      [0.00723267, 0.00799561, 0.00521851, ..., -0.00370789,\n",
              "                       0.00671387, -0.000984192],\n",
              "                      [-0.0375977, -0.0324707, -0.000492096, ..., -0.046875,\n",
              "                       0.00854492, 0.00421143]],\n",
              "              \n",
              "                     [[0.0154419, -0.0148315, -0.0375977, ..., 0.00469971, 0.0214844,\n",
              "                       -0.0375977],\n",
              "                      [-0.0090332, -0.0245361, 0.0402832, ..., 0.00271606, -0.012146,\n",
              "                       -0.000492096],\n",
              "                      [0.036377, -0.000984192, -0.02771, ..., 0.0195312, -0.00622559,\n",
              "                       0.0159912],\n",
              "                      ...,\n",
              "                      [0.0117188, -0.0214844, -0.00234985, ..., -0.0197754,\n",
              "                       -0.00765991, 0.0178223],\n",
              "                      [0.000492096, -0.0065918, -0.0238037, ..., 0.036377, 0.0111694,\n",
              "                       0.00622559],\n",
              "                      [0.00958252, -0.00135803, 0.0122681, ..., -0.013855, -0.0197754,\n",
              "                       -0.00811768]],\n",
              "              \n",
              "                     [[0.0117188, 0.0402832, -0.0284424, ..., 0.0245361, -0.0303955,\n",
              "                       0.00570679],\n",
              "                      [-0.0361328, -0.0132446, -0.0294189, ..., -0.0106201,\n",
              "                       0.000492096, -0.0361328],\n",
              "                      [-0.00958252, 0.0230713, -0.0253906, ..., -0.0106201,\n",
              "                       -0.0267334, 0.00671387],\n",
              "                      ...,\n",
              "                      [0.0303955, 0.0238037, 0.0142212, ..., -0.00334167, 0.0388184,\n",
              "                       -0.00370789],\n",
              "                      [0.0270996, 0.00671387, -0.0201416, ..., 0.0128784, -0.0361328,\n",
              "                       0.0101318],\n",
              "                      [-0.00811768, -0.0349121, 0.00421143, ..., 0.00247192,\n",
              "                       0.00958252, 0.00570679]],\n",
              "              \n",
              "                     ...,\n",
              "              \n",
              "                     [[-0.0238037, 0.00854492, 0.00723267, ..., -0.00622559,\n",
              "                       -0.0132446, -0.0253906],\n",
              "                      [-0.019043, -0.0111694, 0.017334, ..., 0.0388184, -0.0201416,\n",
              "                       -0.0294189],\n",
              "                      [-0.0361328, 0.0032196, -0.0336914, ..., -0.00421143, 0.0349121,\n",
              "                       0.0184326],\n",
              "                      ...,\n",
              "                      [0.0375977, 0.00799561, -0.0375977, ..., 0.0441895, 0.0422363,\n",
              "                       0.0274658],\n",
              "                      [-0.00282288, 0.000492096, -0.0223389, ..., 0.0245361,\n",
              "                       0.00671387, -0.0267334],\n",
              "                      [-0.0336914, 0.00469971, 0.0128784, ..., -0.0375977,\n",
              "                       -0.00421143, -0.0349121]],\n",
              "              \n",
              "                     [[-0.020874, -0.0284424, -0.00811768, ..., 0.00196838, 0.0284424,\n",
              "                       -0.00811768],\n",
              "                      [0.00135803, -0.019043, -0.000984192, ..., 0.0214844,\n",
              "                       0.00196838, 0.0303955],\n",
              "                      [0.0032196, 0.0263672, 0.0117188, ..., -0.0500488, 0,\n",
              "                       0.00521851],\n",
              "                      ...,\n",
              "                      [0.00723267, -0.0197754, 0.0131836, ..., -0.00521851,\n",
              "                       -0.0101318, 0.0201416],\n",
              "                      [-0.019043, 0.0223389, 0.0336914, ..., 0.0314941, -0.0253906,\n",
              "                       0.0159912],\n",
              "                      [-0.0230713, -0.0101318, 0.0136719, ..., 0.00135803, -0.0441895,\n",
              "                       -0.00622559]],\n",
              "              \n",
              "                     [[-0.0159912, 0.0136719, -0.0101318, ..., -0.0407715, 0.0167236,\n",
              "                       0.00854492],\n",
              "                      [0.0159912, 0.0128784, 0.0294189, ..., 0.0090332, 0.0195312,\n",
              "                       -0.0132446],\n",
              "                      [0.00469971, 0.0375977, 0.0111694, ..., 0.00196838, 0.0122681,\n",
              "                       -0.00469971],\n",
              "                      ...,\n",
              "                      [-0.000984192, 0.0349121, 0.0131836, ..., 0.0148315, 0.0184326,\n",
              "                       0.0195312],\n",
              "                      [-0.020874, 0.00421143, -0.0336914, ..., 0.0245361, 0.0294189,\n",
              "                       0.0314941],\n",
              "                      [0.00421143, 0.0184326, 0.0230713, ..., 0.000984192, 0.00570679,\n",
              "                       0.046875]]], dtype=bfloat16),\n",
              "              mesh=None,\n",
              "              sharding=('embed', 'layers', 'mlp'),\n",
              "              sharding_rules=None,\n",
              "              linen_meta_type=LogicallyPartitioned\n",
              "            )\n",
              "          },\n",
              "          'wi_1': {\n",
              "            'kernel': Param( # 603,979,776 (1.2 GB)\n",
              "              value=Array([[[0.00799561, -0.0324707, 0.00570679, ..., -0.0388184,\n",
              "                       0.00799561, 0.0106201],\n",
              "                      [0.00521851, -0.0441895, 0.0178223, ..., -0.0238037, 0.0106201,\n",
              "                       0.019165],\n",
              "                      [0.0142212, -0.0258789, 0.0090332, ..., -0.0090332, 0.0184326,\n",
              "                       -0.0324707],\n",
              "                      ...,\n",
              "                      [-0.0253906, -0.0500488, -0.0388184, ..., -0.0375977,\n",
              "                       0.00421143, -0.0336914],\n",
              "                      [-0.0230713, -0.0201416, 0.0154419, ..., -0.0294189, -0.013855,\n",
              "                       -0.00185394],\n",
              "                      [0.00271606, -0.00135803, 0.0274658, ..., -0.0126953,\n",
              "                       0.00469971, 0.00750732]],\n",
              "              \n",
              "                     [[-0.0336914, 0.00271606, 0.00521851, ..., 0.00521851, 0.0270996,\n",
              "                       0.00570679],\n",
              "                      [-0.00866699, -0.00866699, 0.00521851, ..., 0.0349121,\n",
              "                       0.0142212, 0],\n",
              "                      [-0.00622559, 0.019165, 0.019165, ..., 0.00723267, 0.017334,\n",
              "                       0.00196838],\n",
              "                      ...,\n",
              "                      [-0.0090332, 0.00370789, 0.0303955, ..., -0.0267334, 0.00854492,\n",
              "                       0.0159912],\n",
              "                      [0.020874, -0.00622559, -0.017334, ..., 0.0131836, 0.0245361,\n",
              "                       -0.00521851],\n",
              "                      [-0.0253906, -0.0144653, 0, ..., -0.0111694, 0.0274658,\n",
              "                       0.0274658]],\n",
              "              \n",
              "                     [[0.00799561, 0.0131836, -0.0324707, ..., 0.0131836, -0.0407715,\n",
              "                       0.0111694],\n",
              "                      [-0.012146, -0.012146, 0.0238037, ..., 0.046875, -0.0238037,\n",
              "                       -0.0314941],\n",
              "                      [-0.0159912, -0.0126953, -0.0201416, ..., 0.0128784, 0.0223389,\n",
              "                       0.00247192],\n",
              "                      ...,\n",
              "                      [-0.0294189, -0.0441895, -0.0167236, ..., -0.000492096,\n",
              "                       0.0136719, 0.0201416],\n",
              "                      [-0.0201416, -0.00469971, -0.00958252, ..., 0.0154419,\n",
              "                       0.0422363, -0.0375977],\n",
              "                      [-0.0238037, -0.0144653, 0.00370789, ..., -0.0375977, -0.046875,\n",
              "                       -0.00469971]],\n",
              "              \n",
              "                     ...,\n",
              "              \n",
              "                     [[-0.0132446, -0.00334167, -0.00521851, ..., 0.00469971,\n",
              "                       -0.00282288, 0.00196838],\n",
              "                      [-0.0167236, -0.0294189, -0.0294189, ..., 0.0178223, -0.0284424,\n",
              "                       0.0253906],\n",
              "                      [-0.0303955, -0.00958252, -0.0303955, ..., 0.0441895,\n",
              "                       0.00196838, -0.0090332],\n",
              "                      ...,\n",
              "                      [0.00671387, -0.0197754, -0.00135803, ..., 0.0178223, 0.0245361,\n",
              "                       0.020874],\n",
              "                      [-0.0154419, -0.00185394, 0.0245361, ..., 0.0159912, 0.0284424,\n",
              "                       0.0245361],\n",
              "                      [0.0245361, -0.012146, 0.0195312, ..., 0.00135803, 0.0230713,\n",
              "                       -0.000984192]],\n",
              "              \n",
              "                     [[-0.00234985, -0.020874, -0.02771, ..., -0.0178223, -0.00811768,\n",
              "                       0.0274658],\n",
              "                      [-0.00185394, -0.000984192, 0.00370789, ..., -0.0324707,\n",
              "                       -0.0214844, -0.00370789],\n",
              "                      [-0.02771, -0.00622559, 0.0230713, ..., -0.0238037, -0.0388184,\n",
              "                       0.0245361],\n",
              "                      ...,\n",
              "                      [0.0136719, -0.0407715, -0.0407715, ..., -0.00370789,\n",
              "                       -0.00622559, -0.0090332],\n",
              "                      [-0.00765991, -0.0361328, -0.00185394, ..., -0.0154419,\n",
              "                       0.0117188, 0.0117188],\n",
              "                      [0.0101318, 0.00247192, 0.00421143, ..., -0.0117188, 0.0294189,\n",
              "                       -0.0167236]],\n",
              "              \n",
              "                     [[0.0117188, 0.036377, 0, ..., -0.00521851, 0.0375977,\n",
              "                       -0.0427246],\n",
              "                      [0.0184326, -0.0375977, -0.0303955, ..., -0.0407715,\n",
              "                       0.000492096, 0.0032196],\n",
              "                      [0.0422363, -0.00958252, 0.0154419, ..., -0.00135803,\n",
              "                       -0.0230713, 0.0422363],\n",
              "                      ...,\n",
              "                      [0.0388184, -0.00469971, 0.0238037, ..., -0.019043, 0.0375977,\n",
              "                       0.0245361],\n",
              "                      [-0.00421143, -0.00958252, 0.00196838, ..., -0.0407715,\n",
              "                       -0.00135803, -0.0111694],\n",
              "                      [0.0032196, -0.0154419, -0.00135803, ..., -0.0238037,\n",
              "                       -0.0178223, -0.0349121]]], dtype=bfloat16),\n",
              "              mesh=None,\n",
              "              sharding=('embed', 'layers', 'mlp'),\n",
              "              sharding_rules=None,\n",
              "              linen_meta_type=LogicallyPartitioned\n",
              "            )\n",
              "          },\n",
              "          'wo': {\n",
              "            'kernel': Param( # 603,979,776 (1.2 GB)\n",
              "              value=Array([[[0.00415039, 0.015625, 0.00958252, ..., -0.000999451,\n",
              "                       -0.0133057, 0.00338745],\n",
              "                      [0.00415039, -0.00714111, 0, ..., -0.00946045, 0.000173569,\n",
              "                       -0.015625],\n",
              "                      [-0.0108032, 0, -0.010437, ..., 0.00283813, 0.0133667,\n",
              "                       0.00817871],\n",
              "                      ...,\n",
              "                      [-0.00650024, -0.0119629, -0.0133057, ..., 0.00320435,\n",
              "                       -0.0137939, 0.00592041],\n",
              "                      [-0.00219727, -0.00271606, -0.0128174, ..., -0.0128174,\n",
              "                       -0.00118256, -0.00921631],\n",
              "                      [0.00485229, -0.00946045, -0.00674438, ..., -0.00184631,\n",
              "                       -0.000831604, 0.00692749]],\n",
              "              \n",
              "                     [[0.00897217, -0.0137939, -0.00674438, ..., -0.000480652,\n",
              "                       -0.000173569, -0.0108032],\n",
              "                      [0.000694275, 0.00396729, -0.0108032, ..., 0.0114746,\n",
              "                       0.00338745, -0.00524902],\n",
              "                      [-0.0137939, 0.00283813, -0.0111084, ..., -0.00271606,\n",
              "                       -0.00698853, 0.010437],\n",
              "                      ...,\n",
              "                      [-0.0128174, 0.0114746, -0.00567627, ..., -0.00738525,\n",
              "                       0.0111084, -0.00592041],\n",
              "                      [0.00148773, -0.00872803, 0.0067749, ..., -0.0177002,\n",
              "                       -0.00698853, 0.00762939],\n",
              "                      [-0.00166321, -0.00201416, -0.00897217, ..., -0.000999451,\n",
              "                       0.00219727, 0.000347137]],\n",
              "              \n",
              "                     [[-0.00738525, 0.00238037, 0.00320435, ..., -0.00430298, 0,\n",
              "                       -0.00184631],\n",
              "                      [-0.00131226, 0.00184631, -0.00271606, ..., -0.0108032,\n",
              "                       -0.0144653, 0.00933838],\n",
              "                      [-0.015625, -0.00793457, -0.0133057, ..., 0.00113678,\n",
              "                       -0.00286865, 0.00148773],\n",
              "                      ...,\n",
              "                      [-0.00396729, -0.00396729, -0.0108032, ..., 0.00238037,\n",
              "                       -0.00306702, 0.00454712],\n",
              "                      [0.00302124, 0.000347137, 0.00466919, ..., 0.000347137,\n",
              "                       0.00338745, 0.000873566],\n",
              "                      [-0.00148773, -0.0128174, -0.00650024, ..., 0.015625,\n",
              "                       0.00131226, -0.000656128]],\n",
              "              \n",
              "                     ...,\n",
              "              \n",
              "                     [[0.000873566, -0.0151978, -0.00396729, ..., 0.00933838,\n",
              "                       -0.000480652, 0.00793457],\n",
              "                      [-0.00817871, -0.00738525, -0.0025177, ..., -0.00306702,\n",
              "                       -0.00271606, -0.0111084],\n",
              "                      [-0.00396729, -0.00148773, -0.00233459, ..., -0.000999451,\n",
              "                       -0.0144653, -0.0119629],\n",
              "                      ...,\n",
              "                      [-0.0144653, -0.00166321, 0.00267029, ..., -0.00946045,\n",
              "                       -0.00921631, -0.00546265],\n",
              "                      [-0.00491333, -0.0177002, 0.00131226, ..., -0.00118256,\n",
              "                       0.000173569, 0.00592041],\n",
              "                      [-0.00448608, 0.00148773, 0.00466919, ..., 0.0100708,\n",
              "                       -0.00430298, -0.00219727]],\n",
              "              \n",
              "                     [[0.00320435, 0.0050354, -0.00166321, ..., -0.00233459,\n",
              "                       0.00738525, -0.000173569],\n",
              "                      [-0.00491333, -0.00201416, -0.00184631, ..., -0.00762939,\n",
              "                       -0.0114746, 0.0142822],\n",
              "                      [-0.00184631, -0.00946045, 0.00692749, ..., 0.000694275,\n",
              "                       -0.0144653, -0.00650024],\n",
              "                      ...,\n",
              "                      [-0.00131226, -0.00286865, 0.00933838, ..., 0.0149536,\n",
              "                       0.000347137, 0.00485229],\n",
              "                      [0.00320435, 0, -0.00650024, ..., -0.00921631, -0.00219727,\n",
              "                       0.00897217],\n",
              "                      [0.00454712, -0.00491333, 0.00375366, ..., 0.00166321,\n",
              "                       0.00219727, -0.00714111]],\n",
              "              \n",
              "                     [[-0.00592041, 0.0166016, 0.0123901, ..., 0.00436401, 0.00454712,\n",
              "                       0.00338745],\n",
              "                      [0.0128784, 0.0128784, 0.000480652, ..., 0.010437, 0.00113678,\n",
              "                       0.000694275],\n",
              "                      [0.0123901, -0.00219727, -0.00921631, ..., 0.000694275,\n",
              "                       -0.0151978, -0.00415039],\n",
              "                      ...,\n",
              "                      [-0.00469971, 0.0133667, -0.00338745, ..., 0.00762939,\n",
              "                       0.00219727, 0.000873566],\n",
              "                      [0.00396729, 0.00976562, 0.00634766, ..., 0, -0.00430298,\n",
              "                       -0.000656128],\n",
              "                      [0.00466919, 0.0114746, 0.00238037, ..., -0.0100708,\n",
              "                       -0.00271606, -0.000480652]]], dtype=bfloat16),\n",
              "              mesh=None,\n",
              "              sharding=('mlp', 'layers', 'embed'),\n",
              "              sharding_rules=None,\n",
              "              linen_meta_type=LogicallyPartitioned\n",
              "            )\n",
              "          }\n",
              "        },\n",
              "        'pre_ffw_norm': {\n",
              "          'scale': Param( # 36,864 (73.7 KB)\n",
              "            value=Array([[1, 1, 1, ..., 1, 1, 1],\n",
              "                   [1, 1, 1, ..., 1, 1, 1],\n",
              "                   [1, 1, 1, ..., 1, 1, 1],\n",
              "                   ...,\n",
              "                   [1, 1, 1, ..., 1, 1, 1],\n",
              "                   [1, 1, 1, ..., 1, 1, 1],\n",
              "                   [1, 1, 1, ..., 1, 1, 1]], dtype=bfloat16),\n",
              "            mesh=None,\n",
              "            sharding=('norm', 'layers'),\n",
              "            sharding_rules=None,\n",
              "            linen_meta_type=LogicallyPartitioned\n",
              "          )\n",
              "        },\n",
              "        'pre_self_attention_norm': {\n",
              "          'scale': Param( # 36,864 (73.7 KB)\n",
              "            value=Array([[1, 1, 1, ..., 1, 1, 1],\n",
              "                   [1, 1, 1, ..., 1, 1, 1],\n",
              "                   [1, 1, 1, ..., 1, 1, 1],\n",
              "                   ...,\n",
              "                   [1, 1, 1, ..., 1, 1, 1],\n",
              "                   [1, 1, 1, ..., 1, 1, 1],\n",
              "                   [1, 1, 1, ..., 1, 1, 1]], dtype=bfloat16),\n",
              "            mesh=None,\n",
              "            sharding=('norm', 'layers'),\n",
              "            sharding_rules=None,\n",
              "            linen_meta_type=LogicallyPartitioned\n",
              "          )\n",
              "        },\n",
              "        'self_attention': {\n",
              "          'key': {\n",
              "            'kernel': Param( # 9,437,184 (18.9 MB)\n",
              "              value=Array([[[[0.000107765, 0.0317383, 0.0114136, ..., 0.019165, -0.0466309,\n",
              "                        -0.0220947]],\n",
              "              \n",
              "                      [[-0.0116577, 0.0249023, 0.0167236, ..., 0.000972748,\n",
              "                        -0.00292969, 0.032959]],\n",
              "              \n",
              "                      [[-0.0269775, -0.020752, -0.00469971, ..., -0.0383301,\n",
              "                        -0.00970459, 0.0375977]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[-0.0432129, 0.015564, 0.048584, ..., 0.00854492, 0.0203857,\n",
              "                        -0.0116577]],\n",
              "              \n",
              "                      [[-0.0334473, -0.0269775, 0.0203857, ..., 0.00946045,\n",
              "                        0.00576782, -0.00205994]],\n",
              "              \n",
              "                      [[-0.0126953, -0.0405273, 0.010437, ..., 0.019165, 0.00314331,\n",
              "                        -0.0175781]]],\n",
              "              \n",
              "              \n",
              "                     [[[0.032959, -0.0299072, -0.0405273, ..., 0.0341797, 0.0197754,\n",
              "                        -0.0228271]],\n",
              "              \n",
              "                      [[-0.0158691, -0.00738525, -0.0288086, ..., 0.0375977,\n",
              "                        -0.026123, 0.0203857]],\n",
              "              \n",
              "                      [[0.00534058, -0.0349121, -0.0234375, ..., -0.0142212,\n",
              "                        -0.0251465, 0.0444336]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.0292969, 0.048584, -0.027832, ..., -0.0349121, -0.00970459,\n",
              "                        -0.00830078]],\n",
              "              \n",
              "                      [[-0.00601196, 0.00854492, 0.0255127, ..., 0.017334, 0.0240479,\n",
              "                        0.0395508]],\n",
              "              \n",
              "                      [[-0.0111084, -0.00830078, -0.0220947, ..., -0.0181885,\n",
              "                        0.0249023, 0.00576782]]],\n",
              "              \n",
              "              \n",
              "                     [[[0.0249023, -0.0432129, -0.0214844, ..., 0.0167236, 0.032959,\n",
              "                        0.00759888]],\n",
              "              \n",
              "                      [[-0.0106812, -0.0639648, 0.00402832, ..., -0.0106812,\n",
              "                        -0.000759125, -0.024292]],\n",
              "              \n",
              "                      [[-0.0169678, 0.0129395, 0.0358887, ..., -0.0515137, 0.0129395,\n",
              "                        -0.0078125]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[-0.0164795, -0.00646973, -0.0234375, ..., -0.0142212,\n",
              "                        0.0071106, 0.0071106]],\n",
              "              \n",
              "                      [[0.0556641, -0.0288086, 0.0240479, ..., -0.000324249,\n",
              "                        0.0556641, -0.0299072]],\n",
              "              \n",
              "                      [[0.019165, 0.0071106, 0.00183105, ..., -0.0126953, 0.019165,\n",
              "                        0.010437]]],\n",
              "              \n",
              "              \n",
              "                     ...,\n",
              "              \n",
              "              \n",
              "                     [[[0.0375977, 0.00358582, -0.0383301, ..., 0.0230713,\n",
              "                        -0.0214844, 0.0129395]],\n",
              "              \n",
              "                      [[0.0124512, -0.00379944, -0.0383301, ..., 0.0240479,\n",
              "                        -0.0142212, 0.0255127]],\n",
              "              \n",
              "                      [[-0.0334473, -0.0101929, -0.00738525, ..., -0.0349121,\n",
              "                        -0.000759125, 0.032959]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.0444336, 0.0283203, -0.0432129, ..., -0.012146, 0.00140381,\n",
              "                        -0.0220947]],\n",
              "              \n",
              "                      [[-0.0228271, 0.0395508, 0.0145264, ..., -0.00738525,\n",
              "                        -0.0116577, 0.00759888]],\n",
              "              \n",
              "                      [[0.0203857, -0.0106812, 0.0230713, ..., -0.0111084,\n",
              "                        0.00491333, 0.0230713]]],\n",
              "              \n",
              "              \n",
              "                     [[[0.00946045, -0.00205994, 0.0119019, ..., 0.00759888,\n",
              "                        0.019165, -0.0147705]],\n",
              "              \n",
              "                      [[-0.0111084, -0.00646973, -0.0078125, ..., -0.00337219,\n",
              "                        -0.0405273, 0.0240479]],\n",
              "              \n",
              "                      [[0.0179443, -0.00469971, -0.0158691, ..., -0.0349121,\n",
              "                        0.0556641, 0.010437]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.0317383, 0.0230713, -0.0175781, ..., -0.0116577, 0.0119019,\n",
              "                        -0.0181885]],\n",
              "              \n",
              "                      [[-0.0164795, 0.00314331, 0.0139771, ..., 0.0556641,\n",
              "                        -0.0175781, 0.000107765]],\n",
              "              \n",
              "                      [[0.0292969, 0.0124512, -0.00878906, ..., -0.0187988,\n",
              "                        -0.0234375, 0.00271606]]],\n",
              "              \n",
              "              \n",
              "                     [[[-0.0466309, 0.0124512, 0.0249023, ..., -0.0269775,\n",
              "                        -0.00379944, 0.0071106]],\n",
              "              \n",
              "                      [[0.0305176, -0.00921631, 0.000972748, ..., 0.0197754,\n",
              "                        0.0129395, 0.00227356]],\n",
              "              \n",
              "                      [[-0.0220947, 0.0375977, -0.00921631, ..., 0.0444336,\n",
              "                        -0.0366211, 0.00227356]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.00140381, -0.0078125, -0.00970459, ..., 0.0444336,\n",
              "                        -0.0152588, -0.0158691]],\n",
              "              \n",
              "                      [[-0.0269775, 0.000541687, -0.00248718, ..., -0.00424194,\n",
              "                        0.0224609, 0.0217285]],\n",
              "              \n",
              "                      [[-0.00248718, 0.0444336, -0.0251465, ..., 0.010437,\n",
              "                        -0.00119019, 0.00227356]]]], dtype=bfloat16),\n",
              "              mesh=None,\n",
              "              sharding=('embed', 'layers', 'kv_heads', 'kv_head_dim'),\n",
              "              sharding_rules=None,\n",
              "              linen_meta_type=LogicallyPartitioned\n",
              "            )\n",
              "          },\n",
              "          'out': {\n",
              "            'kernel': Param( # 75,497,472 (151.0 MB)\n",
              "              value=Array([[[[0.0114136, 0.0129395, 0.00854492, ..., 0.00759888,\n",
              "                        -0.0142212, 0.010437],\n",
              "                       [-0.0334473, -0.0288086, 0.019165, ..., -0.00970459, 0.032959,\n",
              "                        0.0161133],\n",
              "                       [0.00491333, 0.0071106, -0.0187988, ..., 0.0185547,\n",
              "                        -0.0311279, 0.017334],\n",
              "                       ...,\n",
              "                       [0.000972748, -0.0194092, 0.019165, ..., 0.048584, -0.0152588,\n",
              "                        -0.0169678],\n",
              "                       [-0.0269775, 0.00854492, 0.0145264, ..., -0.0169678,\n",
              "                        -0.0269775, -0.00738525],\n",
              "                       [0.0139771, -0.0111084, 0.0556641, ..., -0.020752, 0.00854492,\n",
              "                        0.00271606]],\n",
              "              \n",
              "                      [[-0.0251465, -0.0142212, -0.027832, ..., -0.0142212,\n",
              "                        -0.00921631, 0.0224609],\n",
              "                       [0.00140381, -0.00689697, -0.0201416, ..., -0.0106812,\n",
              "                        0.0292969, -0.0311279],\n",
              "                       [-0.00921631, 0.017334, 0.0283203, ..., -0.00738525,\n",
              "                        -0.0194092, 0.00314331],\n",
              "                       ...,\n",
              "                       [-0.0194092, -0.00337219, -0.0299072, ..., 0.048584,\n",
              "                        -0.024292, 0.00674438],\n",
              "                       [0.0185547, -0.012146, -0.00469971, ..., -0.00512695,\n",
              "                        -0.0169678, 0.0217285],\n",
              "                       [-0.020752, -0.0334473, 0.0161133, ..., 0.0197754, 0.0240479,\n",
              "                        -0.00292969]],\n",
              "              \n",
              "                      [[0.0224609, 0.0263672, 0.0358887, ..., -0.00248718, 0.041748,\n",
              "                        -0.0175781],\n",
              "                       [-0.00970459, 0.0109253, 0.00805664, ..., -0.0228271,\n",
              "                        0.0185547, -0.00646973],\n",
              "                       [0.010437, -0.012146, 0.0179443, ..., -0.0288086, -0.0405273,\n",
              "                        0.0129395],\n",
              "                       ...,\n",
              "                       [-0.0288086, 0.000541687, 0.00491333, ..., -0.026123,\n",
              "                        0.0305176, -0.0322266],\n",
              "                       [0.0556641, -0.0234375, 0.0071106, ..., -0.0515137,\n",
              "                        -0.00512695, 0.00271606],\n",
              "                       [-0.0639648, -0.0142212, -0.0142212, ..., -0.0201416,\n",
              "                        -0.0131836, -0.00689697]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.00227356, 0.00445557, 0.00227356, ..., -0.0164795,\n",
              "                        0.0150146, 0.017334],\n",
              "                       [0.0230713, 0.0255127, 0.0119019, ..., 0.0240479, 0.00271606,\n",
              "                        0.00674438],\n",
              "                       [0.000107765, -0.0201416, 0.0263672, ..., 0.0341797,\n",
              "                        -0.00601196, 0.000541687],\n",
              "                       ...,\n",
              "                       [-0.0269775, -0.0111084, 0.0263672, ..., -0.00119019,\n",
              "                        -0.00379944, 0.0255127],\n",
              "                       [0.000107765, 0.0375977, -0.0131836, ..., -0.0111084,\n",
              "                        0.0167236, -0.0126953],\n",
              "                       [-0.00248718, -0.0405273, 0.000541687, ..., 0.0292969,\n",
              "                        0.017334, 0.00140381]],\n",
              "              \n",
              "                      [[-0.00205994, -0.0201416, 0.0556641, ..., -0.00689697,\n",
              "                        -0.0078125, -0.00292969],\n",
              "                       [-0.00689697, 0.0119019, -0.0466309, ..., -0.00689697,\n",
              "                        0.0317383, -0.0131836],\n",
              "                       [-0.020752, 0.0358887, -0.0142212, ..., 0.000107765,\n",
              "                        0.0292969, 0.041748],\n",
              "                       ...,\n",
              "                       [0.00805664, -0.0288086, -0.0169678, ..., -0.0147705,\n",
              "                        -0.0187988, 0.032959],\n",
              "                       [0.0090332, 0.0341797, 0.032959, ..., 0.048584, -0.024292,\n",
              "                        0.00534058],\n",
              "                       [-0.0152588, 0.0167236, -0.0366211, ..., -0.0214844,\n",
              "                        -0.00921631, -0.0639648]],\n",
              "              \n",
              "                      [[0.00314331, -0.0164795, -0.0169678, ..., -0.0152588,\n",
              "                        0.00402832, -0.00921631],\n",
              "                       [-0.00830078, -0.0101929, 0.000107765, ..., -0.00878906,\n",
              "                        -0.0288086, -0.00469971],\n",
              "                       [0.0119019, 0.0240479, 0.00854492, ..., -0.0116577,\n",
              "                        0.00491333, 0.0292969],\n",
              "                       ...,\n",
              "                       [-0.026123, 0.000972748, -0.0147705, ..., -0.0251465,\n",
              "                        -0.0169678, -0.0311279],\n",
              "                       [-0.024292, -0.0349121, 0.00805664, ..., -0.0164795,\n",
              "                        -0.0101929, 0.00183105],\n",
              "                       [-0.0515137, -0.00646973, -0.0220947, ..., -0.0175781,\n",
              "                        -0.0322266, 0.00491333]]],\n",
              "              \n",
              "              \n",
              "                     [[[0.00534058, 0.00994873, 0.000107765, ..., -0.012146,\n",
              "                        -0.0214844, -0.0201416],\n",
              "                       [-0.00379944, 0.0062561, 0.0129395, ..., -0.0366211,\n",
              "                        0.00674438, -0.0515137],\n",
              "                       [-0.020752, -0.0055542, -0.0181885, ..., -0.0078125,\n",
              "                        0.00576782, -0.0126953],\n",
              "                       ...,\n",
              "                       [-0.012146, -0.00337219, 0.00183105, ..., -0.00878906,\n",
              "                        -0.0116577, 0.00946045],\n",
              "                       [-0.0322266, -0.00424194, -0.0169678, ..., 0.0341797,\n",
              "                        -0.000324249, 0.0240479],\n",
              "                       [0.0129395, -0.0515137, 0.032959, ..., 0.00759888, 0.0305176,\n",
              "                        0.00271606]],\n",
              "              \n",
              "                      [[0.0090332, 0.00314331, -0.0432129, ..., -0.020752,\n",
              "                        0.00314331, -0.0515137],\n",
              "                       [0.0129395, -0.0299072, 0.0341797, ..., -0.000759125,\n",
              "                        0.0305176, 0.00674438],\n",
              "                       [-0.0131836, -0.0234375, 0.0145264, ..., -0.0234375,\n",
              "                        0.00491333, -0.000324249],\n",
              "                       ...,\n",
              "                       [0.00805664, -0.0269775, -0.027832, ..., 0.0283203,\n",
              "                        -0.00921631, -0.00292969],\n",
              "                       [-0.0311279, 0.0129395, 0.0444336, ..., -0.0175781,\n",
              "                        -0.00248718, 0.00759888],\n",
              "                       [-0.00379944, -0.00878906, -0.0515137, ..., 0.000541687,\n",
              "                        -0.0078125, 0.00805664]],\n",
              "              \n",
              "                      [[-0.0126953, 0.00805664, 0.010437, ..., -0.0366211,\n",
              "                        -0.0116577, -0.00512695],\n",
              "                       [0.0211182, -0.0106812, -0.00738525, ..., -0.0405273,\n",
              "                        -0.00878906, -0.0383301],\n",
              "                       [0.00183105, -0.0234375, 0.0255127, ..., 0.000972748,\n",
              "                        0.0341797, 0.00946045],\n",
              "                       ...,\n",
              "                       [0.0119019, 0.00759888, 0.0230713, ..., -0.0288086,\n",
              "                        0.00358582, 0.0249023],\n",
              "                       [-0.00292969, -0.0269775, 0.0145264, ..., 0.0203857,\n",
              "                        0.00854492, -0.0147705],\n",
              "                       [-0.0169678, 0.032959, -0.0078125, ..., 0.00227356,\n",
              "                        -0.0322266, -0.0106812]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[-0.0269775, 0.0114136, -0.00601196, ..., 0.00576782,\n",
              "                        -0.0111084, 0.048584],\n",
              "                       [-0.00424194, -0.00738525, 0.0150146, ..., 0.0090332,\n",
              "                        -0.0152588, 0.0203857],\n",
              "                       [0.0062561, 0.00534058, -0.012146, ..., 0.00854492,\n",
              "                        -0.000759125, 0.0305176],\n",
              "                       ...,\n",
              "                       [0.00227356, 0.0556641, 0.0124512, ..., 0.0119019,\n",
              "                        -0.00119019, 0.0197754],\n",
              "                       [0.0145264, -0.0288086, 0.00358582, ..., 0.0139771,\n",
              "                        -0.0311279, -0.0152588],\n",
              "                       [0.00491333, -0.0169678, 0.0305176, ..., -0.00337219,\n",
              "                        0.010437, -0.00469971]],\n",
              "              \n",
              "                      [[-0.00689697, -0.00292969, -0.0220947, ..., -0.020752,\n",
              "                        -0.00689697, -0.00469971],\n",
              "                       [0.0375977, -0.0116577, -0.0152588, ..., 0.017334, 0.00759888,\n",
              "                        -0.0106812],\n",
              "                       [-0.0366211, 0.0150146, -0.0234375, ..., 0.0341797, 0.017334,\n",
              "                        0.0203857],\n",
              "                       ...,\n",
              "                       [-0.0116577, -0.0101929, -0.0366211, ..., 0.0124512,\n",
              "                        0.00491333, 0.0240479],\n",
              "                       [-0.0228271, 0.0217285, -0.0101929, ..., 0.0249023,\n",
              "                        -0.0366211, -0.000759125],\n",
              "                       [-0.00878906, 0.00854492, 0.0134888, ..., -0.00601196,\n",
              "                        -0.00878906, 0.000972748]],\n",
              "              \n",
              "                      [[0.0114136, 0.000107765, 0.00358582, ..., -0.0181885,\n",
              "                        0.041748, -0.0142212],\n",
              "                       [0.00445557, -0.027832, 0.0185547, ..., 0.0185547,\n",
              "                        -0.000324249, 0.0556641],\n",
              "                       [0.0185547, 0.00534058, 0.015564, ..., 0.048584, -0.0137329,\n",
              "                        0.0139771],\n",
              "                       ...,\n",
              "                       [-0.0111084, -0.00424194, 0.00183105, ..., 0.0185547,\n",
              "                        0.0134888, -0.0405273],\n",
              "                       [0.0395508, 0.00445557, -0.0194092, ..., -0.0214844,\n",
              "                        0.00358582, 0.00994873],\n",
              "                       [0.0161133, 0.0375977, -0.0137329, ..., -0.0175781,\n",
              "                        0.00140381, 0.0161133]]],\n",
              "              \n",
              "              \n",
              "                     [[[0.0249023, 0.000541687, -0.00878906, ..., -0.0164795,\n",
              "                        0.017334, -0.0175781],\n",
              "                       [-0.0251465, 0.0249023, 0.00183105, ..., 0.00227356,\n",
              "                        -0.024292, 0.0341797],\n",
              "                       [0.0062561, 0.0114136, 0.0071106, ..., -0.0383301, 0.00271606,\n",
              "                        0.0317383],\n",
              "                       ...,\n",
              "                       [-0.0366211, 0.0145264, -0.0366211, ..., 0.00183105,\n",
              "                        0.0249023, -0.0639648],\n",
              "                       [0.00576782, 0.0305176, 0.000541687, ..., 0.00491333,\n",
              "                        0.048584, -0.00469971],\n",
              "                       [0.0444336, 0.0444336, 0.00946045, ..., -0.0432129, 0.0119019,\n",
              "                        -0.00337219]],\n",
              "              \n",
              "                      [[-0.027832, -0.0405273, 0.0109253, ..., -0.0116577, 0.0134888,\n",
              "                        0.0139771],\n",
              "                       [0.0129395, -0.0116577, 0.0395508, ..., 0.0283203,\n",
              "                        0.000972748, 0.0305176],\n",
              "                       [-0.0158691, 0.0197754, 0.048584, ..., 0.0129395, -0.0220947,\n",
              "                        0.015564],\n",
              "                       ...,\n",
              "                       [0.00271606, -0.0311279, 0.0283203, ..., -0.00970459,\n",
              "                        -0.00163269, -0.0101929],\n",
              "                       [-0.0405273, -0.00830078, -0.0106812, ..., -0.00292969,\n",
              "                        -0.00292969, 0.000972748],\n",
              "                       [0.0341797, -0.0234375, 0.0395508, ..., -0.0432129, 0.048584,\n",
              "                        -0.0228271]],\n",
              "              \n",
              "                      [[-0.0137329, -0.0515137, -0.00292969, ..., -0.0194092,\n",
              "                        -0.000759125, 0.00946045],\n",
              "                       [-0.0181885, -0.0432129, 0.000541687, ..., 0.0139771,\n",
              "                        0.0109253, -0.00424194],\n",
              "                       [-0.0432129, -0.0234375, 0.000107765, ..., -0.00337219,\n",
              "                        0.00534058, 0.017334],\n",
              "                       ...,\n",
              "                       [0.00576782, 0.0255127, 0.0341797, ..., 0.0161133, 0.032959,\n",
              "                        0.0375977],\n",
              "                       [0.0145264, 0.015564, -0.0515137, ..., 0.019165, -0.00424194,\n",
              "                        -0.00119019],\n",
              "                       [0.048584, -0.027832, -0.0322266, ..., 0.041748, -0.0164795,\n",
              "                        0.0114136]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.00946045, -0.0214844, -0.0111084, ..., -0.0164795,\n",
              "                        -0.0349121, 0.0274658],\n",
              "                       [-0.000324249, -0.0055542, 0.0090332, ..., 0.010437,\n",
              "                        0.0090332, 0.00805664],\n",
              "                       [0.0358887, 0.0230713, 0.0185547, ..., -0.000324249,\n",
              "                        0.00759888, -0.00337219],\n",
              "                       ...,\n",
              "                       [-0.00512695, 0.0341797, 0.00805664, ..., 0.000972748,\n",
              "                        0.0217285, -0.0194092],\n",
              "                       [-0.0078125, -0.0220947, -0.0299072, ..., 0.0062561,\n",
              "                        0.0211182, -0.0228271],\n",
              "                       [0.0230713, -0.00646973, -0.0334473, ..., -0.00205994,\n",
              "                        -0.0466309, 0.0129395]],\n",
              "              \n",
              "                      [[0.041748, 0.00227356, 0.00140381, ..., -0.027832, -0.0078125,\n",
              "                        -0.00205994],\n",
              "                       [0.00854492, -0.0181885, -0.0234375, ..., 0.0274658,\n",
              "                        -0.00205994, -0.0169678],\n",
              "                       [0.0185547, 0.0109253, -0.0288086, ..., -0.0234375,\n",
              "                        0.000541687, 0.00759888],\n",
              "                       ...,\n",
              "                       [0.0255127, -0.0432129, -0.012146, ..., -0.0194092, -0.020752,\n",
              "                        0.00759888],\n",
              "                       [0.0217285, 0.0317383, 0.00534058, ..., -0.0055542,\n",
              "                        0.00445557, 0.0167236],\n",
              "                       [0.0109253, -0.0405273, 0.017334, ..., -0.0466309, -0.0334473,\n",
              "                        0.0119019]],\n",
              "              \n",
              "                      [[-0.00970459, 0.0317383, 0.0119019, ..., 0.017334, -0.0158691,\n",
              "                        0.00759888],\n",
              "                       [0.0185547, -0.0220947, -0.0126953, ..., -0.0383301,\n",
              "                        0.0395508, -0.00646973],\n",
              "                       [0.00534058, -0.0288086, -0.026123, ..., -0.026123, 0.0249023,\n",
              "                        0.032959],\n",
              "                       ...,\n",
              "                       [-0.00379944, 0.0375977, -0.0152588, ..., 0.032959, 0.0358887,\n",
              "                        -0.00689697],\n",
              "                       [-0.00970459, -0.0366211, 0.0341797, ..., 0.00271606,\n",
              "                        0.032959, -0.00379944],\n",
              "                       [-0.0126953, 0.000107765, 0.00491333, ..., -0.000759125,\n",
              "                        -0.027832, 0.0203857]]],\n",
              "              \n",
              "              \n",
              "                     ...,\n",
              "              \n",
              "              \n",
              "                     [[[0.0556641, -0.0220947, 0.00759888, ..., -0.0383301,\n",
              "                        -0.00878906, -0.00248718],\n",
              "                       [0.0556641, 0.0197754, -0.0322266, ..., -0.00163269,\n",
              "                        0.0274658, -0.0466309],\n",
              "                       [0.0283203, -0.00646973, -0.026123, ..., -0.024292, 0.0139771,\n",
              "                        -0.0175781],\n",
              "                       ...,\n",
              "                       [-0.0432129, -0.027832, -0.00921631, ..., -0.00512695,\n",
              "                        -0.00119019, 0.0283203],\n",
              "                       [0.00491333, -0.00337219, 0.0292969, ..., -0.0311279,\n",
              "                        0.00314331, 0.00445557],\n",
              "                       [-0.00248718, -0.00292969, -0.0220947, ..., 0.00183105,\n",
              "                        0.00445557, 0.00854492]],\n",
              "              \n",
              "                      [[-0.0515137, -0.0366211, 0.0114136, ..., -0.0078125, 0.010437,\n",
              "                        0.00946045],\n",
              "                       [-0.0126953, -0.0101929, 0.0556641, ..., -0.00292969,\n",
              "                        0.010437, -0.00689697],\n",
              "                       [-0.0187988, -0.00601196, -0.00601196, ..., 0.0211182,\n",
              "                        0.0167236, -0.0366211],\n",
              "                       ...,\n",
              "                       [-0.0131836, -0.0366211, -0.0181885, ..., -0.0175781,\n",
              "                        0.00759888, -0.0126953],\n",
              "                       [-0.00337219, -0.00379944, 0.0249023, ..., 0.0161133,\n",
              "                        0.0062561, 0.00994873],\n",
              "                       [-0.0269775, -0.00830078, 0.0230713, ..., -0.0078125,\n",
              "                        -0.00379944, -0.0055542]],\n",
              "              \n",
              "                      [[-0.0131836, -0.0432129, -0.0078125, ..., -0.00601196,\n",
              "                        -0.0201416, 0.0150146],\n",
              "                       [0.017334, 0.0358887, -0.0515137, ..., 0.00674438, -0.0106812,\n",
              "                        0.048584],\n",
              "                       [-0.0466309, 0.00140381, -0.0131836, ..., -0.0152588,\n",
              "                        0.048584, -0.0311279],\n",
              "                       ...,\n",
              "                       [0.00140381, -0.00379944, -0.0639648, ..., 0.000541687,\n",
              "                        -0.0106812, 0.00491333],\n",
              "                       [0.0071106, -0.00689697, -0.00163269, ..., -0.0405273,\n",
              "                        -0.00163269, -0.000759125],\n",
              "                       [0.0109253, 0.0185547, 0.032959, ..., 0.0185547, 0.000972748,\n",
              "                        0.0375977]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.0139771, -0.0078125, -0.0334473, ..., 0.000541687,\n",
              "                        0.0062561, 0.019165],\n",
              "                       [-0.0299072, 0.00674438, -0.0078125, ..., 0.048584, 0.017334,\n",
              "                        0.00994873],\n",
              "                       [0.0119019, -0.0126953, 0.0211182, ..., 0.017334, -0.0142212,\n",
              "                        -0.0220947],\n",
              "                       ...,\n",
              "                       [0.0375977, -0.00248718, -0.00921631, ..., 0.000972748,\n",
              "                        -0.0405273, 0.000107765],\n",
              "                       [0.0263672, -0.0106812, 0.015564, ..., -0.0131836, 0.0167236,\n",
              "                        0.0230713],\n",
              "                       [-0.0194092, 0.00314331, 0.0185547, ..., 0.0161133,\n",
              "                        0.00445557, 0.0139771]],\n",
              "              \n",
              "                      [[0.00445557, -0.00337219, -0.0158691, ..., 0.00576782,\n",
              "                        -0.0234375, -0.0349121],\n",
              "                       [-0.0101929, -0.00337219, 0.019165, ..., -0.0251465,\n",
              "                        0.0134888, -0.0432129],\n",
              "                       [0.0197754, 0.048584, 0.0119019, ..., -0.00248718, 0.00183105,\n",
              "                        0.0211182],\n",
              "                       ...,\n",
              "                       [0.0129395, 0.0071106, 0.0197754, ..., 0.0556641, 0.0114136,\n",
              "                        -0.0126953],\n",
              "                       [0.00183105, -0.00512695, -0.026123, ..., 0.000541687,\n",
              "                        0.0124512, 0.0109253],\n",
              "                       [-0.00512695, -0.00970459, 0.0129395, ..., 0.00534058,\n",
              "                        -0.00337219, -0.0164795]],\n",
              "              \n",
              "                      [[-0.0101929, 0.00994873, -0.0383301, ..., 0.0134888,\n",
              "                        -0.0116577, 0.000972748],\n",
              "                       [0.00674438, 0.0139771, -0.0269775, ..., -0.00738525,\n",
              "                        -0.00646973, -0.0187988],\n",
              "                       [-0.0158691, 0.0444336, 0.0224609, ..., 0.00140381,\n",
              "                        -0.0175781, -0.0288086],\n",
              "                       ...,\n",
              "                       [0.0114136, -0.0214844, 0.0217285, ..., 0.017334, 0.00271606,\n",
              "                        -0.00646973],\n",
              "                       [-0.0366211, 0.0395508, 0.0255127, ..., 0.00491333,\n",
              "                        -0.0158691, -0.00646973],\n",
              "                       [0.0230713, -0.0288086, 0.0185547, ..., 0.00854492,\n",
              "                        -0.00205994, -0.0639648]]],\n",
              "              \n",
              "              \n",
              "                     [[[0.0395508, 0.00445557, -0.0055542, ..., -0.00337219,\n",
              "                        -0.0126953, -0.0152588],\n",
              "                       [-0.0299072, -0.00163269, 0.0358887, ..., 0.0255127,\n",
              "                        0.00183105, 0.00445557],\n",
              "                       [0.000541687, -0.0299072, 0.00759888, ..., -0.0334473,\n",
              "                        -0.0269775, -0.00646973],\n",
              "                       ...,\n",
              "                       [-0.00205994, 0.0145264, 0.0292969, ..., -0.0322266,\n",
              "                        -0.00424194, -0.00830078],\n",
              "                       [0.0224609, 0.0129395, -0.0228271, ..., 0.00994873,\n",
              "                        0.00314331, -0.0349121],\n",
              "                       [0.0375977, 0.0161133, 0.0179443, ..., -0.0383301, -0.0169678,\n",
              "                        -0.0137329]],\n",
              "              \n",
              "                      [[-0.00646973, 0.0090332, -0.0142212, ..., 0.00227356,\n",
              "                        -0.0639648, -0.027832],\n",
              "                       [-0.027832, -0.00205994, 0.0274658, ..., -0.0164795,\n",
              "                        -0.0299072, -0.0405273],\n",
              "                       [0.00534058, 0.00358582, 0.0119019, ..., -0.0078125,\n",
              "                        -0.00512695, -0.0322266],\n",
              "                       ...,\n",
              "                       [0.0114136, 0.0150146, 0.0317383, ..., -0.00469971, 0.0283203,\n",
              "                        -0.0131836],\n",
              "                       [-0.0111084, 0.0109253, -0.00646973, ..., 0.0230713, 0.041748,\n",
              "                        0.000107765],\n",
              "                       [0.00358582, -0.00337219, 0.0317383, ..., 0.00227356,\n",
              "                        0.0305176, 0.015564]],\n",
              "              \n",
              "                      [[0.032959, -0.0126953, -0.00119019, ..., -0.00512695,\n",
              "                        -0.00119019, 0.0119019],\n",
              "                       [0.0071106, -0.027832, -0.020752, ..., -0.0131836,\n",
              "                        -0.00248718, 0.017334],\n",
              "                       [-0.00205994, 0.00674438, -0.00646973, ..., 0.019165,\n",
              "                        -0.00512695, -0.0432129],\n",
              "                       ...,\n",
              "                       [-0.0116577, -0.0126953, -0.00205994, ..., -0.0466309,\n",
              "                        0.019165, 0.00994873],\n",
              "                       [0.0317383, -0.0515137, -0.0137329, ..., -0.0175781,\n",
              "                        0.0139771, 0.00271606],\n",
              "                       [0.0129395, 0.015564, -0.0101929, ..., 0.00759888,\n",
              "                        -0.00163269, -0.0131836]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.0341797, -0.00248718, -0.00738525, ..., 0.00183105,\n",
              "                        -0.0466309, -0.0201416],\n",
              "                       [0.017334, -0.024292, 0.048584, ..., -0.027832, 0.0230713,\n",
              "                        -0.0137329],\n",
              "                       [0.00271606, -0.00248718, 0.00994873, ..., 0.0179443,\n",
              "                        0.00491333, -0.0126953],\n",
              "                       ...,\n",
              "                       [-0.0101929, 0.0224609, -0.00292969, ..., -0.00738525,\n",
              "                        -0.0269775, -0.00379944],\n",
              "                       [0.00227356, 0.0145264, 0.00445557, ..., 0.0283203,\n",
              "                        0.00227356, 0.00674438],\n",
              "                       [0.0224609, 0.0240479, 0.00854492, ..., 0.0179443, -0.0137329,\n",
              "                        0.00358582]],\n",
              "              \n",
              "                      [[0.0217285, -0.00379944, -0.00469971, ..., 0.0203857,\n",
              "                        0.0114136, 0.010437],\n",
              "                       [-0.00830078, -0.0142212, 0.041748, ..., 0.0305176,\n",
              "                        -0.0055542, -0.0288086],\n",
              "                       [0.0197754, 0.0134888, -0.0220947, ..., 0.0556641, 0.015564,\n",
              "                        0.017334],\n",
              "                       ...,\n",
              "                       [-0.027832, 0.00140381, 0.00227356, ..., 0.0179443,\n",
              "                        0.00314331, -0.00970459],\n",
              "                       [-0.00921631, -0.0349121, -0.00163269, ..., -0.00738525,\n",
              "                        -0.0322266, -0.0126953],\n",
              "                       [0.00402832, 0.0203857, -0.0515137, ..., 0.0139771, 0.0217285,\n",
              "                        -0.00379944]],\n",
              "              \n",
              "                      [[0.00227356, 0.0224609, -0.00921631, ..., 0.0217285,\n",
              "                        0.0358887, -0.0175781],\n",
              "                       [-0.0106812, 0.0124512, -0.0311279, ..., 0.0139771,\n",
              "                        -0.00163269, -0.0187988],\n",
              "                       [0.032959, 0.019165, 0.0240479, ..., 0.0341797, -0.00970459,\n",
              "                        0.0274658],\n",
              "                       ...,\n",
              "                       [0.0358887, 0.0255127, -0.0164795, ..., 0.015564, 0.0263672,\n",
              "                        0.019165],\n",
              "                       [0.000107765, -0.00878906, -0.0334473, ..., 0.0167236,\n",
              "                        0.00183105, 0.0134888],\n",
              "                       [-0.0078125, 0.00402832, 0.0114136, ..., -0.0299072,\n",
              "                        0.0263672, -0.0078125]]],\n",
              "              \n",
              "              \n",
              "                     [[[0.0129395, -0.000324249, 0.00445557, ..., 0.0556641,\n",
              "                        -0.00646973, -0.00878906],\n",
              "                       [0.015564, -0.0147705, 0.0224609, ..., 0.0071106, -0.012146,\n",
              "                        -0.000759125],\n",
              "                       [-0.0078125, 0.00994873, 0.0274658, ..., 0.00358582,\n",
              "                        0.0224609, -0.00119019],\n",
              "                       ...,\n",
              "                       [0.0249023, 0.00674438, 0.00491333, ..., 0.00314331,\n",
              "                        -0.0131836, -0.00163269],\n",
              "                       [-0.00921631, -0.00512695, 0.00183105, ..., 0.0109253,\n",
              "                        -0.0515137, 0.0203857],\n",
              "                       [-0.00248718, 0.0292969, 0.0341797, ..., -0.0055542,\n",
              "                        0.0071106, -0.00163269]],\n",
              "              \n",
              "                      [[-0.00292969, -0.0432129, -0.0101929, ..., -0.0311279,\n",
              "                        -0.00424194, 0.000107765],\n",
              "                       [-0.0111084, 0.0119019, 0.0240479, ..., 0.017334, 0.00402832,\n",
              "                        -0.0234375],\n",
              "                       [-0.0111084, -0.0078125, -0.024292, ..., -0.0055542,\n",
              "                        -0.0147705, -0.00689697],\n",
              "                       ...,\n",
              "                       [0.0134888, -0.00646973, -0.000759125, ..., -0.0214844,\n",
              "                        -0.0101929, -0.0228271],\n",
              "                       [-0.0078125, -0.024292, 0.041748, ..., 0.0444336, -0.024292,\n",
              "                        0.00140381],\n",
              "                       [0.032959, 0.0444336, 0.0358887, ..., 0.00576782, -0.024292,\n",
              "                        0.0197754]],\n",
              "              \n",
              "                      [[-0.000324249, 0.0167236, -0.0169678, ..., 0.00271606,\n",
              "                        0.00759888, -0.0116577],\n",
              "                       [0.00491333, 0.0292969, 0.0341797, ..., 0.0263672, -0.0366211,\n",
              "                        0.00140381],\n",
              "                       [-0.00424194, 0.0150146, -0.00379944, ..., -0.00469971,\n",
              "                        0.041748, 0.00534058],\n",
              "                       ...,\n",
              "                       [-0.0131836, -0.00512695, 0.0134888, ..., -0.0220947,\n",
              "                        0.0217285, -0.0334473],\n",
              "                       [0.00534058, 0.0255127, -0.0055542, ..., -0.0106812,\n",
              "                        0.0062561, 0.0255127],\n",
              "                       [-0.00830078, 0.0124512, -0.0234375, ..., 0.00402832,\n",
              "                        -0.0175781, 0.0185547]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.00674438, -0.0142212, 0.0185547, ..., 0.0249023,\n",
              "                        -0.0405273, -0.0515137],\n",
              "                       [-0.0181885, 0.0375977, -0.0466309, ..., 0.0283203,\n",
              "                        -0.0288086, -0.00424194],\n",
              "                       [-0.00424194, 0.0167236, 0.00674438, ..., 0.015564,\n",
              "                        -0.00424194, 0.019165],\n",
              "                       ...,\n",
              "                       [0.0249023, 0.0119019, 0.0317383, ..., 0.019165, -0.0432129,\n",
              "                        -0.00689697],\n",
              "                       [-0.0055542, -0.00738525, -0.00970459, ..., 0.00576782,\n",
              "                        -0.00646973, 0.00445557],\n",
              "                       [0.000107765, -0.0147705, 0.0263672, ..., -0.00830078,\n",
              "                        0.00759888, -0.0152588]],\n",
              "              \n",
              "                      [[0.00314331, -0.00248718, 0.0129395, ..., -0.0137329,\n",
              "                        0.000541687, -0.00970459],\n",
              "                       [-0.00337219, -0.0201416, 0.0185547, ..., -0.00469971,\n",
              "                        0.0114136, -0.0299072],\n",
              "                       [0.0134888, 0.032959, 0.0556641, ..., -0.0349121, 0.00946045,\n",
              "                        0.041748],\n",
              "                       ...,\n",
              "                       [0.00402832, -0.0269775, 0.0062561, ..., -0.0334473,\n",
              "                        -0.0269775, 0.0139771],\n",
              "                       [0.000107765, -0.00205994, 0.0134888, ..., 0.000541687,\n",
              "                        -0.0220947, 0.00445557],\n",
              "                       [-0.027832, -0.0251465, -0.020752, ..., -0.0116577,\n",
              "                        -0.0175781, 0.019165]],\n",
              "              \n",
              "                      [[0.0124512, -0.0322266, 0.0071106, ..., 0.0109253, 0.010437,\n",
              "                        0.0255127],\n",
              "                       [-0.0175781, -0.0137329, -0.0515137, ..., 0.0255127, 0.032959,\n",
              "                        -0.00248718],\n",
              "                       [-0.00469971, -0.0383301, -0.00970459, ..., -0.00424194,\n",
              "                        0.00140381, 0.0185547],\n",
              "                       ...,\n",
              "                       [-0.000324249, 0.0114136, 0.0240479, ..., -0.0055542,\n",
              "                        0.0197754, 0.0444336],\n",
              "                       [0.0249023, -0.0322266, -0.0142212, ..., -0.0137329,\n",
              "                        0.000107765, 0.0203857],\n",
              "                       [0.048584, 0.00271606, -0.0152588, ..., -0.0131836, 0.015564,\n",
              "                        0.00314331]]]], dtype=bfloat16),\n",
              "              mesh=None,\n",
              "              sharding=('heads', 'layers', 'kv', 'embed'),\n",
              "              sharding_rules=None,\n",
              "              linen_meta_type=LogicallyPartitioned\n",
              "            )\n",
              "          },\n",
              "          'query': {\n",
              "            'kernel': Param( # 75,497,472 (151.0 MB)\n",
              "              value=Array([[[[0.000169754, 0.000873566, 0.00135803, ..., -0.000759125,\n",
              "                        0.000682831, -0.000823975],\n",
              "                       [-0.000858307, -0.000375748, 0.000652313, ..., -0.000858307,\n",
              "                        -0.00109863, -0.000823975],\n",
              "                       [0.000907898, -0.000667572, 0.000360489, ..., -0.00138092,\n",
              "                        0.000142097, 0.0015564],\n",
              "                       ...,\n",
              "                       [0.000682831, 0.000421524, 6.73532e-06, ..., 0.00234985,\n",
              "                        -0.00146484, 0.000114441],\n",
              "                       [0.000360489, -0.0032196, 0.00234985, ..., -0.000953674,\n",
              "                        -0.00109863, 0.00047493],\n",
              "                       [-0.00201416, -0.0018692, 0.00159454, ..., 0.000169754,\n",
              "                        -0.00106049, -0.000923157]],\n",
              "              \n",
              "                      [[-0.00209045, 0.000778198, 0.00104523, ..., 6.73532e-06,\n",
              "                        0.00198364, -0.00157166],\n",
              "                       [0.000307083, 0.000972748, 0.00171661, ..., 0.000938416,\n",
              "                        -7.43866e-05, -0.000155449],\n",
              "                       [-0.000265121, -0.00109863, -7.43866e-05, ..., -0.0014267,\n",
              "                        -0.000759125, 0.00123596],\n",
              "                       ...,\n",
              "                       [-0.00163269, 0.00224304, -0.0032196, ..., -0.000265121,\n",
              "                        -0.00109863, -0.000606537],\n",
              "                       [-0.00239563, 0.00260925, -0.0018692, ..., -0.000293732,\n",
              "                        -0.001297, -0.00209045],\n",
              "                       [0.000743866, -4.74453e-05, -0.000404358, ..., 0.000713348,\n",
              "                        6.07967e-05, -0.000461578]],\n",
              "              \n",
              "                      [[0.00112152, -2.02656e-05, -0.000637054, ..., -7.43866e-05,\n",
              "                        -0.000102043, 0.0027771],\n",
              "                       [-0.000461578, -0.000102043, 0.00047493, ..., 0.00213623,\n",
              "                        -0.000237465, 0.00247192],\n",
              "                       [-0.000320435, -0.000953674, -0.0019455, ..., -0.000858307,\n",
              "                        0.00144196, -0.000102043],\n",
              "                       ...,\n",
              "                       [0.00213623, 0.000972748, 0.00260925, ..., -0.000320435,\n",
              "                        -0.000637054, 0.000333786],\n",
              "                       [-0.0032196, -0.0016861, -0.000759125, ..., -0.000210762,\n",
              "                        6.07967e-05, -0.000549316],\n",
              "                       [-0.00228882, 0.000652313, -0.00109863, ..., -0.000265121,\n",
              "                        -0.00109863, 0.000307083]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[-0.000431061, 0.000278473, -0.00228882, ..., 0.00119781,\n",
              "                        -0.00138092, 0.000621796],\n",
              "                       [0.000534058, -0.000694275, -0.00134277, ..., 0.000333786,\n",
              "                        -0.000888824, -0.00291443],\n",
              "                       [-0.000404358, -0.0018692, 0.000224113, ..., -7.43866e-05,\n",
              "                        -0.000667572, 0.000278473],\n",
              "                       ...,\n",
              "                       [-0.00239563, 0.000534058, 0.000621796, ..., 0.00183105,\n",
              "                        -0.00239563, 0.00213623],\n",
              "                       [8.7738e-05, 0.00177002, 0.000808716, ..., -0.0016861,\n",
              "                        0.000843048, -0.000858307],\n",
              "                       [-0.000375748, 0.00150299, -0.000793457, ..., -2.02656e-05,\n",
              "                        0.00190735, -0.000728607]],\n",
              "              \n",
              "                      [[0.00260925, -0.000102043, 0.000778198, ..., 0.000621796,\n",
              "                        -0.000694275, -0.00228882],\n",
              "                       [-0.00106049, 0.00144196, 0.000224113, ..., -0.00146484,\n",
              "                        0.00131989, 0.000333786],\n",
              "                       [-0.000637054, -0.000694275, 0.00159454, ..., -0.000991821,\n",
              "                        0.00198364, 0.00159454],\n",
              "                       ...,\n",
              "                       [-0.00291443, -0.00109863, -0.00253296, ..., -0.000210762,\n",
              "                        -0.00121307, -0.0032196],\n",
              "                       [-0.000576019, 0.00190735, 0.00127411, ..., 0.00213623,\n",
              "                        -0.000375748, 0.000873566],\n",
              "                       [-0.000667572, 0.00205994, -0.000606537, ..., 0.00177002,\n",
              "                        0.000307083, 0.000591278]],\n",
              "              \n",
              "                      [[0.000843048, -0.000793457, -0.000128746, ..., 0.000333786,\n",
              "                        -0.000576019, -0.001297],\n",
              "                       [0.00127411, -0.000102043, 0.00144196, ..., 0.00131989,\n",
              "                        0.00100708, 0.000278473],\n",
              "                       [-0.00228882, 6.07967e-05, 0.00119781, ..., -0.000728607,\n",
              "                        0.000421524, -0.00157166],\n",
              "                       ...,\n",
              "                       [-0.00102997, 0.0015564, 0.000169754, ..., 0.000114441,\n",
              "                        -0.0018692, 0.00131989],\n",
              "                       [3.38554e-05, -0.001297, 8.7738e-05, ..., 0.00213623,\n",
              "                        0.000682831, 0.000224113],\n",
              "                       [0.000142097, -0.00113678, -0.00157166, ..., -0.0018692,\n",
              "                        -0.00218201, -0.00134277]]],\n",
              "              \n",
              "              \n",
              "                     [[[-0.0014267, -0.000953674, 0.00050354, ..., 0.00100708,\n",
              "                        0.00140381, 0.000591278],\n",
              "                       [0.00112152, 0.00050354, -0.000576019, ..., -0.000375748,\n",
              "                        -0.00163269, -0.000293732],\n",
              "                       [-0.000667572, -0.000461578, 0.00183105, ..., 0.000591278,\n",
              "                        -0.00218201, -0.000823975],\n",
              "                       ...,\n",
              "                       [-7.43866e-05, -0.000759125, -0.00228882, ..., 0.000652313,\n",
              "                        0.000114441, 0.000360489],\n",
              "                       [-0.00146484, -0.00218201, -0.000923157, ..., 0.00213623,\n",
              "                        -0.000953674, -0.00180054],\n",
              "                       [0.00104523, -0.000694275, 0.000591278, ..., 0.00190735,\n",
              "                        -0.00209045, 0.000307083]],\n",
              "              \n",
              "                      [[-0.000858307, -0.00228882, 0.00047493, ..., 0.000333786,\n",
              "                        -0.000728607, -0.000461578],\n",
              "                       [-0.0014267, -0.000858307, -0.00253296, ..., -0.0019455,\n",
              "                        -0.000375748, -0.00134277],\n",
              "                       [0.00108337, -0.00106049, 0.000142097, ..., 0.003479,\n",
              "                        -0.00180054, 0.000621796],\n",
              "                       ...,\n",
              "                       [-0.000210762, -0.00180054, -0.00291443, ..., 0.000421524,\n",
              "                        -0.000991821, 0.000564575],\n",
              "                       [-7.43866e-05, 0.000564575, -0.000293732, ..., 0.00140381,\n",
              "                        0.00190735, 0.000196457],\n",
              "                       [0.000360489, -2.02656e-05, 0.000278473, ..., -0.0017395,\n",
              "                        -0.00121307, 0.00171661]],\n",
              "              \n",
              "                      [[0.000360489, -0.00253296, 0.00025177, ..., 0.00234985,\n",
              "                        -0.00134277, -0.00102997],\n",
              "                       [0.00127411, 0.000713348, 0.00108337, ..., 0.000873566,\n",
              "                        -0.000728607, 6.07967e-05],\n",
              "                       [-0.00151825, -0.000431061, -0.000637054, ..., 0.000534058,\n",
              "                        0.000534058, -0.00180054],\n",
              "                       ...,\n",
              "                       [-0.00134277, 0.00100708, 0.000843048, ..., 0.000391006,\n",
              "                        -0.00163269, -0.00209045],\n",
              "                       [-0.00157166, -0.000637054, 0.000196457, ..., -4.74453e-05,\n",
              "                        -0.000667572, -0.0032196],\n",
              "                       [0.000778198, 6.73532e-06, 0.003479, ..., -0.00209045,\n",
              "                        -0.000461578, 0.000808716]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.0015564, -0.00163269, -0.00146484, ..., -0.0017395,\n",
              "                        -0.0039978, 0.000534058],\n",
              "                       [-0.000637054, 0.000808716, -0.00239563, ..., -0.000606537,\n",
              "                        -0.000347137, -0.0014267],\n",
              "                       [-0.000953674, 0.000907898, 0.000591278, ..., 0.000652313,\n",
              "                        0.000169754, -0.000858307],\n",
              "                       ...,\n",
              "                       [0.000169754, -0.000128746, -2.02656e-05, ..., 0.000778198,\n",
              "                        -0.000183105, 0.00104523],\n",
              "                       [-0.000210762, 0.00115967, 0.000444412, ..., -0.000549316,\n",
              "                        0.000778198, -0.000667572],\n",
              "                       [-0.00253296, 0.003479, 0.00140381, ..., 0.00150299,\n",
              "                        -0.0032196, 0.000391006]],\n",
              "              \n",
              "                      [[-0.0016861, 0.000333786, -0.0018692, ..., 0.00108337,\n",
              "                        0.003479, 0.0027771],\n",
              "                       [-0.00113678, 0.000360489, -0.000155449, ..., -0.0019455,\n",
              "                        -0.000576019, 0.000972748],\n",
              "                       [-0.0016861, 0.0027771, 0.0027771, ..., -0.00163269,\n",
              "                        0.000421524, -0.000549316],\n",
              "                       ...,\n",
              "                       [0.000169754, -0.0018692, -0.0019455, ..., -0.00157166,\n",
              "                        -0.000759125, 6.07967e-05],\n",
              "                       [0.000444412, 0.0027771, 0.000907898, ..., -0.000431061,\n",
              "                        -0.000888824, 0.00104523],\n",
              "                       [-0.000637054, -0.0014267, 0.000907898, ..., 0.0027771,\n",
              "                        0.000224113, 0.000360489]],\n",
              "              \n",
              "                      [[0.00234985, 0.00198364, 0.00260925, ..., 0.00234985,\n",
              "                        0.00050354, 0.00140381],\n",
              "                       [-0.000923157, -0.000210762, 6.73532e-06, ..., -0.000237465,\n",
              "                        -0.000991821, -0.000728607],\n",
              "                       [-0.0018692, -0.00228882, 0.00234985, ..., -0.00138092,\n",
              "                        0.003479, -0.000991821],\n",
              "                       ...,\n",
              "                       [-0.000347137, -0.0014267, 0.000391006, ..., 0.000591278,\n",
              "                        0.000808716, -0.000375748],\n",
              "                       [8.7738e-05, -0.00163269, -0.0016861, ..., -0.000488281,\n",
              "                        0.00183105, 0.000142097],\n",
              "                       [-0.00218201, -0.000155449, -0.0014267, ..., 0.00140381,\n",
              "                        0.00224304, -0.000823975]]],\n",
              "              \n",
              "              \n",
              "                     [[[0.000743866, 0.000713348, 0.00108337, ..., 0.00198364,\n",
              "                        -0.000888824, 0.000169754],\n",
              "                       [0.00115967, -0.00117493, -0.000183105, ..., -0.0017395,\n",
              "                        0.000972748, 0.000391006],\n",
              "                       [0.00119781, 0.000307083, -0.000694275, ..., 0.00260925,\n",
              "                        0.00190735, 0.00135803],\n",
              "                       ...,\n",
              "                       [0.000713348, 0.000333786, 0.00164795, ..., 0.00025177,\n",
              "                        0.00025177, 0.000196457],\n",
              "                       [0.00213623, 0.00144196, -0.000991821, ..., -0.00121307,\n",
              "                        -0.00228882, 0.0015564],\n",
              "                       [0.00047493, 0.00177002, 0.00190735, ..., 0.000682831,\n",
              "                        -0.00138092, -0.000793457]],\n",
              "              \n",
              "                      [[0.000873566, -0.00134277, -0.001297, ..., 0.00025177,\n",
              "                        0.00183105, -0.00138092],\n",
              "                       [0.00108337, -0.000549316, 0.000142097, ..., -0.00113678,\n",
              "                        -0.000265121, 0.00112152],\n",
              "                       [-0.0018692, -0.00209045, -0.000637054, ..., 0.000591278,\n",
              "                        0.00213623, 0.00224304],\n",
              "                       ...,\n",
              "                       [-0.00138092, -0.000667572, 0.000591278, ..., 0.00119781,\n",
              "                        -0.000320435, 0.00119781],\n",
              "                       [0.000444412, 0.00224304, -0.00138092, ..., -0.00209045,\n",
              "                        0.00115967, 0.0015564],\n",
              "                       [0.0030365, -0.000347137, 0.00247192, ..., -0.0017395,\n",
              "                        -0.000293732, -0.000210762]],\n",
              "              \n",
              "                      [[-0.0014267, 0.00247192, 0.000196457, ..., -0.00291443,\n",
              "                        -0.000347137, 0.000142097],\n",
              "                       [0.000169754, -0.000759125, 0.0015564, ..., 0.000778198,\n",
              "                        -0.000991821, 0.000621796],\n",
              "                       [0.00131989, -0.000183105, 0.00164795, ..., 0.00164795,\n",
              "                        -0.000488281, -0.000210762],\n",
              "                       ...,\n",
              "                       [0.000843048, -0.000102043, -0.00201416, ..., -0.000347137,\n",
              "                        0.000391006, 0.000224113],\n",
              "                       [-0.00239563, 0.00234985, 0.00050354, ..., -0.000606537,\n",
              "                        -0.000606537, 0.000278473],\n",
              "                       [-0.000102043, 0.000564575, -0.00138092, ..., -0.00201416,\n",
              "                        -2.02656e-05, -0.000823975]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.00159454, -0.000488281, 0.000307083, ..., 0.00198364,\n",
              "                        0.000391006, -0.00117493],\n",
              "                       [-0.000637054, -0.00113678, 0.00140381, ..., -0.000759125,\n",
              "                        -0.000183105, -0.000265121],\n",
              "                       [0.000591278, -0.000518799, -0.000888824, ..., -0.000888824,\n",
              "                        0.003479, -0.00146484],\n",
              "                       ...,\n",
              "                       [0.00135803, -0.00121307, 0.00112152, ..., -0.000518799,\n",
              "                        -4.74453e-05, 0.00164795],\n",
              "                       [0.000169754, 0.00047493, 0.00047493, ..., -0.0019455,\n",
              "                        -0.000923157, -0.00228882],\n",
              "                       [-0.00239563, 0.000843048, 0.00171661, ..., 0.000114441,\n",
              "                        -0.00134277, -0.0019455]],\n",
              "              \n",
              "                      [[-0.00270081, -0.000293732, 0.000534058, ..., -0.0039978,\n",
              "                        -0.00121307, -4.74453e-05],\n",
              "                       [-0.00253296, -0.000667572, 0.00140381, ..., 0.00150299,\n",
              "                        0.000360489, -0.000953674],\n",
              "                       [0.00213623, -0.00134277, -0.000128746, ..., -0.00180054,\n",
              "                        0.000778198, -0.000576019],\n",
              "                       ...,\n",
              "                       [6.73532e-06, 0.000621796, 0.000743866, ..., -0.000237465,\n",
              "                        -0.000823975, 0.00224304],\n",
              "                       [0.00159454, -0.000694275, 0.00213623, ..., 0.00127411,\n",
              "                        -0.000637054, 0.000682831],\n",
              "                       [0.000652313, -0.000923157, -0.000667572, ..., 0.000360489,\n",
              "                        -0.00253296, -0.000576019]],\n",
              "              \n",
              "                      [[-0.000347137, -0.0018692, 0.00150299, ..., 0.00127411,\n",
              "                        -0.000461578, 0.00234985],\n",
              "                       [0.000444412, 0.00205994, -0.00253296, ..., -0.00117493,\n",
              "                        -0.001297, -0.00117493],\n",
              "                       [0.000224113, -0.000728607, -0.000128746, ..., -0.0018692,\n",
              "                        -0.000293732, -0.00121307],\n",
              "                       ...,\n",
              "                       [0.000421524, 0.0027771, 0.000360489, ..., 0.000360489,\n",
              "                        -0.00117493, 0.00247192],\n",
              "                       [-0.00291443, 0.000196457, -0.0017395, ..., -7.43866e-05,\n",
              "                        -0.00253296, -0.00134277],\n",
              "                       [0.0027771, 0.00104523, -0.00134277, ..., -0.000375748,\n",
              "                        -0.000320435, -0.00163269]]],\n",
              "              \n",
              "              \n",
              "                     ...,\n",
              "              \n",
              "              \n",
              "                     [[[-0.00125885, 0.00205994, -0.000293732, ..., -0.00180054,\n",
              "                        0.00047493, 0.000713348],\n",
              "                       [0.00205994, -0.00209045, -0.000155449, ..., -0.000488281,\n",
              "                        0.00177002, 3.38554e-05],\n",
              "                       [0.000142097, -0.001297, -0.000576019, ..., -4.74453e-05,\n",
              "                        -0.000793457, 0.000534058],\n",
              "                       ...,\n",
              "                       [0.00127411, -0.00151825, -0.0014267, ..., 0.000743866,\n",
              "                        -0.000102043, -0.000858307],\n",
              "                       [-7.43866e-05, -0.00291443, 0.00159454, ..., 0.0030365,\n",
              "                        -0.00209045, 0.000713348],\n",
              "                       [0.00213623, 3.38554e-05, 0.00119781, ..., 0.000621796,\n",
              "                        0.000224113, 0.00260925]],\n",
              "              \n",
              "                      [[0.00260925, -0.000375748, 0.00050354, ..., -0.00146484,\n",
              "                        0.000682831, 0.0027771],\n",
              "                       [-2.02656e-05, 6.07967e-05, -0.000265121, ..., 0.000444412,\n",
              "                        0.000307083, -0.00228882],\n",
              "                       [-0.0018692, -0.000320435, -0.00151825, ..., 0.000114441,\n",
              "                        -0.00138092, 0.000278473],\n",
              "                       ...,\n",
              "                       [0.000713348, 0.000938416, 0.000333786, ..., -0.000991821,\n",
              "                        0.0015564, 0.00047493],\n",
              "                       [0.00144196, -0.000488281, 0.00150299, ..., -0.000347137,\n",
              "                        0.000713348, -0.000637054],\n",
              "                       [-0.00270081, -0.000128746, 0.000591278, ..., -0.000888824,\n",
              "                        -0.000858307, -0.00125885]],\n",
              "              \n",
              "                      [[-0.000793457, 0.00190735, 8.7738e-05, ..., -2.02656e-05,\n",
              "                        0.000196457, -0.00270081],\n",
              "                       [-0.000606537, -0.001297, 0.000972748, ..., -0.000576019,\n",
              "                        0.000142097, 0.000682831],\n",
              "                       [-0.0016861, -0.000858307, -0.000823975, ..., -0.0014267,\n",
              "                        0.00159454, -0.000461578],\n",
              "                       ...,\n",
              "                       [0.00164795, -0.000667572, -0.0014267, ..., 8.7738e-05,\n",
              "                        -0.000606537, -0.000375748],\n",
              "                       [-0.00253296, -0.00291443, -0.00109863, ..., -0.00138092,\n",
              "                        0.00183105, 0.00115967],\n",
              "                       [-0.00109863, 8.7738e-05, -0.000210762, ..., -0.00151825,\n",
              "                        -0.00102997, -0.00117493]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.00100708, -0.000293732, -0.0019455, ..., -0.001297,\n",
              "                        0.00144196, 0.000360489],\n",
              "                       [0.000307083, -0.00239563, -0.00102997, ..., 0.000196457,\n",
              "                        0.00047493, -0.00109863],\n",
              "                       [0.00224304, 0.000621796, -4.74453e-05, ..., -0.0017395,\n",
              "                        -0.00209045, 0.000114441],\n",
              "                       ...,\n",
              "                       [-0.000991821, 0.00198364, 0.000224113, ..., 0.000196457,\n",
              "                        -0.00134277, 0.000682831],\n",
              "                       [0.00171661, -0.0032196, -0.000320435, ..., 0.000196457,\n",
              "                        0.000778198, -0.0032196],\n",
              "                       [0.00119781, -0.000320435, 0.0030365, ..., 0.000169754,\n",
              "                        -0.00151825, -0.00151825]],\n",
              "              \n",
              "                      [[0.000843048, -0.000793457, -0.000128746, ..., 0.000278473,\n",
              "                        -0.000375748, -0.000888824],\n",
              "                       [0.003479, -0.000102043, 0.00119781, ..., -0.00239563,\n",
              "                        0.00119781, -0.00291443],\n",
              "                       [-0.000102043, 3.38554e-05, -0.0016861, ..., 0.000444412,\n",
              "                        0.0015564, 0.000907898],\n",
              "                       ...,\n",
              "                       [-0.0039978, -0.00138092, 0.00100708, ..., -0.000404358,\n",
              "                        -0.00209045, -0.0039978],\n",
              "                       [-0.000549316, -0.00121307, -0.000183105, ..., -0.00125885,\n",
              "                        0.000682831, 0.000907898],\n",
              "                       [0.000778198, 0.00047493, -0.000404358, ..., 0.000114441,\n",
              "                        -0.0014267, -0.000488281]],\n",
              "              \n",
              "                      [[-0.000265121, 0.00112152, 0.000444412, ..., -0.00180054,\n",
              "                        3.38554e-05, -0.0018692],\n",
              "                       [0.00144196, 0.00234985, -0.00253296, ..., -0.000953674,\n",
              "                        -7.43866e-05, -0.000694275],\n",
              "                       [-0.000793457, 0.00144196, -0.000461578, ..., 0.000682831,\n",
              "                        -0.0039978, -0.000183105],\n",
              "                       ...,\n",
              "                       [-0.00138092, -0.000375748, -0.000404358, ..., 0.000972748,\n",
              "                        -0.000953674, -0.00125885],\n",
              "                       [-0.00151825, -0.000375748, -0.000183105, ..., 0.00164795,\n",
              "                        0.00171661, -0.000265121],\n",
              "                       [-0.00134277, -0.000488281, 0.00205994, ..., 0.00177002,\n",
              "                        -0.000320435, -0.00102997]]],\n",
              "              \n",
              "              \n",
              "                     [[[0.00205994, 0.00131989, 0.0027771, ..., 0.000843048,\n",
              "                        0.00224304, -0.000461578],\n",
              "                       [-0.00291443, -0.000694275, -0.000431061, ..., 0.000224113,\n",
              "                        -0.000858307, 0.000444412],\n",
              "                       [-0.000576019, 0.000534058, 0.000682831, ..., -0.00201416,\n",
              "                        -0.0039978, 6.73532e-06],\n",
              "                       ...,\n",
              "                       [0.00104523, -0.000759125, 0.000621796, ..., -0.00113678,\n",
              "                        -0.0032196, 0.000972748],\n",
              "                       [0.000621796, 0.000907898, 6.07967e-05, ..., 0.00050354,\n",
              "                        -0.000858307, 0.000114441],\n",
              "                       [-0.000375748, -0.000347137, 0.00047493, ..., -0.000320435,\n",
              "                        0.00025177, -0.000953674]],\n",
              "              \n",
              "                      [[0.00177002, -0.000265121, -0.0014267, ..., -0.00239563,\n",
              "                        0.00205994, 0.00047493],\n",
              "                       [0.00260925, -0.00291443, 0.00025177, ..., -0.00228882,\n",
              "                        -0.00151825, -7.43866e-05],\n",
              "                       [-0.000375748, -0.00134277, 0.000360489, ..., 0.00150299,\n",
              "                        0.00183105, -0.00117493],\n",
              "                       ...,\n",
              "                       [-0.00253296, 0.00050354, -0.000128746, ..., -0.00163269,\n",
              "                        -0.000953674, 0.000142097],\n",
              "                       [-0.00239563, 0.000743866, -0.000210762, ..., 0.00112152,\n",
              "                        -0.00201416, 0.000444412],\n",
              "                       [0.00050354, 0.00047493, 0.00198364, ..., -0.0032196,\n",
              "                        0.00150299, 0.000360489]],\n",
              "              \n",
              "                      [[0.00144196, 0.00025177, 0.0015564, ..., -0.00125885,\n",
              "                        0.000591278, 0.000444412],\n",
              "                       [0.000938416, 0.000743866, 0.00025177, ..., -0.000858307,\n",
              "                        0.00100708, -0.000155449],\n",
              "                       [0.00183105, -0.000237465, 0.00131989, ..., -0.0014267,\n",
              "                        -0.000637054, -0.000858307],\n",
              "                       ...,\n",
              "                       [0.00112152, 0.000564575, -0.000488281, ..., -0.000549316,\n",
              "                        -0.00253296, 0.00123596],\n",
              "                       [-0.000953674, -0.000461578, -0.00146484, ..., 0.000534058,\n",
              "                        0.00104523, 0.00025177],\n",
              "                       [0.003479, -0.00125885, -0.0019455, ..., -0.00228882,\n",
              "                        -0.000155449, -0.0039978]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[-0.0016861, 0.00025177, 0.000421524, ..., 0.00112152,\n",
              "                        -0.0019455, -0.000155449],\n",
              "                       [-0.00218201, -0.000518799, 0.00104523, ..., 0.000196457,\n",
              "                        -0.000518799, 0.003479],\n",
              "                       [-0.000210762, 0.00205994, 0.00159454, ..., -0.00228882,\n",
              "                        -0.00102997, -0.00239563],\n",
              "                       ...,\n",
              "                       [-2.02656e-05, -0.000637054, -0.000759125, ..., 0.00144196,\n",
              "                        -0.001297, -0.000549316],\n",
              "                       [-0.00106049, 0.000534058, -0.000293732, ..., 0.00205994,\n",
              "                        0.00171661, 0.000142097],\n",
              "                       [0.000873566, 0.00047493, -0.00117493, ..., -0.00270081,\n",
              "                        -0.00201416, -0.000576019]],\n",
              "              \n",
              "                      [[-0.000461578, -0.0018692, -0.000759125, ..., -7.43866e-05,\n",
              "                        -0.000858307, 0.000682831],\n",
              "                       [-7.43866e-05, 0.00123596, -0.000265121, ..., -0.00157166,\n",
              "                        -0.00106049, -0.000320435],\n",
              "                       [0.00213623, -0.00117493, -0.000155449, ..., 0.000591278,\n",
              "                        0.00183105, -0.0019455],\n",
              "                       ...,\n",
              "                       [0.000938416, 0.000421524, -0.0014267, ..., 0.000972748,\n",
              "                        0.000778198, 0.0030365],\n",
              "                       [0.000142097, -0.0019455, -0.000576019, ..., 0.00224304,\n",
              "                        -0.000549316, -0.0017395],\n",
              "                       [-0.000128746, 0.00171661, -0.00102997, ..., 0.000873566,\n",
              "                        -0.00291443, -0.000320435]],\n",
              "              \n",
              "                      [[-7.43866e-05, 0.00047493, -0.000128746, ..., 0.000444412,\n",
              "                        0.00115967, -0.00201416],\n",
              "                       [0.000682831, 0.00183105, 0.00131989, ..., -0.000183105,\n",
              "                        0.000743866, -0.00134277],\n",
              "                       [-0.00102997, -0.00228882, -2.02656e-05, ..., 0.00198364,\n",
              "                        0.000444412, -0.00134277],\n",
              "                       ...,\n",
              "                       [0.00123596, 0.000169754, -0.00134277, ..., -0.00109863,\n",
              "                        8.7738e-05, 0.000843048],\n",
              "                       [-0.00106049, -0.000518799, -0.000728607, ..., 0.000972748,\n",
              "                        0.00224304, -0.00138092],\n",
              "                       [-0.00146484, -0.000320435, -0.000488281, ..., 0.00224304,\n",
              "                        0.00190735, 0.000743866]]],\n",
              "              \n",
              "              \n",
              "                     [[[-0.00218201, -0.00117493, 0.00050354, ..., -0.00109863,\n",
              "                        0.00104523, -4.74453e-05],\n",
              "                       [0.00164795, -0.000728607, -0.000549316, ..., -0.000461578,\n",
              "                        -0.00291443, -0.000759125],\n",
              "                       [0.00159454, 0.00183105, -0.00270081, ..., 0.00247192,\n",
              "                        8.7738e-05, 0.00260925],\n",
              "                       ...,\n",
              "                       [0.00100708, -0.000953674, 0.000564575, ..., 0.00213623,\n",
              "                        0.00123596, -0.000637054],\n",
              "                       [0.0027771, -0.00270081, 0.0030365, ..., 0.000534058,\n",
              "                        -0.000320435, -0.000320435],\n",
              "                       [0.000391006, -0.000991821, -0.000183105, ..., -0.00228882,\n",
              "                        -0.00218201, -0.00138092]],\n",
              "              \n",
              "                      [[0.00224304, 0.00108337, -0.00121307, ..., 0.00131989,\n",
              "                        -0.00146484, 0.000682831],\n",
              "                       [-0.000759125, 0.000224113, -0.0018692, ..., 0.000907898,\n",
              "                        -0.000667572, -0.000102043],\n",
              "                       [0.000873566, -0.000488281, -0.000923157, ..., 0.00190735,\n",
              "                        0.000682831, 0.000778198],\n",
              "                       ...,\n",
              "                       [-0.00253296, -0.000823975, -0.000549316, ..., 0.000972748,\n",
              "                        0.003479, -0.000183105],\n",
              "                       [-0.000728607, -0.000128746, 0.003479, ..., -4.74453e-05,\n",
              "                        0.00224304, -0.00109863],\n",
              "                       [0.000169754, 0.003479, -0.00125885, ..., 0.000360489,\n",
              "                        0.000907898, -0.00218201]],\n",
              "              \n",
              "                      [[-0.0019455, 0.000778198, -0.000375748, ..., 0.00183105,\n",
              "                        -0.000858307, -0.000793457],\n",
              "                       [-0.000404358, -0.000155449, -0.000404358, ..., 0.00205994,\n",
              "                        -0.000576019, -0.000923157],\n",
              "                       [0.000713348, 0.000682831, -0.000237465, ..., -2.02656e-05,\n",
              "                        -0.000728607, 0.00127411],\n",
              "                       ...,\n",
              "                       [0.00047493, 6.07967e-05, 0.000621796, ..., 0.000652313,\n",
              "                        0.00260925, -7.43866e-05],\n",
              "                       [-0.000991821, -0.00209045, 0.00119781, ..., 0.000114441,\n",
              "                        -0.00109863, 0.00183105],\n",
              "                       [0.000360489, -0.0039978, -0.000461578, ..., -0.0014267,\n",
              "                        0.00115967, -0.00134277]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.00260925, -0.000923157, -0.00121307, ..., 0.000444412,\n",
              "                        -0.001297, 0.0015564],\n",
              "                       [-0.000183105, -0.00109863, 0.000743866, ..., -0.000793457,\n",
              "                        -0.0014267, -0.000461578],\n",
              "                       [0.000682831, 0.00213623, 0.000196457, ..., -0.000576019,\n",
              "                        -0.00163269, 0.00127411],\n",
              "                       ...,\n",
              "                       [-0.000375748, 0.00112152, -0.000823975, ..., -0.000210762,\n",
              "                        -0.00109863, -0.000431061],\n",
              "                       [6.73532e-06, 0.000778198, -0.0017395, ..., -0.000320435,\n",
              "                        -0.000293732, 0.00144196],\n",
              "                       [-0.00151825, 0.000444412, 0.000444412, ..., -0.000576019,\n",
              "                        -0.000549316, -0.00180054]],\n",
              "              \n",
              "                      [[0.00213623, 0.000972748, -0.00113678, ..., 0.00112152,\n",
              "                        -0.000404358, 0.00183105],\n",
              "                       [0.00025177, 0.00171661, -0.00228882, ..., 0.000333786,\n",
              "                        0.000808716, -0.000858307],\n",
              "                       [0.003479, 0.00159454, -0.000576019, ..., -0.0017395,\n",
              "                        0.0015564, 6.07967e-05],\n",
              "                       ...,\n",
              "                       [0.000621796, -0.000431061, 0.000682831, ..., 0.003479,\n",
              "                        0.00213623, -0.0017395],\n",
              "                       [-0.000728607, 0.00177002, 0.000743866, ..., -0.000375748,\n",
              "                        0.000278473, -0.00291443],\n",
              "                       [-0.00239563, 0.00025177, 0.00140381, ..., 0.00119781,\n",
              "                        -0.000102043, -0.000265121]],\n",
              "              \n",
              "                      [[-0.00121307, 0.000743866, 0.00047493, ..., 0.00205994,\n",
              "                        -0.00291443, 0.00025177],\n",
              "                       [8.7738e-05, 0.00224304, 0.000778198, ..., -0.000320435,\n",
              "                        0.00177002, -7.43866e-05],\n",
              "                       [-0.000694275, 0.00177002, 0.000114441, ..., 0.00131989,\n",
              "                        -0.000210762, -0.000237465],\n",
              "                       ...,\n",
              "                       [0.000564575, -0.000210762, 0.000938416, ..., 0.000564575,\n",
              "                        3.38554e-05, 0.00224304],\n",
              "                       [-2.02656e-05, -0.000102043, -2.02656e-05, ..., -0.0018692,\n",
              "                        0.00150299, 0.00108337],\n",
              "                       [0.000196457, 0.00150299, 0.00183105, ..., -0.000461578,\n",
              "                        0.00159454, 0.0027771]]]], dtype=bfloat16),\n",
              "              mesh=None,\n",
              "              sharding=('embed', 'layers', 'q_heads', 'kv'),\n",
              "              sharding_rules=None,\n",
              "              linen_meta_type=LogicallyPartitioned\n",
              "            )\n",
              "          },\n",
              "          'value': {\n",
              "            'kernel': Param( # 9,437,184 (18.9 MB)\n",
              "              value=Array([[[[0.0150146, 0.015564, -0.00921631, ..., 0.000541687,\n",
              "                        0.0263672, 0.0263672]],\n",
              "              \n",
              "                      [[-0.0214844, 0.00994873, -0.0137329, ..., 0.00183105,\n",
              "                        0.00674438, -0.027832]],\n",
              "              \n",
              "                      [[0.0556641, -0.0383301, -0.0466309, ..., -0.0432129,\n",
              "                        -0.0111084, 0.00674438]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[-0.0181885, 0.0114136, 0.00674438, ..., 0.0444336, 0.0375977,\n",
              "                        -0.0078125]],\n",
              "              \n",
              "                      [[-0.0126953, -0.0187988, 0.015564, ..., -0.0152588,\n",
              "                        -0.00379944, -0.0299072]],\n",
              "              \n",
              "                      [[0.0197754, 0.010437, 0.0129395, ..., 0.0150146, -0.00248718,\n",
              "                        -0.00205994]]],\n",
              "              \n",
              "              \n",
              "                     [[[-0.00921631, -0.0126953, -0.0158691, ..., 0.0179443,\n",
              "                        0.017334, 0.0395508]],\n",
              "              \n",
              "                      [[0.0556641, 0.0283203, -0.0251465, ..., 0.0203857, 0.0161133,\n",
              "                        0.010437]],\n",
              "              \n",
              "                      [[0.0139771, -0.00119019, -0.012146, ..., -0.00119019,\n",
              "                        -0.00830078, -0.020752]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.0124512, -0.00601196, 0.032959, ..., 0.0274658, -0.0111084,\n",
              "                        -0.00119019]],\n",
              "              \n",
              "                      [[0.0134888, -0.0152588, -0.0639648, ..., -0.0137329,\n",
              "                        -0.0055542, 0.0341797]],\n",
              "              \n",
              "                      [[0.0358887, 0.0145264, 0.048584, ..., 0.0203857, -0.0116577,\n",
              "                        -0.0220947]]],\n",
              "              \n",
              "              \n",
              "                     [[[-0.000324249, -0.0152588, -0.00205994, ..., 0.0145264,\n",
              "                        -0.0175781, 0.00805664]],\n",
              "              \n",
              "                      [[0.0167236, 0.00271606, -0.00646973, ..., -0.00970459,\n",
              "                        -0.00646973, 0.0240479]],\n",
              "              \n",
              "                      [[0.000541687, -0.00119019, 0.0071106, ..., -0.0142212,\n",
              "                        -0.0228271, -0.00119019]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[-0.000759125, -0.0269775, 0.000972748, ..., -0.00970459,\n",
              "                        -0.00830078, -0.00646973]],\n",
              "              \n",
              "                      [[0.00183105, -0.012146, -0.0322266, ..., -0.0111084,\n",
              "                        0.00759888, -0.0126953]],\n",
              "              \n",
              "                      [[0.0395508, -0.00921631, 0.00854492, ..., 0.0179443,\n",
              "                        0.0217285, -0.00163269]]],\n",
              "              \n",
              "              \n",
              "                     ...,\n",
              "              \n",
              "              \n",
              "                     [[[0.032959, 0.000107765, -0.00830078, ..., 0.0305176,\n",
              "                        0.0274658, -0.0299072]],\n",
              "              \n",
              "                      [[-0.0158691, 0.0283203, -0.0164795, ..., 0.00854492,\n",
              "                        -0.0349121, 0.0185547]],\n",
              "              \n",
              "                      [[0.0114136, -0.0106812, -0.0366211, ..., 0.0358887,\n",
              "                        -0.0515137, 0.0305176]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.0129395, -0.00337219, 0.000972748, ..., -0.0126953,\n",
              "                        0.0341797, 0.0129395]],\n",
              "              \n",
              "                      [[0.0358887, 0.0283203, -0.00205994, ..., 0.0150146,\n",
              "                        0.00805664, 0.00576782]],\n",
              "              \n",
              "                      [[-0.0164795, 0.00674438, 0.010437, ..., 0.0185547, 0.019165,\n",
              "                        0.00183105]]],\n",
              "              \n",
              "              \n",
              "                     [[[0.0240479, 0.000972748, -0.0158691, ..., -0.00205994,\n",
              "                        0.000107765, -0.0334473]],\n",
              "              \n",
              "                      [[0.010437, 0.0167236, 0.00183105, ..., -0.0101929, 0.0124512,\n",
              "                        -0.0466309]],\n",
              "              \n",
              "                      [[0.00358582, 0.00402832, 0.000107765, ..., 0.0292969,\n",
              "                        0.010437, 0.0274658]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[-0.0187988, 0.0211182, -0.0078125, ..., -0.00878906,\n",
              "                        -0.00689697, 0.019165]],\n",
              "              \n",
              "                      [[-0.0299072, 0.0161133, 0.00140381, ..., 0.00314331,\n",
              "                        0.0263672, 0.0197754]],\n",
              "              \n",
              "                      [[-0.00512695, -0.00738525, -0.0220947, ..., -0.0131836,\n",
              "                        0.0211182, 0.0145264]]],\n",
              "              \n",
              "              \n",
              "                     [[[0.0263672, -0.0187988, 0.0255127, ..., 0.0109253, 0.0134888,\n",
              "                        -0.027832]],\n",
              "              \n",
              "                      [[-0.00738525, 0.0395508, 0.00227356, ..., 0.0124512,\n",
              "                        -0.012146, -0.026123]],\n",
              "              \n",
              "                      [[0.0129395, -0.00163269, -0.00379944, ..., 0.0179443,\n",
              "                        0.0197754, -0.00601196]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.019165, -0.0251465, 0.0317383, ..., -0.0214844,\n",
              "                        -0.00119019, -0.0169678]],\n",
              "              \n",
              "                      [[0.0185547, 0.00183105, 0.0119019, ..., -0.0366211,\n",
              "                        -0.0152588, -0.026123]],\n",
              "              \n",
              "                      [[0.00445557, -0.012146, -0.0234375, ..., 0.00854492,\n",
              "                        -0.0111084, -0.0126953]]]], dtype=bfloat16),\n",
              "              mesh=None,\n",
              "              sharding=('embed', 'layers', 'kv_heads', 'kv_head_dim'),\n",
              "              sharding_rules=None,\n",
              "              linen_meta_type=LogicallyPartitioned\n",
              "            )\n",
              "          }\n",
              "        }\n",
              "      },\n",
              "      'to_nnx__rngs': {\n",
              "        'dropout': {\n",
              "          'count': RngCount( # 1 (4 B)\n",
              "            value=Array(2, dtype=uint32),\n",
              "            tag='dropout'\n",
              "          ),\n",
              "          'key': RngKey( # 1 (8 B)\n",
              "            value=Array((), dtype=key<fry>) overlaying:\n",
              "            [ 507451445 1853169794],\n",
              "            tag='dropout'\n",
              "          )\n",
              "        },\n",
              "        'params': {\n",
              "          'count': RngCount( # 1 (4 B)\n",
              "            value=Array(2, dtype=uint32),\n",
              "            tag='params'\n",
              "          ),\n",
              "          'key': RngKey( # 1 (8 B)\n",
              "            value=Array((), dtype=key<fry>) overlaying:\n",
              "            [ 928981903 3453687069],\n",
              "            tag='params'\n",
              "          )\n",
              "        }\n",
              "      }\n",
              "    },\n",
              "    'token_embedder': {\n",
              "      'embedding': Param( # 524,550,144 (1.0 GB)\n",
              "        value=Array([[1.15625, -0.353516, 1.4375, ..., -1.57812, 0.0245361, -0.644531],\n",
              "               [1.78906, 0.703125, 1.24219, ..., 1.04688, 0.0439453, 1.38281],\n",
              "               [-0.550781, 1.32812, 0.304688, ..., -0.691406, 0.921875, -1.73438],\n",
              "               ...,\n",
              "               [-0.972656, -0.851562, -1, ..., 0.0439453, -1.73438, 1.4375],\n",
              "               [0.00488281, 0.304688, 0.65625, ..., -1.83594, -0.192383,\n",
              "                0.162109],\n",
              "               [-0.251953, -1.65625, 0.730469, ..., -1.25781, -0.271484,\n",
              "                -0.878906]], dtype=bfloat16),\n",
              "        sharding=('vocab', 'embed')\n",
              "      )\n",
              "    }\n",
              "  }\n",
              "})"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sampler.transformer_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdxn7DQYT3Uv"
      },
      "source": [
        "## Apply LoRA/QLoRA to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3t-7leAT3Uv"
      },
      "outputs": [],
      "source": [
        "def get_lora_model(base_model, mesh):\n",
        "  lora_provider = lora.LoraProvider(\n",
        "      module_path=\".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj\",\n",
        "      rank=RANK,\n",
        "      alpha=ALPHA,\n",
        "      # comment the two args below for LoRA (w/o quantisation).\n",
        "      weight_qtype=\"nf4\",\n",
        "      tile_size=256,\n",
        "  )\n",
        "\n",
        "  model_input = base_model.get_model_input()\n",
        "  lora_model = lora.apply_lora_to_model(\n",
        "      base_model, lora_provider, **model_input\n",
        "  )\n",
        "\n",
        "  with mesh:\n",
        "    state = nnx.state(lora_model)\n",
        "    pspecs = nnx.get_partition_spec(state)\n",
        "    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
        "    nnx.update(lora_model, sharded_state)\n",
        "\n",
        "  return lora_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfmodoqrT3Uv"
      },
      "outputs": [],
      "source": [
        "# # LoRA model\n",
        "# lora_gemma = get_lora_model(gemma, mesh=mesh)\n",
        "# nnx.display(lora_gemma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD6-iU0PT3Uv"
      },
      "source": [
        "## Load Datasets for SFT Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6T_7Cik8T3Uv"
      },
      "outputs": [],
      "source": [
        "# Loads the training and validation datasets\n",
        "train_ds, validation_ds = data_lib.create_datasets(\n",
        "    dataset_name='mtnt/en-fr',\n",
        "    # Uncomment the line below to use a Hugging Face dataset.\n",
        "    # Note that this requires upgrading the 'datasets' package and restarting\n",
        "    # the Colab runtime.\n",
        "    # dataset_name='Helsinki-NLP/opus-100',\n",
        "    global_batch_size=BATCH_SIZE,\n",
        "    max_target_length=256,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    tokenizer=gemma_tokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "def gen_model_input_fn(x: peft_trainer.TrainingInput):\n",
        "  pad_mask = x.input_tokens != gemma_tokenizer.pad_id()\n",
        "  positions = gemma_lib.build_positions_from_mask(pad_mask)\n",
        "  attention_mask = gemma_lib.make_causal_attn_mask(pad_mask)\n",
        "  return {\n",
        "      'input_tokens': x.input_tokens,\n",
        "      'input_mask': x.input_mask,\n",
        "      'positions': positions,\n",
        "      'attention_mask': attention_mask,\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w0PHlVBT3Uv"
      },
      "source": [
        "## SFT Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh1xPieRT3Uv"
      },
      "source": [
        "### Training with full weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oYR9JKNT3Uv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manony-mouse-863749125460230603\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/mazumdera_google_com/tunix/examples/wandb/run-20250802_000105-d9cenrxl</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/anony-mouse-863749125460230603/tunix/runs/d9cenrxl?apiKey=b4d00f0db746a7c14cc080c4b44a878fb3515769' target=\"_blank\">2025-08-02_00-01-05</a></strong> to <a href='https://wandb.ai/anony-mouse-863749125460230603/tunix?apiKey=b4d00f0db746a7c14cc080c4b44a878fb3515769' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/anony-mouse-863749125460230603/tunix?apiKey=b4d00f0db746a7c14cc080c4b44a878fb3515769' target=\"_blank\">https://wandb.ai/anony-mouse-863749125460230603/tunix?apiKey=b4d00f0db746a7c14cc080c4b44a878fb3515769</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/anony-mouse-863749125460230603/tunix/runs/d9cenrxl?apiKey=b4d00f0db746a7c14cc080c4b44a878fb3515769' target=\"_blank\">https://wandb.ai/anony-mouse-863749125460230603/tunix/runs/d9cenrxl?apiKey=b4d00f0db746a7c14cc080c4b44a878fb3515769</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Do NOT share these links with anyone. They can be used to claim your runs."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "ValueError",
          "evalue": "Resource axis: norm of PartitionSpec('norm',) is not found in mesh: ('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive').",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m jax.profiler.trace(os.path.join(PROFILING_DIR, \u001b[33m\"\u001b[39m\u001b[33mfull_training\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m     13\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m mesh:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_ds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/tunix/sft/peft_trainer.py:443\u001b[39m, in \u001b[36mPeftTrainer.train\u001b[39m\u001b[34m(self, train_ds, eval_ds, skip_jit)\u001b[39m\n\u001b[32m    440\u001b[39m mesh = pxla.thread_resources.env.physical_mesh\n\u001b[32m    441\u001b[39m logging.info(\u001b[33m\"\u001b[39m\u001b[33mTraining with mesh: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, mesh)\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m train_step, eval_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjit_train_and_eval_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mskip_jit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.max_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pbar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    446\u001b[39m   \u001b[38;5;28mself\u001b[39m._pbar = progress_bar.ProgressBar(\n\u001b[32m    447\u001b[39m       metrics_logger=\u001b[38;5;28mself\u001b[39m.metrics_logger,\n\u001b[32m    448\u001b[39m       initial_steps=\u001b[38;5;28mself\u001b[39m._train_steps,\n\u001b[32m    449\u001b[39m       max_steps=\u001b[38;5;28mself\u001b[39m.config.max_steps,\n\u001b[32m    450\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/tunix/sft/peft_trainer.py:346\u001b[39m, in \u001b[36mPeftTrainer.jit_train_and_eval_step\u001b[39m\u001b[34m(self, skip_jit)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jitted_train_step_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    345\u001b[39m   mesh = pxla.thread_resources.env.physical_mesh\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_shard_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m   \u001b[38;5;28mself\u001b[39m._jitted_train_step_fn = nnx.jit(\n\u001b[32m    348\u001b[39m       train_step, donate_argnames=(\u001b[33m\"\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m\"\u001b[39m,)\n\u001b[32m    349\u001b[39m   )\n\u001b[32m    350\u001b[39m   \u001b[38;5;28mself\u001b[39m._jitted_eval_step_fn = nnx.jit(\n\u001b[32m    351\u001b[39m       eval_step, donate_argnames=(\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m,)\n\u001b[32m    352\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/tunix/sft/peft_trainer.py:322\u001b[39m, in \u001b[36mPeftTrainer._shard_optimizer\u001b[39m\u001b[34m(self, mesh)\u001b[39m\n\u001b[32m    318\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    319\u001b[39m optimizer_state = nnx.state(\n\u001b[32m    320\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer, nnx.optimizer.OptState\n\u001b[32m    321\u001b[39m )  \u001b[38;5;66;03m# select only the optimizer state\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m optimizer_shardings = \u001b[43mnnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_named_sharding\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m optimizer_sharded_state = jax.lax.with_sharding_constraint(\n\u001b[32m    324\u001b[39m     optimizer_state, optimizer_shardings\n\u001b[32m    325\u001b[39m )\n\u001b[32m    326\u001b[39m nnx.update(\u001b[38;5;28mself\u001b[39m.optimizer, optimizer_sharded_state)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/flax/nnx/spmd.py:151\u001b[39m, in \u001b[36mget_named_sharding\u001b[39m\u001b[34m(tree, mesh)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_named_sharding\u001b[39m(tree: A, mesh: jax.sharding.Mesh) -> A:\n\u001b[32m    150\u001b[39m   spec = get_partition_spec(tree)\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m   sharding = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43msharding\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNamedSharding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m sharding\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/jax/_src/tree.py:155\u001b[39m, in \u001b[36mmap\u001b[39m\u001b[34m(f, tree, is_leaf, *rest)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmap\u001b[39m(f: Callable[..., Any],\n\u001b[32m    116\u001b[39m         tree: Any,\n\u001b[32m    117\u001b[39m         *rest: Any,\n\u001b[32m    118\u001b[39m         is_leaf: Callable[[Any], \u001b[38;5;28mbool\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> Any:\n\u001b[32m    119\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Maps a multi-input function over pytree args to produce a new pytree.\u001b[39;00m\n\u001b[32m    120\u001b[39m \n\u001b[32m    121\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    153\u001b[39m \u001b[33;03m    - :func:`jax.tree.reduce`\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree_util\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/jax/_src/tree_util.py:362\u001b[39m, in \u001b[36mtree_map\u001b[39m\u001b[34m(f, tree, is_leaf, *rest)\u001b[39m\n\u001b[32m    360\u001b[39m leaves, treedef = tree_flatten(tree, is_leaf)\n\u001b[32m    361\u001b[39m all_leaves = [leaves] + [treedef.flatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreedef\u001b[49m\u001b[43m.\u001b[49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_leaves\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/jax/_src/tree_util.py:362\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    360\u001b[39m leaves, treedef = tree_flatten(tree, is_leaf)\n\u001b[32m    361\u001b[39m all_leaves = [leaves] + [treedef.flatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m treedef.unflatten(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(*all_leaves))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/flax/nnx/spmd.py:151\u001b[39m, in \u001b[36mget_named_sharding.<locals>.<lambda>\u001b[39m\u001b[34m(p)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_named_sharding\u001b[39m(tree: A, mesh: jax.sharding.Mesh) -> A:\n\u001b[32m    150\u001b[39m   spec = get_partition_spec(tree)\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m   sharding = jax.tree.map(\u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43msharding\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNamedSharding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m, spec)\n\u001b[32m    152\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m sharding\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/jax/_src/util.py:299\u001b[39m, in \u001b[36mcache.<locals>.wrap.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.check_tracer_leaks.value:\n\u001b[32m    298\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m f(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrace_context_in_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_ignore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m              \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/jax/_src/util.py:293\u001b[39m, in \u001b[36mcache.<locals>.wrap.<locals>.cached\u001b[39m\u001b[34m(_, *args, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache(max_size)\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached\u001b[39m(_, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/jax/_src/named_sharding.py:461\u001b[39m, in \u001b[36mcheck_pspec\u001b[39m\u001b[34m(mesh, spec, _manual_axes)\u001b[39m\n\u001b[32m    458\u001b[39m \u001b[38;5;129m@cache\u001b[39m(max_size=\u001b[32m128\u001b[39m, trace_context_in_key=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_pspec\u001b[39m(mesh, spec, _manual_axes=\u001b[38;5;28mfrozenset\u001b[39m()):\n\u001b[32m    460\u001b[39m   _check_unique_resources(spec, \u001b[33m\"\u001b[39m\u001b[33mNamedSharding spec\u001b[39m\u001b[33m\"\u001b[39m, mesh)\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m   \u001b[43m_check_mesh_resource_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    462\u001b[39m   _check_mesh_unreduced(mesh, spec)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/jax/_src/named_sharding.py:503\u001b[39m, in \u001b[36m_check_mesh_resource_axis\u001b[39m\u001b[34m(mesh, pspec)\u001b[39m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m p:\n\u001b[32m    502\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mesh.axis_names:\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    504\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResource axis: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpspec\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis not found in mesh: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(mesh.shape.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(mesh._name_to_type[p[\u001b[32m0\u001b[39m]] == mesh._name_to_type[r] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m p):\n\u001b[32m    507\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    508\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mAxisTypes should be the same in a tuple subset of PartitionSpec:\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    509\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpspec\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Got subset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with axis\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    510\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m types: (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(mesh._name_to_type[r])\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mr\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mp)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mValueError\u001b[39m: Resource axis: norm of PartitionSpec('norm',) is not found in mesh: ('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive')."
          ]
        }
      ],
      "source": [
        "logging_option = metrics_logger.MetricsLoggerOptions(\n",
        "    log_dir=\"/tmp/tensorboard/full\", flush_every_n_steps=20\n",
        ")\n",
        "training_config = peft_trainer.TrainingConfig(\n",
        "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "    max_steps=MAX_STEPS,\n",
        "    metrics_logging_options=logging_option,\n",
        ")\n",
        "trainer = peft_trainer.PeftTrainer(gemma, optax.adamw(1e-5), training_config)\n",
        "trainer = trainer.with_gen_model_input_fn(gen_model_input_fn)\n",
        "\n",
        "with jax.profiler.trace(os.path.join(PROFILING_DIR, \"full_training\")):\n",
        "  with mesh:\n",
        "    trainer.train(train_ds, validation_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gd-66SRT3Uv"
      },
      "source": [
        "### Training with LoRA/QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZVp0SYMT3Uv"
      },
      "outputs": [],
      "source": [
        "# Restart Colab runtime.\n",
        "\n",
        "training_config = peft_trainer.TrainingConfig(\n",
        "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "    max_steps=MAX_STEPS,\n",
        "    checkpoint_root_directory=CKPT_DIR,\n",
        ")\n",
        "lora_trainer = peft_trainer.PeftTrainer(\n",
        "    lora_gemma, optax.adamw(1e-3), training_config\n",
        ").with_gen_model_input_fn(gen_model_input_fn)\n",
        "\n",
        "with jax.profiler.trace(os.path.join(PROFILING_DIR, \"peft\")):\n",
        "  with mesh:\n",
        "    lora_trainer.train(train_ds, validation_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXUzmyVIT3Uv"
      },
      "source": [
        "### Compare profile results of different training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIw2rIKmT3Uv"
      },
      "source": [
        "<font size=3>Setup<font>           | <font size=3>Train Step Time<font> | <font size=3>Peak Memory Usage<font>\n",
        "---------------------------------- | ---------------------------------- | ------------------------------\n",
        "<font size=3>Full weights<font>        |   <font size=3>~1.22 s<font>     |   <font size=3>43.26 GiB<font>\n",
        "<font size=3>QLoRA<font>        |   <font size=3>~1.19 s<font>     |   <font size=3>28.14 GiB<font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QICLDHTkT3Uv"
      },
      "source": [
        "## Generate with the LoRA/QLoRA model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTsmUCK6T3Uv"
      },
      "source": [
        "The QLoRA model still cannot do English-to-French translation properly since we\n",
        "only trained for 100 steps. If you train it for longer, you will see better\n",
        "results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WohZCuY0T3Uw"
      },
      "outputs": [],
      "source": [
        "gemma_tokenizer = data_lib.GemmaTokenizer(\n",
        "    os.path.join(kaggle_ckpt_path, \"tokenizer.model\")\n",
        ")\n",
        "\n",
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_gemma,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")\n",
        "\n",
        "input_batch = [\n",
        "    \"Translate this into French:\\nHello, my name is Morgane.\\n\",\n",
        "    \"Translate this into French:\\nThis dish is delicious!\\n\",\n",
        "    \"Translate this into French:\\nI am a student.\\n\",\n",
        "    \"Translate this into French:\\nHow's the weather today?\\n\",\n",
        "]\n",
        "\n",
        "out_data = sampler(\n",
        "    input_strings=input_batch,\n",
        "    total_generation_steps=10,  # The number of steps performed when generating a response.\n",
        ")\n",
        "\n",
        "for input_string, out_string in zip(input_batch, out_data.text):\n",
        "  print(f\"----------------------\")\n",
        "  print(f\"Prompt:\\n{input_string}\")\n",
        "  print(f\"Output:\\n{out_string}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//third_party/py/tunix/google/examples/qlora_gemma:qlora_demo_colab",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv-py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
