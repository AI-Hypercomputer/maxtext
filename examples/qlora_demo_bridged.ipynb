{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-gq1gcpyCpT"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/tunix/blob/main/examples/qlora_demo.ipynb\" ><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "## Install necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RTlz7JP7yCpT"
      },
      "outputs": [],
      "source": [
        "!pip install -q kagglehub\n",
        "\n",
        "!pip install -q tensorflow\n",
        "!pip install -q tensorboardX\n",
        "!pip install -q grain\n",
        "!pip install -q jax==0.6.2 jaxlib==0.6.2\n",
        "# !pip install -q git+https://github.com/google/tunix\n",
        "! pip install -q ~/tunix/\n",
        "!pip install -q git+https://github.com/google/qwix\n",
        "\n",
        "!pip uninstall -q -y flax\n",
        "!pip install -q git+https://github.com/google/flax.git@5d5c907d1b5f45c97138289d5aa2e2e8452bf52e\n",
        "\n",
        "\n",
        "!pip install -q tensorflow-datasets\n",
        "\n",
        "!pip install -q git+https://github.com/AI-Hypercomputer/pathways-utils.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NYtRLk-ojcid"
      },
      "outputs": [],
      "source": [
        "# # If you want to upload your metrics to Weights & Biases, please install the package and login. Make sure to install `wandb` before importing `tunix`.\n",
        "# !pip install wandb\n",
        "\n",
        "# import wandb\n",
        "# wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HOQuCB0o7kV"
      },
      "source": [
        "If `wandb` is installed, you'll see a message like the one below when you start the experiment:\n",
        "\n",
        "```\n",
        "Tracking run with wandb version 0.21.0\n",
        "Run data is saved locally in /content/wandb/run-20250717_224322-kmvoi0ho\n",
        "Syncing run 2025-07-17_22-43-22 to Weights & Biases (docs)\n",
        "View project at https://wandb.ai/<wandb_username>/tunix?apiKey=<api_key>\n",
        "View run at https://wandb.ai/<wandb_username>/tunix/runs/kmvoi0ho?apiKey=<api_key>\n",
        "Do NOT share these links with anyone. They can be used to claim your runs.\n",
        "```\n",
        "\n",
        "After clicking the link, you will be directed to the following Weights & Biases metrics page which contain train metrics, eval metrics, system metrics, and various custom metric you wish to report:\n",
        "\n",
        "![wandb_train.png](data:image/png;base64,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)\n",
        "\n",
        "![wandb_eval.png](data:image/png;base64,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)\n",
        "\n",
        "![wandb_system.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-datasets in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (4.9.9)\n",
            "Requirement already satisfied: absl-py in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (2.3.1)\n",
            "Requirement already satisfied: array_record>=0.5.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (0.7.2)\n",
            "Requirement already satisfied: dm-tree in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (0.1.9)\n",
            "Requirement already satisfied: etils>=1.9.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (1.13.0)\n",
            "Requirement already satisfied: immutabledict in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (4.2.1)\n",
            "Requirement already satisfied: numpy in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (2.1.3)\n",
            "Requirement already satisfied: promise in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (5.29.5)\n",
            "Requirement already satisfied: psutil in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (7.0.0)\n",
            "Requirement already satisfied: pyarrow in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (20.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (2.32.4)\n",
            "Requirement already satisfied: simple_parsing in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (1.17.2)\n",
            "Requirement already satisfied: termcolor in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (3.1.0)\n",
            "Requirement already satisfied: toml in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (4.67.1)\n",
            "Requirement already satisfied: wrapt in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets) (1.17.2)\n",
            "Requirement already satisfied: einops in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (0.8.1)\n",
            "Requirement already satisfied: fsspec in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (2025.7.0)\n",
            "Requirement already satisfied: importlib_resources in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (6.5.2)\n",
            "Requirement already satisfied: typing_extensions in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (4.14.1)\n",
            "Requirement already satisfied: zipp in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (3.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow-datasets) (2025.7.9)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from dm-tree->tensorflow-datasets) (25.3.0)\n",
            "Requirement already satisfied: six in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from promise->tensorflow-datasets) (1.17.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from simple_parsing->tensorflow-datasets) (0.16)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow-metadata->tensorflow-datasets) (1.70.0)\n",
            "Requirement already satisfied: aqtp in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (0.8.4)\n",
            "Requirement already satisfied: absl-py in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from aqtp) (2.3.1)\n",
            "Requirement already satisfied: jax in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from aqtp) (0.6.2)\n",
            "Requirement already satisfied: jaxlib in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from aqtp) (0.6.2)\n",
            "Requirement already satisfied: flax in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from aqtp) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->aqtp) (2.1.3)\n",
            "Requirement already satisfied: msgpack in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->aqtp) (1.1.1)\n",
            "Requirement already satisfied: optax in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->aqtp) (0.2.5)\n",
            "Requirement already satisfied: orbax-checkpoint in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->aqtp) (0.11.20)\n",
            "Requirement already satisfied: tensorstore in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->aqtp) (0.1.76)\n",
            "Requirement already satisfied: rich>=11.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->aqtp) (14.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->aqtp) (4.14.1)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->aqtp) (6.0.2)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->aqtp) (0.1.9)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from jax->aqtp) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from jax->aqtp) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.12 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from jax->aqtp) (1.16.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from rich>=11.1->flax->aqtp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from rich>=11.1->flax->aqtp) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax->aqtp) (0.1.2)\n",
            "Requirement already satisfied: chex>=0.1.87 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from optax->flax->aqtp) (0.1.90)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from chex>=0.1.87->optax->flax->aqtp) (1.0.0)\n",
            "Requirement already satisfied: etils[epath,epy] in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint->flax->aqtp) (1.13.0)\n",
            "Requirement already satisfied: nest_asyncio in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint->flax->aqtp) (1.6.0)\n",
            "Requirement already satisfied: aiofiles in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint->flax->aqtp) (24.1.0)\n",
            "Requirement already satisfied: protobuf in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint->flax->aqtp) (5.29.5)\n",
            "Requirement already satisfied: humanize in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint->flax->aqtp) (4.12.3)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint->flax->aqtp) (3.20.1)\n",
            "Requirement already satisfied: fsspec in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from etils[epath,epy]->orbax-checkpoint->flax->aqtp) (2025.7.0)\n",
            "Requirement already satisfied: importlib_resources in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from etils[epath,epy]->orbax-checkpoint->flax->aqtp) (6.5.2)\n",
            "Requirement already satisfied: zipp in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from etils[epath,epy]->orbax-checkpoint->flax->aqtp) (3.23.0)\n",
            "Requirement already satisfied: pillow in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (11.3.0)\n",
            "Requirement already satisfied: omegaconf in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from omegaconf) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from omegaconf) (6.0.2)\n",
            "Requirement already satisfied: google-cloud-storage in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (3.2.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.26.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-cloud-storage) (2.40.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=2.15.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-cloud-storage) (2.25.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-cloud-storage) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.7.2 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.22.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-cloud-storage) (2.32.4)\n",
            "Requirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-cloud-storage) (1.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-api-core<3.0.0,>=2.15.0->google-cloud-storage) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-api-core<3.0.0,>=2.15.0->google-cloud-storage) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-api-core<3.0.0,>=2.15.0->google-cloud-storage) (1.26.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (4.9.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (2025.7.9)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.26.1->google-cloud-storage) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-datasets\n",
        "!pip install aqtp\n",
        "!pip install pillow>=11.1.0\n",
        "!pip install pillow\n",
        "!pip install omegaconf\n",
        "!pip install google-cloud-storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_text in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (2.19.0)\n",
            "Requirement already satisfied: transformers in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (4.53.1)\n",
            "Requirement already satisfied: tiktoken in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (0.9.0)\n",
            "Requirement already satisfied: tensorflow<2.20,>=2.19.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow_text) (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (2.3.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (3.4.0)\n",
            "Requirement already satisfied: packaging in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (65.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (1.73.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (2.1.3)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow_text) (0.37.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (2025.7.9)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (3.1.3)\n",
            "Requirement already satisfied: filelock in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from transformers) (0.33.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.7.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (0.45.1)\n",
            "Requirement already satisfied: rich in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (14.1.0)\n",
            "Requirement already satisfied: namex in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (0.1.0)\n",
            "Requirement already satisfied: optree in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow_text) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_text transformers tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEjoS0-JyCpT"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LshEMmSzx6W6"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "import time\n",
        "\n",
        "from flax import nnx\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import kagglehub\n",
        "import optax\n",
        "from orbax import checkpoint as ocp\n",
        "from qwix import lora\n",
        "from tunix.generate import sampler as sampler_lib\n",
        "from tunix.models.gemma import data as data_lib\n",
        "from tunix.models.gemma import gemma as gemma_lib\n",
        "from tunix.models.gemma import params as params_lib\n",
        "from tunix.sft import metrics_logger\n",
        "from tunix.sft import peft_trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnEZ_jXwypn-"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QBcMaL22T3Uu"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Model\n",
        "MESH = [(1, 8), (\"fsdp\", \"tp\")]\n",
        "# LoRA\n",
        "RANK = 16\n",
        "ALPHA = 2.0\n",
        "\n",
        "# Train\n",
        "MAX_STEPS = 100\n",
        "EVAL_EVERY_N_STEPS = 20\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "\n",
        "# Checkpoint saving\n",
        "INTERMEDIATE_CKPT_DIR = \"~/qlora_expt/content/intermediate_ckpt/\"\n",
        "CKPT_DIR = \"~/qlora_expt/content/ckpts/\"\n",
        "PROFILING_DIR = \"~/qlora_expt/content/profiling/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3s5Qg6xT3Uu"
      },
      "source": [
        "## Load Gemma 2B\n",
        "\n",
        "To load the model, you need to be on [Kaggle](https://www.kaggle.com/) and need\n",
        "to have agreed to the Gemma license\n",
        "[here](https://www.kaggle.com/models/google/gemma/flax/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "o_7Sk8d7T3Uu"
      },
      "outputs": [],
      "source": [
        "# Log in\n",
        "# if \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n",
        "  # kagglehub.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oJC3Hfh9T3Uv"
      },
      "outputs": [],
      "source": [
        "kaggle_ckpt_path = kagglehub.model_download(\"google/gemma/flax/2b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "r_5FWrq-T3Uv"
      },
      "outputs": [],
      "source": [
        "# # This is a workaround. The checkpoints on Kaggle don't work with NNX. So, we\n",
        "# # load the model, save the checkpoint locally, and then reload the model\n",
        "# # (sharded).\n",
        "# params = params_lib.load_and_format_params(os.path.join(kaggle_ckpt_path, \"2b\"))\n",
        "# gemma = gemma_lib.Transformer.from_params(params, version=\"2b\")\n",
        "# checkpointer = ocp.StandardCheckpointer()\n",
        "# _, state = nnx.split(gemma)\n",
        "# checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_ertAr6FT3Uv"
      },
      "outputs": [],
      "source": [
        "# # # Wait for the ckpt to save successfully.\n",
        "# time.sleep(60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nGYKuLyFT3Uv"
      },
      "outputs": [],
      "source": [
        "# # Delete the intermediate model to save memory.\n",
        "# del params\n",
        "# del gemma\n",
        "# del state\n",
        "# gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-01 23:12:41.595954: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754089961.608215 1845531 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754089961.611901 1845531 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754089961.622177 1845531 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754089961.622190 1845531 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754089961.622192 1845531 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754089961.622193 1845531 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# add the parent directory (one level up) to sys.path\n",
        "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../../maxtext')))\n",
        "\n",
        "# ! pip install -r ../../maxtext/requirements.txt\n",
        "\n",
        "import MaxText as mt\n",
        "from MaxText import pyconfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from MaxText.integration.tunix.tunix_adaptor import TunixMaxTextLlama\n",
        "from flax import linen as nn\n",
        "! pip install -q ~/tunix/\n",
        "\n",
        "def get_ref_maxtext_model():\n",
        "\n",
        "  #python3 -m MaxText.train MaxText/configs/base.yml base_output_directory=${BASE_OUTPUT_DIRECTORY} dataset_path=${DATASET_PATH} tokenizer_path=assets/tokenizer.gemma load_parameters_path=${CONVERTED_CHECKPOINT} per_device_batch_size=1 run_name=${FINETUNE_RUN_NAME} max_target_length=8192 steps=10 async_checkpointing=false model_name=gemma-2b checkpoint_period=5\n",
        "\n",
        "  #TODO: @mazumdera: change this to use Gemma2-2b-it\n",
        "  config = pyconfig.initialize(\n",
        "      [\"\", \"../../maxtext/MaxText/configs/base.yml\"], #TODO: @mazumdera: why decode.py?\n",
        "      base_output_directory=\"gs://dummy_output_dir\",  # This is not used in Tunix.\n",
        "      run_name=\"test-tunix-maxtext-llama3-8b\",\n",
        "      # dataset_path=we use Tunix's dataset\n",
        "      load_parameters_path=\"gs://maxtext-gemma/2b/\", #TODO: @mazumdera: change this to use checkpoint\n",
        "      # tokenizer_type=\"tiktoken\",\n",
        "      # tokenizer_path=\"assets/tokenizer_llama3.tiktoken\",\n",
        "      tokenizer_path=\"../../maxtext/assets/tokenizer.gemma\",\n",
        "      per_device_batch_size=1,\n",
        "      max_target_length=8192,\n",
        "      steps=10,\n",
        "      async_checkpointing=\"false\",\n",
        "      # model_name=\"llama3.1-8b\",\n",
        "      model_name=\"gemma-2b\",\n",
        "      checkpoint_period=5,\n",
        "      skip_jax_distributed_system=\"true\",\n",
        "      weight_dtype=\"bfloat16\",\n",
        "  )\n",
        "  \n",
        "  # checkpoint = mt.checkpointing.load_params_from_path(\n",
        "  #     load_parameters_from_path=\"gs://maxtext-gemma/2b/\",\n",
        "  #     abstract_unboxed_params=None,\n",
        "  #     checkpoint_storage_concurrent_gb=None,\n",
        "  # )\n",
        "  checkpoint = {}\n",
        "\n",
        "  def create_model(config):\n",
        "    return mt.from_pretrained(config, rngs=nnx.Rngs(params=0, dropout=1))\n",
        "\n",
        "  model = nnx.eval_shape(create_model, config=config)\n",
        "\n",
        "  @nnx.jit\n",
        "  def partial_init(checkpoint, config):\n",
        "    model = create_model(config)\n",
        "    nnx.update(model, checkpoint)\n",
        "    # shard model\n",
        "    state = nnx.state(model)\n",
        "    specs = nnx.get_partition_spec(state)\n",
        "    state = jax.lax.with_sharding_constraint(state, specs)\n",
        "    nnx.update(model, state)\n",
        "    return model\n",
        "\n",
        "  with jax.sharding.use_mesh(model.mesh), nn.logical_axis_rules(config.logical_axis_rules):\n",
        "    model = partial_init(checkpoint, config)\n",
        "  print(model)\n",
        "\n",
        "  \n",
        "  tunix_model = TunixMaxTextLlama(\n",
        "        base_model=model,\n",
        "        use_attention_mask=False,  # trust Tunix loss masking\n",
        "    )\n",
        "  mesh  = tunix_model.base.mesh\n",
        "  \n",
        "  #TODO: @mazumdera: change this to use llama3.1-8b\n",
        "  # model_config = None\n",
        "  # We can continue to use Tunix's model_config\n",
        "  model_config = gemma_lib.TransformerConfig.gemma2_2b()\n",
        "\n",
        "  # Add these lines to properly get the graph definition and state\n",
        "  graphdef, state = nnx.split(tunix_model)\n",
        "  tunix_model = nnx.merge(graphdef, state)  # Recreate model in proper NNX format\n",
        "    \n",
        "  \n",
        "  return tunix_model, mesh, model_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KCPPEEi3T3Uv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updating keys from env and command line: ['run_name', 'model_name', 'load_parameters_path', 'async_checkpointing', 'checkpoint_period', 'weight_dtype', 'base_output_directory', 'tokenizer_path', 'per_device_batch_size', 'steps', 'skip_jax_distributed_system', 'max_target_length']\n",
            "Running Model: gemma-2b\n",
            "Updating following parameters in config\n",
            "\n",
            "base_emb_dim: 2048\n",
            "base_num_query_heads: 8\n",
            "base_num_kv_heads: 1\n",
            "base_mlp_dim: 16384\n",
            "base_num_decoder_layers: 18\n",
            "head_dim: 256\n",
            "mlp_activations: ['gelu', 'linear']\n",
            "vocab_size: 256128\n",
            "decoder_block: gemma\n",
            "normalization_layer_epsilon: 1e-06\n",
            "logits_via_embedding: True\n",
            "Updating keys from model: ['base_emb_dim', 'base_num_query_heads', 'base_num_kv_heads', 'base_mlp_dim', 'base_num_decoder_layers', 'head_dim', 'mlp_activations', 'vocab_size', 'decoder_block', 'normalization_layer_epsilon', 'logits_via_embedding']\n",
            "Skipping jax distributed system due to skip_jax_distributed_system=True flag.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:2025-08-01 23:12:44,938:jax._src.xla_bridge:798: A Google TPU may be present on this machine, but either a TPU-enabled jaxlib or libtpu is not installed. Falling back to cpu.\n",
            "WARNING:jax._src.xla_bridge:A Google TPU may be present on this machine, but either a TPU-enabled jaxlib or libtpu is not installed. Falling back to cpu.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not using emergency checkpoint, ignoring local_checkpoint_directory, local_checkpoint_period, use_replicator_service and replicator_backup_interval_minutes\n",
            "dataset_type set to tfds, will use keys['dataset_path']='' and keys['dataset_name']='c4/en:3.0.1'\n",
            "Config param activations_in_float32: False\n",
            "Config param adam_b1: 0.9\n",
            "Config param adam_b2: 0.95\n",
            "Config param adam_eps: 1e-08\n",
            "Config param adam_eps_root: 0.0\n",
            "Config param adam_weight_decay: 0.1\n",
            "Config param add_bos: True\n",
            "Config param add_eos: True\n",
            "Config param allow_split_physical_axes: False\n",
            "Config param ar_cache_axis_order: 1,2,0,3\n",
            "Config param async_checkpointing: False\n",
            "Config param attention: autoselected\n",
            "Config param attention_type: global\n",
            "Config param attn_logits_soft_cap: None\n",
            "Config param autoregressive_decode_assert: \n",
            "Config param base_emb_dim: 2048\n",
            "Config param base_mlp_dim: 16384\n",
            "Config param base_moe_mlp_dim: 7168\n",
            "Config param base_num_decoder_layers: 18\n",
            "Config param base_num_kv_heads: 1\n",
            "Config param base_num_query_heads: 8\n",
            "Config param base_output_directory: gs://dummy_output_dir\n",
            "Config param beta_fast: 32\n",
            "Config param beta_slow: 1\n",
            "Config param capacity_factor: -1.0\n",
            "Config param cast_logits_to_fp32: True\n",
            "Config param checkpoint_conversion_fn: None\n",
            "Config param checkpoint_dir: gs://dummy_output_dir/test-tunix-maxtext-llama3-8b/checkpoints/\n",
            "Config param checkpoint_is_quantized: False\n",
            "Config param checkpoint_period: 5\n",
            "Config param checkpoint_storage_concurrent_gb: 96\n",
            "Config param checkpoint_storage_target_data_file_size_bytes: 2147483648\n",
            "Config param checkpoint_storage_use_ocdbt: True\n",
            "Config param checkpoint_storage_use_zarr3: True\n",
            "Config param chunk_attn_window_size: 0\n",
            "Config param collect_stack_trace: False\n",
            "Config param colocated_python_data_input: False\n",
            "Config param compile_topology: \n",
            "Config param compile_topology_num_slices: -1\n",
            "Config param compiled_trainstep_file: \n",
            "Config param compute_axis_order: 0,1,2,3\n",
            "Config param constant_bound_config: []\n",
            "Config param context: remat\n",
            "Config param context_parallel_load_balance: True\n",
            "Config param cosine_learning_rate_final_fraction: 0.1\n",
            "Config param custom_mesh: \n",
            "Config param data_sharding: (('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'),)\n",
            "Config param data_shuffle_seed: 0\n",
            "Config param dataset_name: c4/en:3.0.1\n",
            "Config param dataset_path: \n",
            "Config param dataset_type: tfds\n",
            "Config param dcn_autoregressive_parallelism: 1\n",
            "Config param dcn_context_autoregressive_parallelism: 1\n",
            "Config param dcn_context_parallelism: 1\n",
            "Config param dcn_data_parallelism: -1\n",
            "Config param dcn_expert_parallelism: 1\n",
            "Config param dcn_fsdp_parallelism: 1\n",
            "Config param dcn_fsdp_transpose_parallelism: 1\n",
            "Config param dcn_parallelism: [-1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Config param dcn_pipeline_parallelism: 1\n",
            "Config param dcn_sequence_parallelism: 1\n",
            "Config param dcn_tensor_parallelism: 1\n",
            "Config param dcn_tensor_sequence_parallelism: 1\n",
            "Config param dcn_tensor_transpose_parallelism: 1\n",
            "Config param decode_sampling_nucleus_p: -1\n",
            "Config param decode_sampling_strategy: greedy\n",
            "Config param decode_sampling_temperature: 1.0\n",
            "Config param decode_sampling_top_k: 0\n",
            "Config param decoder_block: DecoderBlockType.GEMMA\n",
            "Config param decoder_layer_input: device\n",
            "Config param dpo_beta: 0.1\n",
            "Config param dpo_label_smoothing: 0.0\n",
            "Config param dropout_rate: 0.0\n",
            "Config param dtype: bfloat16\n",
            "Config param dtype_mm: float32\n",
            "Config param dump_hlo: False\n",
            "Config param dump_hlo_delete_local_after: True\n",
            "Config param dump_hlo_gcs_dir: \n",
            "Config param dump_hlo_local_dir: /tmp/xla_dump/\n",
            "Config param dump_hlo_module_name: jit_train_step\n",
            "Config param dump_hlo_upload_all: False\n",
            "Config param dump_hlo_xla_flags: \n",
            "Config param dump_step: -1\n",
            "Config param emb_dim: 2048\n",
            "Config param enable_checkpoint_cloud_logger: False\n",
            "Config param enable_checkpointing: True\n",
            "Config param enable_data_shuffling: True\n",
            "Config param enable_dropout: True\n",
            "Config param enable_emergency_checkpoint: False\n",
            "Config param enable_gcp_goodput_metrics: True\n",
            "Config param enable_gcp_step_deviation_metrics: True\n",
            "Config param enable_goodput_recording: False\n",
            "Config param enable_jax_profiler: False\n",
            "Config param enable_llm_inference_pool: False\n",
            "Config param enable_model_warmup: False\n",
            "Config param enable_orbax_v1: False\n",
            "Config param enable_padding_causal_mask: True\n",
            "Config param enable_pathways_goodput: False\n",
            "Config param enable_prefix_caching: False\n",
            "Config param enable_single_controller: False\n",
            "Config param enable_single_replica_ckpt_restoring: False\n",
            "Config param enable_tensorboard: True\n",
            "Config param eval_data_columns: ['text']\n",
            "Config param eval_dataset_name: c4/en:3.0.1\n",
            "Config param eval_image_column: image\n",
            "Config param eval_interval: -1\n",
            "Config param eval_per_device_batch_size: 1.0\n",
            "Config param eval_split: validation\n",
            "Config param eval_steps: -1\n",
            "Config param expansion_factor_real_data: -1\n",
            "Config param final_logits_soft_cap: None\n",
            "Config param first_num_dense_layers: 0\n",
            "Config param float32_logits: False\n",
            "Config param float32_qk_product: False\n",
            "Config param force_unroll: False\n",
            "Config param freeze_vision_encoder_params: True\n",
            "Config param fused_mlp: False\n",
            "Config param fused_qkv: False\n",
            "Config param gcs_metrics: False\n",
            "Config param generate_slice: v5e-16\n",
            "Config param global_batch_size_to_eval_on: 1\n",
            "Config param global_batch_size_to_load: 1\n",
            "Config param global_batch_size_to_load_eval: 1\n",
            "Config param global_batch_size_to_train_on: 1\n",
            "Config param global_parameter_scale: 1\n",
            "Config param goodput_upload_interval_seconds: 30\n",
            "Config param gradient_accumulation_steps: 1\n",
            "Config param gradient_clipping_threshold: 1.0\n",
            "Config param grain_eval_files: \n",
            "Config param grain_file_type: arrayrecord\n",
            "Config param grain_train_files: \n",
            "Config param grain_worker_count: 1\n",
            "Config param grain_worker_count_eval: 1\n",
            "Config param hardware: tpu\n",
            "Config param head_dim: 256\n",
            "Config param heartbeat_reporting_interval_in_seconds: 5\n",
            "Config param hf_data_dir: \n",
            "Config param hf_eval_files: \n",
            "Config param hf_eval_split: \n",
            "Config param hf_path: \n",
            "Config param hf_train_files: \n",
            "Config param hidden_size_for_vit: 1408\n",
            "Config param ici_autoregressive_parallelism: 1\n",
            "Config param ici_context_autoregressive_parallelism: 1\n",
            "Config param ici_context_parallelism: 1\n",
            "Config param ici_data_parallelism: 1\n",
            "Config param ici_expert_parallelism: 1\n",
            "Config param ici_fsdp_parallelism: -1\n",
            "Config param ici_fsdp_transpose_parallelism: 1\n",
            "Config param ici_parallelism: [1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Config param ici_pipeline_parallelism: 1\n",
            "Config param ici_sequence_parallelism: 1\n",
            "Config param ici_tensor_parallelism: 1\n",
            "Config param ici_tensor_sequence_parallelism: 1\n",
            "Config param ici_tensor_transpose_parallelism: 1\n",
            "Config param image_path: \n",
            "Config param image_placeholder: <|image|>\n",
            "Config param image_size_for_vit: 896\n",
            "Config param inference_benchmark_test: False\n",
            "Config param inference_metadata_file: \n",
            "Config param inference_microbenchmark_log_file_path: \n",
            "Config param inference_microbenchmark_loop_iters: 10\n",
            "Config param inference_microbenchmark_num_samples: [1, 2, 3, 4, 5]\n",
            "Config param inference_microbenchmark_prefill_lengths: 64,128,256,512,1024\n",
            "Config param inference_microbenchmark_stages: prefill,generate\n",
            "Config param inference_server: MaxtextInterleavedServer\n",
            "Config param inhomogeneous_layer_cycle_interval: 1\n",
            "Config param init_weights_seed: 0\n",
            "Config param input_data_sharding_logical_axes: ['activation_embed_and_logits_batch', 'activation_norm_length']\n",
            "Config param interleave_moe_layer_step: 1\n",
            "Config param intermediate_size_for_vit: 5632\n",
            "Config param jax_cache_dir: ~/jax_cache\n",
            "Config param jax_debug_log_modules: \n",
            "Config param jax_distributed_initialization_timeout: 300\n",
            "Config param jax_profiler_port: 9999\n",
            "Config param key_proj: remat\n",
            "Config param kv_lora_rank: 512\n",
            "Config param kv_quant_axis: heads_and_dkv\n",
            "Config param kv_quant_dtype: int8\n",
            "Config param learning_rate: 3e-05\n",
            "Config param learning_rate_schedule_steps: 10\n",
            "Config param load_balance_loss_weight: 0.01\n",
            "Config param load_from_prefill_dir: False\n",
            "Config param load_full_state_path: \n",
            "Config param load_parameters_path: gs://maxtext-gemma/2b/\n",
            "Config param local_checkpoint_directory: \n",
            "Config param local_checkpoint_period: 0\n",
            "Config param local_rope_max_timescale: -1\n",
            "Config param log_config: True\n",
            "Config param log_period: 100\n",
            "Config param logical_axis_rules: (('activation_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_batch_no_exp', ('data', 'fsdp', 'fsdp_transpose')), ('activation_embed_and_logits_batch', ('data', 'stage', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence', 'autoregressive')), ('activation_kv_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_length', ('sequence', 'context')), ('prefill_activation_length', ('sequence', 'context')), ('activation_length', ('context',)), ('activation_norm_length', ('tensor_sequence', 'context', 'sequence')), ('prefill_activation_norm_length', ('tensor_sequence', 'context', 'sequence')), ('activation_q_length', ('context',)), ('activation_kv_length', ()), ('activation_embed', ('tensor', 'tensor_transpose')), ('activation_mlp', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_kv', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_prefill_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_head_dim', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose')), ('activation_vocab', 'tensor_sequence'), ('activation_vocab', ('sequence', 'context')), ('activation_stage', 'stage'), ('activation_exp', ('expert',)), ('decode_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('decode_length', ('sequence',)), ('mlp', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), ('vocab', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('q_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('kv_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'context', 'expert')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'context')), ('embed_tensor_transpose', ('tensor_transpose',)), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'expert')), ('norm', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('layers', 'stage'), ('kv', ()), ('kv_head_dim', ()), ('cache_batch_prefill', ()), ('cache_batch', ()), ('cache_heads_none', ()), ('cache_heads', ('autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence')), ('cache_heads', ('autoregressive', 'tensor', 'tensor_sequence')), ('cache_kv', ()), ('cache_sequence', ()), ('exp', 'expert'), ('paged_kv_heads', ('tensor',)), ('num_pages', ()), ('tokens_per_page', ()), ('paged_kv_head_dim_size', ()))\n",
            "Config param logits_dot_in_fp32: False\n",
            "Config param logits_via_embedding: True\n",
            "Config param lora_input_adapters_path: \n",
            "Config param matmul_precision: default\n",
            "Config param max_checkify: False\n",
            "Config param max_corpus_chars: 10000000\n",
            "Config param max_position_embeddings: 163840\n",
            "Config param max_prefill_predict_length: 64\n",
            "Config param max_target_length: 8192\n",
            "Config param megablox: True\n",
            "Config param mesh_axes: ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive']\n",
            "Config param metrics_dir: gs://dummy_output_dir/test-tunix-maxtext-llama3-8b/metrics/\n",
            "Config param metrics_file: \n",
            "Config param micro_batch_size_to_eval_on: 1\n",
            "Config param micro_batch_size_to_train_on: 1\n",
            "Config param mla_naive_kvcache: True\n",
            "Config param mlp_activations: ['gelu', 'linear']\n",
            "Config param mlp_dim: 16384\n",
            "Config param mlpwi: remat\n",
            "Config param mlpwi_0: remat\n",
            "Config param mlpwi_1: remat\n",
            "Config param mlpwo: remat\n",
            "Config param model_call_mode: \n",
            "Config param model_fsdp_ag_once: False\n",
            "Config param model_name: gemma-2b\n",
            "Config param moe_mlp_dim: 7168\n",
            "Config param monitor_goodput: False\n",
            "Config param monitor_step_time_deviation: True\n",
            "Config param mscale: 1.0\n",
            "Config param mtp_eval_target_module: 0\n",
            "Config param mtp_loss_scaling_factor: 0.1\n",
            "Config param mtp_num_layers: 0\n",
            "Config param mu_dtype: bfloat16\n",
            "Config param multi_sampling: False\n",
            "Config param n_routing_groups: -1\n",
            "Config param nope_layer_interval: -1\n",
            "Config param normalization_layer_epsilon: 1e-06\n",
            "Config param normalize_embedding_logits: True\n",
            "Config param num_attention_heads_for_vit: 16\n",
            "Config param num_channels_for_vit: 3\n",
            "Config param num_decoder_layers: 18\n",
            "Config param num_epoch: 1\n",
            "Config param num_experts: 1\n",
            "Config param num_experts_per_tok: 1\n",
            "Config param num_hidden_layers_for_vit: 34\n",
            "Config param num_kv_heads: 1\n",
            "Config param num_layers_per_pipeline_stage: 1\n",
            "Config param num_pipeline_microbatches: -1\n",
            "Config param num_pipeline_repeats: -1\n",
            "Config param num_query_heads: 8\n",
            "Config param num_slices: 1\n",
            "Config param opt_type: adamw\n",
            "Config param optimize_mesh_for_tpu_v6e: False\n",
            "Config param optimizer_memory_host_offload: False\n",
            "Config param original_max_position_embeddings: 4096\n",
            "Config param out_proj: remat\n",
            "Config param override_model_config: False\n",
            "Config param packing: True\n",
            "Config param pagedattn_head_dim_alignment: 128\n",
            "Config param pagedattn_max_pages_per_group: 256\n",
            "Config param pagedattn_num_pages: 64\n",
            "Config param pagedattn_pages_per_compute_block: 4\n",
            "Config param pagedattn_tokens_per_page: 32\n",
            "Config param param_scan_axis: 1\n",
            "Config param parameter_memory_host_offload: False\n",
            "Config param patch_size_for_vit: 14\n",
            "Config param per_device_batch_size: 1.0\n",
            "Config param pipeline_delay_activation_forwarding: False\n",
            "Config param pipeline_fsdp_ag_once: False\n",
            "Config param pipeline_parallel_layers: -1\n",
            "Config param pixel_shuffle_ratio_for_vit: 0.5\n",
            "Config param prefill_cache_axis_order: 1,2,0,3\n",
            "Config param prefill_cache_dir: \n",
            "Config param prefill_chunk_size: 256\n",
            "Config param prefill_slice: v5e-16\n",
            "Config param prefix_caching_dram_byte: 100000000000\n",
            "Config param prefix_caching_hbm_byte: 10000000000\n",
            "Config param profile_cleanly: True\n",
            "Config param profile_periodically_period: -1\n",
            "Config param profiler: \n",
            "Config param profiler_steps: 5\n",
            "Config param projector_dropout_for_vit: 0.0\n",
            "Config param projector_input_dim_for_vit: 4096\n",
            "Config param projector_output_dim_for_vit: 4096\n",
            "Config param prometheus_port: 0\n",
            "Config param prompt: I love to\n",
            "Config param q_lora_rank: 0\n",
            "Config param qk_nope_head_dim: 128\n",
            "Config param qk_rope_head_dim: 64\n",
            "Config param qkv_proj: remat\n",
            "Config param quant_cfg_path: \n",
            "Config param quantization: \n",
            "Config param quantization_local_shard_count: 1\n",
            "Config param quantize_kvcache: False\n",
            "Config param query_proj: remat\n",
            "Config param ragged_block_size: 256\n",
            "Config param record_internal_nn_metrics: 0\n",
            "Config param remat_policy: full\n",
            "Config param remat_policy_for_vit: minimal\n",
            "Config param replicate_quant_scale: False\n",
            "Config param replicator_backup_interval_minutes: 0\n",
            "Config param report_heartbeat_metric_for_gcp_monitoring: False\n",
            "Config param report_performance_metric_for_gcp_monitoring: False\n",
            "Config param reshape_q: False\n",
            "Config param return_log_prob: False\n",
            "Config param reuse_example_batch: 0\n",
            "Config param rope_factor: 40\n",
            "Config param rope_max_timescale: 10000\n",
            "Config param rope_min_timescale: 1\n",
            "Config param rope_theta_for_vit: 10000\n",
            "Config param rope_type: default\n",
            "Config param rope_use_scale: True\n",
            "Config param routed_bias: False\n",
            "Config param routed_scaling_factor: 1.0\n",
            "Config param routed_score_func: \n",
            "Config param run_name: test-tunix-maxtext-llama3-8b\n",
            "Config param sa_block_kv: 512\n",
            "Config param sa_block_kv_compute: 512\n",
            "Config param sa_block_kv_dkv: 512\n",
            "Config param sa_block_kv_dkv_compute: 512\n",
            "Config param sa_block_kv_dq: 512\n",
            "Config param sa_block_q: 512\n",
            "Config param sa_block_q_dkv: 512\n",
            "Config param sa_block_q_dq: 512\n",
            "Config param sa_k_layout: HEAD_DIM_MINOR\n",
            "Config param sa_q_layout: HEAD_DIM_MINOR\n",
            "Config param sa_use_fused_bwd_kernel: False\n",
            "Config param sa_v_layout: HEAD_DIM_MINOR\n",
            "Config param save_config_to_gcs: False\n",
            "Config param save_quantized_params_path: \n",
            "Config param scan_layers: True\n",
            "Config param scan_layers_per_stage: False\n",
            "Config param scan_pipeline_iterations: True\n",
            "Config param set_remat_policy_on_layers_per_stage: False\n",
            "Config param set_remat_policy_on_pipeline_iterations: True\n",
            "Config param sft_train_on_completion_only: False\n",
            "Config param sharding_tolerance: 0.02\n",
            "Config param shardy: True\n",
            "Config param shared_experts: 1\n",
            "Config param skip_first_n_steps_for_profiler: 1\n",
            "Config param skip_jax_distributed_system: True\n",
            "Config param sliding_window_size: 0\n",
            "Config param source_checkpoint_layout: orbax\n",
            "Config param sparse_matmul: True\n",
            "Config param stack_prefill_result_cache: False\n",
            "Config param stack_trace_interval_seconds: 600\n",
            "Config param stack_trace_to_cloud: False\n",
            "Config param step_deviation_interval_seconds: 30\n",
            "Config param steps: 10\n",
            "Config param subslice_shape: \n",
            "Config param target_eval_loss: 0.0\n",
            "Config param temperature_tuning: False\n",
            "Config param tensorboard_dir: gs://dummy_output_dir/test-tunix-maxtext-llama3-8b/tensorboard/\n",
            "Config param tile_activation_dim: 1024\n",
            "Config param tile_batch_seq: 512\n",
            "Config param tile_size_for_vit: 336\n",
            "Config param tile_weight_dim: 1024\n",
            "Config param tokenize_eval_data: True\n",
            "Config param tokenize_train_data: True\n",
            "Config param tokenizer_path: ../../maxtext/assets/tokenizer.gemma\n",
            "Config param tokenizer_type: sentencepiece\n",
            "Config param topk_routing_group: -1\n",
            "Config param train_data_columns: ['text']\n",
            "Config param train_image_column: image\n",
            "Config param train_split: train\n",
            "Config param trainable_position_size: -1\n",
            "Config param upload_all_profiler_results: False\n",
            "Config param use_chat_template: False\n",
            "Config param use_chunked_prefill: False\n",
            "Config param use_dpo: False\n",
            "Config param use_iota_embed: False\n",
            "Config param use_multimodal: False\n",
            "Config param use_post_attn_norm: False\n",
            "Config param use_post_ffw_norm: False\n",
            "Config param use_qk_norm: False\n",
            "Config param use_ragged_attention: False\n",
            "Config param use_random_routing: False\n",
            "Config param use_replicator_service: False\n",
            "Config param use_sft: False\n",
            "Config param use_untrainable_positional_embedding: False\n",
            "Config param use_vertex_tensorboard: False\n",
            "Config param using_pipeline_parallelism: False\n",
            "Config param v_head_dim: 128\n",
            "Config param value_proj: remat\n",
            "Config param vertex_tensorboard_project: \n",
            "Config param vertex_tensorboard_region: \n",
            "Config param vision_output_dim_for_vit: 4096\n",
            "Config param vocab_size: 256128\n",
            "Config param warmup_steps_fraction: 0.1\n",
            "Config param weight_dtype: bfloat16\n",
            "Num_devices: 1, shape (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
            "Num_devices: 1, shape (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
            "inputs=[[[1.78906 0.703125 1.24219 ... 1.04688 0.0439453 1.38281]]]\n",
            "lnx=[[[1.80469 0.710938 1.25 ... 1.05469 0.0444336 1.39844]]]\n",
            "attention_lnx=[[[-0.416016 0.960938 -0.0380859 ... -0.257812 0.46875 -2.42188]]]\n",
            "attn_output=[[[0.96875 1.17188 0.847656 ... 0.554688 0.361328 -0.734375]]]\n",
            "next_layer_addition_dropped_out=[[[1.63281 0.34375 -0.390625 ... 0.90625 0.496094 -0.394531]]]\n",
            "inputs=[[[1.63281 0.34375 -0.390625 ... 0.90625 0.496094 -0.394531]]]\n",
            "lnx=[[[1.04688 0.219727 -0.25 ... 0.578125 0.318359 -0.251953]]]\n",
            "attention_lnx=[[[-1 0.206055 -1.76562 ... 1.41406 0.125 0.71875]]]\n",
            "attn_output=[[[0.345703 0.298828 -1.17188 ... 1.26562 0.337891 0.176758]]]\n",
            "next_layer_addition_dropped_out=[[[0.546875 0.0859375 -3.4375 ... 2.01562 0.40625 0.0644531]]]\n",
            "inputs=[[[0.546875 0.0859375 -3.4375 ... 2.01562 0.40625 0.0644531]]]\n",
            "lnx=[[[0.279297 0.0439453 -1.75781 ... 1.03125 0.208008 0.032959]]]\n",
            "attention_lnx=[[[-0.402344 1.67969 0.507812 ... 1.08594 0.929688 -1.30469]]]\n",
            "attn_output=[[[0.065918 0.804688 -1.33594 ... 1.41406 0.609375 -0.566406]]]\n",
            "next_layer_addition_dropped_out=[[[-0.640625 2.8125 -2.73438 ... 3.64062 1.11719 -1.79688]]]\n",
            "inputs=[[[-0.640625 2.8125 -2.73438 ... 3.64062 1.11719 -1.79688]]]\n",
            "lnx=[[[-0.28125 1.23438 -1.19531 ... 1.59375 0.490234 -0.789062]]]\n",
            "attention_lnx=[[[-0.279297 -0.472656 -0.0227051 ... -0.277344 -1.28125 -0.90625]]]\n",
            "attn_output=[[[-0.376953 0.957031 -1.125 ... 1.375 -0.0673828 -1.10938]]]\n",
            "next_layer_addition_dropped_out=[[[-0.808594 1.75 -2.64062 ... 3.5625 -0.188477 -1.84375]]]\n",
            "inputs=[[[-0.808594 1.75 -2.64062 ... 3.5625 -0.188477 -1.84375]]]\n",
            "lnx=[[[-0.318359 0.691406 -1.03906 ... 1.40625 -0.0742188 -0.726562]]]\n",
            "attention_lnx=[[[0.380859 -0.0966797 1.14844 ... -0.277344 -0.131836 1.98438]]]\n",
            "attn_output=[[[-0.15625 0.605469 -0.546875 ... 1.20312 -0.117188 0.0512695]]]\n",
            "next_layer_addition_dropped_out=[[[-0.195312 1.21875 -2.42188 ... 4 -0.255859 2.82812]]]\n",
            "inputs=[[[-0.195312 1.21875 -2.42188 ... 4 -0.255859 2.82812]]]\n",
            "lnx=[[[-0.0688477 0.429688 -0.855469 ... 1.41406 -0.090332 1]]]\n",
            "attention_lnx=[[[-0.847656 0.5625 0.351562 ... 2.45312 -0.178711 -1.64062]]]\n",
            "attn_output=[[[-0.347656 0.59375 -0.691406 ... 2.15625 -0.145508 0.396484]]]\n",
            "next_layer_addition_dropped_out=[[[-2.875 0.976562 -2.54688 ... 6.40625 -0.550781 0.921875]]]\n",
            "inputs=[[[-2.875 0.976562 -2.54688 ... 6.40625 -0.550781 0.921875]]]\n",
            "lnx=[[[-0.9375 0.318359 -0.832031 ... 2.09375 -0.179688 0.300781]]]\n",
            "attention_lnx=[[[0.335938 -0.609375 -1.70312 ... 1.57812 -0.773438 -0.625]]]\n",
            "attn_output=[[[-0.789062 0.114258 -1.32031 ... 2.48438 -0.412109 0.0922852]]]\n",
            "next_layer_addition_dropped_out=[[[-2.28125 -0.167969 -4.6875 ... 8 -1.09375 1.14844]]]\n",
            "inputs=[[[-2.28125 -0.167969 -4.6875 ... 8 -1.09375 1.14844]]]\n",
            "lnx=[[[-0.695312 -0.0512695 -1.42969 ... 2.4375 -0.333984 0.351562]]]\n",
            "attention_lnx=[[[0.19043 -0.535156 0.0810547 ... 1.71875 -0.251953 -1.53125]]]\n",
            "attn_output=[[[-0.609375 -0.205078 -1.34375 ... 2.82812 -0.392578 -0.111328]]]\n",
            "next_layer_addition_dropped_out=[[[-2.375 -0.941406 -6.34375 ... 10.25 -1.33594 0.164062]]]\n",
            "inputs=[[[-2.375 -0.941406 -6.34375 ... 10.25 -1.33594 0.164062]]]\n",
            "lnx=[[[-0.679688 -0.269531 -1.8125 ... 2.9375 -0.382812 0.046875]]]\n",
            "attention_lnx=[[[-0.75 0.574219 -0.279297 ... 1.46875 0.660156 0.00872803]]]\n",
            "attn_output=[[[-0.851562 -0.100098 -1.80469 ... 3.1875 -0.18457 0.0471191]]]\n",
            "next_layer_addition_dropped_out=[[[-4.5625 0.175781 -7.53125 ... 12.4375 -1.32812 0.304688]]]\n",
            "inputs=[[[-4.5625 0.175781 -7.53125 ... 12.4375 -1.32812 0.304688]]]\n",
            "lnx=[[[-1.21875 0.046875 -2.01562 ... 3.3125 -0.353516 0.081543]]]\n",
            "attention_lnx=[[[1.6875 1.53906 -0.335938 ... -0.929688 0.601562 1.14844]]]\n",
            "attn_output=[[[-0.738281 0.441406 -2.01562 ... 2.95312 -0.186523 0.373047]]]\n",
            "next_layer_addition_dropped_out=[[[-3.375 1.95312 -8.0625 ... 11.6875 -2.1875 1.46875]]]\n",
            "inputs=[[[-3.375 1.95312 -8.0625 ... 11.6875 -2.1875 1.46875]]]\n",
            "lnx=[[[-0.851562 0.492188 -2.03125 ... 2.95312 -0.550781 0.371094]]]\n",
            "attention_lnx=[[[-0.890625 -0.188477 -1.10156 ... -0.0272217 0.667969 0.0383301]]]\n",
            "attn_output=[[[-1.03906 0.427734 -2.21875 ... 2.82812 -0.369141 0.365234]]]\n",
            "next_layer_addition_dropped_out=[[[-4.71875 1.82812 -9.75 ... 12.3125 -0.921875 -0.125]]]\n",
            "inputs=[[[-4.71875 1.82812 -9.75 ... 12.3125 -0.921875 -0.125]]]\n",
            "lnx=[[[-1.125 0.435547 -2.32812 ... 2.9375 -0.219727 -0.0297852]]]\n",
            "attention_lnx=[[[0.769531 -0.933594 0.902344 ... -0.699219 -0.535156 -0.644531]]]\n",
            "attn_output=[[[-0.914062 0.207031 -2.04688 ... 2.6875 -0.337891 -0.177734]]]\n",
            "next_layer_addition_dropped_out=[[[-3.39062 0.839844 -9.6875 ... 13.125 -1.21875 -0.816406]]]\n",
            "inputs=[[[-3.39062 0.839844 -9.6875 ... 13.125 -1.21875 -0.816406]]]\n",
            "lnx=[[[-0.773438 0.191406 -2.20312 ... 3 -0.277344 -0.186523]]]\n",
            "attention_lnx=[[[-1.20312 0.566406 0.679688 ... -0.515625 -0.285156 -1.67188]]]\n",
            "attn_output=[[[-1.01562 0.310547 -1.99219 ... 2.78125 -0.332031 -0.550781]]]\n",
            "next_layer_addition_dropped_out=[[[-4.0625 2.8125 -9 ... 13 -1.42188 -2.70312]]]\n",
            "inputs=[[[-4.0625 2.8125 -9 ... 13 -1.42188 -2.70312]]]\n",
            "lnx=[[[-0.882812 0.609375 -1.95312 ... 2.82812 -0.308594 -0.585938]]]\n",
            "attention_lnx=[[[0.75 -0.228516 2 ... -0.375 -1.21094 -0.75]]]\n",
            "attn_output=[[[-0.703125 0.550781 -1.49219 ... 2.6875 -0.558594 -0.734375]]]\n",
            "next_layer_addition_dropped_out=[[[-3.34375 2.48438 -6.625 ... 12.6875 -3.10938 -4.28125]]]\n",
            "inputs=[[[-3.34375 2.48438 -6.625 ... 12.6875 -3.10938 -4.28125]]]\n",
            "lnx=[[[-0.699219 0.519531 -1.39062 ... 2.65625 -0.652344 -0.898438]]]\n",
            "attention_lnx=[[[1.3125 0.0194092 1.22656 ... 1.60938 -0.4375 -0.5625]]]\n",
            "attn_output=[[[-0.416016 0.515625 -1.10938 ... 2.9375 -0.726562 -0.996094]]]\n",
            "next_layer_addition_dropped_out=[[[-2.53125 1.11719 -6.5 ... 13.125 -4.59375 -7.25]]]\n",
            "inputs=[[[-2.53125 1.11719 -6.5 ... 13.125 -4.59375 -7.25]]]\n",
            "lnx=[[[-0.511719 0.225586 -1.3125 ... 2.65625 -0.929688 -1.46094]]]\n",
            "attention_lnx=[[[-1.57031 0.816406 -1.03125 ... -1.99219 -0.847656 0.198242]]]\n",
            "attn_output=[[[-0.8125 0.382812 -1.49219 ... 2.20312 -1.07812 -1.39062]]]\n",
            "next_layer_addition_dropped_out=[[[-4.875 2.70312 -7.78125 ... 10.625 -5.3125 -6.375]]]\n",
            "inputs=[[[-4.875 2.70312 -7.78125 ... 10.625 -5.3125 -6.375]]]\n",
            "lnx=[[[-0.945312 0.523438 -1.50781 ... 2.0625 -1.03125 -1.23438]]]\n",
            "attention_lnx=[[[0.40625 1 -0.114258 ... -0.644531 -1.1875 -0.519531]]]\n",
            "attn_output=[[[-0.847656 0.703125 -1.5 ... 1.89062 -1.23438 -1.30469]]]\n",
            "next_layer_addition_dropped_out=[[[-5.875 3.34375 -7.96875 ... 8.4375 -5.84375 -7.71875]]]\n",
            "inputs=[[[-5.875 3.34375 -7.96875 ... 8.4375 -5.84375 -7.71875]]]\n",
            "lnx=[[[-1.09375 0.621094 -1.48438 ... 1.57031 -1.08594 -1.4375]]]\n",
            "attention_lnx=[[[-1.39844 -0.225586 -2.26562 ... 0.449219 -1.05469 0.160156]]]\n",
            "attn_output=[[[-1.32812 0.566406 -1.86719 ... 1.61719 -1.25781 -1.375]]]\n",
            "next_layer_addition_dropped_out=[[[-6.875 2.51562 -10.1875 ... 7.71875 -6.9375 -8.1875]]]\n",
            "\u001b[38;2;79;201;177mTransformerNNX\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # Param: 2,506,434,560 (5.0 GB), RngState: 4 (24 B), Total: 2,506,434,564 (5.0 GB)\u001b[0m\n",
            "  \u001b[38;2;156;220;254mconfig\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m<MaxText.pyconfig.HyperParameters object at 0x700a0e14a650>,\n",
            "  \u001b[38;2;156;220;254mdecoder\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mToNNX\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # Param: 1,981,884,416 (4.0 GB), RngState: 4 (24 B), Total: 1,981,884,420 (4.0 GB)\u001b[0m\n",
            "    \u001b[38;2;156;220;254mdecoder_norm\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'scale'\u001b[0m: \u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 2,048 (4.1 KB)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mArray\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;156;220;254mshape\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;182;207;169m2048\u001b[0m,\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16)\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mmesh\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'norm'\u001b[0m,\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding_rules\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mlinen_meta_type\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mLogicallyPartitioned\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m,\n",
            "    \u001b[38;2;156;220;254mlayers\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'mlp'\u001b[0m: \u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'wi_0'\u001b[0m: \u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'kernel'\u001b[0m: \u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 603,979,776 (1.2 GB)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mArray\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;156;220;254mshape\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;182;207;169m2048\u001b[0m, \u001b[38;2;182;207;169m18\u001b[0m, \u001b[38;2;182;207;169m16384\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16)\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mmesh\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'embed'\u001b[0m, \u001b[38;2;207;144;120m'layers'\u001b[0m, \u001b[38;2;207;144;120m'mlp'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding_rules\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mlinen_meta_type\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mLogicallyPartitioned\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m, \u001b[38;2;207;144;120m'wi_1'\u001b[0m: \u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'kernel'\u001b[0m: \u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 603,979,776 (1.2 GB)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mArray\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;156;220;254mshape\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;182;207;169m2048\u001b[0m, \u001b[38;2;182;207;169m18\u001b[0m, \u001b[38;2;182;207;169m16384\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16)\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mmesh\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'embed'\u001b[0m, \u001b[38;2;207;144;120m'layers'\u001b[0m, \u001b[38;2;207;144;120m'mlp'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding_rules\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mlinen_meta_type\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mLogicallyPartitioned\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m, \u001b[38;2;207;144;120m'wo'\u001b[0m: \u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'kernel'\u001b[0m: \u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 603,979,776 (1.2 GB)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mArray\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;156;220;254mshape\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;182;207;169m16384\u001b[0m, \u001b[38;2;182;207;169m18\u001b[0m, \u001b[38;2;182;207;169m2048\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16)\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mmesh\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'mlp'\u001b[0m, \u001b[38;2;207;144;120m'layers'\u001b[0m, \u001b[38;2;207;144;120m'embed'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding_rules\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mlinen_meta_type\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mLogicallyPartitioned\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m, \u001b[38;2;207;144;120m'pre_ffw_norm'\u001b[0m: \u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'scale'\u001b[0m: \u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 36,864 (73.7 KB)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mArray\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;156;220;254mshape\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;182;207;169m2048\u001b[0m, \u001b[38;2;182;207;169m18\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16)\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mmesh\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'norm'\u001b[0m, \u001b[38;2;207;144;120m'layers'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding_rules\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mlinen_meta_type\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mLogicallyPartitioned\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m, \u001b[38;2;207;144;120m'pre_self_attention_norm'\u001b[0m: \u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'scale'\u001b[0m: \u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 36,864 (73.7 KB)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mArray\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;156;220;254mshape\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;182;207;169m2048\u001b[0m, \u001b[38;2;182;207;169m18\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16)\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mmesh\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'norm'\u001b[0m, \u001b[38;2;207;144;120m'layers'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding_rules\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mlinen_meta_type\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mLogicallyPartitioned\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m, \u001b[38;2;207;144;120m'self_attention'\u001b[0m: \u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'key'\u001b[0m: \u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'kernel'\u001b[0m: \u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 9,437,184 (18.9 MB)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mArray\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;156;220;254mshape\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;182;207;169m2048\u001b[0m, \u001b[38;2;182;207;169m18\u001b[0m, \u001b[38;2;182;207;169m1\u001b[0m, \u001b[38;2;182;207;169m256\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16)\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mmesh\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'embed'\u001b[0m, \u001b[38;2;207;144;120m'layers'\u001b[0m, \u001b[38;2;207;144;120m'kv_heads'\u001b[0m, \u001b[38;2;207;144;120m'kv_head_dim'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding_rules\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mlinen_meta_type\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mLogicallyPartitioned\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m, \u001b[38;2;207;144;120m'out'\u001b[0m: \u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'kernel'\u001b[0m: \u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 75,497,472 (151.0 MB)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mArray\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;156;220;254mshape\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;182;207;169m8\u001b[0m, \u001b[38;2;182;207;169m18\u001b[0m, \u001b[38;2;182;207;169m256\u001b[0m, \u001b[38;2;182;207;169m2048\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16)\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mmesh\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'heads'\u001b[0m, \u001b[38;2;207;144;120m'layers'\u001b[0m, \u001b[38;2;207;144;120m'kv'\u001b[0m, \u001b[38;2;207;144;120m'embed'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding_rules\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mlinen_meta_type\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mLogicallyPartitioned\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m, \u001b[38;2;207;144;120m'query'\u001b[0m: \u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'kernel'\u001b[0m: \u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 75,497,472 (151.0 MB)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mArray\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;156;220;254mshape\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;182;207;169m2048\u001b[0m, \u001b[38;2;182;207;169m18\u001b[0m, \u001b[38;2;182;207;169m8\u001b[0m, \u001b[38;2;182;207;169m256\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16)\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mmesh\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'embed'\u001b[0m, \u001b[38;2;207;144;120m'layers'\u001b[0m, \u001b[38;2;207;144;120m'q_heads'\u001b[0m, \u001b[38;2;207;144;120m'kv'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding_rules\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mlinen_meta_type\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mLogicallyPartitioned\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m, \u001b[38;2;207;144;120m'value'\u001b[0m: \u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'kernel'\u001b[0m: \u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 9,437,184 (18.9 MB)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mArray\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;156;220;254mshape\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;182;207;169m2048\u001b[0m, \u001b[38;2;182;207;169m18\u001b[0m, \u001b[38;2;182;207;169m1\u001b[0m, \u001b[38;2;182;207;169m256\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16)\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mmesh\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'embed'\u001b[0m, \u001b[38;2;207;144;120m'layers'\u001b[0m, \u001b[38;2;207;144;120m'kv_heads'\u001b[0m, \u001b[38;2;207;144;120m'kv_head_dim'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding_rules\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mlinen_meta_type\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mLogicallyPartitioned\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m,\n",
            "    \u001b[38;2;156;220;254mto_nnx__module\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mDecoder(\n",
            "        # attributes\n",
            "        config = <MaxText.pyconfig.HyperParameters object at 0x700a0e14a650>\n",
            "        mesh = Mesh(device_ids=array([[[[[[[[[[[[0]]]]]]]]]]]]), axis_names=('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'), axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto))\n",
            "        quant = None\n",
            "    ),\n",
            "    \u001b[38;2;156;220;254mto_nnx__rngs\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mRngs\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # RngState: 4 (24 B)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mdropout\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mRngStream\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # RngState: 2 (12 B)\u001b[0m\n",
            "        \u001b[38;2;156;220;254mcount\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mRngCount\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (4 B)\u001b[0m\n",
            "          \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(1, dtype=uint32),\n",
            "          \u001b[38;2;156;220;254mtag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'dropout'\u001b[0m\n",
            "        \u001b[38;2;255;213;3m)\u001b[0m,\n",
            "        \u001b[38;2;156;220;254mkey\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mRngKey\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n",
            "          \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray((), dtype=key<fry>) overlaying:\n",
            "          [ 507451445 1853169794],\n",
            "          \u001b[38;2;156;220;254mtag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'dropout'\u001b[0m\n",
            "        \u001b[38;2;255;213;3m)\u001b[0m,\n",
            "        \u001b[38;2;156;220;254mtag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'dropout'\u001b[0m\n",
            "      \u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mparams\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mRngStream\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # RngState: 2 (12 B)\u001b[0m\n",
            "        \u001b[38;2;156;220;254mcount\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mRngCount\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (4 B)\u001b[0m\n",
            "          \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(1, dtype=uint32),\n",
            "          \u001b[38;2;156;220;254mtag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'params'\u001b[0m\n",
            "        \u001b[38;2;255;213;3m)\u001b[0m,\n",
            "        \u001b[38;2;156;220;254mkey\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mRngKey\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n",
            "          \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray((), dtype=key<fry>) overlaying:\n",
            "          [ 928981903 3453687069],\n",
            "          \u001b[38;2;156;220;254mtag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'params'\u001b[0m\n",
            "        \u001b[38;2;255;213;3m)\u001b[0m,\n",
            "        \u001b[38;2;156;220;254mtag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'params'\u001b[0m\n",
            "      \u001b[38;2;255;213;3m)\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m\n",
            "  \u001b[38;2;255;213;3m)\u001b[0m,\n",
            "  \u001b[38;2;156;220;254mmesh\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mMesh(device_ids=array([[[[[[[[[[[[0]]]]]]]]]]]]), axis_names=('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'), axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)),\n",
            "  \u001b[38;2;156;220;254mquant\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "  \u001b[38;2;156;220;254mtoken_embedder\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mEmbed\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # Param: 524,550,144 (1.0 GB)\u001b[0m\n",
            "    \u001b[38;2;156;220;254mattend_dtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16),\n",
            "    \u001b[38;2;156;220;254mcast_input_dtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "    \u001b[38;2;156;220;254mconfig\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m<MaxText.pyconfig.HyperParameters object at 0x700a0e14a650>,\n",
            "    \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16),\n",
            "    \u001b[38;2;156;220;254membedding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 524,550,144 (1.0 GB)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mArray\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;156;220;254mshape\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;182;207;169m256128\u001b[0m, \u001b[38;2;182;207;169m2048\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16)\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'vocab'\u001b[0m, \u001b[38;2;207;144;120m'embed'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m,\n",
            "    \u001b[38;2;156;220;254mnum_embeddings\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;182;207;169m256128\u001b[0m,\n",
            "    \u001b[38;2;156;220;254mnum_features\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;182;207;169m2048\u001b[0m\n",
            "  \u001b[38;2;255;213;3m)\u001b[0m,\n",
            "  \u001b[38;2;156;220;254mvision_encoder\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m\n",
            "\u001b[38;2;255;213;3m)\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages/treescope/renderers.py:251: UserWarning: Ignoring error inside wrapper hook <function replace_with_canonical_aliases at 0x700b2aff8540>:\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages/treescope/renderers.py\", line 225, in _render_subtree\n",
            "    postprocessed_result = hook(\n",
            "                           ^^^^^\n",
            "  File \"/home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages/treescope/_internal/handlers/canonical_alias_postprocessor.py\", line 78, in replace_with_canonical_aliases\n",
            "    maybe_alias = canonical_aliases.lookup_alias(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages/treescope/canonical_aliases.py\", line 317, in lookup_alias\n",
            "    unwrapped = inspect.unwrap(the_object)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 756, in unwrap\n",
            "    while not isinstance(func, type) and hasattr(func, '__wrapped__'):\n",
            "                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mazumdera_google_com/maxtext/MaxText/pyconfig.py\", line 1120, in __getattr__\n",
            "    return object.__getattribute__(self, \"_config\").keys[attr]\n",
            "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
            "KeyError: '__wrapped__'\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<script> (()=>{ if (customElements.get('treescope-container') === undefined) { class TreescopeContainer extends HTMLElement { constructor() { super(); this.attachShadow({mode: \"open\"}); this.defns = {}; this.state = {}; } } customElements.define(\"treescope-container\", TreescopeContainer); } if (customElements.get('treescope-run-here') === undefined) { class RunHere extends HTMLElement { constructor() { super() } connectedCallback() { const run = child => { const fn = new Function(child.textContent); child.textContent = \"\"; fn.call(this); this.remove(); }; const child = this.querySelector(\"script\"); if (child) { run(child); } else { new MutationObserver(()=>{ run(this.querySelector(\"script\")); }).observe(this, {childList: true}); } } } customElements.define(\"treescope-run-here\", RunHere); } })(); </script> <treescope-container class=\"treescope_out_e918ba094dd847d18e2586b11a706b0e\" style=\"display:block\"></treescope-container> <treescope-run-here><script type=\"application/octet-stream\"> const root = ( Array.from(document.getElementsByClassName( \"treescope_out_e918ba094dd847d18e2586b11a706b0e\")) .filter((elt) => !elt.dataset.setup) )[0]; root.dataset.setup = 1; const msg = document.createElement(\"span\"); msg.style = \"color: #cccccc; font-family: monospace;\"; msg.textContent = \"(Loading...)\"; root.state.loadingMsg = msg; root.shadowRoot.appendChild(msg); root.state.chain = new Promise((resolve, reject) => { const observer = new IntersectionObserver((entries) => { for (const entry of entries) { if (entry.isIntersecting) { resolve(); observer.disconnect(); return; } } }, {rootMargin: \"1000px\"}); window.setTimeout(() => { observer.observe(root); }, 0); }); root.state.deferring = false; const _insertNode = (node) => { for (let oldScript of node.querySelectorAll(\"script\")) { let newScript = document.createElement(\"script\"); newScript.type = oldScript.type; newScript.textContent = oldScript.textContent; oldScript.parentNode.replaceChild(newScript, oldScript); } if (root.state.loadingMsg) { root.state.loadingMsg.remove(); root.state.loadingMsg = null; } root.shadowRoot.appendChild(node); }; root.defns.insertContent = ((contentNode, compressed) => { if (compressed) { root.state.deferring = true; } if (root.state.deferring) { root.state.chain = (async () => { await root.state.chain; if (compressed) { const encoded = contentNode.textContent; const blob = new Blob([ Uint8Array.from(atob(encoded), (m) => m.codePointAt(0)) ]); const reader = blob.stream().pipeThrough( new DecompressionStream(\"deflate\") ).pipeThrough( new TextDecoderStream(\"utf-8\") ).getReader(); const parts = []; while (true) { const step = await reader.read(); if (step.done) { break; } parts.push(step.value); } const tpl = document.createElement('template'); tpl.innerHTML = parts.join(\"\"); _insertNode(tpl.content); } else { _insertNode(contentNode.content); } })(); } else { _insertNode(contentNode.content); } }); </script></treescope-run-here><div style=\"display:none\"> <script type=\"application/octet-stream\" >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</script> <treescope-run-here><script type=\"application/octet-stream\"> const root = ( Array.from(document.getElementsByClassName( \"treescope_out_e918ba094dd847d18e2586b11a706b0e\")) .filter((elt) => !elt.dataset['step0']) )[0]; root.dataset['step0'] = 1; root.defns.insertContent( this.parentNode.querySelector('script[type=\"application/octet-stream\"]'), true ); this.parentNode.remove(); </script></treescope-run-here> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div style=\"display:none\"> <script type=\"application/octet-stream\" >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</script> <treescope-run-here><script type=\"application/octet-stream\"> const root = ( Array.from(document.getElementsByClassName( \"treescope_out_e918ba094dd847d18e2586b11a706b0e\")) .filter((elt) => !elt.dataset['step1']) )[0]; root.dataset['step1'] = 1; root.defns.insertContent( this.parentNode.querySelector('script[type=\"application/octet-stream\"]'), true ); this.parentNode.remove(); </script></treescope-run-here> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized successfully\n",
            "Model mesh shape: OrderedDict([('data', 1), ('stage', 1), ('fsdp', 1), ('fsdp_transpose', 1), ('sequence', 1), ('context', 1), ('context_autoregressive', 1), ('tensor', 1), ('tensor_transpose', 1), ('tensor_sequence', 1), ('expert', 1), ('autoregressive', 1)])\n",
            "Model config: TransformerConfig(num_layers=26, num_embed=256128, embed_dim=2304, hidden_dim=9216, num_heads=8, head_dim=256, num_kv_heads=4, final_logit_softcap=30.0, use_post_attn_norm=True, use_post_ffw_norm=True, attention_types=(<AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>), attn_logits_soft_cap=50.0, sliding_window_size=4096, shd_config=ShardingConfig(emb_vd=('tp', 'fsdp'), q_weight_ndh=('tp', 'fsdp', None), kv_weight_cndh=(None, 'tp', 'fsdp', None), qkv_weight_cndh=(None, 'tp', 'fsdp', None), o_weight_nhd=('tp', None, 'fsdp'), ffw_weight_df=('fsdp', 'tp'), ffw_weight_fd=('tp', 'fsdp'), rms_norm_weight=('tp',), act_btd=('fsdp', None, 'tp'), act_btf=('fsdp', None, 'tp'), act_btnh=('fsdp', None, 'tp', None)))\n"
          ]
        }
      ],
      "source": [
        "# Base model\n",
        "# gemma, mesh, model_config = get_base_model(\n",
        "#     ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\n",
        "# )\n",
        "\n",
        "gemma, mesh, model_config = get_ref_maxtext_model()\n",
        "# gemma_maxtext_nnx = nnx.bridge.ToNNX(gemma)\n",
        "# Instead of:\n",
        "nnx.display(gemma)\n",
        "\n",
        "# Use:\n",
        "print(\"Model initialized successfully\")\n",
        "print(f\"Model mesh shape: {mesh.shape}\")\n",
        "print(f\"Model config: {model_config}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TunixMaxTextLlama( # Param: 2,506,434,560 (5.0 GB), RngState: 4 (24 B), Total: 2,506,434,564 (5.0 GB)\n",
              "  base=TransformerNNX( # Param: 2,506,434,560 (5.0 GB), RngState: 4 (24 B), Total: 2,506,434,564 (5.0 GB)\n",
              "    config=<MaxText.pyconfig.HyperParameters object at 0x700a0e14a650>,\n",
              "    decoder=ToNNX( # Param: 1,981,884,416 (4.0 GB), RngState: 4 (24 B), Total: 1,981,884,420 (4.0 GB)\n",
              "      decoder_norm={'scale': Param( # 2,048 (4.1 KB)\n",
              "        value=Array(shape=(2048,), dtype=dtype(bfloat16)),\n",
              "        mesh=None,\n",
              "        sharding=('norm',),\n",
              "        sharding_rules=None,\n",
              "        linen_meta_type=LogicallyPartitioned\n",
              "      )},\n",
              "      layers={'mlp': {'wi_0': {'kernel': Param( # 603,979,776 (1.2 GB)\n",
              "        value=Array(shape=(2048, 18, 16384), dtype=dtype(bfloat16)),\n",
              "        mesh=None,\n",
              "        sharding=('embed', 'layers', 'mlp'),\n",
              "        sharding_rules=None,\n",
              "        linen_meta_type=LogicallyPartitioned\n",
              "      )}, 'wi_1': {'kernel': Param( # 603,979,776 (1.2 GB)\n",
              "        value=Array(shape=(2048, 18, 16384), dtype=dtype(bfloat16)),\n",
              "        mesh=None,\n",
              "        sharding=('embed', 'layers', 'mlp'),\n",
              "        sharding_rules=None,\n",
              "        linen_meta_type=LogicallyPartitioned\n",
              "      )}, 'wo': {'kernel': Param( # 603,979,776 (1.2 GB)\n",
              "        value=Array(shape=(16384, 18, 2048), dtype=dtype(bfloat16)),\n",
              "        mesh=None,\n",
              "        sharding=('mlp', 'layers', 'embed'),\n",
              "        sharding_rules=None,\n",
              "        linen_meta_type=LogicallyPartitioned\n",
              "      )}}, 'pre_ffw_norm': {'scale': Param( # 36,864 (73.7 KB)\n",
              "        value=Array(shape=(2048, 18), dtype=dtype(bfloat16)),\n",
              "        mesh=None,\n",
              "        sharding=('norm', 'layers'),\n",
              "        sharding_rules=None,\n",
              "        linen_meta_type=LogicallyPartitioned\n",
              "      )}, 'pre_self_attention_norm': {'scale': Param( # 36,864 (73.7 KB)\n",
              "        value=Array(shape=(2048, 18), dtype=dtype(bfloat16)),\n",
              "        mesh=None,\n",
              "        sharding=('norm', 'layers'),\n",
              "        sharding_rules=None,\n",
              "        linen_meta_type=LogicallyPartitioned\n",
              "      )}, 'self_attention': {'key': {'kernel': Param( # 9,437,184 (18.9 MB)\n",
              "        value=Array(shape=(2048, 18, 1, 256), dtype=dtype(bfloat16)),\n",
              "        mesh=None,\n",
              "        sharding=('embed', 'layers', 'kv_heads', 'kv_head_dim'),\n",
              "        sharding_rules=None,\n",
              "        linen_meta_type=LogicallyPartitioned\n",
              "      )}, 'out': {'kernel': Param( # 75,497,472 (151.0 MB)\n",
              "        value=Array(shape=(8, 18, 256, 2048), dtype=dtype(bfloat16)),\n",
              "        mesh=None,\n",
              "        sharding=('heads', 'layers', 'kv', 'embed'),\n",
              "        sharding_rules=None,\n",
              "        linen_meta_type=LogicallyPartitioned\n",
              "      )}, 'query': {'kernel': Param( # 75,497,472 (151.0 MB)\n",
              "        value=Array(shape=(2048, 18, 8, 256), dtype=dtype(bfloat16)),\n",
              "        mesh=None,\n",
              "        sharding=('embed', 'layers', 'q_heads', 'kv'),\n",
              "        sharding_rules=None,\n",
              "        linen_meta_type=LogicallyPartitioned\n",
              "      )}, 'value': {'kernel': Param( # 9,437,184 (18.9 MB)\n",
              "        value=Array(shape=(2048, 18, 1, 256), dtype=dtype(bfloat16)),\n",
              "        mesh=None,\n",
              "        sharding=('embed', 'layers', 'kv_heads', 'kv_head_dim'),\n",
              "        sharding_rules=None,\n",
              "        linen_meta_type=LogicallyPartitioned\n",
              "      )}}},\n",
              "      to_nnx__module=Decoder(\n",
              "          # attributes\n",
              "          config = <MaxText.pyconfig.HyperParameters object at 0x700a0e14a650>\n",
              "          mesh = Mesh(device_ids=array([[[[[[[[[[[[0]]]]]]]]]]]]), axis_names=('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'), axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto))\n",
              "          quant = None\n",
              "      ),\n",
              "      to_nnx__rngs=Rngs( # RngState: 4 (24 B)\n",
              "        dropout=RngStream( # RngState: 2 (12 B)\n",
              "          count=RngCount( # 1 (4 B)\n",
              "            value=Array(1, dtype=uint32),\n",
              "            tag='dropout'\n",
              "          ),\n",
              "          key=RngKey( # 1 (8 B)\n",
              "            value=Array((), dtype=key<fry>) overlaying:\n",
              "            [ 507451445 1853169794],\n",
              "            tag='dropout'\n",
              "          ),\n",
              "          tag='dropout'\n",
              "        ),\n",
              "        params=RngStream( # RngState: 2 (12 B)\n",
              "          count=RngCount( # 1 (4 B)\n",
              "            value=Array(1, dtype=uint32),\n",
              "            tag='params'\n",
              "          ),\n",
              "          key=RngKey( # 1 (8 B)\n",
              "            value=Array((), dtype=key<fry>) overlaying:\n",
              "            [ 928981903 3453687069],\n",
              "            tag='params'\n",
              "          ),\n",
              "          tag='params'\n",
              "        )\n",
              "      )\n",
              "    ),\n",
              "    mesh=Mesh(device_ids=array([[[[[[[[[[[[0]]]]]]]]]]]]), axis_names=('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'), axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)),\n",
              "    quant=None,\n",
              "    token_embedder=Embed( # Param: 524,550,144 (1.0 GB)\n",
              "      attend_dtype=dtype(bfloat16),\n",
              "      cast_input_dtype=None,\n",
              "      config=<MaxText.pyconfig.HyperParameters object at 0x700a0e14a650>,\n",
              "      dtype=dtype(bfloat16),\n",
              "      embedding=Param( # 524,550,144 (1.0 GB)\n",
              "        value=Array(shape=(256128, 2048), dtype=dtype(bfloat16)),\n",
              "        sharding=('vocab', 'embed')\n",
              "      ),\n",
              "      num_embeddings=256128,\n",
              "      num_features=2048\n",
              "    ),\n",
              "    vision_encoder=None\n",
              "  ),\n",
              "  use_attention_mask=False\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gemma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs=[[[-1.35156 -0.482422 -1.30469 ... -0.667969 -0.152344 1.54688]\n",
            "  [-0.353516 1.49219 -0.503906 ... -1.21875 1.28125 -1.57812]\n",
            "  [0.609375 0.162109 -1.30469 ... -0.667969 -0.439453 -0.0344238]\n",
            "  [-0.644531 -2.32812 1.04688 ... -1.03125 1.625 -0.621094]]]\n",
            "lnx=[[[-1.375 -0.490234 -1.32031 ... -0.675781 -0.154297 1.57031]\n",
            "  [-0.355469 1.5 -0.507812 ... -1.22656 1.28906 -1.58594]\n",
            "  [0.597656 0.15918 -1.28125 ... -0.652344 -0.429688 -0.0336914]\n",
            "  [-0.644531 -2.32812 1.04688 ... -1.03125 1.625 -0.621094]]]\n",
            "attention_lnx=[[[-0.617188 -0.996094 0.318359 ... -1.46875 0.585938 -0.371094]\n",
            "  [0.443359 -0.90625 -0.9375 ... 0.412109 0.824219 -0.851562]\n",
            "  [0.255859 -0.228516 0.0761719 ... -0.271484 0.644531 -0.0152588]\n",
            "  [1.10938 0.466797 -0.90625 ... 0.203125 0.542969 -1.10156]]]\n",
            "attn_output=[[[-1.39062 -1.03906 -0.695312 ... -1.50781 0.306641 0.828125]\n",
            "  [0.0698242 0.455078 -1.11719 ... -0.625 1.63281 -1.88281]\n",
            "  [0.699219 -0.0534668 -0.992188 ... -0.757812 0.165039 -0.0400391]\n",
            "  [0.400391 -1.60156 0.121094 ... -0.714844 1.86719 -1.48438]]]\n",
            "next_layer_addition_dropped_out=[[[-2.26562 -1.40625 -1.60938 ... -2.65625 1.98438 1.84375]\n",
            "  [0.490234 0.65625 -0.855469 ... -0.882812 2.01562 -1.875]\n",
            "  [-0.132812 -0.714844 -1.35938 ... -1.125 0.244141 -0.0217285]\n",
            "  [-0.234375 -1.49219 -0.164062 ... -0.320312 3.0625 -2.45312]]]\n",
            "inputs=[[[-2.26562 -1.40625 -1.60938 ... -2.65625 1.98438 1.84375]\n",
            "  [0.490234 0.65625 -0.855469 ... -0.882812 2.01562 -1.875]\n",
            "  [-0.132812 -0.714844 -1.35938 ... -1.125 0.244141 -0.0217285]\n",
            "  [-0.234375 -1.49219 -0.164062 ... -0.320312 3.0625 -2.45312]]]\n",
            "lnx=[[[-1.42969 -0.886719 -1.01562 ... -1.67969 1.25 1.16406]\n",
            "  [0.341797 0.458984 -0.597656 ... -0.617188 1.40625 -1.3125]\n",
            "  [-0.0942383 -0.507812 -0.960938 ... -0.796875 0.172852 -0.0153809]\n",
            "  [-0.171875 -1.09375 -0.120117 ... -0.235352 2.25 -1.79688]]]\n",
            "attention_lnx=[[[1.17969 0.157227 1.22656 ... 0.9375 2.07812 0.243164]\n",
            "  [1.40625 0.28125 2.65625 ... 0.476562 1.39062 0.339844]\n",
            "  [0.679688 -0.691406 2.10938 ... -0.314453 1.21094 -0.394531]\n",
            "  [1.07031 -0.753906 1.77344 ... 0.710938 1.64844 -0.261719]]]\n",
            "attn_output=[[[-0.589844 -0.679688 -0.208008 ... -0.933594 2.20312 1.13281]\n",
            "  [1.13281 0.558594 1.07812 ... -0.242188 2.03125 -0.914062]\n",
            "  [0.347656 -0.894531 0.476562 ... -0.914062 0.925781 -0.265625]\n",
            "  [0.542969 -1.46094 1.04688 ... 0.253906 3.0625 -1.76562]]]\n",
            "next_layer_addition_dropped_out=[[[-1.08594 -1.35938 -1.82812 ... -2.34375 3.90625 0.9375]\n",
            "  [2.28125 0.671875 0.40625 ... -0.644531 3.375 -1.28125]\n",
            "  [1.3125 -1.26562 0.451172 ... -2.60938 0.835938 0.0351562]\n",
            "  [0.0546875 -3 0.84375 ... -0.519531 4.53125 -2.95312]]]\n",
            "inputs=[[[-1.08594 -1.35938 -1.82812 ... -2.34375 3.90625 0.9375]\n",
            "  [2.28125 0.671875 0.40625 ... -0.644531 3.375 -1.28125]\n",
            "  [1.3125 -1.26562 0.451172 ... -2.60938 0.835938 0.0351562]\n",
            "  [0.0546875 -3 0.84375 ... -0.519531 4.53125 -2.95312]]]\n",
            "lnx=[[[-0.550781 -0.691406 -0.929688 ... -1.19531 1.98438 0.476562]\n",
            "  [1.25781 0.371094 0.223633 ... -0.355469 1.85938 -0.707031]\n",
            "  [0.765625 -0.738281 0.263672 ... -1.52344 0.488281 0.0205078]\n",
            "  [0.0327148 -1.78906 0.503906 ... -0.310547 2.70312 -1.75781]]]\n",
            "attention_lnx=[[[-0.412109 -0.455078 -0.59375 ... -2.21875 1.39844 1.11719]\n",
            "  [-0.142578 -0.0834961 -0.135742 ... -2.03125 1.67969 0.941406]\n",
            "  [-0.238281 -0.15918 0.402344 ... -1.24219 1.65625 0.9375]\n",
            "  [-0.209961 0.103516 0.300781 ... -1.39062 1.14062 0.519531]]]\n",
            "attn_output=[[[-0.667969 -0.8125 -1.08594 ... -2.04688 2.375 0.917969]\n",
            "  [1.0625 0.292969 0.134766 ... -1.33594 2.51562 -0.168945]\n",
            "  [0.574219 -0.761719 0.455078 ... -2.0625 1.32812 0.519531]\n",
            "  [-0.0859375 -1.60156 0.632812 ... -1.05469 3.14062 -1.35156]]]\n",
            "next_layer_addition_dropped_out=[[[-0.746094 -1.64062 -1.79688 ... -4.125 5.40625 1.24219]\n",
            "  [2.4375 0.453125 0.273438 ... -3.40625 5.09375 -0.675781]\n",
            "  [1 -0.617188 0.8125 ... -3.07812 2.6875 0.398438]\n",
            "  [0.371094 -3.78125 2.03125 ... -2.76562 5.0625 -2.95312]]]\n",
            "inputs=[[[-0.746094 -1.64062 -1.79688 ... -4.125 5.40625 1.24219]\n",
            "  [2.4375 0.453125 0.273438 ... -3.40625 5.09375 -0.675781]\n",
            "  [1 -0.617188 0.8125 ... -3.07812 2.6875 0.398438]\n",
            "  [0.371094 -3.78125 2.03125 ... -2.76562 5.0625 -2.95312]]]\n",
            "lnx=[[[-0.322266 -0.707031 -0.773438 ... -1.78125 2.32812 0.535156]\n",
            "  [1.16406 0.21582 0.129883 ... -1.625 2.42188 -0.322266]\n",
            "  [0.503906 -0.310547 0.410156 ... -1.55469 1.35156 0.201172]\n",
            "  [0.193359 -1.96875 1.05469 ... -1.4375 2.625 -1.53906]]]\n",
            "attention_lnx=[[[2.20312 -1.00781 -0.267578 ... 0.761719 -0.196289 1.35938]\n",
            "  [0.839844 -1.10156 0.192383 ... 0.173828 -0.193359 -0.121582]\n",
            "  [0.902344 -0.386719 0.445312 ... 0.542969 0.306641 -0.0324707]\n",
            "  [1.48438 -0.855469 0.283203 ... 0.46875 -0.136719 0.746094]]]\n",
            "attn_output=[[[0.582031 -1.0625 -0.828125 ... -1.34375 2.07812 1.03906]\n",
            "  [1.46094 -0.289062 0.207031 ... -1.4375 2.1875 -0.355469]\n",
            "  [0.890625 -0.470703 0.589844 ... -1.1875 1.39844 0.171875]\n",
            "  [0.890625 -2.21875 1.10938 ... -1.10156 2.35938 -1.05469]]]\n",
            "next_layer_addition_dropped_out=[[[0.933594 -2.48438 -3.04688 ... -3.875 4.78125 2.875]\n",
            "  [2.95312 -1.14062 0.628906 ... -2.4375 4.75 -1.00781]\n",
            "  [1.86719 -0.953125 1.55469 ... -3.26562 3.65625 1.6875]\n",
            "  [1.75 -5.40625 2.04688 ... -3.48438 5.40625 -2.96875]]]\n",
            "inputs=[[[0.933594 -2.48438 -3.04688 ... -3.875 4.78125 2.875]\n",
            "  [2.95312 -1.14062 0.628906 ... -2.4375 4.75 -1.00781]\n",
            "  [1.86719 -0.953125 1.55469 ... -3.26562 3.65625 1.6875]\n",
            "  [1.75 -5.40625 2.04688 ... -3.48438 5.40625 -2.96875]]]\n",
            "lnx=[[[0.361328 -0.964844 -1.17969 ... -1.5 1.85156 1.11719]\n",
            "  [1.25781 -0.486328 0.267578 ... -1.03906 2.03125 -0.429688]\n",
            "  [0.832031 -0.423828 0.691406 ... -1.45312 1.63281 0.75]\n",
            "  [0.789062 -2.4375 0.925781 ... -1.57031 2.4375 -1.34375]]]\n",
            "attention_lnx=[[[1.58594 -1.79688 -0.0581055 ... 0.00686646 -0.683594 1.36719]\n",
            "  [0.886719 -1.22656 -1.01562 ... 0.417969 -0.722656 1.34375]\n",
            "  [0.554688 -0.796875 -0.386719 ... 0.382812 -0.251953 1.34375]\n",
            "  [0.605469 -1.28125 0.179688 ... 1.01562 0.0129395 1.53125]]]\n",
            "attn_output=[[[0.910156 -1.54688 -1.11719 ... -1.39844 1.47656 1.53125]\n",
            "  [1.53906 -0.949219 -0.154297 ... -0.808594 1.60938 0.134766]\n",
            "  [1.00781 -0.726562 0.486328 ... -1.20312 1.41406 1.25781]\n",
            "  [1 -2.84375 0.945312 ... -1.04688 2.29688 -0.609375]]]\n",
            "next_layer_addition_dropped_out=[[[2.125 -4.40625 -3.4375 ... -3.25 4.90625 5.46875]\n",
            "  [6.21875 -2.71875 -0.316406 ... -1.90625 3.54688 1.70312]\n",
            "  [2.78125 -2.03125 -0.078125 ... -3.17188 3.25 4.25]\n",
            "  [1.25 -7.875 2.10938 ... -1.88281 5.53125 -1.24219]]]\n",
            "inputs=[[[2.125 -4.40625 -3.4375 ... -3.25 4.90625 5.46875]\n",
            "  [6.21875 -2.71875 -0.316406 ... -1.90625 3.54688 1.70312]\n",
            "  [2.78125 -2.03125 -0.078125 ... -3.17188 3.25 4.25]\n",
            "  [1.25 -7.875 2.10938 ... -1.88281 5.53125 -1.24219]]]\n",
            "lnx=[[[0.742188 -1.53906 -1.19531 ... -1.13281 1.71094 1.90625]\n",
            "  [2.42188 -1.05469 -0.123047 ... -0.742188 1.375 0.660156]\n",
            "  [1.125 -0.824219 -0.0317383 ... -1.28125 1.32031 1.71875]\n",
            "  [0.507812 -3.20312 0.859375 ... -0.765625 2.25 -0.503906]]]\n",
            "attention_lnx=[[[1.05469 -1.25781 0.851562 ... 0.796875 -0.337891 -1.03125]\n",
            "  [1.25781 -0.882812 1.36719 ... 0.734375 -0.0137939 -1.02344]\n",
            "  [0.910156 -1.25 0.683594 ... 0.318359 0.429688 0.0454102]\n",
            "  [0.808594 -1.35938 0.855469 ... 0.503906 0.292969 -0.0644531]]]\n",
            "attn_output=[[[1.04688 -1.86719 -0.851562 ... -0.808594 1.5 1.46094]\n",
            "  [2.71875 -1.30469 0.380859 ... -0.425781 1.28125 0.24707]\n",
            "  [1.40625 -1.25 0.230469 ... -1.08594 1.39844 1.63281]\n",
            "  [0.785156 -3.51562 1.13281 ... -0.523438 2.21875 -0.498047]]]\n",
            "next_layer_addition_dropped_out=[[[3.89062 -5.65625 -1.875 ... -4.5 4.75 3.8125]\n",
            "  [7.4375 -2.71875 1.09375 ... -1.17969 2.42188 1.48438]\n",
            "  [5.125 -2.78125 0.0273438 ... -3.75 2.89062 3.76562]\n",
            "  [1.71094 -9.125 3.70312 ... -1.33594 6.53125 -0.628906]]]\n",
            "inputs=[[[3.89062 -5.65625 -1.875 ... -4.5 4.75 3.8125]\n",
            "  [7.4375 -2.71875 1.09375 ... -1.17969 2.42188 1.48438]\n",
            "  [5.125 -2.78125 0.0273438 ... -3.75 2.89062 3.76562]\n",
            "  [1.71094 -9.125 3.70312 ... -1.33594 6.53125 -0.628906]]]\n",
            "lnx=[[[1.25 -1.8125 -0.601562 ... -1.44531 1.52344 1.22656]\n",
            "  [2.625 -0.957031 0.384766 ... -0.416016 0.851562 0.523438]\n",
            "  [1.89062 -1.02344 0.0100708 ... -1.38281 1.07031 1.39062]\n",
            "  [0.628906 -3.34375 1.35938 ... -0.490234 2.39062 -0.230469]]]\n",
            "attention_lnx=[[[1.20312 0.664062 0.460938 ... -0.996094 0.0140991 0.855469]\n",
            "  [1.71875 0.40625 -0.0334473 ... -1.25781 0.273438 -0.0668945]\n",
            "  [1.39844 0.617188 -0.246094 ... -1.32031 0.431641 -0.176758]\n",
            "  [0.898438 0.644531 -0.632812 ... -0.539062 0.0966797 -0.137695]]]\n",
            "attn_output=[[[1.55469 -1.52344 -0.431641 ... -1.67969 1.45312 1.42188]\n",
            "  [3.03125 -0.765625 0.351562 ... -0.808594 0.894531 0.470703]\n",
            "  [2.29688 -0.761719 -0.0771484 ... -1.78125 1.17188 1.26562]\n",
            "  [0.914062 -2.98438 1.07812 ... -0.65625 2.32812 -0.269531]]]\n",
            "next_layer_addition_dropped_out=[[[4.875 -5.09375 -2.92188 ... -5.28125 4.96875 4.25]\n",
            "  [8.875 -1.71094 0.332031 ... -2.25 3.34375 0.800781]\n",
            "  [4.75 -2.21875 -0.765625 ... -6.28125 2.67188 3.60938]\n",
            "  [2.25 -8.5 2.625 ... -1.61719 6.625 0.0898438]]]\n",
            "inputs=[[[4.875 -5.09375 -2.92188 ... -5.28125 4.96875 4.25]\n",
            "  [8.875 -1.71094 0.332031 ... -2.25 3.34375 0.800781]\n",
            "  [4.75 -2.21875 -0.765625 ... -6.28125 2.67188 3.60938]\n",
            "  [2.25 -8.5 2.625 ... -1.61719 6.625 0.0898438]]]\n",
            "lnx=[[[1.45312 -1.51562 -0.871094 ... -1.57812 1.48438 1.26562]\n",
            "  [2.875 -0.554688 0.107422 ... -0.730469 1.08594 0.259766]\n",
            "  [1.625 -0.757812 -0.261719 ... -2.14062 0.914062 1.23438]\n",
            "  [0.765625 -2.89062 0.890625 ... -0.550781 2.25 0.0305176]]]\n",
            "attention_lnx=[[[-1.14844 -0.632812 0.496094 ... 0.322266 1.53906 0.933594]\n",
            "  [-1.35156 -0.613281 0.710938 ... 0.652344 1.10938 1.0625]\n",
            "  [-0.601562 -0.122559 0.402344 ... 1.03906 1.39844 0.550781]\n",
            "  [-0.486328 -0.447266 0.0588379 ... 0.753906 0.992188 0.453125]]]\n",
            "attn_output=[[[1.0625 -1.63281 -0.691406 ... -1.41406 1.85156 1.47656]\n",
            "  [2.32812 -0.71875 0.322266 ... -0.494141 1.375 0.574219]\n",
            "  [1.36719 -0.773438 -0.119629 ... -1.72656 1.34375 1.375]\n",
            "  [0.578125 -2.92188 0.878906 ... -0.283203 2.5 0.177734]]]\n",
            "next_layer_addition_dropped_out=[[[2.96875 -5.9375 -1.0625 ... -3.92188 6.84375 5.625]\n",
            "  [6.9375 -3.26562 2.46875 ... -0.398438 4.625 2.9375]\n",
            "  [3.15625 -2.82812 -0.115234 ... -4.625 3.90625 5.09375]\n",
            "  [0.617188 -8.0625 2 ... 0.0625 7.03125 1.75]]]\n",
            "inputs=[[[2.96875 -5.9375 -1.0625 ... -3.92188 6.84375 5.625]\n",
            "  [6.9375 -3.26562 2.46875 ... -0.398438 4.625 2.9375]\n",
            "  [3.15625 -2.82812 -0.115234 ... -4.625 3.90625 5.09375]\n",
            "  [0.617188 -8.0625 2 ... 0.0625 7.03125 1.75]]]\n",
            "lnx=[[[0.824219 -1.64844 -0.294922 ... -1.08594 1.89844 1.5625]\n",
            "  [2.09375 -0.984375 0.746094 ... -0.120117 1.39844 0.886719]\n",
            "  [1.01562 -0.914062 -0.0371094 ... -1.49219 1.25781 1.64062]\n",
            "  [0.195312 -2.54688 0.632812 ... 0.0197754 2.21875 0.550781]]]\n",
            "attention_lnx=[[[-0.40625 0.0507812 -0.835938 ... -0.96875 -1.97656 0.410156]\n",
            "  [-0.0512695 -0.222656 -0.53125 ... -0.8125 -1.52344 0.464844]\n",
            "  [-0.207031 -0.863281 -0.402344 ... -0.675781 -0.621094 0.601562]\n",
            "  [0.103027 -0.636719 -0.373047 ... -0.65625 -0.761719 0.878906]]]\n",
            "attn_output=[[[0.675781 -1.55469 -0.5 ... -1.28906 1.28125 1.59375]\n",
            "  [1.99219 -1.00781 0.558594 ... -0.349609 0.894531 0.984375]\n",
            "  [0.917969 -1.14844 -0.161133 ... -1.64844 1.02344 1.77344]\n",
            "  [0.21875 -2.64062 0.494141 ... -0.180664 1.90625 0.796875]]]\n",
            "next_layer_addition_dropped_out=[[[1.92188 -5.5 -2.25 ... -6.5 5.40625 5.3125]\n",
            "  [7.40625 -3.35938 0.972656 ... -2.20312 3.14062 3.67188]\n",
            "  [2.70312 -3.32812 -1.75781 ... -6.59375 3.01562 6.71875]\n",
            "  [0.253906 -8.125 0.785156 ... -0.398438 6.03125 1.95312]]]\n",
            "inputs=[[[1.92188 -5.5 -2.25 ... -6.5 5.40625 5.3125]\n",
            "  [7.40625 -3.35938 0.972656 ... -2.20312 3.14062 3.67188]\n",
            "  [2.70312 -3.32812 -1.75781 ... -6.59375 3.01562 6.71875]\n",
            "  [0.253906 -8.125 0.785156 ... -0.398438 6.03125 1.95312]]]\n",
            "lnx=[[[0.490234 -1.40625 -0.574219 ... -1.66406 1.38281 1.35938]\n",
            "  [2.07812 -0.941406 0.273438 ... -0.617188 0.878906 1.03125]\n",
            "  [0.816406 -1.00781 -0.53125 ... -1.99219 0.914062 2.03125]\n",
            "  [0.0742188 -2.375 0.229492 ... -0.116211 1.76562 0.570312]]]\n",
            "attention_lnx=[[[-0.5 -0.824219 0.898438 ... -3.375 1.08594 0.0120239]\n",
            "  [-0.671875 -0.933594 0.644531 ... -2.85938 1.92188 0.139648]\n",
            "  [-0.699219 -0.333984 0.341797 ... -2.54688 1.36719 0.828125]\n",
            "  [-0.339844 -0.644531 1.05469 ... -2.14062 1.63281 0.222656]]]\n",
            "attn_output=[[[0.349609 -1.5625 -0.333984 ... -2.4375 1.60156 1.3125]\n",
            "  [1.82031 -1.16406 0.4375 ... -1.36719 1.36719 1.03125]\n",
            "  [0.582031 -1.0625 -0.412109 ... -2.65625 1.27344 2.20312]\n",
            "  [-0.0241699 -2.46875 0.515625 ... -0.710938 2.15625 0.609375]]]\n",
            "next_layer_addition_dropped_out=[[[-0.328125 -6.125 -3.0625 ... -9.5625 6.125 5.21875]\n",
            "  [5.3125 -4.21875 0.273438 ... -4.84375 3.71875 4.125]\n",
            "  [1.76562 -3.59375 -1.05469 ... -9.5625 3.25 9.1875]\n",
            "  [-1.14062 -8.9375 1.38281 ... -3.3125 5.9375 2.84375]]]\n",
            "inputs=[[[-0.328125 -6.125 -3.0625 ... -9.5625 6.125 5.21875]\n",
            "  [5.3125 -4.21875 0.273438 ... -4.84375 3.71875 4.125]\n",
            "  [1.76562 -3.59375 -1.05469 ... -9.5625 3.25 9.1875]\n",
            "  [-1.14062 -8.9375 1.38281 ... -3.3125 5.9375 2.84375]]]\n",
            "lnx=[[[-0.078125 -1.45312 -0.726562 ... -2.26562 1.45312 1.24219]\n",
            "  [1.40625 -1.11719 0.0722656 ... -1.28125 0.980469 1.08594]\n",
            "  [0.496094 -1.00781 -0.296875 ... -2.6875 0.914062 2.57812]\n",
            "  [-0.302734 -2.375 0.367188 ... -0.878906 1.57031 0.753906]]]\n",
            "attention_lnx=[[[-0.671875 0.523438 0.792969 ... -0.882812 -0.828125 0.201172]\n",
            "  [-1.34375 -0.168945 1.11719 ... -0.726562 -0.917969 -0.18457]\n",
            "  [-0.410156 0.273438 1.30469 ... -0.730469 -0.589844 -0.292969]\n",
            "  [-0.945312 0.0917969 1.10938 ... -0.53125 -0.628906 -0.486328]]]\n",
            "attn_output=[[[-0.230469 -1.28906 -0.523438 ... -2.40625 1.21875 1.25]\n",
            "  [1.02344 -1.13281 0.359375 ... -1.4375 0.722656 1.01562]\n",
            "  [0.369141 -0.90625 0.0683594 ... -2.8125 0.726562 2.42188]\n",
            "  [-0.539062 -2.29688 0.644531 ... -0.996094 1.375 0.609375]]]\n",
            "next_layer_addition_dropped_out=[[[-1.9375 -5.15625 -0.992188 ... -10.0625 5.3125 6.5]\n",
            "  [4.3125 -4.625 1.30469 ... -5.65625 3.0625 5.375]\n",
            "  [1.42188 -4.375 0.0107422 ... -10.125 1.46094 9.0625]\n",
            "  [-1.57812 -10.125 2.32812 ... -3.67188 5.71875 2.79688]]]\n",
            "inputs=[[[-1.9375 -5.15625 -0.992188 ... -10.0625 5.3125 6.5]\n",
            "  [4.3125 -4.625 1.30469 ... -5.65625 3.0625 5.375]\n",
            "  [1.42188 -4.375 0.0107422 ... -10.125 1.46094 9.0625]\n",
            "  [-1.57812 -10.125 2.32812 ... -3.67188 5.71875 2.79688]]]\n",
            "lnx=[[[-0.427734 -1.14062 -0.21875 ... -2.21875 1.17188 1.4375]\n",
            "  [1.07812 -1.15625 0.326172 ... -1.40625 0.761719 1.34375]\n",
            "  [0.373047 -1.14844 0.00280762 ... -2.65625 0.382812 2.375]\n",
            "  [-0.386719 -2.48438 0.570312 ... -0.898438 1.39844 0.683594]]]\n",
            "attention_lnx=[[[-0.964844 -1.96094 -1 ... 0.103027 -1.14062 0.443359]\n",
            "  [-0.785156 -1.17969 -0.425781 ... 0.453125 -0.400391 0.0703125]\n",
            "  [-0.480469 -0.914062 -0.255859 ... 0.335938 -0.158203 -0.0893555]\n",
            "  [0.133789 -0.800781 -0.675781 ... 0.789062 0.170898 0.259766]]]\n",
            "attn_output=[[[-0.621094 -1.51562 -0.425781 ... -2.125 0.890625 1.48438]\n",
            "  [0.855469 -1.40625 0.212891 ... -1.25781 0.644531 1.32031]\n",
            "  [0.239258 -1.34375 -0.0622559 ... -2.48438 0.332031 2.28125]\n",
            "  [-0.345703 -2.625 0.396484 ... -0.691406 1.41406 0.734375]]]\n",
            "next_layer_addition_dropped_out=[[[-2.96875 -7.40625 -3.07812 ... -9.125 4.46875 7.28125]\n",
            "  [3.125 -5.9375 -0.078125 ... -4 3.4375 4.375]\n",
            "  [1.41406 -5 -0.640625 ... -9.6875 1.20312 8.125]\n",
            "  [-1.21094 -11.4375 1.00781 ... -2.42188 6.6875 3.125]]]\n",
            "inputs=[[[-2.96875 -7.40625 -3.07812 ... -9.125 4.46875 7.28125]\n",
            "  [3.125 -5.9375 -0.078125 ... -4 3.4375 4.375]\n",
            "  [1.41406 -5 -0.640625 ... -9.6875 1.20312 8.125]\n",
            "  [-1.21094 -11.4375 1.00781 ... -2.42188 6.6875 3.125]]]\n",
            "lnx=[[[-0.605469 -1.50781 -0.625 ... -1.85938 0.910156 1.48438]\n",
            "  [0.734375 -1.39062 -0.0183105 ... -0.9375 0.808594 1.03125]\n",
            "  [0.345703 -1.21875 -0.15625 ... -2.35938 0.292969 1.98438]\n",
            "  [-0.277344 -2.60938 0.230469 ... -0.554688 1.53125 0.714844]]]\n",
            "attention_lnx=[[[0.462891 0.217773 -0.460938 ... 1.03906 0.644531 -2.65625]\n",
            "  [0.0149536 0.0664062 -0.320312 ... 0.773438 0.921875 -2.59375]\n",
            "  [0.302734 0.0771484 -0.228516 ... 0.699219 0.730469 -2.70312]\n",
            "  [0.574219 0.165039 -0.345703 ... 0.4375 -0.0830078 -2.26562]]]\n",
            "attn_output=[[[-0.492188 -1.41406 -0.695312 ... -1.58594 1.00781 0.910156]\n",
            "  [0.707031 -1.32812 -0.0898438 ... -0.726562 0.984375 0.402344]\n",
            "  [0.404297 -1.15625 -0.204102 ... -2.10938 0.455078 1.27344]\n",
            "  [-0.139648 -2.46875 0.145508 ... -0.435547 1.44531 0.188477]]]\n",
            "next_layer_addition_dropped_out=[[[-2.67188 -7.40625 -5.5 ... -9.25 5.5 4.34375]\n",
            "  [3.04688 -5.46875 -1.75781 ... -3.92188 4.34375 1.67188]\n",
            "  [3.46875 -5.15625 -2.125 ... -10.1875 0.71875 6.375]\n",
            "  [-0.289062 -12.3125 -1.38281 ... -2.53125 6.125 0.871094]]]\n",
            "inputs=[[[-2.67188 -7.40625 -5.5 ... -9.25 5.5 4.34375]\n",
            "  [3.04688 -5.46875 -1.75781 ... -3.92188 4.34375 1.67188]\n",
            "  [3.46875 -5.15625 -2.125 ... -10.1875 0.71875 6.375]\n",
            "  [-0.289062 -12.3125 -1.38281 ... -2.53125 6.125 0.871094]]]\n",
            "lnx=[[[-0.496094 -1.375 -1.02344 ... -1.71875 1.02344 0.808594]\n",
            "  [0.664062 -1.19531 -0.382812 ... -0.855469 0.949219 0.365234]\n",
            "  [0.773438 -1.14844 -0.474609 ... -2.28125 0.160156 1.42188]\n",
            "  [-0.0593262 -2.53125 -0.283203 ... -0.519531 1.25781 0.178711]]]\n",
            "attention_lnx=[[[-1.70312 1.3125 0.589844 ... -1.5 -0.328125 -0.875]\n",
            "  [-1.375 1.1875 -0.339844 ... -1.46094 -0.419922 -0.839844]\n",
            "  [-1.21875 1.41406 -0.373047 ... -1.4375 -0.324219 -0.953125]\n",
            "  [-1.39062 1.39062 -0.115723 ... -1.15625 -0.345703 -0.789062]]]\n",
            "attn_output=[[[-0.785156 -1.09375 -0.878906 ... -1.92969 0.925781 0.621094]\n",
            "  [0.351562 -0.902344 -0.441406 ... -1.13281 0.828125 0.174805]\n",
            "  [0.488281 -0.8125 -0.542969 ... -2.51562 0.0854492 1.17188]\n",
            "  [-0.335938 -2.1875 -0.298828 ... -0.738281 1.15625 0.0163574]]]\n",
            "next_layer_addition_dropped_out=[[[-5.09375 -5.46875 -4.34375 ... -9.5 5.5625 4.0625]\n",
            "  [1.5625 -4.0625 -2.29688 ... -4.25 3.53125 2.64062]\n",
            "  [2.10938 -3.625 -3.59375 ... -11.8125 0.316406 5]\n",
            "  [-1.3125 -12 -3.23438 ... -1.625 4.875 -1.20312]]]\n",
            "inputs=[[[-5.09375 -5.46875 -4.34375 ... -9.5 5.5625 4.0625]\n",
            "  [1.5625 -4.0625 -2.29688 ... -4.25 3.53125 2.64062]\n",
            "  [2.10938 -3.625 -3.59375 ... -11.8125 0.316406 5]\n",
            "  [-1.3125 -12 -3.23438 ... -1.625 4.875 -1.20312]]]\n",
            "lnx=[[[-0.855469 -0.921875 -0.730469 ... -1.60156 0.9375 0.683594]\n",
            "  [0.314453 -0.820312 -0.462891 ... -0.855469 0.710938 0.53125]\n",
            "  [0.431641 -0.742188 -0.734375 ... -2.40625 0.0644531 1.02344]\n",
            "  [-0.241211 -2.20312 -0.59375 ... -0.298828 0.898438 -0.22168]]]\n",
            "attention_lnx=[[[0.306641 -1.63281 0.141602 ... 0.71875 -0.347656 -0.174805]\n",
            "  [0.120117 -1.45312 -0.0327148 ... 0.746094 -0.198242 -0.200195]\n",
            "  [0.0551758 -0.796875 0.048584 ... 0.503906 0.0299072 0.152344]\n",
            "  [0.00369263 -0.816406 0.0810547 ... 0.597656 -0.316406 -0.107422]]]\n",
            "attn_output=[[[-0.789062 -1.17188 -0.691406 ... -1.44531 0.859375 0.640625]\n",
            "  [0.332031 -1.08594 -0.458984 ... -0.691406 0.65625 0.482422]\n",
            "  [0.431641 -0.882812 -0.707031 ... -2.25 0.0688477 1.02344]\n",
            "  [-0.234375 -2.29688 -0.566406 ... -0.18457 0.816406 -0.235352]]]\n",
            "next_layer_addition_dropped_out=[[[-6.15625 -7.03125 -5.75 ... -7.5625 6.375 5.0625]\n",
            "  [1.57031 -5.40625 -4.0625 ... -2.75 2.39062 3.29688]\n",
            "  [1.40625 -4.78125 -4.1875 ... -11.6875 0.314453 6.59375]\n",
            "  [-1.6875 -13.25 -4.9375 ... -0.71875 3.28125 -0.792969]]]\n",
            "inputs=[[[-6.15625 -7.03125 -5.75 ... -7.5625 6.375 5.0625]\n",
            "  [1.57031 -5.40625 -4.0625 ... -2.75 2.39062 3.29688]\n",
            "  [1.40625 -4.78125 -4.1875 ... -11.6875 0.314453 6.59375]\n",
            "  [-1.6875 -13.25 -4.9375 ... -0.71875 3.28125 -0.792969]]]\n",
            "lnx=[[[-0.945312 -1.07812 -0.882812 ... -1.16406 0.980469 0.777344]\n",
            "  [0.294922 -1.01562 -0.765625 ... -0.519531 0.449219 0.621094]\n",
            "  [0.261719 -0.890625 -0.78125 ... -2.17188 0.0585938 1.22656]\n",
            "  [-0.277344 -2.17188 -0.8125 ... -0.118164 0.539062 -0.129883]]]\n",
            "attention_lnx=[[[-0.175781 0.515625 3.21875 ... -0.316406 -1.67969 0.679688]\n",
            "  [-0.199219 0.429688 3.09375 ... -0.601562 -0.757812 0.675781]\n",
            "  [-0.0344238 0.652344 2.73438 ... -0.376953 -0.365234 0.558594]\n",
            "  [-0.133789 0.578125 2.71875 ... -0.189453 -0.777344 0.384766]]]\n",
            "attn_output=[[[-0.945312 -0.972656 -0.376953 ... -1.17188 0.699219 0.855469]\n",
            "  [0.25 -0.90625 -0.176758 ... -0.609375 0.296875 0.722656]\n",
            "  [0.24707 -0.742188 -0.261719 ... -2.17188 -0.00915527 1.28906]\n",
            "  [-0.291016 -2.01562 -0.353516 ... -0.144531 0.400391 -0.0649414]]]\n",
            "next_layer_addition_dropped_out=[[[-7.3125 -8.6875 -3.90625 ... -9.0625 3.85938 5.34375]\n",
            "  [1.04688 -7.5625 -2.375 ... -3.5625 2.34375 3.65625]\n",
            "  [0.847656 -4.75 -1.59375 ... -12.9375 -0.314453 6.40625]\n",
            "  [-3.3125 -15.375 -3.17188 ... -3.45312 1.21875 -1.4375]]]\n",
            "inputs=[[[-7.3125 -8.6875 -3.90625 ... -9.0625 3.85938 5.34375]\n",
            "  [1.04688 -7.5625 -2.375 ... -3.5625 2.34375 3.65625]\n",
            "  [0.847656 -4.75 -1.59375 ... -12.9375 -0.314453 6.40625]\n",
            "  [-3.3125 -15.375 -3.17188 ... -3.45312 1.21875 -1.4375]]]\n",
            "lnx=[[[-1.01562 -1.20312 -0.539062 ... -1.25781 0.535156 0.738281]\n",
            "  [0.180664 -1.30469 -0.410156 ... -0.613281 0.404297 0.632812]\n",
            "  [0.140625 -0.789062 -0.265625 ... -2.15625 -0.0522461 1.0625]\n",
            "  [-0.482422 -2.23438 -0.460938 ... -0.503906 0.177734 -0.208984]]]\n",
            "attention_lnx=[[[-0.212891 -1.60938 -0.214844 ... -0.158203 -2.15625 1.07031]\n",
            "  [-0.828125 -1.42188 -0.722656 ... -0.710938 -2.07812 1.42969]\n",
            "  [-0.847656 -1.53125 -0.458984 ... -1.10938 -2.29688 1.3125]\n",
            "  [-0.527344 -1.58594 -0.367188 ... -1.20312 -2.26562 1.34375]]]\n",
            "attn_output=[[[-1.00781 -1.375 -0.550781 ... -1.23438 0.227539 0.859375]\n",
            "  [0.036377 -1.49219 -0.515625 ... -0.710938 0.0441895 0.84375]\n",
            "  [0 -1.00781 -0.330078 ... -2.26562 -0.419922 1.24219]\n",
            "  [-0.539062 -2.39062 -0.498047 ... -0.65625 -0.147461 -0.0131836]]]\n",
            "next_layer_addition_dropped_out=[[[-7.375 -9.3125 -4.21875 ... -11.875 0.894531 5.125]\n",
            "  [0.03125 -8.4375 -3.64062 ... -6.53125 -1.625 4.375]\n",
            "  [-0.486328 -6.34375 -2.54688 ... -17.375 -3.42188 8.75]\n",
            "  [-2.65625 -16.875 -3.5625 ... -6.96875 -2.57812 -1.76562]]]\n",
            "inputs=[[[-7.375 -9.3125 -4.21875 ... -11.875 0.894531 5.125]\n",
            "  [0.03125 -8.4375 -3.64062 ... -6.53125 -1.625 4.375]\n",
            "  [-0.486328 -6.34375 -2.54688 ... -17.375 -3.42188 8.75]\n",
            "  [-2.65625 -16.875 -3.5625 ... -6.96875 -2.57812 -1.76562]]]\n",
            "lnx=[[[-0.902344 -1.14062 -0.515625 ... -1.45312 0.109375 0.625]\n",
            "  [0.00485229 -1.3125 -0.566406 ... -1.01562 -0.251953 0.679688]\n",
            "  [-0.0717773 -0.9375 -0.376953 ... -2.5625 -0.503906 1.28906]\n",
            "  [-0.337891 -2.14062 -0.453125 ... -0.886719 -0.328125 -0.224609]]]\n",
            "attention_lnx=[[[-1.45312 -0.621094 -1.61719 ... -0.355469 -0.173828 -1.69531]\n",
            "  [-0.621094 -0.808594 -1.74219 ... -0.263672 -0.476562 -1.05469]\n",
            "  [-0.46875 -0.921875 -1.34375 ... -0.628906 -0.244141 -1.55469]\n",
            "  [-0.578125 -0.984375 -1.11719 ... -0.515625 -0.0336914 -1.39062]]]\n",
            "attn_output=[[[-1.04688 -1.17188 -0.691406 ... -1.44531 0.0854492 0.40625]\n",
            "  [-0.0883789 -1.38281 -0.804688 ... -1.01562 -0.314453 0.496094]\n",
            "  [-0.135742 -1.03906 -0.554688 ... -2.5625 -0.523438 1.02344]\n",
            "  [-0.398438 -2.20312 -0.578125 ... -0.921875 -0.322266 -0.388672]]]\n",
            "next_layer_addition_dropped_out=[[[-6.625 -10.0625 -7.9375 ... -14 -0.476562 1.78125]\n",
            "  [1.23438 -8.9375 -6.9375 ... -8.5625 -1.82812 2.375]\n",
            "  [0.375 -6.125 -6.375 ... -19.875 -3.82812 6.3125]\n",
            "  [-1.38281 -17.125 -8.25 ... -10.875 -3.8125 -4.09375]]]\n",
            "Successfully ran the model!\n",
            "Output shape: (1, 4, 256128)\n",
            "Tokens: [17964, 3722, 1926, 784]\n"
          ]
        }
      ],
      "source": [
        "import jax.numpy as jnp\n",
        "import os\n",
        "# I'm assuming you have these imports in your script.\n",
        "# If not, you'll need to add them.\n",
        "# from tunix.models.gemma import gemma as gemma_lib\n",
        "# from path.to import data_lib\n",
        "\n",
        "TEMP_BATCH_SIZE = 1\n",
        "\n",
        "# Assuming gemma is a pre-loaded instance of gemma_lib.Transformer\n",
        "# and data_lib is available.\n",
        "gemma_tokenizer = data_lib.GemmaTokenizer(\n",
        "    os.path.join(kaggle_ckpt_path, \"tokenizer.model\")\n",
        ")\n",
        "\n",
        "tokens = gemma_tokenizer.encode(\"hakuna matata\")\n",
        "# tokens = [gemma_tokenizer.bos_id()]+gemma_tokenizer.encode(\"The color of the sky is blue but\")\n",
        "\n",
        "repeated_tokens = jnp.repeat(jnp.array(tokens)[None, :], TEMP_BATCH_SIZE, axis=0)\n",
        "positions = jnp.repeat(jnp.arange(0, len(tokens))[None, :], TEMP_BATCH_SIZE, axis=0)\n",
        "\n",
        "# --- FIX STARTS HERE ---\n",
        "\n",
        "# The Gemma model requires an attention mask. Passing `None` causes an error.\n",
        "# We need to create a causal attention mask for the prefill step.\n",
        "\n",
        "# 1. Create a boolean mask for the input tokens (True for valid tokens, False for padding).\n",
        "#    Assuming `gemma_tokenizer.pad_id()` exists. If not, and there's no padding,\n",
        "#    `jnp.ones_like(repeated_tokens, dtype=jnp.bool_)` would also work.\n",
        "#    A common pad_id is 0.\n",
        "pad_id = gemma_tokenizer.pad_id()\n",
        "input_mask = (repeated_tokens != pad_id)\n",
        "\n",
        "# 2. Create a causal attention mask from the input mask.\n",
        "#    This prevents the model from attending to future tokens.\n",
        "attention_mask = gemma_lib.make_causal_attn_mask(input_mask)\n",
        "\n",
        "# 3. Call the model with the correct attention mask.\n",
        "gemma_output_logits, _ = gemma(repeated_tokens, positions, cache=None, attention_mask=attention_mask)  # Test the model to ensure it works\n",
        "\n",
        "# --- FIX ENDS HERE ---\n",
        "\n",
        "# The commented out line below would also need a proper attention_mask.\n",
        "# For example:\n",
        "# dummy_tokens = jnp.ones((TEMP_BATCH_SIZE, 16), jnp.int32)\n",
        "# dummy_positions = jnp.repeat(jnp.arange(0,16)[None,:], TEMP_BATCH_SIZE, axis=0)\n",
        "# dummy_mask = gemma_lib.make_causal_attn_mask(jnp.ones_like(dummy_tokens, dtype=jnp.bool_))\n",
        "# gemma_output = gemma(dummy_tokens, dummy_positions, cache=None, attention_mask=dummy_mask)\n",
        "\n",
        "print(\"Successfully ran the model!\")\n",
        "print(\"Output shape:\", gemma_output_logits.shape)\n",
        "\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"positions: {positions}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 4, 256128)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Array([[17964,  3722,  1926,   784]], dtype=int32)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(gemma_output_logits.shape)\n",
        "jnp.argmax(gemma_output_logits, axis=2)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Predicted next token ID: 708\n",
            "Predicted next token: ' are'\n"
          ]
        }
      ],
      "source": [
        "last_token_logits = gemma_output_logits[:, -1, :]\n",
        "predicted_token_id = jnp.argmax(last_token_logits, axis=-1)\n",
        "# Decode the token ID to see the predicted word.\n",
        "# Since TEMP_BATCH_SIZE is 1, we can just grab the first element.\n",
        "next_token_id = predicted_token_id[0]\n",
        "predicted_token_text = gemma_tokenizer.decode([int(next_token_id)])\n",
        "\n",
        "print(f\"\\nPredicted next token ID: {next_token_id}\")\n",
        "print(f\"Predicted next token: '{predicted_token_text}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array(1.1875, dtype=float32)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gemma_output_logits[0][0,7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "HFKUZT10T3Uv"
      },
      "outputs": [],
      "source": [
        "# def get_base_model(ckpt_path):\n",
        "\n",
        "#   model_config = gemma_lib.TransformerConfig.gemma_2b()\n",
        "#   mesh = jax.make_mesh(*MESH)\n",
        "#   abs_gemma: nnx.Module = nnx.eval_shape(\n",
        "#       lambda: gemma_lib.Transformer(model_config, rngs=nnx.Rngs(params=0))\n",
        "#   )\n",
        "#   abs_state = nnx.state(abs_gemma)\n",
        "#   abs_state = jax.tree.map(\n",
        "#       lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n",
        "#       abs_state,\n",
        "#       nnx.get_named_sharding(abs_state, mesh),\n",
        "#   )\n",
        "#   checkpointer = ocp.StandardCheckpointer()\n",
        "#   restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n",
        "\n",
        "#   graph_def, _ = nnx.split(abs_gemma)\n",
        "#   gemma = nnx.merge(graph_def, restored_params)\n",
        "#   return gemma, mesh, model_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2iDYbnsT3Uv"
      },
      "source": [
        "## Prompt the model\n",
        "\n",
        "Let's see how the model performs on the English-French translation task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "iH82cHpAT3Uv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs=[[[1.15625 -0.353516 1.4375 ... -1.57812 0.0245361 -0.644531]\n",
            "  [1.15625 -0.353516 1.4375 ... -1.57812 0.0245361 -0.644531]\n",
            "  [1.15625 -0.353516 1.4375 ... -1.57812 0.0245361 -0.644531]\n",
            "  ...\n",
            "  [0.539062 -0.333984 0.123047 ... -0.439453 -0.796875 2.51562]\n",
            "  [-1.03125 -0.621094 -0.292969 ... -0.271484 0.8125 0.785156]\n",
            "  [-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.550781 1.32812 0.304688 ... -0.691406 0.921875 -1.73438]\n",
            "  [0.0245361 1.15625 0.515625 ... 0.472656 1.04688 1.625]\n",
            "  [-1 1.24219 -1.83594 ... 1.19531 -0.824219 -0.132812]\n",
            "  ...\n",
            "  [0.472656 0.261719 -0.0146484 ... -0.171875 2.01562 0.65625]\n",
            "  [0.5625 0.632812 -0.292969 ... 2.51562 -0.333984 0.123047]\n",
            "  [-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[1.15625 -0.353516 1.4375 ... -1.57812 0.0245361 -0.644531]\n",
            "  [1.15625 -0.353516 1.4375 ... -1.57812 0.0245361 -0.644531]\n",
            "  [1.15625 -0.353516 1.4375 ... -1.57812 0.0245361 -0.644531]\n",
            "  ...\n",
            "  [-0.0344238 0.103027 0.494141 ... -1.65625 -0.152344 -0.112305]\n",
            "  [-1.25781 0.201172 0.123047 ... -0.644531 -0.271484 -0.333984]\n",
            "  [-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[1.15625 -0.353516 1.4375 ... -1.57812 0.0245361 -0.644531]\n",
            "  [1.15625 -0.353516 1.4375 ... -1.57812 0.0245361 -0.644531]\n",
            "  [1.15625 -0.353516 1.4375 ... -1.57812 0.0245361 -0.644531]\n",
            "  ...\n",
            "  [1.38281 -1.65625 -0.0737305 ... -1.83594 0.632812 -0.152344]\n",
            "  [0.5625 0.632812 -0.292969 ... 2.51562 -0.333984 0.123047]\n",
            "  [-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[1.15625 -0.353516 1.4375 ... -1.57812 0.0245361 -0.644531]\n",
            "  [-0.550781 1.32812 0.304688 ... -0.691406 0.921875 -1.73438]\n",
            "  [0.0245361 1.15625 0.515625 ... 0.472656 1.04688 1.625]\n",
            "  ...\n",
            "  [-0.597656 0.785156 -1.57812 ... 0.515625 0.386719 2.20312]\n",
            "  [0.5625 -0.769531 0.427734 ... 0.449219 2.51562 0.261719]\n",
            "  [-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]]\n",
            "lnx=[[[1.17969 -0.359375 1.46875 ... -1.60938 0.0250244 -0.65625]\n",
            "  [1.17969 -0.359375 1.46875 ... -1.60938 0.0250244 -0.65625]\n",
            "  [1.17969 -0.359375 1.46875 ... -1.60938 0.0250244 -0.65625]\n",
            "  ...\n",
            "  [0.535156 -0.332031 0.122559 ... -0.4375 -0.792969 2.5]\n",
            "  [-1.04688 -0.632812 -0.296875 ... -0.275391 0.824219 0.796875]\n",
            "  [-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.546875 1.32031 0.302734 ... -0.6875 0.914062 -1.71875]\n",
            "  [0.024292 1.14844 0.511719 ... 0.46875 1.03906 1.60938]\n",
            "  [-1.01562 1.25781 -1.85938 ... 1.21094 -0.835938 -0.134766]\n",
            "  ...\n",
            "  [0.474609 0.263672 -0.0147095 ... -0.172852 2.03125 0.660156]\n",
            "  [0.5625 0.632812 -0.292969 ... 2.51562 -0.333984 0.123047]\n",
            "  [-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[1.17969 -0.359375 1.46875 ... -1.60938 0.0250244 -0.65625]\n",
            "  [1.17969 -0.359375 1.46875 ... -1.60938 0.0250244 -0.65625]\n",
            "  [1.17969 -0.359375 1.46875 ... -1.60938 0.0250244 -0.65625]\n",
            "  ...\n",
            "  [-0.0356445 0.106445 0.511719 ... -1.71094 -0.157227 -0.115723]\n",
            "  [-1.24219 0.199219 0.121582 ... -0.636719 -0.267578 -0.330078]\n",
            "  [-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[1.17969 -0.359375 1.46875 ... -1.60938 0.0250244 -0.65625]\n",
            "  [1.17969 -0.359375 1.46875 ... -1.60938 0.0250244 -0.65625]\n",
            "  [1.17969 -0.359375 1.46875 ... -1.60938 0.0250244 -0.65625]\n",
            "  ...\n",
            "  [1.36719 -1.64062 -0.0727539 ... -1.8125 0.625 -0.150391]\n",
            "  [0.5625 0.632812 -0.292969 ... 2.51562 -0.333984 0.123047]\n",
            "  [-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[1.17969 -0.359375 1.46875 ... -1.60938 0.0250244 -0.65625]\n",
            "  [-0.546875 1.32031 0.302734 ... -0.6875 0.914062 -1.71875]\n",
            "  [0.024292 1.14844 0.511719 ... 0.46875 1.03906 1.60938]\n",
            "  ...\n",
            "  [-0.585938 0.773438 -1.55469 ... 0.507812 0.380859 2.17188]\n",
            "  [0.550781 -0.753906 0.417969 ... 0.439453 2.45312 0.255859]\n",
            "  [-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]]\n",
            "attention_lnx=[[[-0.464844 0.351562 -0.326172 ... 0.375 -0.380859 -0.320312]\n",
            "  [-0.464844 0.351562 -0.326172 ... 0.375 -0.380859 -0.320312]\n",
            "  [-0.466797 0.355469 -0.326172 ... 0.373047 -0.378906 -0.322266]\n",
            "  ...\n",
            "  [-0.330078 0.179688 0.0581055 ... -0.476562 -0.112793 -0.134766]\n",
            "  [0.333984 0.400391 -1.03125 ... -0.257812 -0.230469 -0.205078]\n",
            "  [-0.21582 -0.0344238 0.359375 ... -0.0800781 -0.271484 -0.0559082]]\n",
            "\n",
            " [[-0.124023 1.59375 0.886719 ... -0.15332 -0.251953 0.0473633]\n",
            "  [-0.00860596 1.33594 0.369141 ... -0.691406 -0.462891 -0.566406]\n",
            "  [-0.667969 0.0756836 1.5 ... -0.679688 -1.58594 -0.953125]\n",
            "  ...\n",
            "  [-0.0128174 0.112793 -0.0324707 ... -0.378906 0.371094 -0.458984]\n",
            "  [0.120117 0.0471191 -0.625 ... -0.283203 -0.0639648 -0.460938]\n",
            "  [0.0149536 0.21875 -0.425781 ... -0.224609 -0.123535 -0.427734]]\n",
            "\n",
            " [[-0.464844 0.351562 -0.326172 ... 0.375 -0.380859 -0.320312]\n",
            "  [-0.464844 0.351562 -0.326172 ... 0.375 -0.380859 -0.320312]\n",
            "  [-0.466797 0.355469 -0.326172 ... 0.373047 -0.378906 -0.322266]\n",
            "  ...\n",
            "  [-0.65625 -0.361328 -0.470703 ... -0.474609 -0.683594 -0.081543]\n",
            "  [-0.388672 -0.263672 -0.308594 ... -0.243164 -0.404297 -0.511719]\n",
            "  [-0.0541992 -0.910156 0.108887 ... 0.0732422 -0.601562 -0.585938]]\n",
            "\n",
            " [[-0.464844 0.351562 -0.326172 ... 0.375 -0.380859 -0.320312]\n",
            "  [-0.464844 0.351562 -0.326172 ... 0.375 -0.380859 -0.320312]\n",
            "  [-0.466797 0.355469 -0.326172 ... 0.373047 -0.378906 -0.322266]\n",
            "  ...\n",
            "  [-0.839844 -0.0233154 -0.106445 ... -0.185547 -0.234375 -0.449219]\n",
            "  [-0.371094 0.0415039 -0.078125 ... -0.180664 -0.617188 -0.652344]\n",
            "  [-0.199219 -0.11377 0.390625 ... -0.0830078 -0.601562 -0.507812]]\n",
            "\n",
            " [[-0.464844 0.351562 -0.326172 ... 0.375 -0.380859 -0.320312]\n",
            "  [-0.605469 1.14062 0.194336 ... 0.628906 -0.251953 0.367188]\n",
            "  [-0.132812 1.10156 0.0383301 ... -0.636719 -0.447266 -0.730469]\n",
            "  ...\n",
            "  [-0.365234 -0.213867 0.597656 ... -0.359375 -0.511719 -0.384766]\n",
            "  [-0.198242 -0.141602 0.112793 ... -0.15625 -0.554688 -0.707031]\n",
            "  [0.000379562 -0.640625 0.832031 ... -0.492188 -0.625 -0.566406]]]\n",
            "attn_output=[[[0.494141 -0.00139618 0.792969 ... -0.859375 -0.253906 -0.6875]\n",
            "  [0.494141 -0.00139618 0.792969 ... -0.859375 -0.253906 -0.6875]\n",
            "  [0.492188 0.00139618 0.792969 ... -0.859375 -0.251953 -0.691406]\n",
            "  ...\n",
            "  [0.175781 -0.129883 0.152344 ... -0.769531 -0.765625 2]\n",
            "  [-0.628906 -0.198242 -1.19531 ... -0.476562 0.523438 0.523438]\n",
            "  [-0.304688 -0.90625 0.0581055 ... -0.239258 0.652344 1.07031]]\n",
            "\n",
            " [[-0.472656 2.04688 0.832031 ... -0.589844 0.46875 -1.17969]\n",
            "  [0.0129395 2.01562 0.71875 ... -0.177734 0.474609 0.859375]\n",
            "  [-1.35156 1.07031 -0.273438 ... 0.417969 -1.95312 -0.882812]\n",
            "  ...\n",
            "  [0.419922 0.341797 -0.0432129 ... -0.503906 2.1875 0.180664]\n",
            "  [0.648438 0.644531 -0.871094 ... 2.10938 -0.376953 -0.320312]\n",
            "  [-0.111328 -0.738281 -0.679688 ... -0.392578 0.839844 0.804688]]\n",
            "\n",
            " [[0.494141 -0.00139618 0.792969 ... -0.859375 -0.253906 -0.6875]\n",
            "  [0.494141 -0.00139618 0.792969 ... -0.859375 -0.253906 -0.6875]\n",
            "  [0.492188 0.00139618 0.792969 ... -0.859375 -0.251953 -0.691406]\n",
            "  ...\n",
            "  [-0.65625 -0.245117 0.0222168 ... -2.01562 -0.792969 -0.183594]\n",
            "  [-1.52344 -0.0578613 -0.171875 ... -0.824219 -0.625 -0.785156]\n",
            "  [-0.169922 -1.73438 -0.166992 ... -0.10791 0.375 0.632812]]\n",
            "\n",
            " [[0.494141 -0.00139618 0.792969 ... -0.859375 -0.253906 -0.6875]\n",
            "  [0.494141 -0.00139618 0.792969 ... -0.859375 -0.253906 -0.6875]\n",
            "  [0.492188 0.00139618 0.792969 ... -0.859375 -0.251953 -0.691406]\n",
            "  ...\n",
            "  [0.5 -1.54688 -0.166016 ... -1.85938 0.367188 -0.554688]\n",
            "  [0.178711 0.628906 -0.345703 ... 2.1875 -0.886719 -0.494141]\n",
            "  [-0.304688 -1.02344 0.0898438 ... -0.251953 0.380859 0.710938]]\n",
            "\n",
            " [[0.494141 -0.00139618 0.792969 ... -0.859375 -0.253906 -0.6875]\n",
            "  [-0.886719 1.89062 0.382812 ... -0.0478516 0.515625 -1.04688]\n",
            "  [-0.090332 1.88281 0.462891 ... -0.136719 0.5 0.746094]\n",
            "  ...\n",
            "  [-0.890625 0.527344 -0.90625 ... 0.144531 -0.115234 1.67969]\n",
            "  [0.337891 -0.84375 0.5 ... 0.271484 1.8125 -0.412109]\n",
            "  [-0.124512 -1.54688 0.507812 ... -0.644531 0.367188 0.671875]]]\n",
            "next_layer_addition_dropped_out=[[[1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.304688 0.867188 ... -1.51562 -1.29688 -1.49219]\n",
            "  ...\n",
            "  [0.0576172 -0.664062 0.78125 ... -0.0195312 -0.625 2.03125]\n",
            "  [-0.792969 -0.0654297 -2.01562 ... -0.496094 0.644531 -0.179688]\n",
            "  [-1.42188 -0.957031 0.582031 ... -0.347656 0.96875 0.382812]]\n",
            "\n",
            " [[-1.01562 3.1875 0.980469 ... -0.164062 -0.03125 -1.77344]\n",
            "  [0.255859 2.1875 1.71875 ... -0.441406 0.726562 1.96875]\n",
            "  [-2.875 1.48438 -0.0546875 ... 1.17969 -2.84375 -0.4375]\n",
            "  ...\n",
            "  [1.00781 0.0859375 0.423828 ... 1.6875 1.79688 0.00390625]\n",
            "  [-0.785156 -0.015625 -0.785156 ... 2.3125 0.1875 0.695312]\n",
            "  [-0.957031 -0.380859 -0.0898438 ... 0.714844 1.35938 -0.226562]]\n",
            "\n",
            " [[1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.304688 0.867188 ... -1.51562 -1.29688 -1.49219]\n",
            "  ...\n",
            "  [0.125 -0.100586 -0.369141 ... -1.45312 -1.52344 -0.355469]\n",
            "  [-1.46094 -0.898438 -1.04688 ... -0.203125 -1.28125 0.304688]\n",
            "  [-1.375 -1.85938 -0.0527344 ... 0.277344 0.0488281 -0.414062]]\n",
            "\n",
            " [[1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.304688 0.867188 ... -1.51562 -1.29688 -1.49219]\n",
            "  ...\n",
            "  [0.242188 -0.84375 -0.859375 ... -1.53125 0.365234 -0.808594]\n",
            "  [-0.675781 0.253906 -0.198242 ... 2.53125 -0.640625 0.140625]\n",
            "  [-1.64062 -0.96875 0.644531 ... 0.0917969 0.353516 -0.183594]]\n",
            "\n",
            " [[1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [-1.3125 2.4375 0.167969 ... -0.132812 0.316406 -1.03906]\n",
            "  [-0.453125 2.14062 1.24219 ... -0.609375 0.992188 0.671875]\n",
            "  ...\n",
            "  [-0.558594 -0.429688 -0.664062 ... 0.449219 0.410156 3.09375]\n",
            "  [-0.429688 -1.48438 0.730469 ... 0.161133 2.9375 -0.457031]\n",
            "  [-1.32812 -1.01562 1.21875 ... -0.419922 0.410156 -0.402344]]]\n",
            "inputs=[[[1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.304688 0.867188 ... -1.51562 -1.29688 -1.49219]\n",
            "  ...\n",
            "  [0.0576172 -0.664062 0.78125 ... -0.0195312 -0.625 2.03125]\n",
            "  [-0.792969 -0.0654297 -2.01562 ... -0.496094 0.644531 -0.179688]\n",
            "  [-1.42188 -0.957031 0.582031 ... -0.347656 0.96875 0.382812]]\n",
            "\n",
            " [[-1.01562 3.1875 0.980469 ... -0.164062 -0.03125 -1.77344]\n",
            "  [0.255859 2.1875 1.71875 ... -0.441406 0.726562 1.96875]\n",
            "  [-2.875 1.48438 -0.0546875 ... 1.17969 -2.84375 -0.4375]\n",
            "  ...\n",
            "  [1.00781 0.0859375 0.423828 ... 1.6875 1.79688 0.00390625]\n",
            "  [-0.785156 -0.015625 -0.785156 ... 2.3125 0.1875 0.695312]\n",
            "  [-0.957031 -0.380859 -0.0898438 ... 0.714844 1.35938 -0.226562]]\n",
            "\n",
            " [[1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.304688 0.867188 ... -1.51562 -1.29688 -1.49219]\n",
            "  ...\n",
            "  [0.125 -0.100586 -0.369141 ... -1.45312 -1.52344 -0.355469]\n",
            "  [-1.46094 -0.898438 -1.04688 ... -0.203125 -1.28125 0.304688]\n",
            "  [-1.375 -1.85938 -0.0527344 ... 0.277344 0.0488281 -0.414062]]\n",
            "\n",
            " [[1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [1.34375 -0.304688 0.867188 ... -1.51562 -1.29688 -1.49219]\n",
            "  ...\n",
            "  [0.242188 -0.84375 -0.859375 ... -1.53125 0.365234 -0.808594]\n",
            "  [-0.675781 0.253906 -0.198242 ... 2.53125 -0.640625 0.140625]\n",
            "  [-1.64062 -0.96875 0.644531 ... 0.0917969 0.353516 -0.183594]]\n",
            "\n",
            " [[1.34375 -0.308594 0.875 ... -1.51562 -1.29688 -1.49219]\n",
            "  [-1.3125 2.4375 0.167969 ... -0.132812 0.316406 -1.03906]\n",
            "  [-0.453125 2.14062 1.24219 ... -0.609375 0.992188 0.671875]\n",
            "  ...\n",
            "  [-0.558594 -0.429688 -0.664062 ... 0.449219 0.410156 3.09375]\n",
            "  [-0.429688 -1.48438 0.730469 ... 0.161133 2.9375 -0.457031]\n",
            "  [-1.32812 -1.01562 1.21875 ... -0.419922 0.410156 -0.402344]]]\n",
            "lnx=[[[0.855469 -0.196289 0.558594 ... -0.964844 -0.828125 -0.949219]\n",
            "  [0.855469 -0.196289 0.558594 ... -0.964844 -0.828125 -0.949219]\n",
            "  [0.855469 -0.194336 0.550781 ... -0.964844 -0.828125 -0.949219]\n",
            "  ...\n",
            "  [0.0424805 -0.490234 0.578125 ... -0.0144043 -0.460938 1.5]\n",
            "  [-0.625 -0.0517578 -1.59375 ... -0.392578 0.507812 -0.141602]\n",
            "  [-1.07031 -0.722656 0.439453 ... -0.261719 0.730469 0.289062]]\n",
            "\n",
            " [[-0.640625 2.01562 0.621094 ... -0.104004 -0.0197754 -1.125]\n",
            "  [0.18457 1.57812 1.24219 ... -0.318359 0.523438 1.42188]\n",
            "  [-2.09375 1.07812 -0.0397949 ... 0.859375 -2.0625 -0.318359]\n",
            "  ...\n",
            "  [0.792969 0.0678711 0.333984 ... 1.32812 1.41406 0.00308228]\n",
            "  [-0.621094 -0.0123901 -0.621094 ... 1.83594 0.148438 0.550781]\n",
            "  [-0.773438 -0.306641 -0.0722656 ... 0.578125 1.09375 -0.182617]]\n",
            "\n",
            " [[0.855469 -0.196289 0.558594 ... -0.964844 -0.828125 -0.949219]\n",
            "  [0.855469 -0.196289 0.558594 ... -0.964844 -0.828125 -0.949219]\n",
            "  [0.855469 -0.194336 0.550781 ... -0.964844 -0.828125 -0.949219]\n",
            "  ...\n",
            "  [0.101562 -0.081543 -0.298828 ... -1.17969 -1.23438 -0.289062]\n",
            "  [-1.14062 -0.703125 -0.820312 ... -0.15918 -1 0.238281]\n",
            "  [-1.07031 -1.44531 -0.0410156 ... 0.21582 0.0378418 -0.322266]]\n",
            "\n",
            " [[0.855469 -0.196289 0.558594 ... -0.964844 -0.828125 -0.949219]\n",
            "  [0.855469 -0.196289 0.558594 ... -0.964844 -0.828125 -0.949219]\n",
            "  [0.855469 -0.194336 0.550781 ... -0.964844 -0.828125 -0.949219]\n",
            "  ...\n",
            "  [0.189453 -0.660156 -0.671875 ... -1.19531 0.285156 -0.632812]\n",
            "  [-0.53125 0.200195 -0.15625 ... 1.99219 -0.503906 0.11084]\n",
            "  [-1.28125 -0.753906 0.503906 ... 0.0717773 0.275391 -0.143555]]\n",
            "\n",
            " [[0.855469 -0.196289 0.558594 ... -0.964844 -0.828125 -0.949219]\n",
            "  [-0.898438 1.66406 0.114746 ... -0.0908203 0.216797 -0.710938]\n",
            "  [-0.333984 1.57812 0.914062 ... -0.449219 0.730469 0.494141]\n",
            "  ...\n",
            "  [-0.457031 -0.351562 -0.542969 ... 0.367188 0.335938 2.53125]\n",
            "  [-0.34375 -1.1875 0.582031 ... 0.128906 2.34375 -0.365234]\n",
            "  [-1.07031 -0.816406 0.980469 ... -0.337891 0.330078 -0.322266]]]\n",
            "attention_lnx=[[[2.20312 0.679688 0.71875 ... 0.492188 -0.3125 -0.628906]\n",
            "  [2.20312 0.679688 0.71875 ... 0.492188 -0.3125 -0.628906]\n",
            "  [2.1875 0.679688 0.71875 ... 0.486328 -0.314453 -0.625]\n",
            "  ...\n",
            "  [1.50781 1.10156 0.925781 ... 0.149414 0.0471191 -0.0439453]\n",
            "  [1.14062 0.558594 0.429688 ... -0.298828 0.578125 -0.412109]\n",
            "  [1.63281 1.09375 0.910156 ... 0.0234375 -0.0410156 -0.0393066]]\n",
            "\n",
            " [[0.9375 1.53125 1.10156 ... -0.380859 -1.25781 2.04688]\n",
            "  [0.410156 1.40625 1.55469 ... -0.269531 -0.859375 1.69531]\n",
            "  [1.05469 1.38281 1.16406 ... -0.239258 -0.648438 0.882812]\n",
            "  ...\n",
            "  [0.691406 0.5625 0.8125 ... -0.730469 0.398438 0.164062]\n",
            "  [0.503906 0.601562 0.832031 ... -0.878906 0.188477 0.130859]\n",
            "  [0.0541992 0.523438 0.644531 ... -0.302734 -0.245117 -0.074707]]\n",
            "\n",
            " [[2.20312 0.679688 0.71875 ... 0.492188 -0.3125 -0.628906]\n",
            "  [2.20312 0.679688 0.71875 ... 0.492188 -0.3125 -0.628906]\n",
            "  [2.1875 0.679688 0.71875 ... 0.486328 -0.314453 -0.625]\n",
            "  ...\n",
            "  [0.941406 0.859375 1.09375 ... 0.0461426 0.235352 0.0588379]\n",
            "  [0.984375 0.824219 0.925781 ... -0.170898 0.0220947 -0.251953]\n",
            "  [1.07031 0.761719 0.447266 ... -0.28125 -0.46875 0.158203]]\n",
            "\n",
            " [[2.20312 0.679688 0.71875 ... 0.492188 -0.3125 -0.628906]\n",
            "  [2.20312 0.679688 0.71875 ... 0.492188 -0.3125 -0.628906]\n",
            "  [2.1875 0.679688 0.71875 ... 0.486328 -0.314453 -0.625]\n",
            "  ...\n",
            "  [1.00781 1.04688 0.742188 ... -0.363281 -0.460938 -0.878906]\n",
            "  [0.46875 0.404297 1.16406 ... -0.546875 0.10498 0.0883789]\n",
            "  [1.17969 0.78125 0.910156 ... -0.09375 0.155273 0.0192871]]\n",
            "\n",
            " [[2.20312 0.679688 0.71875 ... 0.492188 -0.3125 -0.628906]\n",
            "  [1.16406 0.707031 1.34375 ... 0.466797 -0.427734 0.671875]\n",
            "  [0.875 1.1875 0.667969 ... 0.574219 -0.761719 0.59375]\n",
            "  ...\n",
            "  [0.130859 0.816406 0.695312 ... -0.376953 -0.114258 -0.419922]\n",
            "  [0.691406 1.25781 0.337891 ... -0.193359 -0.134766 -0.104004]\n",
            "  [0.695312 0.871094 0.283203 ... 0.0174561 0.0620117 0.294922]]]\n",
            "attn_output=[[[1.89062 0.198242 0.847656 ... -0.546875 -0.859375 -1.13281]\n",
            "  [1.89062 0.198242 0.847656 ... -0.546875 -0.859375 -1.13281]\n",
            "  [1.88281 0.200195 0.84375 ... -0.546875 -0.859375 -1.125]\n",
            "  ...\n",
            "  [1.02344 0.285156 1.10938 ... 0.0844727 -0.376953 1.29688]\n",
            "  [0.237305 0.335938 -1.07812 ... -0.542969 0.832031 -0.404297]\n",
            "  [0.143555 0.0927734 1.01562 ... -0.220703 0.628906 0.233398]]\n",
            "\n",
            " [[-0.0427246 2.57812 1.14062 ... -0.298828 -0.707031 0.149414]\n",
            "  [0.412109 2.21875 2.03125 ... -0.439453 -0.0820312 2.26562]\n",
            "  [-1.17188 1.84375 0.714844 ... 0.605469 -2.25 0.287109]\n",
            "  ...\n",
            "  [1.28125 0.488281 0.933594 ... 0.722656 1.65625 0.126953]\n",
            "  [-0.209961 0.4375 0.0349121 ... 1.07031 0.28125 0.617188]\n",
            "  [-0.695312 0.109375 0.425781 ... 0.316406 0.855469 -0.231445]]\n",
            "\n",
            " [[1.89062 0.198242 0.847656 ... -0.546875 -0.859375 -1.13281]\n",
            "  [1.89062 0.198242 0.847656 ... -0.546875 -0.859375 -1.13281]\n",
            "  [1.88281 0.200195 0.84375 ... -0.546875 -0.859375 -1.125]\n",
            "  ...\n",
            "  [0.796875 0.566406 0.542969 ... -1.05469 -0.964844 -0.22168]\n",
            "  [-0.345703 -0.0537109 -0.0874023 ... -0.271484 -0.910156 0.0380859]\n",
            "  [-0.219727 -0.792969 0.285156 ... -0.00282288 -0.302734 -0.18457]]\n",
            "\n",
            " [[1.89062 0.198242 0.847656 ... -0.546875 -0.859375 -1.13281]\n",
            "  [1.89062 0.198242 0.847656 ... -0.546875 -0.859375 -1.13281]\n",
            "  [1.88281 0.200195 0.84375 ... -0.546875 -0.859375 -1.125]\n",
            "  ...\n",
            "  [0.90625 0.147461 -0.0849609 ... -1.375 -0.0693359 -1.21875]\n",
            "  [-0.151367 0.482422 0.707031 ... 1.45312 -0.392578 0.167969]\n",
            "  [-0.339844 -0.137695 1.14844 ... -0.00144196 0.375 -0.121094]]\n",
            "\n",
            " [[1.89062 0.198242 0.847656 ... -0.546875 -0.859375 -1.13281]\n",
            "  [-0.0874023 1.85938 0.890625 ... 0.197266 -0.065918 -0.216797]\n",
            "  [0.267578 2.125 1.21875 ... -0.0223389 0.146484 0.804688]\n",
            "  ...\n",
            "  [-0.330078 0.298828 0.0241699 ... 0.0559082 0.228516 2.0625]\n",
            "  [0.198242 -0.171875 0.808594 ... -0.0244141 2.125 -0.425781]\n",
            "  [-0.482422 -0.109863 1.14062 ... -0.306641 0.359375 -0.0820312]]]\n",
            "next_layer_addition_dropped_out=[[[2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.5625 0.722656 1.5625 ... -1.15625 -1.52344 -2.20312]\n",
            "  ...\n",
            "  [1.47656 0.84375 1.17188 ... -0.00292969 -0.21875 2.53125]\n",
            "  [0.462891 0.152344 -1.875 ... -0.433594 0.488281 -0.539062]\n",
            "  [-0.523438 -0.605469 1.375 ... -1.15625 0.976562 -0.21875]]\n",
            "\n",
            " [[0.135742 5.125 2.48438 ... -0.515625 -1.74219 0.119141]\n",
            "  [1.40625 2.75 2.3125 ... -0.992188 0.173828 3.78125]\n",
            "  [-0.765625 2.07812 1.02344 ... 1.48438 -2.5 0.796875]\n",
            "  ...\n",
            "  [2.0625 0.345703 1.14062 ... 1.10156 2 -0.496094]\n",
            "  [-0.253906 0.398438 0.0957031 ... 1.53906 0.578125 0.964844]\n",
            "  [-0.875 -0.148438 0.503906 ... -0.859375 1.92188 -1.34375]]\n",
            "\n",
            " [[2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.5625 0.722656 1.5625 ... -1.15625 -1.52344 -2.20312]\n",
            "  ...\n",
            "  [1.375 1.5 0.730469 ... -2.10938 -2.15625 -0.816406]\n",
            "  [-1.34375 0.320312 -0.808594 ... -1.17969 -1.75 0.065918]\n",
            "  [-0.996094 -1.17969 -0.0527344 ... -0.722656 0.123047 -0.6875]]\n",
            "\n",
            " [[2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.5625 0.722656 1.5625 ... -1.15625 -1.52344 -2.20312]\n",
            "  ...\n",
            "  [0.679688 0.792969 -0.523438 ... -2.42188 -0.382812 -1.77344]\n",
            "  [-0.235352 0.332031 0.851562 ... 2.20312 -0.371094 -0.416016]\n",
            "  [-1.17188 -0.648438 1.39844 ... -1.17969 1.08594 -1.09375]]\n",
            "\n",
            " [[2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [0.617188 3.14062 1.34375 ... 0.40625 -0.84375 -0.820312]\n",
            "  [0.789062 2.26562 1.60156 ... -0.507812 -0.0234375 1.95312]\n",
            "  ...\n",
            "  [-0.328125 1.73438 0.365234 ... -0.632812 0.78125 3.23438]\n",
            "  [0.269531 -0.219727 1.1875 ... -0.71875 2.875 -0.890625]\n",
            "  [-1.5 -0.0620117 0.304688 ... -1.15625 1.33594 -1.19531]]]\n",
            "inputs=[[[2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.5625 0.722656 1.5625 ... -1.15625 -1.52344 -2.20312]\n",
            "  ...\n",
            "  [1.47656 0.84375 1.17188 ... -0.00292969 -0.21875 2.53125]\n",
            "  [0.462891 0.152344 -1.875 ... -0.433594 0.488281 -0.539062]\n",
            "  [-0.523438 -0.605469 1.375 ... -1.15625 0.976562 -0.21875]]\n",
            "\n",
            " [[0.135742 5.125 2.48438 ... -0.515625 -1.74219 0.119141]\n",
            "  [1.40625 2.75 2.3125 ... -0.992188 0.173828 3.78125]\n",
            "  [-0.765625 2.07812 1.02344 ... 1.48438 -2.5 0.796875]\n",
            "  ...\n",
            "  [2.0625 0.345703 1.14062 ... 1.10156 2 -0.496094]\n",
            "  [-0.253906 0.398438 0.0957031 ... 1.53906 0.578125 0.964844]\n",
            "  [-0.875 -0.148438 0.503906 ... -0.859375 1.92188 -1.34375]]\n",
            "\n",
            " [[2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.5625 0.722656 1.5625 ... -1.15625 -1.52344 -2.20312]\n",
            "  ...\n",
            "  [1.375 1.5 0.730469 ... -2.10938 -2.15625 -0.816406]\n",
            "  [-1.34375 0.320312 -0.808594 ... -1.17969 -1.75 0.065918]\n",
            "  [-0.996094 -1.17969 -0.0527344 ... -0.722656 0.123047 -0.6875]]\n",
            "\n",
            " [[2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [2.5625 0.722656 1.5625 ... -1.15625 -1.52344 -2.20312]\n",
            "  ...\n",
            "  [0.679688 0.792969 -0.523438 ... -2.42188 -0.382812 -1.77344]\n",
            "  [-0.235352 0.332031 0.851562 ... 2.20312 -0.371094 -0.416016]\n",
            "  [-1.17188 -0.648438 1.39844 ... -1.17969 1.08594 -1.09375]]\n",
            "\n",
            " [[2.59375 0.726562 1.57031 ... -1.16406 -1.52344 -2.20312]\n",
            "  [0.617188 3.14062 1.34375 ... 0.40625 -0.84375 -0.820312]\n",
            "  [0.789062 2.26562 1.60156 ... -0.507812 -0.0234375 1.95312]\n",
            "  ...\n",
            "  [-0.328125 1.73438 0.365234 ... -0.632812 0.78125 3.23438]\n",
            "  [0.269531 -0.219727 1.1875 ... -0.71875 2.875 -0.890625]\n",
            "  [-1.5 -0.0620117 0.304688 ... -1.15625 1.33594 -1.19531]]]\n",
            "lnx=[[[1.29688 0.363281 0.785156 ... -0.582031 -0.761719 -1.10156]\n",
            "  [1.29688 0.363281 0.785156 ... -0.582031 -0.761719 -1.10156]\n",
            "  [1.28125 0.361328 0.78125 ... -0.578125 -0.761719 -1.10156]\n",
            "  ...\n",
            "  [0.875 0.5 0.695312 ... -0.00173187 -0.129883 1.5]\n",
            "  [0.291016 0.0957031 -1.17969 ... -0.273438 0.306641 -0.339844]\n",
            "  [-0.322266 -0.371094 0.84375 ... -0.710938 0.601562 -0.134766]]\n",
            "\n",
            " [[0.0703125 2.64062 1.28125 ... -0.265625 -0.898438 0.0615234]\n",
            "  [0.804688 1.57031 1.32031 ... -0.566406 0.0991211 2.15625]\n",
            "  [-0.453125 1.23438 0.605469 ... 0.878906 -1.48438 0.472656]\n",
            "  ...\n",
            "  [1.41406 0.236328 0.78125 ... 0.753906 1.36719 -0.339844]\n",
            "  [-0.167969 0.263672 0.0634766 ... 1.02344 0.382812 0.640625]\n",
            "  [-0.605469 -0.103027 0.349609 ... -0.597656 1.33594 -0.929688]]\n",
            "\n",
            " [[1.29688 0.363281 0.785156 ... -0.582031 -0.761719 -1.10156]\n",
            "  [1.29688 0.363281 0.785156 ... -0.582031 -0.761719 -1.10156]\n",
            "  [1.28125 0.361328 0.78125 ... -0.578125 -0.761719 -1.10156]\n",
            "  ...\n",
            "  [0.941406 1.02344 0.5 ... -1.44531 -1.47656 -0.558594]\n",
            "  [-0.882812 0.210938 -0.53125 ... -0.773438 -1.14844 0.0432129]\n",
            "  [-0.652344 -0.773438 -0.034668 ... -0.474609 0.0805664 -0.451172]]\n",
            "\n",
            " [[1.29688 0.363281 0.785156 ... -0.582031 -0.761719 -1.10156]\n",
            "  [1.29688 0.363281 0.785156 ... -0.582031 -0.761719 -1.10156]\n",
            "  [1.28125 0.361328 0.78125 ... -0.578125 -0.761719 -1.10156]\n",
            "  ...\n",
            "  [0.449219 0.523438 -0.345703 ... -1.60156 -0.253906 -1.17188]\n",
            "  [-0.155273 0.21875 0.5625 ... 1.45312 -0.245117 -0.273438]\n",
            "  [-0.773438 -0.427734 0.921875 ... -0.777344 0.714844 -0.722656]]\n",
            "\n",
            " [[1.29688 0.363281 0.785156 ... -0.582031 -0.761719 -1.10156]\n",
            "  [0.341797 1.73438 0.742188 ... 0.224609 -0.466797 -0.453125]\n",
            "  [0.462891 1.32812 0.941406 ... -0.298828 -0.0137939 1.14844]\n",
            "  ...\n",
            "  [-0.228516 1.21094 0.253906 ... -0.441406 0.542969 2.25]\n",
            "  [0.180664 -0.146484 0.792969 ... -0.480469 1.92188 -0.59375]\n",
            "  [-1.03125 -0.0424805 0.208984 ... -0.792969 0.917969 -0.820312]]]\n",
            "attention_lnx=[[[-1.27344 -1.07812 1.3125 ... -0.410156 0.984375 -0.294922]\n",
            "  [-1.27344 -1.07812 1.3125 ... -0.410156 0.984375 -0.294922]\n",
            "  [-1.27344 -1.08594 1.30469 ... -0.412109 0.984375 -0.298828]\n",
            "  ...\n",
            "  [-0.734375 -1.01562 1.1875 ... 0.245117 0.330078 0.0263672]\n",
            "  [-0.0717773 -1.03125 1.01562 ... 0.699219 0.203125 0.257812]\n",
            "  [-0.738281 -0.613281 1.01562 ... 0.414062 0.0368652 0.0849609]]\n",
            "\n",
            " [[0.769531 -0.609375 0.96875 ... -1.74219 0.273438 0.00454712]\n",
            "  [0.632812 0.439453 1.42188 ... -0.71875 0.351562 0.277344]\n",
            "  [0.267578 0.388672 0.279297 ... -0.166992 0.480469 0.695312]\n",
            "  ...\n",
            "  [0.032959 0.539062 0.308594 ... -0.609375 0.208008 -0.267578]\n",
            "  [0.511719 0.3125 0.601562 ... -0.523438 0.226562 -0.460938]\n",
            "  [0.230469 0.636719 0.283203 ... -0.511719 0.0961914 0.154297]]\n",
            "\n",
            " [[-1.27344 -1.07812 1.3125 ... -0.410156 0.984375 -0.294922]\n",
            "  [-1.27344 -1.07812 1.3125 ... -0.410156 0.984375 -0.294922]\n",
            "  [-1.27344 -1.08594 1.30469 ... -0.412109 0.984375 -0.298828]\n",
            "  ...\n",
            "  [-0.394531 -0.0161133 0.217773 ... 0.193359 0.404297 0.173828]\n",
            "  [-0.386719 0.0825195 0.648438 ... -0.257812 0.753906 0.412109]\n",
            "  [0.326172 -0.105957 0.324219 ... -0.261719 0.542969 0.492188]]\n",
            "\n",
            " [[-1.27344 -1.07812 1.3125 ... -0.410156 0.984375 -0.294922]\n",
            "  [-1.27344 -1.07812 1.3125 ... -0.410156 0.984375 -0.294922]\n",
            "  [-1.27344 -1.08594 1.30469 ... -0.412109 0.984375 -0.298828]\n",
            "  ...\n",
            "  [-0.349609 -0.0495605 0.210938 ... 0.09375 0.213867 0.0319824]\n",
            "  [-0.316406 0.160156 0.251953 ... 0.136719 -0.0888672 0.570312]\n",
            "  [-0.0412598 0.0942383 0.102051 ... -0.0541992 0.149414 0.613281]]\n",
            "\n",
            " [[-1.27344 -1.07812 1.3125 ... -0.410156 0.984375 -0.294922]\n",
            "  [-0.585938 -1.14062 1.23438 ... -0.337891 0.660156 0.28125]\n",
            "  [-0.605469 0.239258 1.32812 ... -0.0742188 0.515625 0.414062]\n",
            "  ...\n",
            "  [-0.435547 0.902344 0.617188 ... -0.474609 0.310547 0.839844]\n",
            "  [-0.134766 0.5 0.414062 ... 0.128906 0.589844 0.304688]\n",
            "  [-0.253906 0.6875 0.0893555 ... -0.0126343 0.380859 0.371094]]]\n",
            "attn_output=[[[0.582031 -0.154297 1.26562 ... -0.691406 -0.237305 -1.09375]\n",
            "  [0.582031 -0.154297 1.26562 ... -0.691406 -0.237305 -1.09375]\n",
            "  [0.566406 -0.15918 1.25781 ... -0.6875 -0.237305 -1.10156]\n",
            "  ...\n",
            "  [0.394531 -0.0913086 1.25 ... 0.128906 0.059082 1.35938]\n",
            "  [0.223633 -0.503906 -0.492188 ... 0.152344 0.396484 -0.161133]\n",
            "  [-0.699219 -0.675781 1.32812 ... -0.412109 0.5625 -0.0742188]]\n",
            "\n",
            " [[0.417969 2.07812 1.59375 ... -1.03906 -0.675781 0.0571289]\n",
            "  [1.05469 1.64844 1.92969 ... -0.882812 0.271484 2.09375]\n",
            "  [-0.275391 1.36719 0.71875 ... 0.726562 -1.11719 0.824219]\n",
            "  ...\n",
            "  [1.375 0.578125 0.949219 ... 0.322266 1.44531 -0.5]\n",
            "  [0.163086 0.451172 0.441406 ... 0.644531 0.511719 0.320312]\n",
            "  [-0.423828 0.322266 0.519531 ... -0.902344 1.32812 -0.78125]]\n",
            "\n",
            " [[0.582031 -0.154297 1.26562 ... -0.691406 -0.237305 -1.09375]\n",
            "  [0.582031 -0.154297 1.26562 ... -0.691406 -0.237305 -1.09375]\n",
            "  [0.566406 -0.15918 1.25781 ... -0.6875 -0.237305 -1.10156]\n",
            "  ...\n",
            "  [0.625 0.945312 0.605469 ... -1.21875 -1.11719 -0.410156]\n",
            "  [-1.05469 0.245117 -0.0976562 ... -0.875 -0.605469 0.291016]\n",
            "  [-0.410156 -0.789062 0.166016 ... -0.601562 0.408203 -0.119629]]\n",
            "\n",
            " [[0.582031 -0.154297 1.26562 ... -0.691406 -0.237305 -1.09375]\n",
            "  [0.582031 -0.154297 1.26562 ... -0.691406 -0.237305 -1.09375]\n",
            "  [0.566406 -0.15918 1.25781 ... -0.6875 -0.237305 -1.10156]\n",
            "  ...\n",
            "  [0.203125 0.457031 -0.192383 ... -1.42969 -0.104004 -1.07031]\n",
            "  [-0.339844 0.302734 0.679688 ... 1.44531 -0.283203 0.0952148]\n",
            "  [-0.75 -0.34375 0.929688 ... -0.761719 0.765625 -0.296875]]\n",
            "\n",
            " [[0.582031 -0.154297 1.26562 ... -0.691406 -0.237305 -1.09375]\n",
            "  [0.015564 0.996094 1.28125 ... 0.0339355 -0.0913086 -0.267578]\n",
            "  [0.0981445 1.33594 1.5625 ... -0.310547 0.263672 1.26562]\n",
            "  ...\n",
            "  [-0.498047 1.71875 0.640625 ... -0.722656 0.710938 2.65625]\n",
            "  [0.0859375 0.178711 1.02344 ... -0.376953 2.20312 -0.373047]\n",
            "  [-1.13281 0.402344 0.253906 ... -0.753906 1.10938 -0.53125]]]\n",
            "next_layer_addition_dropped_out=[[[1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.77344 -0.0683594 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  ...\n",
            "  [0.349609 -0.0283203 2.71875 ... -0.193359 0.34375 2.23438]\n",
            "  [0.578125 -1.07812 0.859375 ... 0.59375 0.386719 -0.382812]\n",
            "  [-1.96875 -1.23438 2.64062 ... -0.710938 0.730469 -1.26562]]\n",
            "\n",
            " [[1.20312 4.03125 2.92188 ... -1.57812 -1.0625 0.0336914]\n",
            "  [0.820312 3 3.21875 ... -1.07812 1.75 3.46875]\n",
            "  [-0.867188 1.96875 1.4375 ... 1.20312 -1.85938 1.78906]\n",
            "  ...\n",
            "  [3.21875 0.152344 1.60156 ... 0.363281 2.0625 -1.02344]\n",
            "  [-0.0332031 0.703125 0.972656 ... 0.570312 0.746094 -0.09375]\n",
            "  [-1.64062 -0.253906 -0.382812 ... -1.94531 1.91406 -2.34375]]\n",
            "\n",
            " [[1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.77344 -0.0683594 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  ...\n",
            "  [0.882812 1.36719 1.28125 ... -2.15625 -2.75 -1.03125]\n",
            "  [-2.03125 0.427734 0.585938 ... -1.57031 -1.89844 -0.0839844]\n",
            "  [-1.07031 -2.40625 0.166016 ... -1.4375 0.0273438 -1.35156]]\n",
            "\n",
            " [[1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.77344 -0.0683594 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  ...\n",
            "  [0.765625 1.29688 -0.433594 ... -1.41406 -0.726562 -1.71094]\n",
            "  [-0.6875 -0.210938 0.53125 ... 2.53125 -0.236328 -0.392578]\n",
            "  [-2.125 -1.32812 0.777344 ... -1.35156 0.992188 -1.92188]]\n",
            "\n",
            " [[1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.21094 1.82031 2.25 ... -0.101562 -0.871094 -0.878906]\n",
            "  [-0.21875 2.96875 2.35938 ... 0.0976562 1.21875 0.976562]\n",
            "  ...\n",
            "  [-0.738281 3.28125 0.800781 ... -1.0625 1.28125 3.96875]\n",
            "  [0.6875 0.116211 1.4375 ... -0.75 4.125 -1.46875]\n",
            "  [-2.5625 0.498047 -0.34375 ... -1.46875 1.40625 -2.20312]]]\n",
            "inputs=[[[1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.77344 -0.0683594 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  ...\n",
            "  [0.349609 -0.0283203 2.71875 ... -0.193359 0.34375 2.23438]\n",
            "  [0.578125 -1.07812 0.859375 ... 0.59375 0.386719 -0.382812]\n",
            "  [-1.96875 -1.23438 2.64062 ... -0.710938 0.730469 -1.26562]]\n",
            "\n",
            " [[1.20312 4.03125 2.92188 ... -1.57812 -1.0625 0.0336914]\n",
            "  [0.820312 3 3.21875 ... -1.07812 1.75 3.46875]\n",
            "  [-0.867188 1.96875 1.4375 ... 1.20312 -1.85938 1.78906]\n",
            "  ...\n",
            "  [3.21875 0.152344 1.60156 ... 0.363281 2.0625 -1.02344]\n",
            "  [-0.0332031 0.703125 0.972656 ... 0.570312 0.746094 -0.09375]\n",
            "  [-1.64062 -0.253906 -0.382812 ... -1.94531 1.91406 -2.34375]]\n",
            "\n",
            " [[1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.77344 -0.0683594 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  ...\n",
            "  [0.882812 1.36719 1.28125 ... -2.15625 -2.75 -1.03125]\n",
            "  [-2.03125 0.427734 0.585938 ... -1.57031 -1.89844 -0.0839844]\n",
            "  [-1.07031 -2.40625 0.166016 ... -1.4375 0.0273438 -1.35156]]\n",
            "\n",
            " [[1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.77344 -0.0683594 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  ...\n",
            "  [0.765625 1.29688 -0.433594 ... -1.41406 -0.726562 -1.71094]\n",
            "  [-0.6875 -0.210938 0.53125 ... 2.53125 -0.236328 -0.392578]\n",
            "  [-2.125 -1.32812 0.777344 ... -1.35156 0.992188 -1.92188]]\n",
            "\n",
            " [[1.8125 -0.0566406 3.375 ... -2.57812 -1.71094 -2.9375]\n",
            "  [1.21094 1.82031 2.25 ... -0.101562 -0.871094 -0.878906]\n",
            "  [-0.21875 2.96875 2.35938 ... 0.0976562 1.21875 0.976562]\n",
            "  ...\n",
            "  [-0.738281 3.28125 0.800781 ... -1.0625 1.28125 3.96875]\n",
            "  [0.6875 0.116211 1.4375 ... -0.75 4.125 -1.46875]\n",
            "  [-2.5625 0.498047 -0.34375 ... -1.46875 1.40625 -2.20312]]]\n",
            "lnx=[[[0.769531 -0.0240479 1.42969 ... -1.09375 -0.726562 -1.24219]\n",
            "  [0.769531 -0.0240479 1.42969 ... -1.09375 -0.726562 -1.24219]\n",
            "  [0.753906 -0.0290527 1.42969 ... -1.09375 -0.726562 -1.24219]\n",
            "  ...\n",
            "  [0.174805 -0.0141602 1.35938 ... -0.0966797 0.171875 1.11719]\n",
            "  [0.3125 -0.582031 0.464844 ... 0.320312 0.208984 -0.207031]\n",
            "  [-1.01562 -0.636719 1.36719 ... -0.367188 0.376953 -0.652344]]\n",
            "\n",
            " [[0.535156 1.78906 1.29688 ... -0.699219 -0.470703 0.0149536]\n",
            "  [0.40625 1.48438 1.59375 ... -0.535156 0.867188 1.71875]\n",
            "  [-0.455078 1.03125 0.753906 ... 0.632812 -0.976562 0.9375]\n",
            "  ...\n",
            "  [1.9375 0.0917969 0.964844 ... 0.21875 1.24219 -0.617188]\n",
            "  [-0.019043 0.402344 0.558594 ... 0.326172 0.427734 -0.0537109]\n",
            "  [-0.980469 -0.152344 -0.229492 ... -1.16406 1.14844 -1.40625]]\n",
            "\n",
            " [[0.769531 -0.0240479 1.42969 ... -1.09375 -0.726562 -1.24219]\n",
            "  [0.769531 -0.0240479 1.42969 ... -1.09375 -0.726562 -1.24219]\n",
            "  [0.753906 -0.0290527 1.42969 ... -1.09375 -0.726562 -1.24219]\n",
            "  ...\n",
            "  [0.519531 0.804688 0.753906 ... -1.27344 -1.625 -0.609375]\n",
            "  [-1.17188 0.246094 0.337891 ... -0.902344 -1.09375 -0.0483398]\n",
            "  [-0.601562 -1.35156 0.0932617 ... -0.808594 0.0153809 -0.757812]]\n",
            "\n",
            " [[0.769531 -0.0240479 1.42969 ... -1.09375 -0.726562 -1.24219]\n",
            "  [0.769531 -0.0240479 1.42969 ... -1.09375 -0.726562 -1.24219]\n",
            "  [0.753906 -0.0290527 1.42969 ... -1.09375 -0.726562 -1.24219]\n",
            "  ...\n",
            "  [0.439453 0.746094 -0.249023 ... -0.8125 -0.417969 -0.984375]\n",
            "  [-0.388672 -0.119141 0.300781 ... 1.42969 -0.133789 -0.22168]\n",
            "  [-1.19531 -0.746094 0.435547 ... -0.757812 0.558594 -1.07812]]\n",
            "\n",
            " [[0.769531 -0.0240479 1.42969 ... -1.09375 -0.726562 -1.24219]\n",
            "  [0.574219 0.867188 1.07031 ... -0.0483398 -0.414062 -0.417969]\n",
            "  [-0.111328 1.50781 1.19531 ... 0.0495605 0.621094 0.496094]\n",
            "  ...\n",
            "  [-0.441406 1.96094 0.478516 ... -0.636719 0.765625 2.375]\n",
            "  [0.40625 0.0683594 0.847656 ... -0.441406 2.4375 -0.867188]\n",
            "  [-1.49219 0.289062 -0.200195 ... -0.855469 0.816406 -1.28125]]]\n",
            "attention_lnx=[[[0.244141 -0.337891 -0.0585938 ... -0.433594 1.28125 0.232422]\n",
            "  [0.244141 -0.337891 -0.0585938 ... -0.433594 1.28125 0.232422]\n",
            "  [0.242188 -0.341797 -0.0612793 ... -0.435547 1.27344 0.229492]\n",
            "  ...\n",
            "  [-0.0825195 -0.824219 0.00854492 ... 0.267578 1.1875 0.0612793]\n",
            "  [-0.00289917 -0.914062 -0.138672 ... 0.482422 1.04688 0.105957]\n",
            "  [-0.0175781 -0.710938 -0.216797 ... 0.335938 1.03125 -0.0512695]]\n",
            "\n",
            " [[0.511719 -0.933594 0.976562 ... 0.417969 -0.734375 0.0820312]\n",
            "  [0.333984 -0.318359 0.613281 ... 0.0766602 -0.640625 -0.020874]\n",
            "  [0.388672 -0.0634766 0.400391 ... -0.0722656 -0.492188 0.179688]\n",
            "  ...\n",
            "  [0.207031 0.675781 0.138672 ... -0.730469 -0.347656 -0.285156]\n",
            "  [0.285156 0.703125 0.539062 ... -0.128906 -0.188477 -0.353516]\n",
            "  [0.269531 0.679688 0.597656 ... -0.507812 -0.333984 -0.410156]]\n",
            "\n",
            " [[0.244141 -0.337891 -0.0585938 ... -0.433594 1.28125 0.232422]\n",
            "  [0.244141 -0.337891 -0.0585938 ... -0.433594 1.28125 0.232422]\n",
            "  [0.242188 -0.341797 -0.0612793 ... -0.435547 1.27344 0.229492]\n",
            "  ...\n",
            "  [-0.302734 0.0407715 0.308594 ... 0.283203 0.957031 0.425781]\n",
            "  [-0.478516 0.100586 0.4375 ... 0.398438 0.992188 0.165039]\n",
            "  [-0.0800781 0.0245361 0.000249863 ... 0.640625 0.765625 -0.0864258]]\n",
            "\n",
            " [[0.244141 -0.337891 -0.0585938 ... -0.433594 1.28125 0.232422]\n",
            "  [0.244141 -0.337891 -0.0585938 ... -0.433594 1.28125 0.232422]\n",
            "  [0.242188 -0.341797 -0.0612793 ... -0.435547 1.27344 0.229492]\n",
            "  ...\n",
            "  [-0.474609 0.423828 -0.116699 ... 0.462891 0.59375 -0.269531]\n",
            "  [-0.53125 0.277344 -0.251953 ... 0.777344 0.304688 -0.597656]\n",
            "  [-0.020874 0.200195 -0.546875 ... 0.660156 0.636719 -0.699219]]\n",
            "\n",
            " [[0.244141 -0.337891 -0.0585938 ... -0.433594 1.28125 0.232422]\n",
            "  [-0.0478516 -0.445312 0.511719 ... -0.582031 0.898438 -0.10791]\n",
            "  [-0.179688 -0.363281 0.191406 ... -0.255859 0.421875 -0.219727]\n",
            "  ...\n",
            "  [-0.347656 0.585938 0.0961914 ... 0.273438 0.21875 -0.121094]\n",
            "  [-0.241211 0.652344 0.341797 ... 0.314453 0.28125 -0.398438]\n",
            "  [-0.0324707 0.628906 -0.155273 ... 0.287109 0.12793 -0.496094]]]\n",
            "attn_output=[[[0.800781 -0.15332 1.28906 ... -1.17188 -0.166992 -1.05469]\n",
            "  [0.800781 -0.15332 1.28906 ... -1.17188 -0.166992 -1.05469]\n",
            "  [0.785156 -0.15918 1.28906 ... -1.17188 -0.169922 -1.05469]\n",
            "  ...\n",
            "  [0.124023 -0.396484 1.26562 ... 0.0344238 0.710938 1.0625]\n",
            "  [0.289062 -1 0.363281 ... 0.542969 0.722656 -0.139648]\n",
            "  [-0.960938 -0.941406 1.17188 ... -0.181641 0.851562 -0.636719]]\n",
            "\n",
            " [[0.695312 1.25781 1.58594 ... -0.470703 -0.730469 0.046875]\n",
            "  [0.53125 1.23438 1.76562 ... -0.462891 0.511719 1.59375]\n",
            "  [-0.231445 0.921875 0.890625 ... 0.546875 -1.14062 0.953125]\n",
            "  ...\n",
            "  [1.95312 0.472656 0.992188 ... -0.209961 0.980469 -0.746094]\n",
            "  [0.137695 0.765625 0.824219 ... 0.241211 0.304688 -0.244141]\n",
            "  [-0.789062 0.245117 0.123535 ... -1.41406 0.910156 -1.58594]]\n",
            "\n",
            " [[0.800781 -0.15332 1.28906 ... -1.17188 -0.166992 -1.05469]\n",
            "  [0.800781 -0.15332 1.28906 ... -1.17188 -0.166992 -1.05469]\n",
            "  [0.785156 -0.15918 1.28906 ... -1.17188 -0.169922 -1.05469]\n",
            "  ...\n",
            "  [0.320312 0.777344 0.878906 ... -1.03906 -0.992188 -0.333984]\n",
            "  [-1.36719 0.289062 0.558594 ... -0.640625 -0.494141 0.0441895]\n",
            "  [-0.613281 -1.26562 0.0883789 ... -0.423828 0.421875 -0.765625]]\n",
            "\n",
            " [[0.800781 -0.15332 1.28906 ... -1.17188 -0.166992 -1.05469]\n",
            "  [0.800781 -0.15332 1.28906 ... -1.17188 -0.166992 -1.05469]\n",
            "  [0.785156 -0.15918 1.28906 ... -1.17188 -0.169922 -1.05469]\n",
            "  ...\n",
            "  [0.15625 0.925781 -0.294922 ... -0.511719 -0.0712891 -1.0625]\n",
            "  [-0.65625 0.0356445 0.150391 ... 1.78125 0.0368652 -0.53125]\n",
            "  [-1.13281 -0.597656 0.12207 ... -0.365234 0.859375 -1.38281]]\n",
            "\n",
            " [[0.800781 -0.15332 1.28906 ... -1.17188 -0.166992 -1.05469]\n",
            "  [0.515625 0.609375 1.22656 ... -0.302734 0.012146 -0.4375]\n",
            "  [-0.189453 1.23438 1.21094 ... -0.0751953 0.777344 0.359375]\n",
            "  ...\n",
            "  [-0.621094 2.21875 0.511719 ... -0.451172 0.859375 2.20312]\n",
            "  [0.251953 0.433594 1.00781 ... -0.246094 2.48438 -1.05469]\n",
            "  [-1.44531 0.628906 -0.279297 ... -0.660156 0.855469 -1.50781]]]\n",
            "next_layer_addition_dropped_out=[[[1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.92188 0.109375 3.39062 ... -3.25 0.546875 -2.4375]\n",
            "  ...\n",
            "  [0.0625 -0.585938 2.79688 ... 0.21582 1.95312 1.5625]\n",
            "  [0.402344 -2.42188 0.90625 ... 0.539062 2.65625 0.148438]\n",
            "  [-2.53125 -2.89062 3.125 ... -0.0527344 1.41406 -3.09375]]\n",
            "\n",
            " [[1.57812 4.28125 3.0625 ... -0.148438 -2.375 0.484375]\n",
            "  [1.51562 3.46875 4.25 ... -1.125 1.0625 1.92188]\n",
            "  [0.765625 1.48438 1.77344 ... 0.753906 -1.79688 0.773438]\n",
            "  ...\n",
            "  [4.0625 1.40625 1.44531 ... -0.453125 2.29688 -0.789062]\n",
            "  [-0.00976562 1.4375 0.695312 ... 1.20312 0.205078 -0.357422]\n",
            "  [-1.84375 1.26562 0.6875 ... -2.21875 1.07031 -4.5]]\n",
            "\n",
            " [[1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.92188 0.109375 3.39062 ... -3.25 0.546875 -2.4375]\n",
            "  ...\n",
            "  [1.75 1.21094 -0.109375 ... -2.6875 -2.875 0.511719]\n",
            "  [-2.73438 1.39062 1.52344 ... -1.77344 -0.800781 -0.4375]\n",
            "  [-0.964844 -2.5625 -0.158203 ... -0.113281 -0.441406 -2.34375]]\n",
            "\n",
            " [[1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.92188 0.109375 3.39062 ... -3.25 0.546875 -2.4375]\n",
            "  ...\n",
            "  [0.742188 1.5625 -0.625 ... -0.59375 0.0908203 -3.25]\n",
            "  [-1.35156 -0.113281 -0.0253906 ... 3.79688 -0.283203 -2.15625]\n",
            "  [-1.625 -0.957031 0.480469 ... -0.00390625 1.28125 -4.09375]]\n",
            "\n",
            " [[1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [0.460938 2.84375 2.64062 ... -1.14062 0.53125 -0.925781]\n",
            "  [-0.202148 3.65625 2.125 ... -1.60156 1.72656 -0.515625]\n",
            "  ...\n",
            "  [-2.4375 3.875 1.09375 ... -0.8125 1.08594 3.78125]\n",
            "  [1.16406 1.875 0.757812 ... -0.208984 3.25 -2.84375]\n",
            "  [-2.125 1.07812 -0.742188 ... -0.171875 0.992188 -3.48438]]]\n",
            "inputs=[[[1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.92188 0.109375 3.39062 ... -3.25 0.546875 -2.4375]\n",
            "  ...\n",
            "  [0.0625 -0.585938 2.79688 ... 0.21582 1.95312 1.5625]\n",
            "  [0.402344 -2.42188 0.90625 ... 0.539062 2.65625 0.148438]\n",
            "  [-2.53125 -2.89062 3.125 ... -0.0527344 1.41406 -3.09375]]\n",
            "\n",
            " [[1.57812 4.28125 3.0625 ... -0.148438 -2.375 0.484375]\n",
            "  [1.51562 3.46875 4.25 ... -1.125 1.0625 1.92188]\n",
            "  [0.765625 1.48438 1.77344 ... 0.753906 -1.79688 0.773438]\n",
            "  ...\n",
            "  [4.0625 1.40625 1.44531 ... -0.453125 2.29688 -0.789062]\n",
            "  [-0.00976562 1.4375 0.695312 ... 1.20312 0.205078 -0.357422]\n",
            "  [-1.84375 1.26562 0.6875 ... -2.21875 1.07031 -4.5]]\n",
            "\n",
            " [[1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.92188 0.109375 3.39062 ... -3.25 0.546875 -2.4375]\n",
            "  ...\n",
            "  [1.75 1.21094 -0.109375 ... -2.6875 -2.875 0.511719]\n",
            "  [-2.73438 1.39062 1.52344 ... -1.77344 -0.800781 -0.4375]\n",
            "  [-0.964844 -2.5625 -0.158203 ... -0.113281 -0.441406 -2.34375]]\n",
            "\n",
            " [[1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [1.92188 0.109375 3.39062 ... -3.25 0.546875 -2.4375]\n",
            "  ...\n",
            "  [0.742188 1.5625 -0.625 ... -0.59375 0.0908203 -3.25]\n",
            "  [-1.35156 -0.113281 -0.0253906 ... 3.79688 -0.283203 -2.15625]\n",
            "  [-1.625 -0.957031 0.480469 ... -0.00390625 1.28125 -4.09375]]\n",
            "\n",
            " [[1.96875 0.121094 3.39062 ... -3.25 0.539062 -2.42188]\n",
            "  [0.460938 2.84375 2.64062 ... -1.14062 0.53125 -0.925781]\n",
            "  [-0.202148 3.65625 2.125 ... -1.60156 1.72656 -0.515625]\n",
            "  ...\n",
            "  [-2.4375 3.875 1.09375 ... -0.8125 1.08594 3.78125]\n",
            "  [1.16406 1.875 0.757812 ... -0.208984 3.25 -2.84375]\n",
            "  [-2.125 1.07812 -0.742188 ... -0.171875 0.992188 -3.48438]]]\n",
            "lnx=[[[0.738281 0.0454102 1.27344 ... -1.21875 0.202148 -0.910156]\n",
            "  [0.738281 0.0454102 1.27344 ... -1.21875 0.202148 -0.910156]\n",
            "  [0.722656 0.0410156 1.27344 ... -1.21875 0.205078 -0.914062]\n",
            "  ...\n",
            "  [0.027832 -0.259766 1.24219 ... 0.0957031 0.867188 0.695312]\n",
            "  [0.193359 -1.16406 0.435547 ... 0.259766 1.28125 0.0712891]\n",
            "  [-1.14844 -1.3125 1.41406 ... -0.0239258 0.640625 -1.40625]]\n",
            "\n",
            " [[0.621094 1.6875 1.20312 ... -0.0583496 -0.933594 0.19043]\n",
            "  [0.667969 1.53125 1.875 ... -0.496094 0.46875 0.847656]\n",
            "  [0.359375 0.699219 0.832031 ... 0.353516 -0.84375 0.363281]\n",
            "  ...\n",
            "  [2.14062 0.742188 0.761719 ... -0.239258 1.21094 -0.416016]\n",
            "  [-0.00500488 0.738281 0.357422 ... 0.617188 0.105469 -0.183594]\n",
            "  [-0.988281 0.679688 0.369141 ... -1.1875 0.574219 -2.40625]]\n",
            "\n",
            " [[0.738281 0.0454102 1.27344 ... -1.21875 0.202148 -0.910156]\n",
            "  [0.738281 0.0454102 1.27344 ... -1.21875 0.202148 -0.910156]\n",
            "  [0.722656 0.0410156 1.27344 ... -1.21875 0.205078 -0.914062]\n",
            "  ...\n",
            "  [0.914062 0.632812 -0.0571289 ... -1.40625 -1.5 0.267578]\n",
            "  [-1.41406 0.71875 0.789062 ... -0.917969 -0.414062 -0.226562]\n",
            "  [-0.480469 -1.28125 -0.0791016 ... -0.0563965 -0.219727 -1.17188]]\n",
            "\n",
            " [[0.738281 0.0454102 1.27344 ... -1.21875 0.202148 -0.910156]\n",
            "  [0.738281 0.0454102 1.27344 ... -1.21875 0.202148 -0.910156]\n",
            "  [0.722656 0.0410156 1.27344 ... -1.21875 0.205078 -0.914062]\n",
            "  ...\n",
            "  [0.375 0.789062 -0.316406 ... -0.300781 0.0458984 -1.64062]\n",
            "  [-0.679688 -0.0571289 -0.0127563 ... 1.91406 -0.142578 -1.08594]\n",
            "  [-0.800781 -0.472656 0.237305 ... -0.00193024 0.632812 -2.01562]]\n",
            "\n",
            " [[0.738281 0.0454102 1.27344 ... -1.21875 0.202148 -0.910156]\n",
            "  [0.196289 1.21094 1.125 ... -0.484375 0.225586 -0.392578]\n",
            "  [-0.0908203 1.64062 0.953125 ... -0.71875 0.777344 -0.231445]\n",
            "  ...\n",
            "  [-1.3125 2.07812 0.585938 ... -0.435547 0.582031 2.03125]\n",
            "  [0.621094 1 0.404297 ... -0.111328 1.73438 -1.51562]\n",
            "  [-1.09375 0.554688 -0.382812 ... -0.0883789 0.511719 -1.79688]]]\n",
            "attention_lnx=[[[1.60156 -1.25781 1.13281 ... 1.33594 -0.808594 0.433594]\n",
            "  [1.60156 -1.25781 1.13281 ... 1.33594 -0.808594 0.433594]\n",
            "  [1.59375 -1.25781 1.125 ... 1.33594 -0.808594 0.429688]\n",
            "  ...\n",
            "  [1.19531 -1.46875 0.847656 ... 1.05469 -0.882812 0.234375]\n",
            "  [1.05469 -1.13281 0.773438 ... 1.125 -0.789062 0.382812]\n",
            "  [0.882812 -1.17969 0.730469 ... 0.910156 -1.05469 0.225586]]\n",
            "\n",
            " [[-1.26562 -0.890625 -1.19531 ... -0.585938 -0.988281 0.255859]\n",
            "  [-1.5 -1.04688 -0.769531 ... 0.929688 -1.08594 0.367188]\n",
            "  [-1.10938 -1.25781 -0.339844 ... -0.328125 -0.464844 0.119629]\n",
            "  ...\n",
            "  [-0.605469 0.129883 -0.554688 ... 0.503906 -0.0537109 -0.402344]\n",
            "  [-0.691406 -0.410156 -0.435547 ... -0.209961 0.000471115 -0.75]\n",
            "  [-0.589844 0.0878906 -0.394531 ... -0.0253906 -0.110352 -0.196289]]\n",
            "\n",
            " [[1.60156 -1.25781 1.13281 ... 1.33594 -0.808594 0.433594]\n",
            "  [1.60156 -1.25781 1.13281 ... 1.33594 -0.808594 0.433594]\n",
            "  [1.59375 -1.25781 1.125 ... 1.33594 -0.808594 0.429688]\n",
            "  ...\n",
            "  [0.400391 -1.20312 0.0585938 ... 1.03125 -0.761719 -0.0035553]\n",
            "  [0.539062 -1.25781 0.183594 ... 0.785156 -0.851562 -0.205078]\n",
            "  [0.734375 -1.22656 0.523438 ... 0.447266 -0.734375 0.175781]]\n",
            "\n",
            " [[1.60156 -1.25781 1.13281 ... 1.33594 -0.808594 0.433594]\n",
            "  [1.60156 -1.25781 1.13281 ... 1.33594 -0.808594 0.433594]\n",
            "  [1.59375 -1.25781 1.125 ... 1.33594 -0.808594 0.429688]\n",
            "  ...\n",
            "  [0.597656 -0.574219 0.345703 ... 0.53125 -0.523438 -0.429688]\n",
            "  [0.648438 -0.476562 0.225586 ... 0.777344 -0.304688 -0.796875]\n",
            "  [0.617188 -0.890625 0.304688 ... 0.671875 -0.496094 -0.128906]]\n",
            "\n",
            " [[1.60156 -1.25781 1.13281 ... 1.33594 -0.808594 0.433594]\n",
            "  [1.30469 -1.57031 0.726562 ... -0.212891 -1.24219 -0.0927734]\n",
            "  [0.5 -1.23438 0.227539 ... 1.03906 -1.17188 -0.554688]\n",
            "  ...\n",
            "  [-0.186523 -0.792969 -0.10498 ... 0.0419922 -0.585938 -0.0737305]\n",
            "  [0.11377 -1.02344 0.119141 ... 0.043457 -0.386719 -0.554688]\n",
            "  [-0.0299072 -1.05469 0.416016 ... 0.306641 -0.769531 0.353516]]]\n",
            "attn_output=[[[1.25781 -0.400391 1.59375 ... -0.675781 -0.0952148 -0.699219]\n",
            "  [1.25781 -0.400391 1.59375 ... -0.675781 -0.0952148 -0.699219]\n",
            "  [1.24219 -0.404297 1.59375 ... -0.675781 -0.0922852 -0.707031]\n",
            "  ...\n",
            "  [0.53125 -0.867188 1.53906 ... 0.535156 0.451172 0.757812]\n",
            "  [0.65625 -1.60156 0.753906 ... 0.746094 0.839844 0.239258]\n",
            "  [-0.703125 -1.74219 1.64844 ... 0.367188 0.15332 -1.22656]]\n",
            "\n",
            " [[0.114258 1.24219 0.683594 ... -0.269531 -1.23438 0.271484]\n",
            "  [0.00640869 0.996094 1.42969 ... -0.0800781 -0.00964355 0.941406]\n",
            "  [-0.151367 0.100098 0.632812 ... 0.188477 -1 0.394531]\n",
            "  ...\n",
            "  [1.75781 0.78125 0.451172 ... 0.0257568 1.14062 -0.605469]\n",
            "  [-0.345703 0.503906 0.12793 ... 0.488281 0.101074 -0.542969]\n",
            "  [-1.25781 0.699219 0.151367 ... -1.15625 0.496094 -2.42188]]\n",
            "\n",
            " [[1.25781 -0.400391 1.59375 ... -0.675781 -0.0952148 -0.699219]\n",
            "  [1.25781 -0.400391 1.59375 ... -0.675781 -0.0952148 -0.699219]\n",
            "  [1.24219 -0.404297 1.59375 ... -0.675781 -0.0922852 -0.707031]\n",
            "  ...\n",
            "  [1.04688 0.0038147 -0.0247803 ... -0.808594 -1.77344 0.248047]\n",
            "  [-1.07031 0.0644531 0.832031 ... -0.480469 -0.804688 -0.3125]\n",
            "  [-0.108887 -1.78906 0.172852 ... 0.158203 -0.554688 -1.02344]]\n",
            "\n",
            " [[1.25781 -0.400391 1.59375 ... -0.675781 -0.0952148 -0.699219]\n",
            "  [1.25781 -0.400391 1.59375 ... -0.675781 -0.0952148 -0.699219]\n",
            "  [1.24219 -0.404297 1.59375 ... -0.675781 -0.0922852 -0.707031]\n",
            "  ...\n",
            "  [0.636719 0.46875 -0.132812 ... -0.0296631 -0.205078 -1.75]\n",
            "  [-0.335938 -0.283203 0.0957031 ... 2.1875 -0.28125 -1.41406]\n",
            "  [-0.474609 -0.867188 0.369141 ... 0.314453 0.369141 -1.98438]]\n",
            "\n",
            " [[1.25781 -0.400391 1.59375 ... -0.675781 -0.0952148 -0.699219]\n",
            "  [0.703125 0.507812 1.33594 ... -0.539062 -0.283203 -0.404297]\n",
            "  [0.125977 1.02344 0.996094 ... -0.238281 0.235352 -0.453125]\n",
            "  ...\n",
            "  [-1.35938 1.59375 0.511719 ... -0.398438 0.257812 1.91406]\n",
            "  [0.652344 0.435547 0.447266 ... -0.0844727 1.46094 -1.73438]\n",
            "  [-1.07031 0.0116577 -0.162109 ... 0.0668945 0.11084 -1.55469]]]\n",
            "next_layer_addition_dropped_out=[[[3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3 -2.6875 4.03125 ... -1.375 -0.253906 -2.0625]\n",
            "  ...\n",
            "  [0.753906 -2.53125 3.17188 ... 0.867188 1.11719 2.1875]\n",
            "  [1.94531 -3.78125 2.25 ... 2.17188 1.98438 1.14062]\n",
            "  [-3.09375 -4.40625 4.5625 ... 0.613281 1.14062 -3.28125]]\n",
            "\n",
            " [[-0.773438 2.32812 1.20312 ... -0.353516 -3.60938 1.75781]\n",
            "  [-0.345703 1.90625 3.60938 ... -0.371094 -0.484375 2.79688]\n",
            "  [-0.488281 0.464844 1.27344 ... 0.652344 -1.39062 0.144531]\n",
            "  ...\n",
            "  [3.35938 1.89844 0.0078125 ... 0.314453 1.44531 -1.42969]\n",
            "  [-0.617188 0.195312 0.929688 ... 1.82031 -0.310547 -1.5]\n",
            "  [-3.17188 2.21875 0.953125 ... -0.84375 0.390625 -4]]\n",
            "\n",
            " [[3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3 -2.6875 4.03125 ... -1.375 -0.253906 -2.0625]\n",
            "  ...\n",
            "  [2.10938 -1.44531 -0.90625 ... -0.660156 -3.67188 -0.671875]\n",
            "  [-3.5625 -0.839844 1.25781 ... -1.35156 -1.47656 -1.39844]\n",
            "  [-1.76562 -4 0.632812 ... -0.193359 -1.28906 -1.5625]]\n",
            "\n",
            " [[3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3 -2.6875 4.03125 ... -1.375 -0.253906 -2.0625]\n",
            "  ...\n",
            "  [-0.46875 -0.152344 0.287109 ... -0.208008 -1.6875 -3.48438]\n",
            "  [-1.35156 -2.21875 0.0244141 ... 4.5 -1.10938 -4]\n",
            "  [-2.59375 -2.07812 0.945312 ... 0.109375 1.09375 -3.84375]]\n",
            "\n",
            " [[3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [1.21094 -0.117188 2.78125 ... -1.74219 -0.546875 -0.703125]\n",
            "  [-0.132812 2.09375 1.95312 ... -0.820312 0.765625 -1.49219]\n",
            "  ...\n",
            "  [-2.89062 2.59375 1.40625 ... -0.988281 0.367188 3.79688]\n",
            "  [1.40625 0.457031 0.671875 ... 0.6875 3.09375 -3.5]\n",
            "  [-3.40625 0.0476074 -0.291016 ... 1.02344 0.332031 -3.1875]]]\n",
            "inputs=[[[3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3 -2.6875 4.03125 ... -1.375 -0.253906 -2.0625]\n",
            "  ...\n",
            "  [0.753906 -2.53125 3.17188 ... 0.867188 1.11719 2.1875]\n",
            "  [1.94531 -3.78125 2.25 ... 2.17188 1.98438 1.14062]\n",
            "  [-3.09375 -4.40625 4.5625 ... 0.613281 1.14062 -3.28125]]\n",
            "\n",
            " [[-0.773438 2.32812 1.20312 ... -0.353516 -3.60938 1.75781]\n",
            "  [-0.345703 1.90625 3.60938 ... -0.371094 -0.484375 2.79688]\n",
            "  [-0.488281 0.464844 1.27344 ... 0.652344 -1.39062 0.144531]\n",
            "  ...\n",
            "  [3.35938 1.89844 0.0078125 ... 0.314453 1.44531 -1.42969]\n",
            "  [-0.617188 0.195312 0.929688 ... 1.82031 -0.310547 -1.5]\n",
            "  [-3.17188 2.21875 0.953125 ... -0.84375 0.390625 -4]]\n",
            "\n",
            " [[3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3 -2.6875 4.03125 ... -1.375 -0.253906 -2.0625]\n",
            "  ...\n",
            "  [2.10938 -1.44531 -0.90625 ... -0.660156 -3.67188 -0.671875]\n",
            "  [-3.5625 -0.839844 1.25781 ... -1.35156 -1.47656 -1.39844]\n",
            "  [-1.76562 -4 0.632812 ... -0.193359 -1.28906 -1.5625]]\n",
            "\n",
            " [[3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [3 -2.6875 4.03125 ... -1.375 -0.253906 -2.0625]\n",
            "  ...\n",
            "  [-0.46875 -0.152344 0.287109 ... -0.208008 -1.6875 -3.48438]\n",
            "  [-1.35156 -2.21875 0.0244141 ... 4.5 -1.10938 -4]\n",
            "  [-2.59375 -2.07812 0.945312 ... 0.109375 1.09375 -3.84375]]\n",
            "\n",
            " [[3.0625 -2.6875 4.0625 ... -1.38281 -0.263672 -2.04688]\n",
            "  [1.21094 -0.117188 2.78125 ... -1.74219 -0.546875 -0.703125]\n",
            "  [-0.132812 2.09375 1.95312 ... -0.820312 0.765625 -1.49219]\n",
            "  ...\n",
            "  [-2.89062 2.59375 1.40625 ... -0.988281 0.367188 3.79688]\n",
            "  [1.40625 0.457031 0.671875 ... 0.6875 3.09375 -3.5]\n",
            "  [-3.40625 0.0476074 -0.291016 ... 1.02344 0.332031 -3.1875]]]\n",
            "lnx=[[[1.04688 -0.921875 1.39062 ... -0.474609 -0.090332 -0.703125]\n",
            "  [1.04688 -0.921875 1.39062 ... -0.474609 -0.090332 -0.703125]\n",
            "  [1.03125 -0.921875 1.38281 ... -0.470703 -0.0869141 -0.707031]\n",
            "  ...\n",
            "  [0.306641 -1.03125 1.28906 ... 0.351562 0.453125 0.886719]\n",
            "  [0.84375 -1.64062 0.976562 ... 0.941406 0.859375 0.494141]\n",
            "  [-1.24219 -1.77344 1.83594 ... 0.24707 0.458984 -1.32031]]\n",
            "\n",
            " [[-0.275391 0.828125 0.427734 ... -0.125977 -1.28125 0.625]\n",
            "  [-0.136719 0.753906 1.42969 ... -0.146484 -0.191406 1.10938]\n",
            "  [-0.205078 0.195312 0.535156 ... 0.273438 -0.582031 0.060791]\n",
            "  ...\n",
            "  [1.625 0.917969 0.00378418 ... 0.152344 0.699219 -0.691406]\n",
            "  [-0.285156 0.0898438 0.427734 ... 0.839844 -0.143555 -0.691406]\n",
            "  [-1.52344 1.07031 0.458984 ... -0.40625 0.1875 -1.92188]]\n",
            "\n",
            " [[1.04688 -0.921875 1.39062 ... -0.474609 -0.090332 -0.703125]\n",
            "  [1.04688 -0.921875 1.39062 ... -0.474609 -0.090332 -0.703125]\n",
            "  [1.03125 -0.921875 1.38281 ... -0.470703 -0.0869141 -0.707031]\n",
            "  ...\n",
            "  [0.992188 -0.679688 -0.425781 ... -0.310547 -1.72656 -0.316406]\n",
            "  [-1.66406 -0.390625 0.585938 ... -0.628906 -0.6875 -0.652344]\n",
            "  [-0.78125 -1.77344 0.279297 ... -0.0854492 -0.570312 -0.691406]]\n",
            "\n",
            " [[1.04688 -0.921875 1.39062 ... -0.474609 -0.090332 -0.703125]\n",
            "  [1.04688 -0.921875 1.39062 ... -0.474609 -0.090332 -0.703125]\n",
            "  [1.03125 -0.921875 1.38281 ... -0.470703 -0.0869141 -0.707031]\n",
            "  ...\n",
            "  [-0.213867 -0.0693359 0.130859 ... -0.0947266 -0.769531 -1.58594]\n",
            "  [-0.613281 -1.00781 0.0111084 ... 2.04688 -0.503906 -1.82031]\n",
            "  [-1.14844 -0.917969 0.417969 ... 0.0483398 0.482422 -1.69531]]\n",
            "\n",
            " [[1.04688 -0.921875 1.39062 ... -0.474609 -0.090332 -0.703125]\n",
            "  [0.46875 -0.0454102 1.07812 ... -0.675781 -0.211914 -0.271484]\n",
            "  [-0.0541992 0.855469 0.796875 ... -0.333984 0.3125 -0.609375]\n",
            "  ...\n",
            "  [-1.42969 1.28125 0.695312 ... -0.490234 0.181641 1.88281]\n",
            "  [0.683594 0.22168 0.326172 ... 0.333984 1.5 -1.70312]\n",
            "  [-1.59375 0.0222168 -0.135742 ... 0.478516 0.155273 -1.49219]]]\n",
            "attention_lnx=[[[0.523438 0.149414 0.0262451 ... 0.945312 -0.894531 0.494141]\n",
            "  [0.523438 0.149414 0.0262451 ... 0.945312 -0.894531 0.494141]\n",
            "  [0.527344 0.150391 0.0273438 ... 0.941406 -0.894531 0.492188]\n",
            "  ...\n",
            "  [0.988281 0.0141602 -0.314453 ... 0.15332 -0.816406 0.582031]\n",
            "  [0.890625 -0.169922 -0.470703 ... 0.0888672 -0.730469 0.392578]\n",
            "  [1.02344 0.111816 -0.230469 ... 0.259766 -0.703125 0.333984]]\n",
            "\n",
            " [[0.147461 -2.10938 -0.3125 ... -0.460938 -0.894531 0.65625]\n",
            "  [-0.0563965 -1.34375 -0.474609 ... -0.636719 -2.03125 0.9375]\n",
            "  [-0.306641 -0.828125 -0.753906 ... -0.605469 -1.29688 0.201172]\n",
            "  ...\n",
            "  [0.243164 0.322266 -0.910156 ... 0.03125 -0.566406 0.482422]\n",
            "  [-0.365234 0.789062 -0.078125 ... -0.296875 -0.726562 0.832031]\n",
            "  [-0.298828 0.601562 -0.245117 ... -0.257812 -0.5625 0.371094]]\n",
            "\n",
            " [[0.523438 0.149414 0.0262451 ... 0.945312 -0.894531 0.494141]\n",
            "  [0.523438 0.149414 0.0262451 ... 0.945312 -0.894531 0.494141]\n",
            "  [0.527344 0.150391 0.0273438 ... 0.941406 -0.894531 0.492188]\n",
            "  ...\n",
            "  [1.10938 0.220703 -0.318359 ... 0.322266 -0.683594 0.441406]\n",
            "  [1.14062 0.060791 -0.255859 ... -0.157227 -0.96875 0.660156]\n",
            "  [1.11719 0.349609 0.181641 ... 0.102539 -0.699219 0.462891]]\n",
            "\n",
            " [[0.523438 0.149414 0.0262451 ... 0.945312 -0.894531 0.494141]\n",
            "  [0.523438 0.149414 0.0262451 ... 0.945312 -0.894531 0.494141]\n",
            "  [0.527344 0.150391 0.0273438 ... 0.941406 -0.894531 0.492188]\n",
            "  ...\n",
            "  [0.898438 -0.304688 -0.133789 ... -0.131836 -0.890625 0.609375]\n",
            "  [0.773438 0.228516 -0.0473633 ... -0.216797 -1.28125 0.613281]\n",
            "  [1.02344 0.419922 0.11084 ... -0.105469 -1.08594 0.476562]]\n",
            "\n",
            " [[0.523438 0.149414 0.0262451 ... 0.945312 -0.894531 0.494141]\n",
            "  [1.91406 -0.765625 -0.228516 ... 0.114746 -0.855469 0.496094]\n",
            "  [1.48438 -0.675781 0.201172 ... 0.245117 -1.19531 0.5]\n",
            "  ...\n",
            "  [0.808594 0.621094 0.0371094 ... -0.585938 -1.19531 0.304688]\n",
            "  [0.875 0.332031 -0.0332031 ... -0.392578 -1.02344 0.19043]\n",
            "  [0.953125 0.792969 0.0449219 ... -0.263672 -0.632812 -0.146484]]]\n",
            "attn_output=[[[1.17188 -0.828125 1.33594 ... -0.142578 -0.378906 -0.507812]\n",
            "  [1.17188 -0.828125 1.33594 ... -0.142578 -0.378906 -0.507812]\n",
            "  [1.15625 -0.828125 1.32812 ... -0.141602 -0.375 -0.515625]\n",
            "  ...\n",
            "  [0.679688 -0.980469 1.10938 ... 0.396484 0.117188 1.07812]\n",
            "  [1.17969 -1.64062 0.738281 ... 0.9375 0.519531 0.636719]\n",
            "  [-0.800781 -1.66406 1.67969 ... 0.337891 0.169922 -1.14062]]\n",
            "\n",
            " [[-0.207031 0.0727539 0.294922 ... -0.269531 -1.49219 0.800781]\n",
            "  [-0.148438 0.207031 1.15625 ... -0.371094 -0.925781 1.375]\n",
            "  [-0.314453 -0.143555 0.205078 ... 0.0185547 -1.0625 0.136719]\n",
            "  ...\n",
            "  [1.66406 1.02344 -0.416016 ... 0.15918 0.40625 -0.4375]\n",
            "  [-0.427734 0.429688 0.371094 ... 0.664062 -0.453125 -0.291016]\n",
            "  [-1.59375 1.29688 0.326172 ... -0.507812 -0.0791016 -1.67188]]\n",
            "\n",
            " [[1.17188 -0.828125 1.33594 ... -0.142578 -0.378906 -0.507812]\n",
            "  [1.17188 -0.828125 1.33594 ... -0.142578 -0.378906 -0.507812]\n",
            "  [1.15625 -0.828125 1.32812 ... -0.141602 -0.375 -0.515625]\n",
            "  ...\n",
            "  [1.42969 -0.542969 -0.542969 ... -0.150391 -1.9375 -0.102539]\n",
            "  [-1.07812 -0.345703 0.445312 ... -0.671875 -1.08594 -0.328125]\n",
            "  [-0.271484 -1.53125 0.341797 ... -0.0380859 -0.832031 -0.460938]]\n",
            "\n",
            " [[1.17188 -0.828125 1.33594 ... -0.142578 -0.378906 -0.507812]\n",
            "  [1.17188 -0.828125 1.33594 ... -0.142578 -0.378906 -0.507812]\n",
            "  [1.15625 -0.828125 1.32812 ... -0.141602 -0.375 -0.515625]\n",
            "  ...\n",
            "  [0.1875 -0.199219 0.0668945 ... -0.148438 -1.125 -1.25]\n",
            "  [-0.251953 -0.867188 -0.0100098 ... 1.86719 -1.04688 -1.47656]\n",
            "  [-0.664062 -0.699219 0.445312 ... 0.00164795 0.0032959 -1.42188]]\n",
            "\n",
            " [[1.17188 -0.828125 1.33594 ... -0.142578 -0.378906 -0.507812]\n",
            "  [1.15625 -0.326172 0.945312 ... -0.601562 -0.519531 -0.0766602]\n",
            "  [0.519531 0.546875 0.832031 ... -0.22168 -0.166016 -0.382812]\n",
            "  ...\n",
            "  [-0.980469 1.51562 0.679688 ... -0.742188 -0.390625 1.92969]\n",
            "  [1.07031 0.371094 0.300781 ... 0.138672 0.972656 -1.55469]\n",
            "  [-1.10156 0.378906 -0.11084 ... 0.341797 -0.135742 -1.5]]]\n",
            "next_layer_addition_dropped_out=[[[3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.6875 -2.8125 4.5625 ... -1.125 -1.85938 -1.91406]\n",
            "  ...\n",
            "  [1.82031 -2.09375 3.34375 ... -0.523438 -0.0371094 3.28125]\n",
            "  [2.79688 -4.625 1.4375 ... 1.64062 1.88281 1.75]\n",
            "  [-2.14062 -3.89062 4.1875 ... -0.0234375 1.3125 -2.35938]]\n",
            "\n",
            " [[-0.6875 0.824219 0.933594 ... -0.554688 -5.3125 3.39062]\n",
            "  [-0.808594 0.769531 2.59375 ... -1.53125 -2.73438 4.03125]\n",
            "  [-0.328125 0.273438 0.539062 ... 0.441406 -1.47656 -1]\n",
            "  ...\n",
            "  [3.71875 3.84375 -1.17188 ... 0.804688 1.69531 -0.447266]\n",
            "  [-1.32812 1.27344 0.652344 ... 0.683594 -1.625 -0.132812]\n",
            "  [-2.75 2.65625 2.42188 ... -1.5 -0.249023 -4.09375]]\n",
            "\n",
            " [[3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.6875 -2.8125 4.5625 ... -1.125 -1.85938 -1.91406]\n",
            "  ...\n",
            "  [2.9375 -1.39844 -2.625 ... -0.106445 -4.75 -0.00488281]\n",
            "  [-2.45312 -1.11719 0.710938 ... -2.14062 -3.59375 -1.26562]\n",
            "  [-0.8125 -3.45312 1.10938 ... -1.625 -1.35156 -1.07031]]\n",
            "\n",
            " [[3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.6875 -2.8125 4.5625 ... -1.125 -1.85938 -1.91406]\n",
            "  ...\n",
            "  [0.875 -0.00976562 0.925781 ... 0.15625 -2.57812 -2.10938]\n",
            "  [-0.660156 -2.04688 0.703125 ... 2.42188 -1.97656 -2.0625]\n",
            "  [-2.15625 -0.945312 1.48438 ... -1.09375 0.457031 -3.51562]]\n",
            "\n",
            " [[3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [2.28125 -0.300781 3.10938 ... -2.35938 -0.695312 0.222656]\n",
            "  [1 1.10156 2.35938 ... -0.177734 -1.35938 -1.28125]\n",
            "  ...\n",
            "  [-2.14062 3.14062 1.13281 ... -1.67969 -0.539062 4.46875]\n",
            "  [1.57812 1.10156 0.523438 ... 0.00976562 2.375 -3.625]\n",
            "  [-2.8125 1.57812 1.25 ... -0.515625 -0.431641 -2.67188]]]\n",
            "inputs=[[[3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.6875 -2.8125 4.5625 ... -1.125 -1.85938 -1.91406]\n",
            "  ...\n",
            "  [1.82031 -2.09375 3.34375 ... -0.523438 -0.0371094 3.28125]\n",
            "  [2.79688 -4.625 1.4375 ... 1.64062 1.88281 1.75]\n",
            "  [-2.14062 -3.89062 4.1875 ... -0.0234375 1.3125 -2.35938]]\n",
            "\n",
            " [[-0.6875 0.824219 0.933594 ... -0.554688 -5.3125 3.39062]\n",
            "  [-0.808594 0.769531 2.59375 ... -1.53125 -2.73438 4.03125]\n",
            "  [-0.328125 0.273438 0.539062 ... 0.441406 -1.47656 -1]\n",
            "  ...\n",
            "  [3.71875 3.84375 -1.17188 ... 0.804688 1.69531 -0.447266]\n",
            "  [-1.32812 1.27344 0.652344 ... 0.683594 -1.625 -0.132812]\n",
            "  [-2.75 2.65625 2.42188 ... -1.5 -0.249023 -4.09375]]\n",
            "\n",
            " [[3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.6875 -2.8125 4.5625 ... -1.125 -1.85938 -1.91406]\n",
            "  ...\n",
            "  [2.9375 -1.39844 -2.625 ... -0.106445 -4.75 -0.00488281]\n",
            "  [-2.45312 -1.11719 0.710938 ... -2.14062 -3.59375 -1.26562]\n",
            "  [-0.8125 -3.45312 1.10938 ... -1.625 -1.35156 -1.07031]]\n",
            "\n",
            " [[3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [3.6875 -2.8125 4.5625 ... -1.125 -1.85938 -1.91406]\n",
            "  ...\n",
            "  [0.875 -0.00976562 0.925781 ... 0.15625 -2.57812 -2.10938]\n",
            "  [-0.660156 -2.04688 0.703125 ... 2.42188 -1.97656 -2.0625]\n",
            "  [-2.15625 -0.945312 1.48438 ... -1.09375 0.457031 -3.51562]]\n",
            "\n",
            " [[3.75 -2.8125 4.5625 ... -1.10938 -1.875 -1.89062]\n",
            "  [2.28125 -0.300781 3.10938 ... -2.35938 -0.695312 0.222656]\n",
            "  [1 1.10156 2.35938 ... -0.177734 -1.35938 -1.28125]\n",
            "  ...\n",
            "  [-2.14062 3.14062 1.13281 ... -1.67969 -0.539062 4.46875]\n",
            "  [1.57812 1.10156 0.523438 ... 0.00976562 2.375 -3.625]\n",
            "  [-2.8125 1.57812 1.25 ... -0.515625 -0.431641 -2.67188]]]\n",
            "lnx=[[[1.19531 -0.894531 1.45312 ... -0.353516 -0.597656 -0.601562]\n",
            "  [1.19531 -0.894531 1.45312 ... -0.353516 -0.597656 -0.601562]\n",
            "  [1.17188 -0.898438 1.45312 ... -0.359375 -0.59375 -0.609375]\n",
            "  ...\n",
            "  [0.6875 -0.789062 1.26562 ... -0.197266 -0.0140381 1.24219]\n",
            "  [1.11719 -1.85156 0.574219 ... 0.65625 0.753906 0.699219]\n",
            "  [-0.792969 -1.44531 1.55469 ... -0.00866699 0.486328 -0.875]]\n",
            "\n",
            " [[-0.222656 0.267578 0.302734 ... -0.179688 -1.71875 1.10156]\n",
            "  [-0.291016 0.277344 0.9375 ... -0.554688 -0.988281 1.45312]\n",
            "  [-0.125 0.104492 0.206055 ... 0.167969 -0.5625 -0.380859]\n",
            "  ...\n",
            "  [1.64062 1.69531 -0.515625 ... 0.355469 0.746094 -0.197266]\n",
            "  [-0.550781 0.527344 0.271484 ... 0.283203 -0.675781 -0.0551758]\n",
            "  [-1.1875 1.14844 1.04688 ... -0.648438 -0.107422 -1.76562]]\n",
            "\n",
            " [[1.19531 -0.894531 1.45312 ... -0.353516 -0.597656 -0.601562]\n",
            "  [1.19531 -0.894531 1.45312 ... -0.353516 -0.597656 -0.601562]\n",
            "  [1.17188 -0.898438 1.45312 ... -0.359375 -0.59375 -0.609375]\n",
            "  ...\n",
            "  [1.25 -0.597656 -1.11719 ... -0.0454102 -2.03125 -0.00209045]\n",
            "  [-1.05469 -0.478516 0.304688 ... -0.917969 -1.53906 -0.542969]\n",
            "  [-0.324219 -1.38281 0.443359 ... -0.648438 -0.539062 -0.427734]]\n",
            "\n",
            " [[1.19531 -0.894531 1.45312 ... -0.353516 -0.597656 -0.601562]\n",
            "  [1.19531 -0.894531 1.45312 ... -0.353516 -0.597656 -0.601562]\n",
            "  [1.17188 -0.898438 1.45312 ... -0.359375 -0.59375 -0.609375]\n",
            "  ...\n",
            "  [0.369141 -0.00411987 0.390625 ... 0.065918 -1.08594 -0.886719]\n",
            "  [-0.28125 -0.871094 0.298828 ... 1.03125 -0.839844 -0.878906]\n",
            "  [-0.867188 -0.380859 0.597656 ... -0.439453 0.183594 -1.41406]]\n",
            "\n",
            " [[1.19531 -0.894531 1.45312 ... -0.353516 -0.597656 -0.601562]\n",
            "  [0.824219 -0.108887 1.125 ... -0.855469 -0.251953 0.0805664]\n",
            "  [0.373047 0.412109 0.882812 ... -0.0664062 -0.507812 -0.478516]\n",
            "  ...\n",
            "  [-0.953125 1.39844 0.503906 ... -0.746094 -0.239258 1.98438]\n",
            "  [0.703125 0.490234 0.232422 ... 0.0043335 1.05469 -1.60938]\n",
            "  [-1.20312 0.675781 0.535156 ... -0.220703 -0.18457 -1.14062]]]\n",
            "attention_lnx=[[[-1.14844 0.226562 -0.53125 ... -0.314453 0.0366211 -0.746094]\n",
            "  [-1.14844 0.226562 -0.53125 ... -0.314453 0.0366211 -0.746094]\n",
            "  [-1.14844 0.217773 -0.53125 ... -0.322266 0.0415039 -0.746094]\n",
            "  ...\n",
            "  [-1.19531 0.507812 -0.394531 ... -0.275391 -0.00485229 -0.511719]\n",
            "  [-1.32031 0.429688 -0.457031 ... 0.0256348 0.431641 -0.347656]\n",
            "  [-1.25 0.392578 -0.519531 ... -0.107422 0.0529785 -0.589844]]\n",
            "\n",
            " [[1.07031 -0.621094 -0.0150757 ... -0.208984 0.996094 -0.130859]\n",
            "  [-0.197266 -0.566406 -0.78125 ... -0.480469 1.21875 -0.275391]\n",
            "  [-0.261719 -0.878906 -0.427734 ... -0.941406 1.32812 -0.00787354]\n",
            "  ...\n",
            "  [-1.34375 0.363281 -0.78125 ... -1.10938 1.46094 0.237305]\n",
            "  [-0.90625 0.125977 -0.542969 ... -1.19531 0.945312 0.0810547]\n",
            "  [-0.933594 0.163086 -0.59375 ... -0.914062 0.9375 0.601562]]\n",
            "\n",
            " [[-1.14844 0.226562 -0.53125 ... -0.314453 0.0366211 -0.746094]\n",
            "  [-1.14844 0.226562 -0.53125 ... -0.314453 0.0366211 -0.746094]\n",
            "  [-1.14844 0.217773 -0.53125 ... -0.322266 0.0415039 -0.746094]\n",
            "  ...\n",
            "  [-0.90625 0.570312 -0.119141 ... -0.660156 -0.457031 -0.196289]\n",
            "  [-1.38281 0.335938 -0.259766 ... -0.703125 -0.535156 0.0615234]\n",
            "  [-1.26562 0.267578 -0.373047 ... -0.636719 -0.597656 0.251953]]\n",
            "\n",
            " [[-1.14844 0.226562 -0.53125 ... -0.314453 0.0366211 -0.746094]\n",
            "  [-1.14844 0.226562 -0.53125 ... -0.314453 0.0366211 -0.746094]\n",
            "  [-1.14844 0.217773 -0.53125 ... -0.322266 0.0415039 -0.746094]\n",
            "  ...\n",
            "  [-0.957031 0.0864258 -0.458984 ... -0.972656 -0.601562 -0.503906]\n",
            "  [-1.17188 0.365234 -0.328125 ... -0.984375 -0.652344 -0.145508]\n",
            "  [-1.0625 0.240234 -0.441406 ... -1.02344 -0.992188 0.0839844]]\n",
            "\n",
            " [[-1.14844 0.226562 -0.53125 ... -0.314453 0.0366211 -0.746094]\n",
            "  [-0.984375 0.660156 -0.4375 ... -0.213867 -0.279297 -0.328125]\n",
            "  [-1.59375 0.414062 -0.410156 ... -0.205078 0.0893555 -0.243164]\n",
            "  ...\n",
            "  [-1.32031 0.0422363 -0.275391 ... -1.25781 -0.429688 -0.476562]\n",
            "  [-1.17188 -0.140625 -0.0402832 ... -1.53906 -0.324219 -0.349609]\n",
            "  [-1.35938 0.00582886 0.0888672 ... -1.28906 -0.6875 -0.261719]]]\n",
            "attn_output=[[[0.792969 -0.789062 1.22656 ... -0.433594 -0.558594 -0.804688]\n",
            "  [0.792969 -0.789062 1.22656 ... -0.433594 -0.558594 -0.804688]\n",
            "  [0.773438 -0.789062 1.22656 ... -0.441406 -0.554688 -0.8125]\n",
            "  ...\n",
            "  [0.225586 -0.574219 1.0625 ... -0.289062 -0.0151367 1]\n",
            "  [0.558594 -1.58594 0.371094 ... 0.628906 0.875 0.53125]\n",
            "  [-1.20312 -1.23438 1.29688 ... -0.0463867 0.482422 -1.04688]]\n",
            "\n",
            " [[0.119629 0.0634766 0.287109 ... -0.239258 -1.35156 1.02344]\n",
            "  [-0.347656 0.0703125 0.625 ... -0.695312 -0.523438 1.29688]\n",
            "  [-0.216797 -0.22168 0.0407715 ... -0.183594 -0.0544434 -0.369141]\n",
            "  ...\n",
            "  [1.02344 1.80469 -0.839844 ... -0.130859 1.35938 -0.090332]\n",
            "  [-0.90625 0.566406 0.0444336 ... -0.208008 -0.275391 -0.0209961]\n",
            "  [-1.54688 1.17969 0.765625 ... -1.00781 0.289062 -1.46094]]\n",
            "\n",
            " [[0.792969 -0.789062 1.22656 ... -0.433594 -0.558594 -0.804688]\n",
            "  [0.792969 -0.789062 1.22656 ... -0.433594 -0.558594 -0.804688]\n",
            "  [0.773438 -0.789062 1.22656 ... -0.441406 -0.554688 -0.8125]\n",
            "  ...\n",
            "  [0.824219 -0.335938 -1.11719 ... -0.310547 -2.10938 -0.081543]\n",
            "  [-1.5625 -0.318359 0.183594 ... -1.15625 -1.6875 -0.490234]\n",
            "  [-0.792969 -1.21875 0.28125 ... -0.863281 -0.742188 -0.3125]]\n",
            "\n",
            " [[0.792969 -0.789062 1.22656 ... -0.433594 -0.558594 -0.804688]\n",
            "  [0.792969 -0.789062 1.22656 ... -0.433594 -0.558594 -0.804688]\n",
            "  [0.773438 -0.789062 1.22656 ... -0.441406 -0.554688 -0.8125]\n",
            "  ...\n",
            "  [-0.0327148 0.0306396 0.186523 ... -0.326172 -1.27344 -1.04688]\n",
            "  [-0.742188 -0.683594 0.152344 ... 0.582031 -1.0625 -0.894531]\n",
            "  [-1.23438 -0.271484 0.400391 ... -0.8125 -0.206055 -1.32031]]\n",
            "\n",
            " [[0.792969 -0.789062 1.22656 ... -0.433594 -0.558594 -0.804688]\n",
            "  [0.449219 0.124023 0.921875 ... -0.890625 -0.337891 -0.036377]\n",
            "  [-0.213867 0.546875 0.703125 ... -0.137695 -0.457031 -0.546875]\n",
            "  ...\n",
            "  [-1.48438 1.35938 0.367188 ... -1.25781 -0.414062 1.71094]\n",
            "  [0.173828 0.410156 0.207031 ... -0.652344 0.878906 -1.70312]\n",
            "  [-1.71875 0.652344 0.550781 ... -0.742188 -0.460938 -1.21094]]]\n",
            "next_layer_addition_dropped_out=[[[1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.84375 -3.0625 2.85938 ... -1.29688 -1.14062 -2.45312]\n",
            "  ...\n",
            "  [0.972656 -1.20312 2.3125 ... -1.32812 1.30469 3.3125]\n",
            "  [0.625 -5.25 1.89844 ... 1.76562 3.5625 1.07812]\n",
            "  [-3.53125 -3.51562 3.57812 ... -1.13281 1.72656 -3.28125]]\n",
            "\n",
            " [[-0.0566406 0.152344 0.1875 ... 0.03125 -4.1875 3.125]\n",
            "  [-0.4375 -0.0878906 1.76562 ... -1.21875 -0.648438 4.15625]\n",
            "  [-0.804688 -1.125 -0.376953 ... -0.373047 -0.554688 -0.710938]\n",
            "  ...\n",
            "  [3.53125 3.53125 -2.96875 ... -0.24707 2.34375 -0.671875]\n",
            "  [-1.64062 2.4375 -0.105469 ... 0.164062 -2.03125 -1.92188]\n",
            "  [-3.4375 4.125 2.29688 ... -3.21875 0.757812 -2.9375]]\n",
            "\n",
            " [[1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.84375 -3.0625 2.85938 ... -1.29688 -1.14062 -2.45312]\n",
            "  ...\n",
            "  [2.01562 -1.5625 -2.78125 ... -0.539062 -4.6875 -0.328125]\n",
            "  [-3.54688 -1.36719 0.632812 ... -3.21875 -3.04688 -1.39062]\n",
            "  [-2.14062 -2.25 1.26562 ... -3.20312 -1.39844 -1.90625]]\n",
            "\n",
            " [[1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.84375 -3.0625 2.85938 ... -1.29688 -1.14062 -2.45312]\n",
            "  ...\n",
            "  [0.308594 0.773438 0.628906 ... 0.105469 -2.5 -3.84375]\n",
            "  [-2.8125 -1.33594 1.17969 ... 1.40625 -1.03906 -3.40625]\n",
            "  [-3.26562 0.4375 1.59375 ... -2.53125 -0.179688 -4.25]]\n",
            "\n",
            " [[1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.90625 -0.351562 1.57031 ... -1.65625 -0.167969 -0.0351562]\n",
            "  [0.421875 0.96875 2.20312 ... 0.214844 -1.01562 -2.6875]\n",
            "  ...\n",
            "  [-4.03125 3.8125 1.03125 ... -2.40625 -0.535156 4]\n",
            "  [0.597656 0.421875 1.17188 ... -0.550781 2.17188 -4.59375]\n",
            "  [-4.90625 3.125 2.28125 ... -2.34375 -1.74219 -3.39062]]]\n",
            "inputs=[[[1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.84375 -3.0625 2.85938 ... -1.29688 -1.14062 -2.45312]\n",
            "  ...\n",
            "  [0.972656 -1.20312 2.3125 ... -1.32812 1.30469 3.3125]\n",
            "  [0.625 -5.25 1.89844 ... 1.76562 3.5625 1.07812]\n",
            "  [-3.53125 -3.51562 3.57812 ... -1.13281 1.72656 -3.28125]]\n",
            "\n",
            " [[-0.0566406 0.152344 0.1875 ... 0.03125 -4.1875 3.125]\n",
            "  [-0.4375 -0.0878906 1.76562 ... -1.21875 -0.648438 4.15625]\n",
            "  [-0.804688 -1.125 -0.376953 ... -0.373047 -0.554688 -0.710938]\n",
            "  ...\n",
            "  [3.53125 3.53125 -2.96875 ... -0.24707 2.34375 -0.671875]\n",
            "  [-1.64062 2.4375 -0.105469 ... 0.164062 -2.03125 -1.92188]\n",
            "  [-3.4375 4.125 2.29688 ... -3.21875 0.757812 -2.9375]]\n",
            "\n",
            " [[1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.84375 -3.0625 2.85938 ... -1.29688 -1.14062 -2.45312]\n",
            "  ...\n",
            "  [2.01562 -1.5625 -2.78125 ... -0.539062 -4.6875 -0.328125]\n",
            "  [-3.54688 -1.36719 0.632812 ... -3.21875 -3.04688 -1.39062]\n",
            "  [-2.14062 -2.25 1.26562 ... -3.20312 -1.39844 -1.90625]]\n",
            "\n",
            " [[1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.84375 -3.0625 2.85938 ... -1.29688 -1.14062 -2.45312]\n",
            "  ...\n",
            "  [0.308594 0.773438 0.628906 ... 0.105469 -2.5 -3.84375]\n",
            "  [-2.8125 -1.33594 1.17969 ... 1.40625 -1.03906 -3.40625]\n",
            "  [-3.26562 0.4375 1.59375 ... -2.53125 -0.179688 -4.25]]\n",
            "\n",
            " [[1.92969 -3.0625 2.875 ... -1.27344 -1.15625 -2.45312]\n",
            "  [1.90625 -0.351562 1.57031 ... -1.65625 -0.167969 -0.0351562]\n",
            "  [0.421875 0.96875 2.20312 ... 0.214844 -1.01562 -2.6875]\n",
            "  ...\n",
            "  [-4.03125 3.8125 1.03125 ... -2.40625 -0.535156 4]\n",
            "  [0.597656 0.421875 1.17188 ... -0.550781 2.17188 -4.59375]\n",
            "  [-4.90625 3.125 2.28125 ... -2.34375 -1.74219 -3.39062]]]\n",
            "lnx=[[[0.574219 -0.910156 0.855469 ... -0.378906 -0.34375 -0.730469]\n",
            "  [0.574219 -0.910156 0.855469 ... -0.378906 -0.34375 -0.730469]\n",
            "  [0.546875 -0.910156 0.851562 ... -0.384766 -0.339844 -0.730469]\n",
            "  ...\n",
            "  [0.339844 -0.419922 0.808594 ... -0.464844 0.455078 1.15625]\n",
            "  [0.230469 -1.9375 0.699219 ... 0.652344 1.3125 0.398438]\n",
            "  [-1.21094 -1.21094 1.22656 ... -0.388672 0.59375 -1.125]]\n",
            "\n",
            " [[-0.017334 0.0466309 0.057373 ... 0.00958252 -1.28125 0.957031]\n",
            "  [-0.147461 -0.0296631 0.597656 ... -0.412109 -0.21875 1.40625]\n",
            "  [-0.285156 -0.398438 -0.133789 ... -0.131836 -0.196289 -0.251953]\n",
            "  ...\n",
            "  [1.46094 1.46094 -1.22656 ... -0.102051 0.96875 -0.277344]\n",
            "  [-0.625 0.929688 -0.0402832 ... 0.0625 -0.777344 -0.734375]\n",
            "  [-1.39062 1.67188 0.929688 ... -1.30469 0.306641 -1.1875]]\n",
            "\n",
            " [[0.574219 -0.910156 0.855469 ... -0.378906 -0.34375 -0.730469]\n",
            "  [0.574219 -0.910156 0.855469 ... -0.378906 -0.34375 -0.730469]\n",
            "  [0.546875 -0.910156 0.851562 ... -0.384766 -0.339844 -0.730469]\n",
            "  ...\n",
            "  [0.785156 -0.609375 -1.08594 ... -0.209961 -1.82812 -0.12793]\n",
            "  [-1.38281 -0.535156 0.24707 ... -1.25781 -1.1875 -0.542969]\n",
            "  [-0.773438 -0.8125 0.457031 ... -1.15625 -0.503906 -0.6875]]\n",
            "\n",
            " [[0.574219 -0.910156 0.855469 ... -0.378906 -0.34375 -0.730469]\n",
            "  [0.574219 -0.910156 0.855469 ... -0.378906 -0.34375 -0.730469]\n",
            "  [0.546875 -0.910156 0.851562 ... -0.384766 -0.339844 -0.730469]\n",
            "  ...\n",
            "  [0.118652 0.296875 0.241211 ... 0.0405273 -0.960938 -1.47656]\n",
            "  [-1.10156 -0.523438 0.462891 ... 0.550781 -0.40625 -1.33594]\n",
            "  [-1.19531 0.160156 0.582031 ... -0.925781 -0.0654297 -1.55469]]\n",
            "\n",
            " [[0.574219 -0.910156 0.855469 ... -0.378906 -0.34375 -0.730469]\n",
            "  [0.640625 -0.118164 0.527344 ... -0.558594 -0.0566406 -0.0118408]\n",
            "  [0.146484 0.335938 0.765625 ... 0.074707 -0.351562 -0.933594]\n",
            "  ...\n",
            "  [-1.66406 1.57812 0.425781 ... -0.992188 -0.220703 1.64844]\n",
            "  [0.244141 0.171875 0.478516 ... -0.224609 0.886719 -1.875]\n",
            "  [-1.92969 1.22656 0.894531 ... -0.921875 -0.683594 -1.32812]]]\n",
            "attention_lnx=[[[-0.0167236 0.451172 -0.53125 ... 0.0473633 -0.414062 -0.511719]\n",
            "  [-0.0167236 0.451172 -0.53125 ... 0.0473633 -0.414062 -0.511719]\n",
            "  [-0.0133057 0.457031 -0.535156 ... 0.0478516 -0.414062 -0.507812]\n",
            "  ...\n",
            "  [0.103027 0.289062 -0.773438 ... 0.267578 -0.714844 -0.427734]\n",
            "  [-0.0306396 0.205078 -0.648438 ... 0.137695 -0.757812 -0.173828]\n",
            "  [0.0732422 0.384766 -0.949219 ... 0.239258 -0.632812 -0.214844]]\n",
            "\n",
            " [[0.131836 1.36719 -0.265625 ... -1 0.257812 0.0957031]\n",
            "  [-0.246094 1.58594 -0.628906 ... -1.00781 0.373047 -0.394531]\n",
            "  [-0.131836 1.21094 -0.640625 ... -0.636719 1.39844 -0.160156]\n",
            "  ...\n",
            "  [0.0849609 1.72656 -1.57812 ... -1.10156 0.84375 0.53125]\n",
            "  [-0.0878906 1.75 -1.65625 ... -1.20312 0.652344 0.707031]\n",
            "  [-0.134766 1.59375 -1.32031 ... -0.988281 0.566406 0.417969]]\n",
            "\n",
            " [[-0.0167236 0.451172 -0.53125 ... 0.0473633 -0.414062 -0.511719]\n",
            "  [-0.0167236 0.451172 -0.53125 ... 0.0473633 -0.414062 -0.511719]\n",
            "  [-0.0133057 0.457031 -0.535156 ... 0.0478516 -0.414062 -0.507812]\n",
            "  ...\n",
            "  [-0.046875 0.0859375 -1.67969 ... 0.0825195 -0.147461 0.237305]\n",
            "  [0.298828 0.324219 -1.64844 ... 0.162109 -0.220703 0.0732422]\n",
            "  [-0.00582886 0.271484 -1.86719 ... 0.0458984 -0.421875 0.0454102]]\n",
            "\n",
            " [[-0.0167236 0.451172 -0.53125 ... 0.0473633 -0.414062 -0.511719]\n",
            "  [-0.0167236 0.451172 -0.53125 ... 0.0473633 -0.414062 -0.511719]\n",
            "  [-0.0133057 0.457031 -0.535156 ... 0.0478516 -0.414062 -0.507812]\n",
            "  ...\n",
            "  [0.503906 0.617188 -1.85156 ... 0.355469 -0.197266 0.119141]\n",
            "  [0.777344 0.628906 -2.26562 ... 0.189453 -0.152344 0.421875]\n",
            "  [0.365234 0.363281 -1.82031 ... 0.134766 -0.546875 0.118164]]\n",
            "\n",
            " [[-0.0167236 0.451172 -0.53125 ... 0.0473633 -0.414062 -0.511719]\n",
            "  [-0.0458984 0.472656 -0.90625 ... -0.0429688 0.118164 -0.390625]\n",
            "  [-0.316406 0.828125 -0.730469 ... 0.0402832 0.131836 -0.486328]\n",
            "  ...\n",
            "  [0.285156 0.726562 -1.96094 ... -0.0383301 -0.0441895 -0.335938]\n",
            "  [0.178711 0.765625 -1.99219 ... 0.0795898 0.435547 -0.104492]\n",
            "  [0.251953 0.255859 -2.15625 ... -0.155273 0.124023 0.0240479]]]\n",
            "attn_output=[[[0.546875 -0.746094 0.667969 ... -0.349609 -0.447266 -0.84375]\n",
            "  [0.546875 -0.746094 0.667969 ... -0.349609 -0.447266 -0.84375]\n",
            "  [0.523438 -0.742188 0.664062 ... -0.355469 -0.443359 -0.84375]\n",
            "  ...\n",
            "  [0.363281 -0.308594 0.519531 ... -0.357422 0.198242 0.972656]\n",
            "  [0.208984 -1.77344 0.439453 ... 0.667969 0.984375 0.318359]\n",
            "  [-1.14062 -1.03125 0.863281 ... -0.292969 0.359375 -1.14844]]\n",
            "\n",
            " [[0.0219727 0.443359 -0.0228271 ... -0.283203 -1.14844 0.9375]\n",
            "  [-0.22168 0.486328 0.369141 ... -0.722656 -0.0893555 1.21875]\n",
            "  [-0.318359 0.0291748 -0.345703 ... -0.341797 0.285156 -0.294922]\n",
            "  ...\n",
            "  [1.42969 2.07812 -1.79688 ... -0.535156 1.25781 -0.0556641]\n",
            "  [-0.636719 1.54688 -0.648438 ... -0.382812 -0.507812 -0.447266]\n",
            "  [-1.39844 2.23438 0.380859 ... -1.64062 0.515625 -0.984375]]\n",
            "\n",
            " [[0.546875 -0.746094 0.667969 ... -0.349609 -0.447266 -0.84375]\n",
            "  [0.546875 -0.746094 0.667969 ... -0.349609 -0.447266 -0.84375]\n",
            "  [0.523438 -0.742188 0.664062 ... -0.355469 -0.443359 -0.84375]\n",
            "  ...\n",
            "  [0.730469 -0.546875 -1.65625 ... -0.169922 -1.79688 -0.0336914]\n",
            "  [-1.21875 -0.392578 -0.380859 ... -1.14844 -1.22656 -0.494141]\n",
            "  [-0.746094 -0.6875 -0.208984 ... -1.09375 -0.632812 -0.648438]]\n",
            "\n",
            " [[0.546875 -0.746094 0.667969 ... -0.349609 -0.447266 -0.84375]\n",
            "  [0.546875 -0.746094 0.667969 ... -0.349609 -0.447266 -0.84375]\n",
            "  [0.523438 -0.742188 0.664062 ... -0.355469 -0.443359 -0.84375]\n",
            "  ...\n",
            "  [0.298828 0.511719 -0.449219 ... 0.169922 -0.992188 -1.36719]\n",
            "  [-0.769531 -0.267578 -0.410156 ... 0.601562 -0.449219 -1.125]\n",
            "  [-1.02344 0.283203 -0.0800781 ... -0.84375 -0.255859 -1.45312]]\n",
            "\n",
            " [[0.546875 -0.746094 0.667969 ... -0.349609 -0.447266 -0.84375]\n",
            "  [0.597656 0.0390625 0.213867 ... -0.546875 -0.0159912 -0.136719]\n",
            "  [0.0351562 0.597656 0.490234 ... 0.0849609 -0.292969 -1.05469]\n",
            "  ...\n",
            "  [-1.48438 1.79688 -0.367188 ... -0.96875 -0.229492 1.45312]\n",
            "  [0.304688 0.466797 -0.322266 ... -0.185547 1.02344 -1.84375]\n",
            "  [-1.77344 1.28906 0.0476074 ... -0.953125 -0.617188 -1.28125]]]\n",
            "next_layer_addition_dropped_out=[[[1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1 -2.5625 2.54688 ... -1.48438 -0.742188 -3.21875]\n",
            "  ...\n",
            "  [0.46875 -1 1.53125 ... -1.08594 0.230469 2.98438]\n",
            "  [0.306641 -5.90625 -0.421875 ... 1.95312 3.21875 0.796875]\n",
            "  [-3.46875 -3.1875 2.32812 ... -1.39062 0.914062 -3.59375]]\n",
            "\n",
            " [[-0.78125 1.09375 0.667969 ... -0.601562 -4.90625 3.0625]\n",
            "  [0.0976562 1.44531 1.75 ... -1.99219 0.462891 3.90625]\n",
            "  [-0.914062 -1.00781 -0.8125 ... -0.742188 1.13281 -0.566406]\n",
            "  ...\n",
            "  [3.53125 4.875 -4.78125 ... -1.55469 3.04688 -0.988281]\n",
            "  [-2.54688 3.29688 -1.375 ... -1.54688 -1.04688 -0.890625]\n",
            "  [-4.5 5.53125 1.3125 ... -5 0.796875 -4.125]]\n",
            "\n",
            " [[1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1 -2.5625 2.54688 ... -1.48438 -0.742188 -3.21875]\n",
            "  ...\n",
            "  [2.21875 -3.10938 -3.78125 ... 0.539062 -4.4375 -0.921875]\n",
            "  [-3.29688 -2.03125 -0.394531 ... -2.90625 -3.82812 -1.60938]\n",
            "  [-3.09375 -3.0625 -0.984375 ... -3.40625 -2.6875 -2.4375]]\n",
            "\n",
            " [[1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1 -2.5625 2.54688 ... -1.48438 -0.742188 -3.21875]\n",
            "  ...\n",
            "  [0.416016 1.49219 -1.03125 ... -0.371094 -3.03125 -3.6875]\n",
            "  [-1.35156 -0.703125 -2.39062 ... 0.917969 -1.875 -3.23438]\n",
            "  [-3.70312 -0.511719 -0.236328 ... -3.40625 -2.0625 -5.3125]]\n",
            "\n",
            " [[1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1.625 0.292969 0.71875 ... -1.85156 -0.277344 -0.152344]\n",
            "  [0.24707 1.55469 1.41406 ... 0.211914 -0.458984 -3]\n",
            "  ...\n",
            "  [-3.85938 4.875 -1.85156 ... -1.125 -0.253906 3.125]\n",
            "  [-0.753906 1.38281 -1.48438 ... -0.197266 3.20312 -4.59375]\n",
            "  [-6.0625 2.65625 0.546875 ... -2.34375 -3.1875 -4.15625]]]\n",
            "inputs=[[[1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1 -2.5625 2.54688 ... -1.48438 -0.742188 -3.21875]\n",
            "  ...\n",
            "  [0.46875 -1 1.53125 ... -1.08594 0.230469 2.98438]\n",
            "  [0.306641 -5.90625 -0.421875 ... 1.95312 3.21875 0.796875]\n",
            "  [-3.46875 -3.1875 2.32812 ... -1.39062 0.914062 -3.59375]]\n",
            "\n",
            " [[-0.78125 1.09375 0.667969 ... -0.601562 -4.90625 3.0625]\n",
            "  [0.0976562 1.44531 1.75 ... -1.99219 0.462891 3.90625]\n",
            "  [-0.914062 -1.00781 -0.8125 ... -0.742188 1.13281 -0.566406]\n",
            "  ...\n",
            "  [3.53125 4.875 -4.78125 ... -1.55469 3.04688 -0.988281]\n",
            "  [-2.54688 3.29688 -1.375 ... -1.54688 -1.04688 -0.890625]\n",
            "  [-4.5 5.53125 1.3125 ... -5 0.796875 -4.125]]\n",
            "\n",
            " [[1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1 -2.5625 2.54688 ... -1.48438 -0.742188 -3.21875]\n",
            "  ...\n",
            "  [2.21875 -3.10938 -3.78125 ... 0.539062 -4.4375 -0.921875]\n",
            "  [-3.29688 -2.03125 -0.394531 ... -2.90625 -3.82812 -1.60938]\n",
            "  [-3.09375 -3.0625 -0.984375 ... -3.40625 -2.6875 -2.4375]]\n",
            "\n",
            " [[1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1 -2.5625 2.54688 ... -1.48438 -0.742188 -3.21875]\n",
            "  ...\n",
            "  [0.416016 1.49219 -1.03125 ... -0.371094 -3.03125 -3.6875]\n",
            "  [-1.35156 -0.703125 -2.39062 ... 0.917969 -1.875 -3.23438]\n",
            "  [-3.70312 -0.511719 -0.236328 ... -3.40625 -2.0625 -5.3125]]\n",
            "\n",
            " [[1.09375 -2.5625 2.5625 ... -1.44531 -0.738281 -3.21875]\n",
            "  [1.625 0.292969 0.71875 ... -1.85156 -0.277344 -0.152344]\n",
            "  [0.24707 1.55469 1.41406 ... 0.211914 -0.458984 -3]\n",
            "  ...\n",
            "  [-3.85938 4.875 -1.85156 ... -1.125 -0.253906 3.125]\n",
            "  [-0.753906 1.38281 -1.48438 ... -0.197266 3.20312 -4.59375]\n",
            "  [-6.0625 2.65625 0.546875 ... -2.34375 -3.1875 -4.15625]]]\n",
            "lnx=[[[0.304688 -0.710938 0.710938 ... -0.402344 -0.205078 -0.894531]\n",
            "  [0.304688 -0.710938 0.710938 ... -0.402344 -0.205078 -0.894531]\n",
            "  [0.277344 -0.710938 0.707031 ... -0.412109 -0.206055 -0.894531]\n",
            "  ...\n",
            "  [0.155273 -0.330078 0.507812 ... -0.359375 0.0761719 0.988281]\n",
            "  [0.10498 -2.01562 -0.144531 ... 0.667969 1.10156 0.271484]\n",
            "  [-1.10156 -1.00781 0.738281 ... -0.441406 0.289062 -1.14062]]\n",
            "\n",
            " [[-0.222656 0.3125 0.19043 ... -0.171875 -1.39844 0.875]\n",
            "  [0.03125 0.462891 0.558594 ... -0.636719 0.148438 1.25]\n",
            "  [-0.302734 -0.333984 -0.269531 ... -0.246094 0.375 -0.1875]\n",
            "  ...\n",
            "  [1.34375 1.85938 -1.82031 ... -0.59375 1.16406 -0.376953]\n",
            "  [-0.894531 1.15625 -0.484375 ... -0.542969 -0.367188 -0.3125]\n",
            "  [-1.67188 2.0625 0.488281 ... -1.85938 0.296875 -1.53906]]\n",
            "\n",
            " [[0.304688 -0.710938 0.710938 ... -0.402344 -0.205078 -0.894531]\n",
            "  [0.304688 -0.710938 0.710938 ... -0.402344 -0.205078 -0.894531]\n",
            "  [0.277344 -0.710938 0.707031 ... -0.412109 -0.206055 -0.894531]\n",
            "  ...\n",
            "  [0.789062 -1.10156 -1.34375 ... 0.191406 -1.57812 -0.328125]\n",
            "  [-1.19531 -0.734375 -0.142578 ... -1.04688 -1.38281 -0.582031]\n",
            "  [-1.02344 -1.00781 -0.324219 ... -1.125 -0.886719 -0.804688]]\n",
            "\n",
            " [[0.304688 -0.710938 0.710938 ... -0.402344 -0.205078 -0.894531]\n",
            "  [0.304688 -0.710938 0.710938 ... -0.402344 -0.205078 -0.894531]\n",
            "  [0.277344 -0.710938 0.707031 ... -0.412109 -0.206055 -0.894531]\n",
            "  ...\n",
            "  [0.147461 0.53125 -0.365234 ... -0.131836 -1.07812 -1.3125]\n",
            "  [-0.494141 -0.255859 -0.871094 ... 0.335938 -0.683594 -1.17969]\n",
            "  [-1.22656 -0.169922 -0.078125 ... -1.125 -0.683594 -1.75781]]\n",
            "\n",
            " [[0.304688 -0.710938 0.710938 ... -0.402344 -0.205078 -0.894531]\n",
            "  [0.511719 0.0922852 0.225586 ... -0.582031 -0.0874023 -0.0478516]\n",
            "  [0.0795898 0.5 0.455078 ... 0.0678711 -0.147461 -0.964844]\n",
            "  ...\n",
            "  [-1.46875 1.85938 -0.707031 ... -0.427734 -0.0966797 1.1875]\n",
            "  [-0.287109 0.527344 -0.566406 ... -0.0751953 1.22656 -1.75781]\n",
            "  [-2.20312 0.964844 0.199219 ... -0.851562 -1.15625 -1.50781]]]\n",
            "attention_lnx=[[[0.347656 -0.162109 -0.0844727 ... -0.142578 -0.996094 0.703125]\n",
            "  [0.347656 -0.162109 -0.0844727 ... -0.142578 -0.996094 0.703125]\n",
            "  [0.34375 -0.160156 -0.0834961 ... -0.139648 -0.996094 0.707031]\n",
            "  ...\n",
            "  [0.447266 0.052002 -0.443359 ... -0.371094 -0.683594 1.08594]\n",
            "  [0.322266 0.0120239 -0.431641 ... -0.324219 -0.664062 0.960938]\n",
            "  [0.523438 0.0722656 -0.431641 ... -0.570312 -0.671875 0.847656]]\n",
            "\n",
            " [[0.0103149 1.80469 -0.211914 ... -2.03125 -1.88281 -0.96875]\n",
            "  [0.193359 0.792969 -0.472656 ... -1.20312 -1.69531 -0.302734]\n",
            "  [0.394531 0.648438 -0.0678711 ... -0.980469 -1.21875 -0.0107422]\n",
            "  ...\n",
            "  [0.330078 0.0722656 -0.419922 ... -0.0437012 -1.10938 -0.135742]\n",
            "  [-0.429688 -0.265625 -0.714844 ... -0.341797 -1.25781 0.125]\n",
            "  [-0.330078 -0.0134888 -0.349609 ... -0.316406 -1.58594 0.121582]]\n",
            "\n",
            " [[0.347656 -0.162109 -0.0844727 ... -0.142578 -0.996094 0.703125]\n",
            "  [0.347656 -0.162109 -0.0844727 ... -0.142578 -0.996094 0.703125]\n",
            "  [0.34375 -0.160156 -0.0834961 ... -0.139648 -0.996094 0.707031]\n",
            "  ...\n",
            "  [0.757812 -0.0299072 -0.404297 ... -0.738281 -0.244141 0.96875]\n",
            "  [0.90625 -0.048584 -0.302734 ... -0.546875 -0.0071106 0.71875]\n",
            "  [0.714844 -0.0932617 -0.660156 ... -0.527344 -0.0732422 0.574219]]\n",
            "\n",
            " [[0.347656 -0.162109 -0.0844727 ... -0.142578 -0.996094 0.703125]\n",
            "  [0.347656 -0.162109 -0.0844727 ... -0.142578 -0.996094 0.703125]\n",
            "  [0.34375 -0.160156 -0.0834961 ... -0.139648 -0.996094 0.707031]\n",
            "  ...\n",
            "  [0.765625 -0.296875 -0.601562 ... -0.570312 -0.455078 1.11719]\n",
            "  [0.835938 -0.164062 -0.792969 ... -0.539062 -0.498047 1.10938]\n",
            "  [0.65625 -0.128906 -0.65625 ... -0.457031 -0.480469 0.847656]]\n",
            "\n",
            " [[0.347656 -0.162109 -0.0844727 ... -0.142578 -0.996094 0.703125]\n",
            "  [0.824219 0.746094 -0.347656 ... -0.695312 -0.917969 0.275391]\n",
            "  [0.796875 0.617188 -0.535156 ... -0.507812 -0.785156 0.886719]\n",
            "  ...\n",
            "  [0.392578 0.0688477 -0.511719 ... -0.219727 -0.388672 0.714844]\n",
            "  [0.65625 0.111816 -0.269531 ... -0.310547 -0.589844 0.267578]\n",
            "  [0.554688 -0.255859 -0.158203 ... -0.271484 -0.369141 0.367188]]]\n",
            "attn_output=[[[0.388672 -0.734375 0.667969 ... -0.427734 -0.46875 -0.679688]\n",
            "  [0.388672 -0.734375 0.667969 ... -0.427734 -0.46875 -0.679688]\n",
            "  [0.363281 -0.734375 0.664062 ... -0.439453 -0.46875 -0.679688]\n",
            "  ...\n",
            "  [0.287109 -0.298828 0.341797 ... -0.457031 -0.142578 1.28125]\n",
            "  [0.206055 -1.92969 -0.279297 ... 0.535156 0.835938 0.574219]\n",
            "  [-0.902344 -0.957031 0.582031 ... -0.601562 0.0742188 -0.84375]]\n",
            "\n",
            " [[-0.208984 0.785156 0.123535 ... -0.710938 -1.83594 0.566406]\n",
            "  [0.0883789 0.679688 0.388672 ... -0.972656 -0.375 1.09375]\n",
            "  [-0.163086 -0.112793 -0.277344 ... -0.542969 -0.0269775 -0.181641]\n",
            "  ...\n",
            "  [1.4375 1.84375 -1.9375 ... -0.59375 0.722656 -0.417969]\n",
            "  [-1.01562 1.03125 -0.710938 ... -0.644531 -0.785156 -0.261719]\n",
            "  [-1.75 2 0.349609 ... -1.92969 -0.287109 -1.45312]]\n",
            "\n",
            " [[0.388672 -0.734375 0.667969 ... -0.427734 -0.46875 -0.679688]\n",
            "  [0.388672 -0.734375 0.667969 ... -0.427734 -0.46875 -0.679688]\n",
            "  [0.363281 -0.734375 0.664062 ... -0.439453 -0.46875 -0.679688]\n",
            "  ...\n",
            "  [1.02344 -1.07812 -1.4375 ... -0.0683594 -1.60938 0.0161133]\n",
            "  [-0.828125 -0.71875 -0.241211 ... -1.19531 -1.32812 -0.308594]\n",
            "  [-0.761719 -1.00781 -0.527344 ... -1.25781 -0.882812 -0.59375]]\n",
            "\n",
            " [[0.388672 -0.734375 0.667969 ... -0.427734 -0.46875 -0.679688]\n",
            "  [0.388672 -0.734375 0.667969 ... -0.427734 -0.46875 -0.679688]\n",
            "  [0.363281 -0.734375 0.664062 ... -0.439453 -0.46875 -0.679688]\n",
            "  ...\n",
            "  [0.40625 0.410156 -0.558594 ... -0.322266 -1.19531 -0.882812]\n",
            "  [-0.179688 -0.302734 -1.10938 ... 0.132812 -0.828125 -0.742188]\n",
            "  [-0.980469 -0.206055 -0.287109 ... -1.24219 -0.820312 -1.4375]]\n",
            "\n",
            " [[0.388672 -0.734375 0.667969 ... -0.427734 -0.46875 -0.679688]\n",
            "  [0.738281 0.3125 0.111816 ... -0.769531 -0.361328 0.0371094]\n",
            "  [0.322266 0.671875 0.271484 ... -0.0913086 -0.384766 -0.652344]\n",
            "  ...\n",
            "  [-1.28125 1.82812 -0.875 ... -0.498047 -0.237305 1.42188]\n",
            "  [-0.0361328 0.550781 -0.648438 ... -0.1875 0.964844 -1.60156]\n",
            "  [-1.95312 0.851562 0.137695 ... -0.925781 -1.25781 -1.34375]]]\n",
            "next_layer_addition_dropped_out=[[[0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.296875 -3.34375 1.70312 ... -1.79688 -2.29688 -1.6875]\n",
            "  ...\n",
            "  [-0.351562 -1.28906 1.23438 ... -0.8125 -0.435547 4.6875]\n",
            "  [0.386719 -6.0625 -2.28125 ... 2.35938 2.90625 1.23438]\n",
            "  [-5.6875 -2.9375 1.89844 ... -1.40625 -0.984375 -3.1875]]\n",
            "\n",
            " [[-0.0976562 3.42188 0.242188 ... -2.57812 -5.9375 2.04688]\n",
            "  [1.67188 2.01562 0.941406 ... -2.75 -1.16406 4.03125]\n",
            "  [-0.341797 -0.320312 -1.39062 ... -0.90625 -0.476562 -0.882812]\n",
            "  ...\n",
            "  [4.78125 5.75 -5.125 ... -1.64844 2.07812 0]\n",
            "  [-2.9375 4 -2.76562 ... -2.35938 -2.01562 -0.835938]\n",
            "  [-5.34375 6.15625 0.878906 ... -5.15625 0.0078125 -3.96875]]\n",
            "\n",
            " [[0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.296875 -3.34375 1.70312 ... -1.79688 -2.29688 -1.6875]\n",
            "  ...\n",
            "  [2.21875 -3.3125 -4.65625 ... 0.242188 -5.75 0.233398]\n",
            "  [-2.54688 -0.921875 -2.0625 ... -3.53125 -3.9375 -0.976562]\n",
            "  [-5 -2.32812 -2.90625 ... -4.125 -4.59375 -1.75]]\n",
            "\n",
            " [[0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.296875 -3.34375 1.70312 ... -1.79688 -2.29688 -1.6875]\n",
            "  ...\n",
            "  [0.929688 1.19531 -1.59375 ... -0.738281 -3.5 -2.71875]\n",
            "  [-0.984375 0.121094 -3.48438 ... 1.125 -2.98438 -1.52344]\n",
            "  [-6.09375 -0.59375 -2.1875 ... -3.8125 -3.59375 -4.84375]]\n",
            "\n",
            " [[0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [2.1875 1.65625 0.296875 ... -2.51562 -0.75 0.149414]\n",
            "  [1.14062 1.75781 0.474609 ... 0.371094 -2.32812 -2.39062]\n",
            "  ...\n",
            "  [-3.67188 5.75 -3.09375 ... -1.36719 -1.65625 4.4375]\n",
            "  [-0.285156 1.35938 -2.78125 ... 0.378906 1.23438 -3.17188]\n",
            "  [-7.34375 2.75 -0.953125 ... -3.46875 -3.75 -4.4375]]]\n",
            "inputs=[[[0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.296875 -3.34375 1.70312 ... -1.79688 -2.29688 -1.6875]\n",
            "  ...\n",
            "  [-0.351562 -1.28906 1.23438 ... -0.8125 -0.435547 4.6875]\n",
            "  [0.386719 -6.0625 -2.28125 ... 2.35938 2.90625 1.23438]\n",
            "  [-5.6875 -2.9375 1.89844 ... -1.40625 -0.984375 -3.1875]]\n",
            "\n",
            " [[-0.0976562 3.42188 0.242188 ... -2.57812 -5.9375 2.04688]\n",
            "  [1.67188 2.01562 0.941406 ... -2.75 -1.16406 4.03125]\n",
            "  [-0.341797 -0.320312 -1.39062 ... -0.90625 -0.476562 -0.882812]\n",
            "  ...\n",
            "  [4.78125 5.75 -5.125 ... -1.64844 2.07812 0]\n",
            "  [-2.9375 4 -2.76562 ... -2.35938 -2.01562 -0.835938]\n",
            "  [-5.34375 6.15625 0.878906 ... -5.15625 0.0078125 -3.96875]]\n",
            "\n",
            " [[0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.296875 -3.34375 1.70312 ... -1.79688 -2.29688 -1.6875]\n",
            "  ...\n",
            "  [2.21875 -3.3125 -4.65625 ... 0.242188 -5.75 0.233398]\n",
            "  [-2.54688 -0.921875 -2.0625 ... -3.53125 -3.9375 -0.976562]\n",
            "  [-5 -2.32812 -2.90625 ... -4.125 -4.59375 -1.75]]\n",
            "\n",
            " [[0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [0.296875 -3.34375 1.70312 ... -1.79688 -2.29688 -1.6875]\n",
            "  ...\n",
            "  [0.929688 1.19531 -1.59375 ... -0.738281 -3.5 -2.71875]\n",
            "  [-0.984375 0.121094 -3.48438 ... 1.125 -2.98438 -1.52344]\n",
            "  [-6.09375 -0.59375 -2.1875 ... -3.8125 -3.59375 -4.84375]]\n",
            "\n",
            " [[0.382812 -3.34375 1.72656 ... -1.76562 -2.3125 -1.67188]\n",
            "  [2.1875 1.65625 0.296875 ... -2.51562 -0.75 0.149414]\n",
            "  [1.14062 1.75781 0.474609 ... 0.371094 -2.32812 -2.39062]\n",
            "  ...\n",
            "  [-3.67188 5.75 -3.09375 ... -1.36719 -1.65625 4.4375]\n",
            "  [-0.285156 1.35938 -2.78125 ... 0.378906 1.23438 -3.17188]\n",
            "  [-7.34375 2.75 -0.953125 ... -3.46875 -3.75 -4.4375]]]\n",
            "lnx=[[[0.101562 -0.886719 0.457031 ... -0.466797 -0.613281 -0.443359]\n",
            "  [0.101562 -0.886719 0.457031 ... -0.466797 -0.613281 -0.443359]\n",
            "  [0.0786133 -0.886719 0.451172 ... -0.476562 -0.609375 -0.447266]\n",
            "  ...\n",
            "  [-0.109375 -0.400391 0.384766 ... -0.251953 -0.135742 1.46094]\n",
            "  [0.124512 -1.95312 -0.734375 ... 0.761719 0.9375 0.398438]\n",
            "  [-1.69531 -0.875 0.566406 ... -0.417969 -0.292969 -0.949219]]\n",
            "\n",
            " [[-0.026001 0.910156 0.0644531 ... -0.6875 -1.57812 0.542969]\n",
            "  [0.5 0.601562 0.28125 ... -0.820312 -0.347656 1.20312]\n",
            "  [-0.104492 -0.0981445 -0.425781 ... -0.277344 -0.145508 -0.269531]\n",
            "  ...\n",
            "  [1.71875 2.07812 -1.84375 ... -0.59375 0.75 0]\n",
            "  [-0.953125 1.29688 -0.898438 ... -0.765625 -0.65625 -0.271484]\n",
            "  [-1.83594 2.10938 0.300781 ... -1.77344 0.00268555 -1.35938]]\n",
            "\n",
            " [[0.101562 -0.886719 0.457031 ... -0.466797 -0.613281 -0.443359]\n",
            "  [0.101562 -0.886719 0.457031 ... -0.466797 -0.613281 -0.443359]\n",
            "  [0.0786133 -0.886719 0.451172 ... -0.476562 -0.609375 -0.447266]\n",
            "  ...\n",
            "  [0.734375 -1.09375 -1.53906 ... 0.0800781 -1.89844 0.0771484]\n",
            "  [-0.851562 -0.308594 -0.6875 ... -1.17969 -1.3125 -0.326172]\n",
            "  [-1.5 -0.699219 -0.875 ... -1.24219 -1.38281 -0.527344]]\n",
            "\n",
            " [[0.101562 -0.886719 0.457031 ... -0.466797 -0.613281 -0.443359]\n",
            "  [0.101562 -0.886719 0.457031 ... -0.466797 -0.613281 -0.443359]\n",
            "  [0.0786133 -0.886719 0.451172 ... -0.476562 -0.609375 -0.447266]\n",
            "  ...\n",
            "  [0.310547 0.400391 -0.53125 ... -0.24707 -1.17188 -0.910156]\n",
            "  [-0.333984 0.0410156 -1.17969 ... 0.380859 -1.00781 -0.515625]\n",
            "  [-1.85938 -0.180664 -0.667969 ... -1.16406 -1.09375 -1.47656]]\n",
            "\n",
            " [[0.101562 -0.886719 0.457031 ... -0.466797 -0.613281 -0.443359]\n",
            "  [0.648438 0.492188 0.0883789 ... -0.746094 -0.222656 0.0444336]\n",
            "  [0.34375 0.527344 0.142578 ... 0.111816 -0.699219 -0.71875]\n",
            "  ...\n",
            "  [-1.3125 2.0625 -1.10938 ... -0.490234 -0.59375 1.59375]\n",
            "  [-0.103516 0.492188 -1.00781 ... 0.137695 0.447266 -1.14844]\n",
            "  [-2.45312 0.917969 -0.318359 ... -1.15625 -1.25 -1.48438]]]\n",
            "attention_lnx=[[[-0.065918 0.773438 -3.0625 ... 0.34375 -0.382812 0.369141]\n",
            "  [-0.065918 0.773438 -3.0625 ... 0.34375 -0.382812 0.369141]\n",
            "  [-0.0678711 0.769531 -3.0625 ... 0.347656 -0.382812 0.367188]\n",
            "  ...\n",
            "  [0.00457764 0.402344 -2.8125 ... 0.333984 0.0490723 0.396484]\n",
            "  [-0.0214844 0.3125 -2.8125 ... 0.683594 0.135742 0.398438]\n",
            "  [-0.0529785 0.378906 -2.70312 ... 0.535156 0.134766 0.404297]]\n",
            "\n",
            " [[-0.365234 0.601562 -2.32812 ... -0.929688 0.957031 0.722656]\n",
            "  [-0.378906 1.44531 -1.96094 ... -0.75 0.902344 0.652344]\n",
            "  [-0.671875 1.55469 -2.20312 ... -0.212891 0.945312 0.294922]\n",
            "  ...\n",
            "  [-0.208008 1.64844 -1.70312 ... -0.486328 0.275391 -0.917969]\n",
            "  [-0.318359 1.28125 -1.55469 ... -0.632812 0.394531 -0.065918]\n",
            "  [-0.349609 1.21094 -1.74219 ... -0.28125 0.294922 -0.15332]]\n",
            "\n",
            " [[-0.065918 0.773438 -3.0625 ... 0.34375 -0.382812 0.369141]\n",
            "  [-0.065918 0.773438 -3.0625 ... 0.34375 -0.382812 0.369141]\n",
            "  [-0.0678711 0.769531 -3.0625 ... 0.347656 -0.382812 0.367188]\n",
            "  ...\n",
            "  [-0.431641 0.398438 -2.1875 ... 0.5 0.10498 0.265625]\n",
            "  [-0.464844 0.255859 -2.20312 ... 0.628906 0.550781 -0.00110626]\n",
            "  [-0.59375 0.157227 -2.15625 ... 0.542969 0.367188 0.3125]]\n",
            "\n",
            " [[-0.065918 0.773438 -3.0625 ... 0.34375 -0.382812 0.369141]\n",
            "  [-0.065918 0.773438 -3.0625 ... 0.34375 -0.382812 0.369141]\n",
            "  [-0.0678711 0.769531 -3.0625 ... 0.347656 -0.382812 0.367188]\n",
            "  ...\n",
            "  [-0.267578 0.333984 -2.0625 ... 0.421875 0.102051 0.242188]\n",
            "  [-0.394531 0.490234 -1.82031 ... 0.404297 -0.00616455 0.449219]\n",
            "  [-0.287109 0.207031 -1.96875 ... 0.378906 0.200195 0.0913086]]\n",
            "\n",
            " [[-0.065918 0.773438 -3.0625 ... 0.34375 -0.382812 0.369141]\n",
            "  [0.117188 0.472656 -2.70312 ... 0.314453 0.108887 0.726562]\n",
            "  [-0.107422 0.167969 -2.1875 ... 0.0507812 0.3125 0.359375]\n",
            "  ...\n",
            "  [-0.578125 0.349609 -1.91406 ... 0.0678711 0.558594 0.0825195]\n",
            "  [-0.388672 0.145508 -2.09375 ... 0.316406 0.412109 0.181641]\n",
            "  [-0.277344 -0.0761719 -2.29688 ... 0.11377 0.359375 -0.0742188]]]\n",
            "attn_output=[[[0.081543 -0.660156 -0.34375 ... -0.365234 -0.691406 -0.333984]\n",
            "  [0.081543 -0.660156 -0.34375 ... -0.365234 -0.691406 -0.333984]\n",
            "  [0.0588379 -0.660156 -0.349609 ... -0.373047 -0.6875 -0.339844]\n",
            "  ...\n",
            "  [-0.104004 -0.265625 -0.474609 ... -0.143555 -0.116211 1.52344]\n",
            "  [0.114746 -1.80469 -1.60156 ... 0.953125 0.953125 0.511719]\n",
            "  [-1.66406 -0.742188 -0.233398 ... -0.251953 -0.246094 -0.808594]]\n",
            "\n",
            " [[-0.119141 1.03906 -0.539062 ... -0.90625 -1.28125 0.714844]\n",
            "  [0.373047 1 -0.294922 ... -1.00781 -0.0756836 1.35156]\n",
            "  [-0.296875 0.361328 -1.05469 ... -0.328125 0.137695 -0.172852]\n",
            "  ...\n",
            "  [1.59375 2.57812 -2.375 ... -0.742188 0.820312 -0.320312]\n",
            "  [-1.03125 1.67969 -1.375 ... -0.953125 -0.515625 -0.287109]\n",
            "  [-1.89844 2.45312 -0.289062 ... -1.8125 0.101074 -1.375]]\n",
            "\n",
            " [[0.081543 -0.660156 -0.34375 ... -0.365234 -0.691406 -0.333984]\n",
            "  [0.081543 -0.660156 -0.34375 ... -0.365234 -0.691406 -0.333984]\n",
            "  [0.0588379 -0.660156 -0.349609 ... -0.373047 -0.6875 -0.339844]\n",
            "  ...\n",
            "  [0.574219 -0.933594 -2.1875 ... 0.238281 -1.8125 0.160156]\n",
            "  [-0.976562 -0.21582 -1.38281 ... -0.941406 -1.09375 -0.316406]\n",
            "  [-1.64062 -0.636719 -1.48438 ... -1.04688 -1.23438 -0.421875]]\n",
            "\n",
            " [[0.081543 -0.660156 -0.34375 ... -0.365234 -0.691406 -0.333984]\n",
            "  [0.081543 -0.660156 -0.34375 ... -0.365234 -0.691406 -0.333984]\n",
            "  [0.0588379 -0.660156 -0.349609 ... -0.373047 -0.6875 -0.339844]\n",
            "  ...\n",
            "  [0.217773 0.503906 -1.20312 ... -0.104004 -1.11719 -0.816406]\n",
            "  [-0.458984 0.203125 -1.76562 ... 0.507812 -0.992188 -0.357422]\n",
            "  [-1.90625 -0.115234 -1.24219 ... -1.02344 -1.01562 -1.41406]]\n",
            "\n",
            " [[0.081543 -0.660156 -0.34375 ... -0.365234 -0.691406 -0.333984]\n",
            "  [0.667969 0.617188 -0.699219 ... -0.636719 -0.185547 0.253906]\n",
            "  [0.300781 0.5625 -0.5 ... 0.123047 -0.589844 -0.59375]\n",
            "  ...\n",
            "  [-1.47656 2.125 -1.74219 ... -0.453125 -0.382812 1.57031]\n",
            "  [-0.239258 0.535156 -1.72656 ... 0.24707 0.585938 -1.0625]\n",
            "  [-2.46875 0.863281 -1.05469 ... -1.08594 -1.09375 -1.46094]]]\n",
            "next_layer_addition_dropped_out=[[[-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.175781 -2.875 -1.13281 ... -1.125 -3.01562 -0.808594]\n",
            "  ...\n",
            "  [-0.765625 -0.964844 -1.26562 ... -0.0527344 -0.410156 5.4375]\n",
            "  [0.859375 -5.625 -5.03125 ... 3.5625 2.5 0.507812]\n",
            "  [-5 -2.875 -1.92188 ... 0.410156 -0.470703 -3.23438]]\n",
            "\n",
            " [[0.248047 3.53125 -3.125 ... -4.0625 -3.45312 2.0625]\n",
            "  [2.5625 2.32812 -1.52344 ... -3.95312 1.09375 3.95312]\n",
            "  [-0.5 1.10938 -4.4375 ... -1.03125 1.33594 -2.04688]\n",
            "  ...\n",
            "  [5.375 6.34375 -7.53125 ... -2.53125 2.875 -0.410156]\n",
            "  [-3.34375 4.59375 -3.6875 ... -4.96875 -0.894531 0.0976562]\n",
            "  [-6.0625 5.5 -1.24219 ... -4.59375 0.632812 -4.21875]]\n",
            "\n",
            " [[-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.175781 -2.875 -1.13281 ... -1.125 -3.01562 -0.808594]\n",
            "  ...\n",
            "  [3.375 -2.45312 -7.625 ... -0.0273438 -6.96875 0.621094]\n",
            "  [-1.96875 -2.15625 -3.70312 ... -2.5625 -2.64062 -1.3125]\n",
            "  [-4.71875 -2.75 -5.65625 ... -3.04688 -3.73438 -1.5]]\n",
            "\n",
            " [[-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.175781 -2.875 -1.13281 ... -1.125 -3.01562 -0.808594]\n",
            "  ...\n",
            "  [1.48438 1.45312 -3 ... -0.203125 -3.5625 -2.89062]\n",
            "  [-0.296875 -0.132812 -6.09375 ... 1.22656 -1.95312 -1.05469]\n",
            "  [-5.3125 -0.792969 -5.09375 ... -3.20312 -2.84375 -5.75]]\n",
            "\n",
            " [[-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [2.73438 2.29688 -2.25 ... -1.78125 -0.980469 -0.28125]\n",
            "  [1.3125 0.820312 -1.60156 ... 0.0175781 -2.21875 -2.46875]\n",
            "  ...\n",
            "  [-4.3125 5.9375 -4.59375 ... -2.32812 0.882812 4.25]\n",
            "  [-0.410156 1.69531 -5.09375 ... 0.554688 2.5625 -2.90625]\n",
            "  [-7.25 1.53906 -3.96875 ... -3.76562 -2.53125 -5.09375]]]\n",
            "inputs=[[[-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.175781 -2.875 -1.13281 ... -1.125 -3.01562 -0.808594]\n",
            "  ...\n",
            "  [-0.765625 -0.964844 -1.26562 ... -0.0527344 -0.410156 5.4375]\n",
            "  [0.859375 -5.625 -5.03125 ... 3.5625 2.5 0.507812]\n",
            "  [-5 -2.875 -1.92188 ... 0.410156 -0.470703 -3.23438]]\n",
            "\n",
            " [[0.248047 3.53125 -3.125 ... -4.0625 -3.45312 2.0625]\n",
            "  [2.5625 2.32812 -1.52344 ... -3.95312 1.09375 3.95312]\n",
            "  [-0.5 1.10938 -4.4375 ... -1.03125 1.33594 -2.04688]\n",
            "  ...\n",
            "  [5.375 6.34375 -7.53125 ... -2.53125 2.875 -0.410156]\n",
            "  [-3.34375 4.59375 -3.6875 ... -4.96875 -0.894531 0.0976562]\n",
            "  [-6.0625 5.5 -1.24219 ... -4.59375 0.632812 -4.21875]]\n",
            "\n",
            " [[-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.175781 -2.875 -1.13281 ... -1.125 -3.01562 -0.808594]\n",
            "  ...\n",
            "  [3.375 -2.45312 -7.625 ... -0.0273438 -6.96875 0.621094]\n",
            "  [-1.96875 -2.15625 -3.70312 ... -2.5625 -2.64062 -1.3125]\n",
            "  [-4.71875 -2.75 -5.65625 ... -3.04688 -3.73438 -1.5]]\n",
            "\n",
            " [[-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [-0.175781 -2.875 -1.13281 ... -1.125 -3.01562 -0.808594]\n",
            "  ...\n",
            "  [1.48438 1.45312 -3 ... -0.203125 -3.5625 -2.89062]\n",
            "  [-0.296875 -0.132812 -6.09375 ... 1.22656 -1.95312 -1.05469]\n",
            "  [-5.3125 -0.792969 -5.09375 ... -3.20312 -2.84375 -5.75]]\n",
            "\n",
            " [[-0.0957031 -2.85938 -1.10156 ... -1.09375 -3.03125 -0.804688]\n",
            "  [2.73438 2.29688 -2.25 ... -1.78125 -0.980469 -0.28125]\n",
            "  [1.3125 0.820312 -1.60156 ... 0.0175781 -2.21875 -2.46875]\n",
            "  ...\n",
            "  [-4.3125 5.9375 -4.59375 ... -2.32812 0.882812 4.25]\n",
            "  [-0.410156 1.69531 -5.09375 ... 0.554688 2.5625 -2.90625]\n",
            "  [-7.25 1.53906 -3.96875 ... -3.76562 -2.53125 -5.09375]]]\n",
            "lnx=[[[-0.0240479 -0.71875 -0.277344 ... -0.275391 -0.761719 -0.202148]\n",
            "  [-0.0240479 -0.71875 -0.277344 ... -0.275391 -0.761719 -0.202148]\n",
            "  [-0.0441895 -0.722656 -0.285156 ... -0.283203 -0.757812 -0.203125]\n",
            "  ...\n",
            "  [-0.225586 -0.283203 -0.373047 ... -0.0155029 -0.120605 1.60156]\n",
            "  [0.263672 -1.72656 -1.53906 ... 1.09375 0.765625 0.155273]\n",
            "  [-1.39844 -0.804688 -0.539062 ... 0.114746 -0.131836 -0.90625]]\n",
            "\n",
            " [[0.0629883 0.894531 -0.792969 ... -1.03125 -0.875 0.523438]\n",
            "  [0.726562 0.660156 -0.433594 ... -1.125 0.310547 1.125]\n",
            "  [-0.144531 0.320312 -1.28125 ... -0.298828 0.386719 -0.59375]\n",
            "  ...\n",
            "  [1.82031 2.15625 -2.54688 ... -0.859375 0.976562 -0.138672]\n",
            "  [-1.01562 1.39062 -1.11719 ... -1.50781 -0.271484 0.029541]\n",
            "  [-1.9375 1.75781 -0.396484 ... -1.46875 0.202148 -1.34375]]\n",
            "\n",
            " [[-0.0240479 -0.71875 -0.277344 ... -0.275391 -0.761719 -0.202148]\n",
            "  [-0.0240479 -0.71875 -0.277344 ... -0.275391 -0.761719 -0.202148]\n",
            "  [-0.0441895 -0.722656 -0.285156 ... -0.283203 -0.757812 -0.203125]\n",
            "  ...\n",
            "  [1.04688 -0.757812 -2.35938 ... -0.00848389 -2.15625 0.192383]\n",
            "  [-0.613281 -0.671875 -1.15625 ... -0.800781 -0.824219 -0.410156]\n",
            "  [-1.29688 -0.757812 -1.55469 ... -0.839844 -1.02344 -0.412109]]\n",
            "\n",
            " [[-0.0240479 -0.71875 -0.277344 ... -0.275391 -0.761719 -0.202148]\n",
            "  [-0.0240479 -0.71875 -0.277344 ... -0.275391 -0.761719 -0.202148]\n",
            "  [-0.0441895 -0.722656 -0.285156 ... -0.283203 -0.757812 -0.203125]\n",
            "  ...\n",
            "  [0.478516 0.46875 -0.96875 ... -0.0654297 -1.14844 -0.933594]\n",
            "  [-0.0966797 -0.0432129 -1.98438 ... 0.400391 -0.636719 -0.34375]\n",
            "  [-1.5 -0.223633 -1.4375 ... -0.902344 -0.800781 -1.625]]\n",
            "\n",
            " [[-0.0240479 -0.71875 -0.277344 ... -0.275391 -0.761719 -0.202148]\n",
            "  [0.777344 0.65625 -0.640625 ... -0.507812 -0.279297 -0.0800781]\n",
            "  [0.376953 0.235352 -0.458984 ... 0.0050354 -0.636719 -0.707031]\n",
            "  ...\n",
            "  [-1.46875 2.03125 -1.57031 ... -0.792969 0.300781 1.45312]\n",
            "  [-0.141602 0.585938 -1.75781 ... 0.191406 0.886719 -1.00781]\n",
            "  [-2.23438 0.474609 -1.21875 ... -1.15625 -0.777344 -1.57031]]]\n",
            "attention_lnx=[[[-1.59375 0.847656 -1.61719 ... -0.734375 1.64062 1.32031]\n",
            "  [-1.59375 0.847656 -1.61719 ... -0.734375 1.64062 1.32031]\n",
            "  [-1.60156 0.847656 -1.60938 ... -0.738281 1.64062 1.32031]\n",
            "  ...\n",
            "  [-1.33594 0.984375 -1.74219 ... -0.710938 1.15625 1.40625]\n",
            "  [-1.28906 1.01562 -1.63281 ... -0.765625 1.30469 1.10156]\n",
            "  [-1.36719 1.02344 -1.35938 ... -0.539062 0.882812 1.48438]]\n",
            "\n",
            " [[-1.07812 0.388672 0.539062 ... 0.152344 -0.0712891 -0.431641]\n",
            "  [-0.22168 0.511719 0.104004 ... 0.535156 -0.116211 -0.691406]\n",
            "  [-0.451172 0.546875 0.589844 ... 0.164062 -0.186523 -0.392578]\n",
            "  ...\n",
            "  [-1.03125 -0.0571289 0.0240479 ... -0.106445 -0.180664 0.472656]\n",
            "  [-1.28906 -0.00622559 0.0927734 ... 0.283203 -0.314453 0.691406]\n",
            "  [-1.23438 0.125 -0.0140991 ... -0.0424805 -0.585938 0.78125]]\n",
            "\n",
            " [[-1.59375 0.847656 -1.61719 ... -0.734375 1.64062 1.32031]\n",
            "  [-1.59375 0.847656 -1.61719 ... -0.734375 1.64062 1.32031]\n",
            "  [-1.60156 0.847656 -1.60938 ... -0.738281 1.64062 1.32031]\n",
            "  ...\n",
            "  [-1.35156 0.578125 -1.28906 ... 0.480469 1.19531 1.35938]\n",
            "  [-1.625 0.628906 -1.03906 ... 0.0510254 1.14062 1.36719]\n",
            "  [-1.16406 0.722656 -1.21094 ... 0.166016 1.01562 1.29688]]\n",
            "\n",
            " [[-1.59375 0.847656 -1.61719 ... -0.734375 1.64062 1.32031]\n",
            "  [-1.59375 0.847656 -1.61719 ... -0.734375 1.64062 1.32031]\n",
            "  [-1.60156 0.847656 -1.60938 ... -0.738281 1.64062 1.32031]\n",
            "  ...\n",
            "  [-1.28125 0.757812 -1.19531 ... 0.277344 0.792969 1.55469]\n",
            "  [-1.21094 0.539062 -1.32812 ... -0.0175781 0.875 1.53906]\n",
            "  [-1.07031 0.761719 -1.09375 ... 0.0302734 0.355469 1.67969]]\n",
            "\n",
            " [[-1.59375 0.847656 -1.61719 ... -0.734375 1.64062 1.32031]\n",
            "  [-1.5 0.945312 -1.5 ... -0.28125 1.15625 0.792969]\n",
            "  [-1.25 0.730469 -0.808594 ... -0.0429688 1.95312 0.773438]\n",
            "  ...\n",
            "  [-1.46094 0.124512 -1.50781 ... 0.204102 1.11719 0.886719]\n",
            "  [-1.41406 0.15918 -1.04688 ... 0.3125 1.44531 0.847656]\n",
            "  [-1.21875 0.421875 -0.96875 ... 0.145508 0.753906 1.11719]]]\n",
            "attn_output=[[[-0.414062 -0.494141 -0.667969 ... -0.449219 -0.341797 0.126953]\n",
            "  [-0.414062 -0.494141 -0.667969 ... -0.449219 -0.341797 0.126953]\n",
            "  [-0.435547 -0.498047 -0.671875 ... -0.457031 -0.337891 0.125977]\n",
            "  ...\n",
            "  [-0.601562 0.00558472 -0.863281 ... -0.21875 0.213867 1.96094]\n",
            "  [-0.128906 -1.38281 -2 ... 0.839844 1.14062 0.482422]\n",
            "  [-1.73438 -0.503906 -0.894531 ... -0.0351562 0.112305 -0.476562]]\n",
            "\n",
            " [[-0.205078 0.96875 -0.640625 ... -0.964844 -0.871094 0.402344]\n",
            "  [0.648438 0.785156 -0.392578 ... -0.945312 0.269531 0.902344]\n",
            "  [-0.269531 0.466797 -1.08594 ... -0.245117 0.324219 -0.6875]\n",
            "  ...\n",
            "  [1.42969 2.07812 -2.46875 ... -0.871094 0.886719 0.0206299]\n",
            "  [-1.375 1.35938 -1.07031 ... -1.39062 -0.359375 0.234375]\n",
            "  [-2.26562 1.75 -0.390625 ... -1.44531 0.0145874 -1.07031]]\n",
            "\n",
            " [[-0.414062 -0.494141 -0.667969 ... -0.449219 -0.341797 0.126953]\n",
            "  [-0.414062 -0.494141 -0.667969 ... -0.449219 -0.341797 0.126953]\n",
            "  [-0.435547 -0.498047 -0.671875 ... -0.457031 -0.337891 0.125977]\n",
            "  ...\n",
            "  [0.609375 -0.5625 -2.67188 ... 0.135742 -1.73438 0.59375]\n",
            "  [-1.08594 -0.460938 -1.42969 ... -0.757812 -0.453125 0.0164795]\n",
            "  [-1.57031 -0.539062 -1.82812 ... -0.769531 -0.726562 -0.0541992]]\n",
            "\n",
            " [[-0.414062 -0.494141 -0.667969 ... -0.449219 -0.341797 0.126953]\n",
            "  [-0.414062 -0.494141 -0.667969 ... -0.449219 -0.341797 0.126953]\n",
            "  [-0.435547 -0.498047 -0.671875 ... -0.457031 -0.337891 0.125977]\n",
            "  ...\n",
            "  [0.0634766 0.691406 -1.3125 ... 0.0231934 -0.867188 -0.417969]\n",
            "  [-0.476562 0.128906 -2.34375 ... 0.382812 -0.341797 0.15332]\n",
            "  [-1.75 -0.00854492 -1.69531 ... -0.871094 -0.683594 -1.11719]]\n",
            "\n",
            " [[-0.414062 -0.494141 -0.667969 ... -0.449219 -0.341797 0.126953]\n",
            "  [0.339844 0.890625 -1.03125 ... -0.566406 0.0483398 0.140625]\n",
            "  [0.0174561 0.431641 -0.671875 ... -0.00708008 -0.0737305 -0.472656]\n",
            "  ...\n",
            "  [-1.91406 2.01562 -2.01562 ... -0.703125 0.664062 1.70312]\n",
            "  [-0.613281 0.621094 -2.0625 ... 0.291016 1.34375 -0.691406]\n",
            "  [-2.53125 0.585938 -1.47656 ... -1.07812 -0.53125 -1.1875]]]\n",
            "next_layer_addition_dropped_out=[[[-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.6875 -3.0625 -3.65625 ... -0.757812 -1.64062 -0.535156]\n",
            "  ...\n",
            "  [-2.85938 -0.427734 -3.9375 ... 0.679688 1.35938 6.90625]\n",
            "  [-0.746094 -5.09375 -6 ... 2.79688 3.40625 1.28125]\n",
            "  [-7.21875 -2.9375 -2.76562 ... -0.5 0.373047 -3.96875]]\n",
            "\n",
            " [[-0.597656 3.53125 -2.14062 ... -3.625 -4.34375 0.960938]\n",
            "  [2.59375 2.35938 -2.3125 ... -3.04688 0.910156 3.39062]\n",
            "  [-1.33594 1.70312 -3.28125 ... -1.30469 0.976562 -3.26562]\n",
            "  ...\n",
            "  [5 6.9375 -7.1875 ... -2.71875 2.32812 0.574219]\n",
            "  [-3.96875 3.3125 -4.3125 ... -4.875 -2.0625 -0.0429688]\n",
            "  [-7.84375 4.90625 -0.597656 ... -5.1875 0.205078 -4.40625]]\n",
            "\n",
            " [[-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.6875 -3.0625 -3.65625 ... -0.757812 -1.64062 -0.535156]\n",
            "  ...\n",
            "  [1.94531 -2.78125 -8.9375 ... 1.03125 -6.875 0.992188]\n",
            "  [-5.40625 -3.0625 -4.03125 ... -2.5625 -2 -0.960938]\n",
            "  [-6.09375 -3.57812 -6.1875 ... -4.5 -2.75 -1.46094]]\n",
            "\n",
            " [[-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.6875 -3.0625 -3.65625 ... -0.757812 -1.64062 -0.535156]\n",
            "  ...\n",
            "  [-0.992188 2.3125 -4.3125 ... 0.847656 -2.95312 -0.902344]\n",
            "  [-1.21094 0.675781 -7.15625 ... 0.914062 -0.847656 -0.460938]\n",
            "  [-6.09375 -1.59375 -5.5625 ... -4.0625 -2.54688 -5.6875]]\n",
            "\n",
            " [[-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [0.652344 2.60938 -3.29688 ... -1.09375 -0.679688 -0.113281]\n",
            "  [0.828125 0.1875 -3.09375 ... 0.609375 -1.65625 -2.73438]\n",
            "  ...\n",
            "  [-5.125 5.78125 -6.0625 ... -1.05469 1.46094 3.9375]\n",
            "  [-1.21875 2.14062 -6.625 ... 2.17188 3.4375 -2.59375]\n",
            "  [-7.0625 0.257812 -4.875 ... -4.40625 -1.75 -5.09375]]]\n",
            "inputs=[[[-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.6875 -3.0625 -3.65625 ... -0.757812 -1.64062 -0.535156]\n",
            "  ...\n",
            "  [-2.85938 -0.427734 -3.9375 ... 0.679688 1.35938 6.90625]\n",
            "  [-0.746094 -5.09375 -6 ... 2.79688 3.40625 1.28125]\n",
            "  [-7.21875 -2.9375 -2.76562 ... -0.5 0.373047 -3.96875]]\n",
            "\n",
            " [[-0.597656 3.53125 -2.14062 ... -3.625 -4.34375 0.960938]\n",
            "  [2.59375 2.35938 -2.3125 ... -3.04688 0.910156 3.39062]\n",
            "  [-1.33594 1.70312 -3.28125 ... -1.30469 0.976562 -3.26562]\n",
            "  ...\n",
            "  [5 6.9375 -7.1875 ... -2.71875 2.32812 0.574219]\n",
            "  [-3.96875 3.3125 -4.3125 ... -4.875 -2.0625 -0.0429688]\n",
            "  [-7.84375 4.90625 -0.597656 ... -5.1875 0.205078 -4.40625]]\n",
            "\n",
            " [[-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.6875 -3.0625 -3.65625 ... -0.757812 -1.64062 -0.535156]\n",
            "  ...\n",
            "  [1.94531 -2.78125 -8.9375 ... 1.03125 -6.875 0.992188]\n",
            "  [-5.40625 -3.0625 -4.03125 ... -2.5625 -2 -0.960938]\n",
            "  [-6.09375 -3.57812 -6.1875 ... -4.5 -2.75 -1.46094]]\n",
            "\n",
            " [[-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [-2.6875 -3.0625 -3.65625 ... -0.757812 -1.64062 -0.535156]\n",
            "  ...\n",
            "  [-0.992188 2.3125 -4.3125 ... 0.847656 -2.95312 -0.902344]\n",
            "  [-1.21094 0.675781 -7.15625 ... 0.914062 -0.847656 -0.460938]\n",
            "  [-6.09375 -1.59375 -5.5625 ... -4.0625 -2.54688 -5.6875]]\n",
            "\n",
            " [[-2.59375 -3.07812 -3.625 ... -0.734375 -1.64844 -0.539062]\n",
            "  [0.652344 2.60938 -3.29688 ... -1.09375 -0.679688 -0.113281]\n",
            "  [0.828125 0.1875 -3.09375 ... 0.609375 -1.65625 -2.73438]\n",
            "  ...\n",
            "  [-5.125 5.78125 -6.0625 ... -1.05469 1.46094 3.9375]\n",
            "  [-1.21875 2.14062 -6.625 ... 2.17188 3.4375 -2.59375]\n",
            "  [-7.0625 0.257812 -4.875 ... -4.40625 -1.75 -5.09375]]]\n",
            "lnx=[[[-0.621094 -0.738281 -0.867188 ... -0.175781 -0.394531 -0.128906]\n",
            "  [-0.621094 -0.738281 -0.867188 ... -0.175781 -0.394531 -0.128906]\n",
            "  [-0.644531 -0.734375 -0.875 ... -0.181641 -0.392578 -0.12793]\n",
            "  ...\n",
            "  [-0.804688 -0.120117 -1.10938 ... 0.191406 0.382812 1.94531]\n",
            "  [-0.219727 -1.5 -1.77344 ... 0.824219 1.00781 0.378906]\n",
            "  [-1.89062 -0.769531 -0.722656 ... -0.130859 0.0976562 -1.03906]]\n",
            "\n",
            " [[-0.145508 0.859375 -0.519531 ... -0.882812 -1.05469 0.233398]\n",
            "  [0.707031 0.640625 -0.628906 ... -0.828125 0.248047 0.921875]\n",
            "  [-0.371094 0.474609 -0.914062 ... -0.363281 0.271484 -0.910156]\n",
            "  ...\n",
            "  [1.60938 2.23438 -2.3125 ... -0.875 0.75 0.185547]\n",
            "  [-1.10938 0.925781 -1.20312 ... -1.36719 -0.578125 -0.0120239]\n",
            "  [-2.35938 1.47656 -0.179688 ... -1.5625 0.0617676 -1.32812]]\n",
            "\n",
            " [[-0.621094 -0.738281 -0.867188 ... -0.175781 -0.394531 -0.128906]\n",
            "  [-0.621094 -0.738281 -0.867188 ... -0.175781 -0.394531 -0.128906]\n",
            "  [-0.644531 -0.734375 -0.875 ... -0.181641 -0.392578 -0.12793]\n",
            "  ...\n",
            "  [0.566406 -0.808594 -2.59375 ... 0.300781 -2 0.289062]\n",
            "  [-1.57031 -0.890625 -1.17188 ... -0.746094 -0.582031 -0.279297]\n",
            "  [-1.52344 -0.894531 -1.54688 ... -1.125 -0.6875 -0.365234]]\n",
            "\n",
            " [[-0.621094 -0.738281 -0.867188 ... -0.175781 -0.394531 -0.128906]\n",
            "  [-0.621094 -0.738281 -0.867188 ... -0.175781 -0.394531 -0.128906]\n",
            "  [-0.644531 -0.734375 -0.875 ... -0.181641 -0.392578 -0.12793]\n",
            "  ...\n",
            "  [-0.304688 0.710938 -1.32812 ... 0.259766 -0.90625 -0.277344]\n",
            "  [-0.376953 0.209961 -2.21875 ... 0.283203 -0.263672 -0.143555]\n",
            "  [-1.57031 -0.410156 -1.42969 ... -1.04688 -0.65625 -1.46094]]\n",
            "\n",
            " [[-0.621094 -0.738281 -0.867188 ... -0.175781 -0.394531 -0.128906]\n",
            "  [0.176758 0.707031 -0.894531 ... -0.296875 -0.18457 -0.0306396]\n",
            "  [0.226562 0.0512695 -0.84375 ... 0.166992 -0.453125 -0.746094]\n",
            "  ...\n",
            "  [-1.66406 1.875 -1.96875 ... -0.341797 0.474609 1.28125]\n",
            "  [-0.400391 0.703125 -2.1875 ... 0.714844 1.13281 -0.855469]\n",
            "  [-1.98438 0.0722656 -1.36719 ... -1.23438 -0.492188 -1.42969]]]\n",
            "attention_lnx=[[[-0.204102 -0.202148 -0.878906 ... -0.53125 -0.285156 -1.23438]\n",
            "  [-0.204102 -0.202148 -0.878906 ... -0.53125 -0.285156 -1.23438]\n",
            "  [-0.199219 -0.198242 -0.882812 ... -0.527344 -0.291016 -1.23438]\n",
            "  ...\n",
            "  [-0.259766 -0.773438 -0.730469 ... -0.322266 0.220703 -0.839844]\n",
            "  [-0.277344 -0.667969 -0.769531 ... -0.511719 0.157227 -0.796875]\n",
            "  [-0.241211 -0.699219 -0.753906 ... -0.816406 0.143555 -0.675781]]\n",
            "\n",
            " [[-0.421875 -1.47656 0.0078125 ... 0.921875 0.714844 0.847656]\n",
            "  [-0.675781 -0.953125 0.832031 ... 1.19531 0.542969 0.15918]\n",
            "  [-0.400391 -1.05469 0.330078 ... 1.07031 0.78125 0.0864258]\n",
            "  ...\n",
            "  [0.109863 -0.00265503 -1.35156 ... 0.917969 0.443359 -0.419922]\n",
            "  [-0.0178223 -0.386719 -1.10938 ... 0.75 0.992188 -0.24707]\n",
            "  [-0.148438 0.0292969 -1.03906 ... 0.851562 0.722656 -0.144531]]\n",
            "\n",
            " [[-0.204102 -0.202148 -0.878906 ... -0.53125 -0.285156 -1.23438]\n",
            "  [-0.204102 -0.202148 -0.878906 ... -0.53125 -0.285156 -1.23438]\n",
            "  [-0.199219 -0.198242 -0.882812 ... -0.527344 -0.291016 -1.23438]\n",
            "  ...\n",
            "  [0.102539 -0.949219 -0.761719 ... -0.363281 -0.257812 -0.271484]\n",
            "  [0.0412598 -0.867188 -0.824219 ... -0.613281 -0.300781 -0.248047]\n",
            "  [0.0927734 -0.664062 -1.07031 ... -0.820312 -0.168945 -0.425781]]\n",
            "\n",
            " [[-0.204102 -0.202148 -0.878906 ... -0.53125 -0.285156 -1.23438]\n",
            "  [-0.204102 -0.202148 -0.878906 ... -0.53125 -0.285156 -1.23438]\n",
            "  [-0.199219 -0.198242 -0.882812 ... -0.527344 -0.291016 -1.23438]\n",
            "  ...\n",
            "  [0.0170898 -0.847656 -0.902344 ... -0.0294189 -0.138672 -0.353516]\n",
            "  [0.041748 -0.472656 -0.441406 ... -0.605469 -0.0116577 -0.695312]\n",
            "  [-0.0888672 -0.185547 -0.84375 ... -0.984375 -0.0471191 -0.345703]]\n",
            "\n",
            " [[-0.204102 -0.202148 -0.878906 ... -0.53125 -0.285156 -1.23438]\n",
            "  [-0.128906 -1.25781 -0.992188 ... 0.515625 0.558594 -0.0201416]\n",
            "  [0.300781 -1.03906 -0.652344 ... 0.792969 0.660156 -0.0167236]\n",
            "  ...\n",
            "  [0.376953 -1.25 -1.53125 ... 0.423828 0.114746 -0.000289917]\n",
            "  [0.238281 -1.28125 -1.46875 ... 0.209961 -0.118164 -0.210938]\n",
            "  [-0.347656 -1.25781 -1.58594 ... -0.0869141 -0.0378418 -0.292969]]]\n",
            "attn_output=[[[-0.652344 -0.761719 -1.04688 ... -0.294922 -0.449219 -0.412109]\n",
            "  [-0.652344 -0.761719 -1.04688 ... -0.294922 -0.449219 -0.412109]\n",
            "  [-0.671875 -0.757812 -1.05469 ... -0.298828 -0.449219 -0.412109]\n",
            "  ...\n",
            "  [-0.847656 -0.326172 -1.26562 ... 0.097168 0.429688 1.64844]\n",
            "  [-0.291016 -1.64062 -1.92188 ... 0.648438 1.01562 0.137695]\n",
            "  [-1.88281 -0.917969 -0.886719 ... -0.332031 0.129883 -1.17188]]\n",
            "\n",
            " [[-0.240234 0.484375 -0.503906 ... -0.636719 -0.855469 0.425781]\n",
            "  [0.503906 0.369141 -0.388672 ... -0.486328 0.382812 0.933594]\n",
            "  [-0.46875 0.174805 -0.796875 ... -0.0634766 0.474609 -0.859375]\n",
            "  ...\n",
            "  [1.60156 2.17188 -2.67188 ... -0.5625 0.867188 0.0483398]\n",
            "  [-1.09375 0.800781 -1.48438 ... -1.13281 -0.292969 -0.0795898]\n",
            "  [-2.34375 1.45312 -0.480469 ... -1.27344 0.271484 -1.33594]]\n",
            "\n",
            " [[-0.652344 -0.761719 -1.04688 ... -0.294922 -0.449219 -0.412109]\n",
            "  [-0.652344 -0.761719 -1.04688 ... -0.294922 -0.449219 -0.412109]\n",
            "  [-0.671875 -0.757812 -1.05469 ... -0.298828 -0.449219 -0.412109]\n",
            "  ...\n",
            "  [0.582031 -1.05469 -2.75 ... 0.189453 -2.01562 0.204102]\n",
            "  [-1.51562 -1.10938 -1.36719 ... -0.894531 -0.648438 -0.341797]\n",
            "  [-1.45312 -1.02344 -1.75781 ... -1.28906 -0.707031 -0.457031]]\n",
            "\n",
            " [[-0.652344 -0.761719 -1.04688 ... -0.294922 -0.449219 -0.412109]\n",
            "  [-0.652344 -0.761719 -1.04688 ... -0.294922 -0.449219 -0.412109]\n",
            "  [-0.671875 -0.757812 -1.05469 ... -0.298828 -0.449219 -0.412109]\n",
            "  ...\n",
            "  [-0.291016 0.4375 -1.55469 ... 0.244141 -0.921875 -0.375]\n",
            "  [-0.351562 0.0610352 -2.28125 ... 0.0927734 -0.257812 -0.347656]\n",
            "  [-1.53906 -0.443359 -1.60156 ... -1.25781 -0.648438 -1.50781]]\n",
            "\n",
            " [[-0.652344 -0.761719 -1.04688 ... -0.294922 -0.449219 -0.412109]\n",
            "  [0.138672 0.357422 -1.13281 ... -0.152344 -0.0319824 -0.0351562]\n",
            "  [0.302734 -0.227539 -1 ... 0.375 -0.265625 -0.734375]\n",
            "  ...\n",
            "  [-1.49219 1.42969 -2.39062 ... -0.199219 0.496094 1.24219]\n",
            "  [-0.314453 0.275391 -2.59375 ... 0.765625 1.0625 -0.902344]\n",
            "  [-2.03125 -0.273438 -1.76562 ... -1.22656 -0.488281 -1.47656]]]\n",
            "next_layer_addition_dropped_out=[[[-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.92188 -3.10938 -4.59375 ... -1.875 -0.570312 -3.5625]\n",
            "  ...\n",
            "  [-2.64062 -1.01562 -4.59375 ... 0.0546875 2.3125 5.59375]\n",
            "  [-1.07812 -5.25 -6.3125 ... 2.04688 4.09375 0.828125]\n",
            "  [-7.59375 -4.03125 -3.25 ... -3.45312 0.291016 -6.75]]\n",
            "\n",
            " [[-2.0625 1.85156 -1.97656 ... -3.35938 -3.75 2.29688]\n",
            "  [2.39062 1.25781 -1.32031 ... -1.94531 1.75 3.75]\n",
            "  [-2.51562 0.71875 -2.65625 ... 0.0107422 1.625 -3.65625]\n",
            "  ...\n",
            "  [5.9375 7.4375 -9.125 ... -2.20312 1.83594 0.804688]\n",
            "  [-5.125 3.57812 -5.84375 ... -3.23438 -2.21875 -0.589844]\n",
            "  [-7 5.03125 -2.96875 ... -4.53125 0.523438 -3.45312]]\n",
            "\n",
            " [[-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.92188 -3.10938 -4.59375 ... -1.875 -0.570312 -3.5625]\n",
            "  ...\n",
            "  [2.25 -5.125 -10.625 ... 0.296875 -6.59375 -0.265625]\n",
            "  [-5.25 -4.28125 -3.875 ... -4.375 -2.29688 -2.51562]\n",
            "  [-6.53125 -5.03125 -7.84375 ... -6.75 -2.78125 -3.25]]\n",
            "\n",
            " [[-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.92188 -3.10938 -4.59375 ... -1.875 -0.570312 -3.5625]\n",
            "  ...\n",
            "  [-1.28125 1.42188 -3.84375 ... 0.144531 -2.15625 -1.84375]\n",
            "  [-2.25 0.535156 -6.6875 ... 0.4375 -0.203125 -2.09375]\n",
            "  [-6.09375 -2.8125 -7.1875 ... -6.3125 -2.21875 -7.0625]]\n",
            "\n",
            " [[-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [1.26562 1.70312 -4.0625 ... -1.32812 0.644531 -0.402344]\n",
            "  [1.42188 -0.3125 -3.54688 ... 0.640625 -0.367188 -4.59375]\n",
            "  ...\n",
            "  [-4.375 3.89062 -7.875 ... -0.585938 2.15625 3.65625]\n",
            "  [-0.265625 0.140625 -8.25 ... 2.875 3.42188 -3.34375]\n",
            "  [-6.5625 -1.48438 -6.6875 ... -5.15625 -1.72656 -6.4375]]]\n",
            "inputs=[[[-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.92188 -3.10938 -4.59375 ... -1.875 -0.570312 -3.5625]\n",
            "  ...\n",
            "  [-2.64062 -1.01562 -4.59375 ... 0.0546875 2.3125 5.59375]\n",
            "  [-1.07812 -5.25 -6.3125 ... 2.04688 4.09375 0.828125]\n",
            "  [-7.59375 -4.03125 -3.25 ... -3.45312 0.291016 -6.75]]\n",
            "\n",
            " [[-2.0625 1.85156 -1.97656 ... -3.35938 -3.75 2.29688]\n",
            "  [2.39062 1.25781 -1.32031 ... -1.94531 1.75 3.75]\n",
            "  [-2.51562 0.71875 -2.65625 ... 0.0107422 1.625 -3.65625]\n",
            "  ...\n",
            "  [5.9375 7.4375 -9.125 ... -2.20312 1.83594 0.804688]\n",
            "  [-5.125 3.57812 -5.84375 ... -3.23438 -2.21875 -0.589844]\n",
            "  [-7 5.03125 -2.96875 ... -4.53125 0.523438 -3.45312]]\n",
            "\n",
            " [[-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.92188 -3.10938 -4.59375 ... -1.875 -0.570312 -3.5625]\n",
            "  ...\n",
            "  [2.25 -5.125 -10.625 ... 0.296875 -6.59375 -0.265625]\n",
            "  [-5.25 -4.28125 -3.875 ... -4.375 -2.29688 -2.51562]\n",
            "  [-6.53125 -5.03125 -7.84375 ... -6.75 -2.78125 -3.25]]\n",
            "\n",
            " [[-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [-1.92188 -3.10938 -4.59375 ... -1.875 -0.570312 -3.5625]\n",
            "  ...\n",
            "  [-1.28125 1.42188 -3.84375 ... 0.144531 -2.15625 -1.84375]\n",
            "  [-2.25 0.535156 -6.6875 ... 0.4375 -0.203125 -2.09375]\n",
            "  [-6.09375 -2.8125 -7.1875 ... -6.3125 -2.21875 -7.0625]]\n",
            "\n",
            " [[-1.8125 -3.125 -4.59375 ... -1.86719 -0.570312 -3.5625]\n",
            "  [1.26562 1.70312 -4.0625 ... -1.32812 0.644531 -0.402344]\n",
            "  [1.42188 -0.3125 -3.54688 ... 0.640625 -0.367188 -4.59375]\n",
            "  ...\n",
            "  [-4.375 3.89062 -7.875 ... -0.585938 2.15625 3.65625]\n",
            "  [-0.265625 0.140625 -8.25 ... 2.875 3.42188 -3.34375]\n",
            "  [-6.5625 -1.48438 -6.6875 ... -5.15625 -1.72656 -6.4375]]]\n",
            "lnx=[[[-0.412109 -0.710938 -1.04688 ... -0.423828 -0.129883 -0.808594]\n",
            "  [-0.412109 -0.710938 -1.04688 ... -0.423828 -0.129883 -0.808594]\n",
            "  [-0.4375 -0.707031 -1.04688 ... -0.425781 -0.129883 -0.808594]\n",
            "  ...\n",
            "  [-0.707031 -0.271484 -1.22656 ... 0.0146484 0.617188 1.5]\n",
            "  [-0.302734 -1.46875 -1.76562 ... 0.574219 1.14844 0.231445]\n",
            "  [-1.8125 -0.960938 -0.773438 ... -0.824219 0.0693359 -1.60938]]\n",
            "\n",
            " [[-0.476562 0.427734 -0.457031 ... -0.777344 -0.867188 0.53125]\n",
            "  [0.621094 0.326172 -0.341797 ... -0.503906 0.453125 0.972656]\n",
            "  [-0.667969 0.19043 -0.703125 ... 0.00285339 0.429688 -0.96875]\n",
            "  ...\n",
            "  [1.83594 2.29688 -2.8125 ... -0.679688 0.566406 0.248047]\n",
            "  [-1.32031 0.921875 -1.50781 ... -0.835938 -0.570312 -0.152344]\n",
            "  [-1.96094 1.40625 -0.832031 ... -1.27344 0.146484 -0.96875]]\n",
            "\n",
            " [[-0.412109 -0.710938 -1.04688 ... -0.423828 -0.129883 -0.808594]\n",
            "  [-0.412109 -0.710938 -1.04688 ... -0.423828 -0.129883 -0.808594]\n",
            "  [-0.4375 -0.707031 -1.04688 ... -0.425781 -0.129883 -0.808594]\n",
            "  ...\n",
            "  [0.613281 -1.39844 -2.89062 ... 0.0810547 -1.79688 -0.0722656]\n",
            "  [-1.41406 -1.14844 -1.03906 ... -1.17969 -0.617188 -0.675781]\n",
            "  [-1.46875 -1.13281 -1.76562 ... -1.51562 -0.625 -0.730469]]\n",
            "\n",
            " [[-0.412109 -0.710938 -1.04688 ... -0.423828 -0.129883 -0.808594]\n",
            "  [-0.412109 -0.710938 -1.04688 ... -0.423828 -0.129883 -0.808594]\n",
            "  [-0.4375 -0.707031 -1.04688 ... -0.425781 -0.129883 -0.808594]\n",
            "  ...\n",
            "  [-0.371094 0.412109 -1.11719 ... 0.0419922 -0.625 -0.535156]\n",
            "  [-0.65625 0.15625 -1.95312 ... 0.12793 -0.0593262 -0.609375]\n",
            "  [-1.41406 -0.652344 -1.66406 ... -1.46094 -0.515625 -1.64062]]\n",
            "\n",
            " [[-0.412109 -0.710938 -1.04688 ... -0.423828 -0.129883 -0.808594]\n",
            "  [0.328125 0.441406 -1.05469 ... -0.34375 0.166992 -0.104004]\n",
            "  [0.373047 -0.0820312 -0.933594 ... 0.167969 -0.0966797 -1.20312]\n",
            "  ...\n",
            "  [-1.35156 1.20312 -2.4375 ... -0.180664 0.664062 1.13281]\n",
            "  [-0.0834961 0.0441895 -2.59375 ... 0.902344 1.07031 -1.04688]\n",
            "  [-1.67969 -0.380859 -1.71094 ... -1.32031 -0.441406 -1.64844]]]\n",
            "attention_lnx=[[[-0.197266 -0.296875 0.330078 ... -0.0131836 0.605469 -1.46094]\n",
            "  [-0.197266 -0.296875 0.330078 ... -0.0131836 0.605469 -1.46094]\n",
            "  [-0.202148 -0.294922 0.333984 ... -0.0186768 0.597656 -1.46094]\n",
            "  ...\n",
            "  [-0.492188 0.0371094 0.585938 ... 0.306641 0.816406 -1.34375]\n",
            "  [-0.5 -0.0957031 0.539062 ... 0.145508 0.738281 -1.25781]\n",
            "  [-0.488281 -0.0480957 0.347656 ... 0.337891 0.585938 -1.15625]]\n",
            "\n",
            " [[0.355469 1.41406 1.21875 ... 0.566406 0.0622559 1.5625]\n",
            "  [0.363281 0.910156 0.636719 ... 0.507812 -0.11084 1.4375]\n",
            "  [0.484375 1.36719 0.898438 ... 0.410156 -0.396484 1.49219]\n",
            "  ...\n",
            "  [0.238281 0.523438 1.03906 ... 0.972656 -0.412109 0.851562]\n",
            "  [-0.259766 1.10156 0.621094 ... 1.28125 -0.237305 0.306641]\n",
            "  [-0.285156 0.761719 0.65625 ... 1.44531 -0.233398 0.675781]]\n",
            "\n",
            " [[-0.197266 -0.296875 0.330078 ... -0.0131836 0.605469 -1.46094]\n",
            "  [-0.197266 -0.296875 0.330078 ... -0.0131836 0.605469 -1.46094]\n",
            "  [-0.202148 -0.294922 0.333984 ... -0.0186768 0.597656 -1.46094]\n",
            "  ...\n",
            "  [-0.3125 0.0366211 0.101074 ... 0.980469 0.554688 -0.498047]\n",
            "  [-0.523438 0.026001 -0.125 ... 1.0625 0.625 -0.546875]\n",
            "  [-0.181641 -0.0393066 -0.474609 ... 1.35938 0.388672 -1.29688]]\n",
            "\n",
            " [[-0.197266 -0.296875 0.330078 ... -0.0131836 0.605469 -1.46094]\n",
            "  [-0.197266 -0.296875 0.330078 ... -0.0131836 0.605469 -1.46094]\n",
            "  [-0.202148 -0.294922 0.333984 ... -0.0186768 0.597656 -1.46094]\n",
            "  ...\n",
            "  [-0.239258 0.351562 -0.0795898 ... 0.660156 0.722656 -0.628906]\n",
            "  [-0.546875 0.249023 0.0917969 ... 0.511719 0.980469 -0.498047]\n",
            "  [-0.326172 0.200195 -0.324219 ... 0.882812 0.585938 -1.01562]]\n",
            "\n",
            " [[-0.197266 -0.296875 0.330078 ... -0.0131836 0.605469 -1.46094]\n",
            "  [0.144531 0.8125 1.36719 ... 0.181641 0.695312 -0.558594]\n",
            "  [-0.158203 0.455078 0.628906 ... -0.00588989 0.287109 -0.535156]\n",
            "  ...\n",
            "  [-0.217773 -0.0148315 -0.178711 ... 0.949219 0.0375977 0.0742188]\n",
            "  [0.236328 0.235352 -0.105957 ... 0.90625 0.0966797 0.285156]\n",
            "  [-0.494141 -0.167969 -0.279297 ... 1.19531 0.0517578 -0.605469]]]\n",
            "attn_output=[[[-0.441406 -0.75 -0.933594 ... -0.412109 0.00772095 -1.10156]\n",
            "  [-0.441406 -0.75 -0.933594 ... -0.412109 0.00772095 -1.10156]\n",
            "  [-0.464844 -0.746094 -0.933594 ... -0.416016 0.00598145 -1.10156]\n",
            "  ...\n",
            "  [-0.8125 -0.253906 -1.03906 ... 0.09375 0.8125 1.10156]\n",
            "  [-0.427734 -1.44531 -1.5625 ... 0.59375 1.30469 -0.116211]\n",
            "  [-1.85938 -0.9375 -0.667969 ... -0.714844 0.202148 -1.82031]]\n",
            "\n",
            " [[-0.384766 0.734375 -0.170898 ... -0.628906 -0.832031 0.871094]\n",
            "  [0.703125 0.550781 -0.173828 ... -0.367188 0.417969 1.32031]\n",
            "  [-0.527344 0.542969 -0.457031 ... 0.109863 0.320312 -0.5625]\n",
            "  ...\n",
            "  [1.86719 2.40625 -2.45312 ... -0.373047 0.431641 0.5]\n",
            "  [-1.35156 1.17188 -1.3125 ... -0.490234 -0.617188 -0.0712891]\n",
            "  [-1.99219 1.58594 -0.632812 ... -0.84375 0.0791016 -0.757812]]\n",
            "\n",
            " [[-0.441406 -0.75 -0.933594 ... -0.412109 0.00772095 -1.10156]\n",
            "  [-0.441406 -0.75 -0.933594 ... -0.412109 0.00772095 -1.10156]\n",
            "  [-0.464844 -0.746094 -0.933594 ... -0.416016 0.00598145 -1.10156]\n",
            "  ...\n",
            "  [0.511719 -1.34375 -2.76562 ... 0.335938 -1.59375 -0.201172]\n",
            "  [-1.50781 -1.10938 -1.04688 ... -0.863281 -0.4375 -0.800781]\n",
            "  [-1.45312 -1.10156 -1.80469 ... -1.17188 -0.519531 -0.984375]]\n",
            "\n",
            " [[-0.441406 -0.75 -0.933594 ... -0.412109 0.00772095 -1.10156]\n",
            "  [-0.441406 -0.75 -0.933594 ... -0.412109 0.00772095 -1.10156]\n",
            "  [-0.464844 -0.746094 -0.933594 ... -0.416016 0.00598145 -1.10156]\n",
            "  ...\n",
            "  [-0.427734 0.498047 -1.10156 ... 0.225586 -0.402344 -0.695312]\n",
            "  [-0.78125 0.219727 -1.84375 ... 0.265625 0.217773 -0.726562]\n",
            "  [-1.4375 -0.585938 -1.67969 ... -1.21094 -0.365234 -1.80469]]\n",
            "\n",
            " [[-0.441406 -0.75 -0.933594 ... -0.412109 0.00772095 -1.10156]\n",
            "  [0.353516 0.628906 -0.675781 ... -0.287109 0.335938 -0.240234]\n",
            "  [0.322266 0.036377 -0.746094 ... 0.162109 -0.0203857 -1.3125]\n",
            "  ...\n",
            "  [-1.375 1.16406 -2.42188 ... 0.108887 0.65625 1.11719]\n",
            "  [-0.00891113 0.114746 -2.54688 ... 1.15625 1.07031 -0.933594]\n",
            "  [-1.75 -0.410156 -1.72656 ... -0.984375 -0.416016 -1.75]]]\n",
            "next_layer_addition_dropped_out=[[[-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.57812 -1.69531 -4.46875 ... -3.23438 0.632812 -4.25]\n",
            "  ...\n",
            "  [-2.67188 -0.429688 -4.3125 ... -0.279297 3.40625 3.9375]\n",
            "  [-0.59375 -4.40625 -5.375 ... 2.40625 5.125 0.679688]\n",
            "  [-7.4375 -4.59375 -2.90625 ... -4.125 0.5 -7.28125]]\n",
            "\n",
            " [[-1.15625 3.42188 -1.32812 ... -3.5625 -4.15625 3.57812]\n",
            "  [3.23438 3.125 0.25 ... -1.50781 0.941406 4.53125]\n",
            "  [-0.820312 2.17188 -1.42969 ... 0.333984 0.835938 -1.82812]\n",
            "  ...\n",
            "  [6.34375 9.3125 -8.875 ... -1.5 3.125 2.78125]\n",
            "  [-6.375 4.625 -6.53125 ... -4.875 -2.32812 -1.10156]\n",
            "  [-8.5 5.59375 -2.48438 ... -4.09375 0.476562 -4.0625]]\n",
            "\n",
            " [[-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.57812 -1.69531 -4.46875 ... -3.23438 0.632812 -4.25]\n",
            "  ...\n",
            "  [1.80469 -3.78125 -10.25 ... 0.8125 -5.6875 -0.964844]\n",
            "  [-5.8125 -3.34375 -4.34375 ... -3.0625 -2.5 -3.15625]\n",
            "  [-7 -6 -7.3125 ... -6.5 -4 -4.53125]]\n",
            "\n",
            " [[-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.57812 -1.69531 -4.46875 ... -3.23438 0.632812 -4.25]\n",
            "  ...\n",
            "  [-1.82031 2.40625 -3.92188 ... -0.00390625 -1.89062 -1.39062]\n",
            "  [-2.92188 1.27344 -7.125 ... 0.261719 0.482422 -3.28125]\n",
            "  [-5.59375 -4.03125 -6.65625 ... -6.40625 -2.90625 -7.1875]]\n",
            "\n",
            " [[-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [1.60938 2.51562 -2.89062 ... -0.691406 2.10938 -0.371094]\n",
            "  [0.53125 1.17969 -2.78125 ... 0.167969 0.507812 -5]\n",
            "  ...\n",
            "  [-4.09375 3.95312 -8.125 ... 0.248047 1.39062 3.125]\n",
            "  [0.294922 -0.265625 -9.625 ... 3.65625 4.09375 -1.46875]\n",
            "  [-6.4375 -2.10938 -6.90625 ... -5 -2.39062 -6.90625]]]\n",
            "inputs=[[[-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.57812 -1.69531 -4.46875 ... -3.23438 0.632812 -4.25]\n",
            "  ...\n",
            "  [-2.67188 -0.429688 -4.3125 ... -0.279297 3.40625 3.9375]\n",
            "  [-0.59375 -4.40625 -5.375 ... 2.40625 5.125 0.679688]\n",
            "  [-7.4375 -4.59375 -2.90625 ... -4.125 0.5 -7.28125]]\n",
            "\n",
            " [[-1.15625 3.42188 -1.32812 ... -3.5625 -4.15625 3.57812]\n",
            "  [3.23438 3.125 0.25 ... -1.50781 0.941406 4.53125]\n",
            "  [-0.820312 2.17188 -1.42969 ... 0.333984 0.835938 -1.82812]\n",
            "  ...\n",
            "  [6.34375 9.3125 -8.875 ... -1.5 3.125 2.78125]\n",
            "  [-6.375 4.625 -6.53125 ... -4.875 -2.32812 -1.10156]\n",
            "  [-8.5 5.59375 -2.48438 ... -4.09375 0.476562 -4.0625]]\n",
            "\n",
            " [[-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.57812 -1.69531 -4.46875 ... -3.23438 0.632812 -4.25]\n",
            "  ...\n",
            "  [1.80469 -3.78125 -10.25 ... 0.8125 -5.6875 -0.964844]\n",
            "  [-5.8125 -3.34375 -4.34375 ... -3.0625 -2.5 -3.15625]\n",
            "  [-7 -6 -7.3125 ... -6.5 -4 -4.53125]]\n",
            "\n",
            " [[-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [-1.57812 -1.69531 -4.46875 ... -3.23438 0.632812 -4.25]\n",
            "  ...\n",
            "  [-1.82031 2.40625 -3.92188 ... -0.00390625 -1.89062 -1.39062]\n",
            "  [-2.92188 1.27344 -7.125 ... 0.261719 0.482422 -3.28125]\n",
            "  [-5.59375 -4.03125 -6.65625 ... -6.40625 -2.90625 -7.1875]]\n",
            "\n",
            " [[-1.46094 -1.71875 -4.5 ... -3.21875 0.640625 -4.25]\n",
            "  [1.60938 2.51562 -2.89062 ... -0.691406 2.10938 -0.371094]\n",
            "  [0.53125 1.17969 -2.78125 ... 0.167969 0.507812 -5]\n",
            "  ...\n",
            "  [-4.09375 3.95312 -8.125 ... 0.248047 1.39062 3.125]\n",
            "  [0.294922 -0.265625 -9.625 ... 3.65625 4.09375 -1.46875]\n",
            "  [-6.4375 -2.10938 -6.90625 ... -5 -2.39062 -6.90625]]]\n",
            "lnx=[[[-0.3125 -0.367188 -0.960938 ... -0.6875 0.136719 -0.910156]\n",
            "  [-0.3125 -0.367188 -0.960938 ... -0.6875 0.136719 -0.910156]\n",
            "  [-0.337891 -0.363281 -0.957031 ... -0.691406 0.135742 -0.910156]\n",
            "  ...\n",
            "  [-0.683594 -0.109863 -1.10156 ... -0.0712891 0.871094 1.00781]\n",
            "  [-0.157227 -1.17188 -1.42969 ... 0.640625 1.35938 0.180664]\n",
            "  [-1.60938 -0.996094 -0.628906 ... -0.894531 0.108398 -1.57812]]\n",
            "\n",
            " [[-0.257812 0.761719 -0.294922 ... -0.792969 -0.925781 0.796875]\n",
            "  [0.8125 0.785156 0.0629883 ... -0.378906 0.237305 1.14062]\n",
            "  [-0.211914 0.558594 -0.369141 ... 0.0859375 0.21582 -0.470703]\n",
            "  ...\n",
            "  [1.86719 2.75 -2.60938 ... -0.441406 0.921875 0.820312]\n",
            "  [-1.5 1.08594 -1.53906 ... -1.14844 -0.546875 -0.259766]\n",
            "  [-2.20312 1.45312 -0.644531 ... -1.0625 0.123535 -1.05469]]\n",
            "\n",
            " [[-0.3125 -0.367188 -0.960938 ... -0.6875 0.136719 -0.910156]\n",
            "  [-0.3125 -0.367188 -0.960938 ... -0.6875 0.136719 -0.910156]\n",
            "  [-0.337891 -0.363281 -0.957031 ... -0.691406 0.135742 -0.910156]\n",
            "  ...\n",
            "  [0.453125 -0.949219 -2.57812 ... 0.204102 -1.42969 -0.242188]\n",
            "  [-1.42969 -0.824219 -1.07031 ... -0.753906 -0.617188 -0.777344]\n",
            "  [-1.375 -1.17969 -1.4375 ... -1.28125 -0.785156 -0.890625]]\n",
            "\n",
            " [[-0.3125 -0.367188 -0.960938 ... -0.6875 0.136719 -0.910156]\n",
            "  [-0.3125 -0.367188 -0.960938 ... -0.6875 0.136719 -0.910156]\n",
            "  [-0.337891 -0.363281 -0.957031 ... -0.691406 0.135742 -0.910156]\n",
            "  ...\n",
            "  [-0.498047 0.660156 -1.07812 ... -0.00106812 -0.519531 -0.380859]\n",
            "  [-0.796875 0.347656 -1.94531 ... 0.0712891 0.131836 -0.894531]\n",
            "  [-1.13281 -0.816406 -1.34375 ... -1.29688 -0.589844 -1.45312]]\n",
            "\n",
            " [[-0.3125 -0.367188 -0.960938 ... -0.6875 0.136719 -0.910156]\n",
            "  [0.394531 0.617188 -0.707031 ... -0.168945 0.515625 -0.0908203]\n",
            "  [0.132812 0.294922 -0.699219 ... 0.0422363 0.126953 -1.25]\n",
            "  ...\n",
            "  [-1.19531 1.15625 -2.375 ... 0.0722656 0.40625 0.910156]\n",
            "  [0.0878906 -0.0795898 -2.875 ... 1.09375 1.22656 -0.439453]\n",
            "  [-1.46094 -0.478516 -1.5625 ... -1.13281 -0.542969 -1.5625]]]\n",
            "attention_lnx=[[[-0.5 0.0218506 -1.44531 ... -0.0722656 -1.25 -0.644531]\n",
            "  [-0.5 0.0218506 -1.44531 ... -0.0722656 -1.25 -0.644531]\n",
            "  [-0.503906 0.0233154 -1.45312 ... -0.0703125 -1.25 -0.640625]\n",
            "  ...\n",
            "  [-0.361328 0.515625 -1.375 ... -0.22168 -1.4375 -0.427734]\n",
            "  [-0.398438 0.515625 -1.39062 ... -0.207031 -1.30469 -0.574219]\n",
            "  [-0.609375 0.330078 -1.35156 ... -0.234375 -1.21875 -0.582031]]\n",
            "\n",
            " [[-0.386719 -0.0751953 -0.097168 ... -0.957031 -0.679688 -0.925781]\n",
            "  [-0.722656 0.151367 -0.722656 ... -0.726562 0.00674438 -0.404297]\n",
            "  [-0.71875 0.0554199 -1.17188 ... -0.255859 0.227539 -0.558594]\n",
            "  ...\n",
            "  [-0.291016 -0.263672 -0.824219 ... 0.15625 0.130859 -0.796875]\n",
            "  [-0.21582 -0.585938 -0.208984 ... 0.251953 0.107422 -0.679688]\n",
            "  [-0.53125 -0.597656 -0.141602 ... 0.191406 -0.0228271 -0.574219]]\n",
            "\n",
            " [[-0.5 0.0218506 -1.44531 ... -0.0722656 -1.25 -0.644531]\n",
            "  [-0.5 0.0218506 -1.44531 ... -0.0722656 -1.25 -0.644531]\n",
            "  [-0.503906 0.0233154 -1.45312 ... -0.0703125 -1.25 -0.640625]\n",
            "  ...\n",
            "  [-0.208984 0.291016 -1.32031 ... 0.00576782 -0.660156 -0.269531]\n",
            "  [-0.239258 0.396484 -1.10938 ... -0.0578613 -0.652344 -0.318359]\n",
            "  [-0.392578 0.710938 -1.17188 ... -0.00473022 -0.5625 -0.365234]]\n",
            "\n",
            " [[-0.5 0.0218506 -1.44531 ... -0.0722656 -1.25 -0.644531]\n",
            "  [-0.5 0.0218506 -1.44531 ... -0.0722656 -1.25 -0.644531]\n",
            "  [-0.503906 0.0233154 -1.45312 ... -0.0703125 -1.25 -0.640625]\n",
            "  ...\n",
            "  [-0.226562 0.367188 -1.41406 ... -0.182617 -0.738281 -0.376953]\n",
            "  [-0.419922 0.414062 -1.5 ... 0.0500488 -0.835938 -0.40625]\n",
            "  [-0.5 0.46875 -1.53125 ... -0.0169678 -0.839844 -0.453125]]\n",
            "\n",
            " [[-0.5 0.0218506 -1.44531 ... -0.0722656 -1.25 -0.644531]\n",
            "  [-0.386719 0.470703 -1.5 ... -0.421875 -1.21875 -0.445312]\n",
            "  [-0.570312 0.65625 -1.63281 ... -0.527344 -1.17969 -0.316406]\n",
            "  ...\n",
            "  [-0.84375 0.482422 -1.63281 ... 0.474609 -0.804688 -0.613281]\n",
            "  [-0.871094 0.722656 -1.55469 ... 0.78125 -0.992188 -0.617188]\n",
            "  [-0.828125 0.439453 -1.32812 ... 0.318359 -0.738281 -0.648438]]]\n",
            "attn_output=[[[-0.410156 -0.355469 -1.24219 ... -0.6875 -0.12793 -1.02344]\n",
            "  [-0.410156 -0.355469 -1.24219 ... -0.6875 -0.12793 -1.02344]\n",
            "  [-0.435547 -0.349609 -1.24219 ... -0.691406 -0.128906 -1.02344]\n",
            "  ...\n",
            "  [-0.757812 0.0214844 -1.42188 ... -0.125 0.492188 0.875]\n",
            "  [-0.257812 -1.00781 -1.75781 ... 0.570312 0.992188 0.0274658]\n",
            "  [-1.70312 -0.902344 -0.898438 ... -0.921875 -0.152344 -1.66406]]\n",
            "\n",
            " [[-0.335938 0.726562 -0.308594 ... -0.980469 -1.04688 0.574219]\n",
            "  [0.621094 0.808594 -0.116699 ... -0.550781 0.234375 1.01562]\n",
            "  [-0.392578 0.570312 -0.664062 ... 0.0198975 0.271484 -0.609375]\n",
            "  ...\n",
            "  [1.75 2.60938 -2.79688 ... -0.388672 0.941406 0.574219]\n",
            "  [-1.50781 0.925781 -1.53906 ... -1.05469 -0.507812 -0.408203]\n",
            "  [-2.29688 1.27344 -0.667969 ... -0.992188 0.115234 -1.17969]]\n",
            "\n",
            " [[-0.410156 -0.355469 -1.24219 ... -0.6875 -0.12793 -1.02344]\n",
            "  [-0.410156 -0.355469 -1.24219 ... -0.6875 -0.12793 -1.02344]\n",
            "  [-0.435547 -0.349609 -1.24219 ... -0.691406 -0.128906 -1.02344]\n",
            "  ...\n",
            "  [0.392578 -0.859375 -2.85938 ... 0.202148 -1.5625 -0.304688]\n",
            "  [-1.45312 -0.707031 -1.3125 ... -0.75 -0.757812 -0.835938]\n",
            "  [-1.42969 -1.02344 -1.64062 ... -1.25781 -0.878906 -0.945312]]\n",
            "\n",
            " [[-0.410156 -0.355469 -1.24219 ... -0.6875 -0.12793 -1.02344]\n",
            "  [-0.410156 -0.355469 -1.24219 ... -0.6875 -0.12793 -1.02344]\n",
            "  [-0.435547 -0.349609 -1.24219 ... -0.691406 -0.128906 -1.02344]\n",
            "  ...\n",
            "  [-0.546875 0.742188 -1.42969 ... -0.0500488 -0.703125 -0.474609]\n",
            "  [-0.886719 0.449219 -2.29688 ... 0.0830078 -0.09375 -0.980469]\n",
            "  [-1.21094 -0.707031 -1.625 ... -1.27344 -0.742188 -1.51562]]\n",
            "\n",
            " [[-0.410156 -0.355469 -1.24219 ... -0.6875 -0.12793 -1.02344]\n",
            "  [0.292969 0.714844 -1.05469 ... -0.265625 0.212891 -0.195312]\n",
            "  [-0.00958252 0.451172 -1.08594 ... -0.0883789 -0.165039 -1.30469]\n",
            "  ...\n",
            "  [-1.40625 1.26562 -2.78125 ... 0.206055 0.166992 0.714844]\n",
            "  [-0.167969 0.132812 -3.26562 ... 1.29688 0.90625 -0.609375]\n",
            "  [-1.60938 -0.369141 -1.82031 ... -1.03906 -0.691406 -1.67188]]]\n",
            "next_layer_addition_dropped_out=[[[-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.5 -1.66406 -5.78125 ... -2.0625 -1.21875 -3.32812]\n",
            "  ...\n",
            "  [-3.15625 0.226562 -6.09375 ... 0.578125 2.71875 3.8125]\n",
            "  [-0.328125 -3.96875 -6.875 ... 2.875 3.1875 0.722656]\n",
            "  [-7.40625 -4.5625 -6.59375 ... -2.125 -1.03125 -6.9375]]\n",
            "\n",
            " [[-1.48438 3.34375 -1.9375 ... -4.875 -4.5625 2.6875]\n",
            "  [2.6875 2.59375 -1.34375 ... -2.21875 1.17969 4.03125]\n",
            "  [-1.9375 1.76562 -3.23438 ... -1.14062 0.449219 -2.95312]\n",
            "  ...\n",
            "  [4.9375 8.5 -10.5625 ... -0.671875 3.07812 3.15625]\n",
            "  [-7.71875 3.07812 -6.875 ... -4.875 -2.79688 -2.6875]\n",
            "  [-8.9375 4.375 -2.59375 ... -5.0625 0.0371094 -4.25]]\n",
            "\n",
            " [[-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.5 -1.66406 -5.78125 ... -2.0625 -1.21875 -3.32812]\n",
            "  ...\n",
            "  [0.773438 -3.90625 -11.0625 ... 2.53125 -7.625 0.757812]\n",
            "  [-6.8125 -3.9375 -7.3125 ... -0.046875 -2.9375 -3.1875]\n",
            "  [-6.53125 -6.6875 -10.4375 ... -3.84375 -4.4375 -4.09375]]\n",
            "\n",
            " [[-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.5 -1.66406 -5.78125 ... -2.0625 -1.21875 -3.32812]\n",
            "  ...\n",
            "  [-3.03125 3.03125 -5.8125 ... -0.378906 -3.125 -1.74219]\n",
            "  [-3.82812 1.10938 -9.125 ... 1.78906 -1 -3.26562]\n",
            "  [-6.03125 -5.25 -10.5625 ... -3.92188 -3.75 -5.9375]]\n",
            "\n",
            " [[-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [1.42188 3.5625 -4.78125 ... -0.851562 0.4375 0.574219]\n",
            "  [-0.640625 2.10938 -5.21875 ... 0.328125 0.117188 -4.125]\n",
            "  ...\n",
            "  [-4.375 3.98438 -9.375 ... 0.894531 -0.421875 2.6875]\n",
            "  [-1.32812 0.462891 -12.4375 ... 5.03125 3.625 -1.3125]\n",
            "  [-7.25 -2.60938 -9.75 ... -3.71875 -4 -7.3125]]]\n",
            "inputs=[[[-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.5 -1.66406 -5.78125 ... -2.0625 -1.21875 -3.32812]\n",
            "  ...\n",
            "  [-3.15625 0.226562 -6.09375 ... 0.578125 2.71875 3.8125]\n",
            "  [-0.328125 -3.96875 -6.875 ... 2.875 3.1875 0.722656]\n",
            "  [-7.40625 -4.5625 -6.59375 ... -2.125 -1.03125 -6.9375]]\n",
            "\n",
            " [[-1.48438 3.34375 -1.9375 ... -4.875 -4.5625 2.6875]\n",
            "  [2.6875 2.59375 -1.34375 ... -2.21875 1.17969 4.03125]\n",
            "  [-1.9375 1.76562 -3.23438 ... -1.14062 0.449219 -2.95312]\n",
            "  ...\n",
            "  [4.9375 8.5 -10.5625 ... -0.671875 3.07812 3.15625]\n",
            "  [-7.71875 3.07812 -6.875 ... -4.875 -2.79688 -2.6875]\n",
            "  [-8.9375 4.375 -2.59375 ... -5.0625 0.0371094 -4.25]]\n",
            "\n",
            " [[-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.5 -1.66406 -5.78125 ... -2.0625 -1.21875 -3.32812]\n",
            "  ...\n",
            "  [0.773438 -3.90625 -11.0625 ... 2.53125 -7.625 0.757812]\n",
            "  [-6.8125 -3.9375 -7.3125 ... -0.046875 -2.9375 -3.1875]\n",
            "  [-6.53125 -6.6875 -10.4375 ... -3.84375 -4.4375 -4.09375]]\n",
            "\n",
            " [[-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [-1.5 -1.66406 -5.78125 ... -2.0625 -1.21875 -3.32812]\n",
            "  ...\n",
            "  [-3.03125 3.03125 -5.8125 ... -0.378906 -3.125 -1.74219]\n",
            "  [-3.82812 1.10938 -9.125 ... 1.78906 -1 -3.26562]\n",
            "  [-6.03125 -5.25 -10.5625 ... -3.92188 -3.75 -5.9375]]\n",
            "\n",
            " [[-1.39062 -1.70312 -5.8125 ... -2.0625 -1.21875 -3.34375]\n",
            "  [1.42188 3.5625 -4.78125 ... -0.851562 0.4375 0.574219]\n",
            "  [-0.640625 2.10938 -5.21875 ... 0.328125 0.117188 -4.125]\n",
            "  ...\n",
            "  [-4.375 3.98438 -9.375 ... 0.894531 -0.421875 2.6875]\n",
            "  [-1.32812 0.462891 -12.4375 ... 5.03125 3.625 -1.3125]\n",
            "  [-7.25 -2.60938 -9.75 ... -3.71875 -4 -7.3125]]]\n",
            "lnx=[[[-0.28125 -0.345703 -1.17969 ... -0.417969 -0.24707 -0.675781]\n",
            "  [-0.28125 -0.345703 -1.17969 ... -0.417969 -0.24707 -0.675781]\n",
            "  [-0.304688 -0.337891 -1.17188 ... -0.417969 -0.24707 -0.675781]\n",
            "  ...\n",
            "  [-0.773438 0.0556641 -1.5 ... 0.141602 0.667969 0.9375]\n",
            "  [-0.0830078 -1 -1.74219 ... 0.726562 0.804688 0.182617]\n",
            "  [-1.46094 -0.902344 -1.30469 ... -0.419922 -0.204102 -1.36719]]\n",
            "\n",
            " [[-0.318359 0.714844 -0.414062 ... -1.04688 -0.976562 0.574219]\n",
            "  [0.65625 0.632812 -0.328125 ... -0.542969 0.289062 0.984375]\n",
            "  [-0.484375 0.441406 -0.808594 ... -0.285156 0.112305 -0.738281]\n",
            "  ...\n",
            "  [1.40625 2.42188 -3 ... -0.191406 0.875 0.898438]\n",
            "  [-1.625 0.648438 -1.44531 ... -1.02344 -0.585938 -0.566406]\n",
            "  [-2.125 1.03906 -0.617188 ... -1.20312 0.0088501 -1.00781]]\n",
            "\n",
            " [[-0.28125 -0.345703 -1.17969 ... -0.417969 -0.24707 -0.675781]\n",
            "  [-0.28125 -0.345703 -1.17969 ... -0.417969 -0.24707 -0.675781]\n",
            "  [-0.304688 -0.337891 -1.17188 ... -0.417969 -0.24707 -0.675781]\n",
            "  ...\n",
            "  [0.179688 -0.910156 -2.57812 ... 0.589844 -1.77344 0.176758]\n",
            "  [-1.53125 -0.886719 -1.64062 ... -0.0105591 -0.660156 -0.714844]\n",
            "  [-1.13281 -1.16406 -1.8125 ... -0.667969 -0.773438 -0.710938]]\n",
            "\n",
            " [[-0.28125 -0.345703 -1.17969 ... -0.417969 -0.24707 -0.675781]\n",
            "  [-0.28125 -0.345703 -1.17969 ... -0.417969 -0.24707 -0.675781]\n",
            "  [-0.304688 -0.337891 -1.17188 ... -0.417969 -0.24707 -0.675781]\n",
            "  ...\n",
            "  [-0.792969 0.792969 -1.51562 ... -0.0991211 -0.816406 -0.455078]\n",
            "  [-0.976562 0.283203 -2.32812 ... 0.457031 -0.255859 -0.832031]\n",
            "  [-1.08594 -0.945312 -1.89844 ... -0.707031 -0.675781 -1.07031]]\n",
            "\n",
            " [[-0.28125 -0.345703 -1.17969 ... -0.417969 -0.24707 -0.675781]\n",
            "  [0.332031 0.832031 -1.11719 ... -0.199219 0.102539 0.134766]\n",
            "  [-0.154297 0.507812 -1.25781 ... 0.0791016 0.0281982 -0.996094]\n",
            "  ...\n",
            "  [-1.21875 1.10938 -2.625 ... 0.25 -0.117676 0.75]\n",
            "  [-0.378906 0.131836 -3.54688 ... 1.4375 1.03125 -0.375]\n",
            "  [-1.46875 -0.527344 -1.96875 ... -0.753906 -0.808594 -1.47656]]]\n",
            "attention_lnx=[[[0.722656 -0.871094 0.417969 ... 1.26562 -0.0578613 -1.30469]\n",
            "  [0.722656 -0.871094 0.417969 ... 1.26562 -0.0578613 -1.30469]\n",
            "  [0.714844 -0.863281 0.410156 ... 1.26562 -0.0578613 -1.29688]\n",
            "  ...\n",
            "  [0.863281 -0.683594 0.503906 ... 1.13281 -0.194336 -0.890625]\n",
            "  [0.976562 -0.554688 0.449219 ... 0.886719 -0.0795898 -0.9375]\n",
            "  [0.671875 -0.863281 0.542969 ... 0.988281 0.0932617 -1.0625]]\n",
            "\n",
            " [[0.808594 0.417969 -1.05469 ... -1.90625 1.26562 -0.929688]\n",
            "  [0.333984 0.652344 -1.875 ... -1.22656 0.458984 -0.367188]\n",
            "  [0.0537109 0.765625 -1.61719 ... -1.46094 0.170898 -0.142578]\n",
            "  ...\n",
            "  [-0.361328 -0.217773 -1.59375 ... -0.244141 -0.695312 -0.761719]\n",
            "  [-0.546875 -1.22656 -0.871094 ... 0.328125 -0.439453 -0.78125]\n",
            "  [-0.582031 -1.10156 -1 ... 0.447266 -0.503906 -0.498047]]\n",
            "\n",
            " [[0.722656 -0.871094 0.417969 ... 1.26562 -0.0578613 -1.30469]\n",
            "  [0.722656 -0.871094 0.417969 ... 1.26562 -0.0578613 -1.30469]\n",
            "  [0.714844 -0.863281 0.410156 ... 1.26562 -0.0578613 -1.29688]\n",
            "  ...\n",
            "  [0.5625 -0.898438 0.298828 ... 0.535156 0.226562 -0.255859]\n",
            "  [0.648438 -1 0.353516 ... 0.8125 0.177734 -0.431641]\n",
            "  [0.367188 -1.875 0.425781 ... 0.488281 0.310547 -0.472656]]\n",
            "\n",
            " [[0.722656 -0.871094 0.417969 ... 1.26562 -0.0578613 -1.30469]\n",
            "  [0.722656 -0.871094 0.417969 ... 1.26562 -0.0578613 -1.30469]\n",
            "  [0.714844 -0.863281 0.410156 ... 1.26562 -0.0578613 -1.29688]\n",
            "  ...\n",
            "  [0.886719 -0.578125 0.494141 ... 0.980469 -0.191406 -0.283203]\n",
            "  [0.683594 -0.859375 0.402344 ... 0.679688 -0.0512695 -0.609375]\n",
            "  [0.285156 -1.36719 0.542969 ... 0.71875 0.180664 -0.476562]]\n",
            "\n",
            " [[0.722656 -0.871094 0.417969 ... 1.26562 -0.0578613 -1.30469]\n",
            "  [1.13281 -0.921875 0.126953 ... 0.439453 0.353516 -1.28125]\n",
            "  [0.59375 -0.867188 -0.0776367 ... 0.433594 -0.179688 -0.878906]\n",
            "  ...\n",
            "  [0.382812 -1.03906 0.109863 ... 0.773438 -0.0717773 -0.194336]\n",
            "  [0.0698242 -0.714844 -0.245117 ... 0.582031 -0.00732422 -0.207031]\n",
            "  [-0.0559082 -1.42188 0.0683594 ... 0.726562 0.116699 0.0388184]]]\n",
            "attn_output=[[[-0.131836 -0.507812 -1.0625 ... -0.157227 -0.251953 -0.917969]\n",
            "  [-0.131836 -0.507812 -1.0625 ... -0.157227 -0.251953 -0.917969]\n",
            "  [-0.155273 -0.498047 -1.0625 ... -0.157227 -0.251953 -0.910156]\n",
            "  ...\n",
            "  [-0.546875 -0.109375 -1.33594 ... 0.408203 0.601562 0.699219]\n",
            "  [0.15918 -1.10938 -1.57812 ... 0.921875 0.761719 -0.0527344]\n",
            "  [-1.29688 -1.04688 -1.16406 ... -0.21875 -0.180664 -1.53906]]\n",
            "\n",
            " [[-0.140625 0.785156 -0.625 ... -1.41406 -0.6875 0.367188]\n",
            "  [0.722656 0.777344 -0.769531 ... -0.824219 0.390625 0.875]\n",
            "  [-0.462891 0.621094 -1.19531 ... -0.640625 0.152344 -0.761719]\n",
            "  ...\n",
            "  [1.27344 2.3125 -3.39062 ... -0.255859 0.664062 0.667969]\n",
            "  [-1.69531 0.380859 -1.59375 ... -0.933594 -0.664062 -0.710938]\n",
            "  [-2.20312 0.757812 -0.832031 ... -1.07031 -0.108398 -1.10156]]\n",
            "\n",
            " [[-0.131836 -0.507812 -1.0625 ... -0.157227 -0.251953 -0.917969]\n",
            "  [-0.131836 -0.507812 -1.0625 ... -0.157227 -0.251953 -0.917969]\n",
            "  [-0.155273 -0.498047 -1.0625 ... -0.157227 -0.251953 -0.910156]\n",
            "  ...\n",
            "  [0.300781 -1.07812 -2.42188 ... 0.691406 -1.66406 0.112793]\n",
            "  [-1.34375 -1.07812 -1.52344 ... 0.166992 -0.601562 -0.789062]\n",
            "  [-1.03906 -1.44531 -1.69531 ... -0.566406 -0.699219 -0.773438]]\n",
            "\n",
            " [[-0.131836 -0.507812 -1.0625 ... -0.157227 -0.251953 -0.917969]\n",
            "  [-0.131836 -0.507812 -1.0625 ... -0.157227 -0.251953 -0.917969]\n",
            "  [-0.155273 -0.498047 -1.0625 ... -0.157227 -0.251953 -0.910156]\n",
            "  ...\n",
            "  [-0.546875 0.625 -1.35156 ... 0.15332 -0.84375 -0.515625]\n",
            "  [-0.78125 0.0622559 -2.17188 ... 0.613281 -0.261719 -0.964844]\n",
            "  [-1 -1.15625 -1.75 ... -0.558594 -0.625 -1.11719]]\n",
            "\n",
            " [[-0.131836 -0.507812 -1.0625 ... -0.157227 -0.251953 -0.917969]\n",
            "  [0.582031 0.601562 -1.05469 ... -0.09375 0.179688 -0.160156]\n",
            "  [-0.0109863 0.291016 -1.24219 ... 0.178711 -0.0146484 -1.17188]\n",
            "  ...\n",
            "  [-1.08594 0.800781 -2.51562 ... 0.453125 -0.133789 0.679688]\n",
            "  [-0.349609 -0.0703125 -3.53125 ... 1.5625 1.00781 -0.423828]\n",
            "  [-1.4375 -0.796875 -1.90625 ... -0.589844 -0.765625 -1.4375]]]\n",
            "next_layer_addition_dropped_out=[[[-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.92188 -1.98438 -5.5625 ... -0.808594 -0.117188 -4.96875]\n",
            "  ...\n",
            "  [-3.07812 0.34375 -6.5625 ... 1.49219 2.65625 3.35938]\n",
            "  [0.158203 -4.09375 -7.25 ... 3.73438 3.71875 0.0722656]\n",
            "  [-8.1875 -4.75 -8.0625 ... -0.859375 -1.35156 -6.8125]]\n",
            "\n",
            " [[0.113281 4.53125 -2.73438 ... -4.40625 -4.15625 1.60156]\n",
            "  [3.54688 3.8125 -3 ... -2.59375 1.78125 4.6875]\n",
            "  [-0.492188 2.34375 -4.625 ... -1.8125 0.396484 -3.3125]\n",
            "  ...\n",
            "  [5.125 7.5625 -11.6875 ... -0.90625 0.945312 2.40625]\n",
            "  [-8.5 1.21875 -10.625 ... -3.8125 -2.96875 -2.90625]\n",
            "  [-9.4375 1.36719 -6.53125 ... -4.5 0.451172 -3.71875]]\n",
            "\n",
            " [[-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.92188 -1.98438 -5.5625 ... -0.808594 -0.117188 -4.96875]\n",
            "  ...\n",
            "  [0.460938 -5.28125 -13.375 ... 3.28125 -8.9375 1.375]\n",
            "  [-7.5625 -5.875 -8.6875 ... -0.0351562 -3.8125 -3.67188]\n",
            "  [-7.03125 -9.8125 -13.1875 ... -3.4375 -3.84375 -3.40625]]\n",
            "\n",
            " [[-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.92188 -1.98438 -5.5625 ... -0.808594 -0.117188 -4.96875]\n",
            "  ...\n",
            "  [-2 2.85938 -5.40625 ... 1.32812 -2.625 -2.34375]\n",
            "  [-2.26562 0.761719 -10.0625 ... 1.8125 -1.88281 -3.875]\n",
            "  [-6.6875 -8.375 -13 ... -2.79688 -3.125 -4.90625]]\n",
            "\n",
            " [[-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [2.17188 2.78125 -5.65625 ... -0.298828 1.00781 -0.207031]\n",
            "  [-0.00292969 0.992188 -6.40625 ... -0.128906 0.0273438 -4.09375]\n",
            "  ...\n",
            "  [-2.65625 2.84375 -9.5625 ... 3.375 0.158203 2.23438]\n",
            "  [-0.851562 -0.929688 -12.5 ... 5.34375 2.54688 -1.34375]\n",
            "  [-6.8125 -5.375 -12.1875 ... -2.46875 -3.59375 -6.1875]]]\n",
            "inputs=[[[-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.92188 -1.98438 -5.5625 ... -0.808594 -0.117188 -4.96875]\n",
            "  ...\n",
            "  [-3.07812 0.34375 -6.5625 ... 1.49219 2.65625 3.35938]\n",
            "  [0.158203 -4.09375 -7.25 ... 3.73438 3.71875 0.0722656]\n",
            "  [-8.1875 -4.75 -8.0625 ... -0.859375 -1.35156 -6.8125]]\n",
            "\n",
            " [[0.113281 4.53125 -2.73438 ... -4.40625 -4.15625 1.60156]\n",
            "  [3.54688 3.8125 -3 ... -2.59375 1.78125 4.6875]\n",
            "  [-0.492188 2.34375 -4.625 ... -1.8125 0.396484 -3.3125]\n",
            "  ...\n",
            "  [5.125 7.5625 -11.6875 ... -0.90625 0.945312 2.40625]\n",
            "  [-8.5 1.21875 -10.625 ... -3.8125 -2.96875 -2.90625]\n",
            "  [-9.4375 1.36719 -6.53125 ... -4.5 0.451172 -3.71875]]\n",
            "\n",
            " [[-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.92188 -1.98438 -5.5625 ... -0.808594 -0.117188 -4.96875]\n",
            "  ...\n",
            "  [0.460938 -5.28125 -13.375 ... 3.28125 -8.9375 1.375]\n",
            "  [-7.5625 -5.875 -8.6875 ... -0.0351562 -3.8125 -3.67188]\n",
            "  [-7.03125 -9.8125 -13.1875 ... -3.4375 -3.84375 -3.40625]]\n",
            "\n",
            " [[-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [-1.92188 -1.98438 -5.5625 ... -0.808594 -0.117188 -4.96875]\n",
            "  ...\n",
            "  [-2 2.85938 -5.40625 ... 1.32812 -2.625 -2.34375]\n",
            "  [-2.26562 0.761719 -10.0625 ... 1.8125 -1.88281 -3.875]\n",
            "  [-6.6875 -8.375 -13 ... -2.79688 -3.125 -4.90625]]\n",
            "\n",
            " [[-1.79688 -2.03125 -5.59375 ... -0.789062 -0.09375 -4.96875]\n",
            "  [2.17188 2.78125 -5.65625 ... -0.298828 1.00781 -0.207031]\n",
            "  [-0.00292969 0.992188 -6.40625 ... -0.128906 0.0273438 -4.09375]\n",
            "  ...\n",
            "  [-2.65625 2.84375 -9.5625 ... 3.375 0.158203 2.23438]\n",
            "  [-0.851562 -0.929688 -12.5 ... 5.34375 2.54688 -1.34375]\n",
            "  [-6.8125 -5.375 -12.1875 ... -2.46875 -3.59375 -6.1875]]]\n",
            "lnx=[[[-0.341797 -0.386719 -1.0625 ... -0.150391 -0.0178223 -0.945312]\n",
            "  [-0.341797 -0.386719 -1.0625 ... -0.150391 -0.0178223 -0.945312]\n",
            "  [-0.365234 -0.376953 -1.0625 ... -0.154297 -0.0223389 -0.945312]\n",
            "  ...\n",
            "  [-0.714844 0.0800781 -1.52344 ... 0.347656 0.617188 0.78125]\n",
            "  [0.0378418 -0.980469 -1.73438 ... 0.894531 0.890625 0.017334]\n",
            "  [-1.44531 -0.839844 -1.42188 ... -0.151367 -0.238281 -1.20312]]\n",
            "\n",
            " [[0.0231934 0.929688 -0.558594 ... -0.902344 -0.851562 0.328125]\n",
            "  [0.839844 0.902344 -0.710938 ... -0.613281 0.421875 1.10938]\n",
            "  [-0.119629 0.570312 -1.125 ... -0.439453 0.0961914 -0.804688]\n",
            "  ...\n",
            "  [1.39844 2.0625 -3.1875 ... -0.24707 0.257812 0.65625]\n",
            "  [-1.58594 0.227539 -1.98438 ... -0.710938 -0.554688 -0.542969]\n",
            "  [-2.01562 0.291016 -1.39062 ... -0.957031 0.0961914 -0.792969]]\n",
            "\n",
            " [[-0.341797 -0.386719 -1.0625 ... -0.150391 -0.0178223 -0.945312]\n",
            "  [-0.341797 -0.386719 -1.0625 ... -0.150391 -0.0178223 -0.945312]\n",
            "  [-0.365234 -0.376953 -1.0625 ... -0.154297 -0.0223389 -0.945312]\n",
            "  ...\n",
            "  [0.0976562 -1.11719 -2.82812 ... 0.695312 -1.89062 0.291016]\n",
            "  [-1.52344 -1.1875 -1.75 ... -0.00708008 -0.769531 -0.742188]\n",
            "  [-1.05469 -1.47656 -1.97656 ... -0.515625 -0.578125 -0.511719]]\n",
            "\n",
            " [[-0.341797 -0.386719 -1.0625 ... -0.150391 -0.0178223 -0.945312]\n",
            "  [-0.341797 -0.386719 -1.0625 ... -0.150391 -0.0178223 -0.945312]\n",
            "  [-0.365234 -0.376953 -1.0625 ... -0.154297 -0.0223389 -0.945312]\n",
            "  ...\n",
            "  [-0.494141 0.707031 -1.33594 ... 0.328125 -0.648438 -0.578125]\n",
            "  [-0.539062 0.181641 -2.40625 ... 0.431641 -0.449219 -0.925781]\n",
            "  [-1.03906 -1.30469 -2.01562 ... -0.435547 -0.486328 -0.761719]]\n",
            "\n",
            " [[-0.341797 -0.386719 -1.0625 ... -0.150391 -0.0178223 -0.945312]\n",
            "  [0.480469 0.617188 -1.25 ... -0.065918 0.222656 -0.0458984]\n",
            "  [-0.000671387 0.227539 -1.46875 ... -0.029541 0.0062561 -0.9375]\n",
            "  ...\n",
            "  [-0.703125 0.753906 -2.53125 ... 0.894531 0.041748 0.589844]\n",
            "  [-0.232422 -0.253906 -3.40625 ... 1.46094 0.695312 -0.367188]\n",
            "  [-1.20312 -0.949219 -2.15625 ... -0.4375 -0.636719 -1.09375]]]\n",
            "attention_lnx=[[[1.03125 -0.143555 -0.542969 ... -0.804688 0.480469 -0.277344]\n",
            "  [1.03125 -0.143555 -0.542969 ... -0.804688 0.480469 -0.277344]\n",
            "  [1.03125 -0.134766 -0.558594 ... -0.804688 0.482422 -0.275391]\n",
            "  ...\n",
            "  [0.703125 -0.0917969 -0.3125 ... -1.125 0.78125 -0.458984]\n",
            "  [0.660156 -0.152344 -0.199219 ... -1.28125 0.734375 -0.390625]\n",
            "  [0.417969 0.00515747 -0.101074 ... -1.23438 0.542969 -0.283203]]\n",
            "\n",
            " [[-0.753906 0.185547 -0.241211 ... 2.51562 -0.753906 -1.98438]\n",
            "  [-0.255859 -0.11084 -0.984375 ... 1.35938 -0.449219 -1.48438]\n",
            "  [-0.181641 0.0737305 -0.984375 ... 1.25 -0.753906 -1.71094]\n",
            "  ...\n",
            "  [0.102539 0.824219 -0.298828 ... 0.0090332 -0.640625 -1.19531]\n",
            "  [-0.0947266 1.20312 0.149414 ... -0.15332 -1.15625 -1.04688]\n",
            "  [-0.012085 1.13281 0.0178223 ... -0.251953 -1.15625 -1.03125]]\n",
            "\n",
            " [[1.03125 -0.143555 -0.542969 ... -0.804688 0.480469 -0.277344]\n",
            "  [1.03125 -0.143555 -0.542969 ... -0.804688 0.480469 -0.277344]\n",
            "  [1.03125 -0.134766 -0.558594 ... -0.804688 0.482422 -0.275391]\n",
            "  ...\n",
            "  [0.12793 -0.402344 0.149414 ... -0.945312 -0.108887 -0.769531]\n",
            "  [0.0500488 -0.546875 0.347656 ... -1.00781 -0.335938 -0.867188]\n",
            "  [-0.132812 -0.328125 0.671875 ... -0.953125 -0.886719 -0.570312]]\n",
            "\n",
            " [[1.03125 -0.143555 -0.542969 ... -0.804688 0.480469 -0.277344]\n",
            "  [1.03125 -0.143555 -0.542969 ... -0.804688 0.480469 -0.277344]\n",
            "  [1.03125 -0.134766 -0.558594 ... -0.804688 0.482422 -0.275391]\n",
            "  ...\n",
            "  [-0.166992 -0.601562 0.134766 ... -0.902344 -0.148438 -0.734375]\n",
            "  [0.0212402 -0.507812 -0.147461 ... -1.14844 -0.0463867 -0.636719]\n",
            "  [-0.169922 -0.304688 0.464844 ... -1.30469 -0.636719 -0.285156]]\n",
            "\n",
            " [[1.03125 -0.143555 -0.542969 ... -0.804688 0.480469 -0.277344]\n",
            "  [0.714844 -0.179688 -0.722656 ... -0.628906 1.17188 -0.828125]\n",
            "  [0.566406 -0.890625 -0.597656 ... -0.546875 0.554688 -1.28906]\n",
            "  ...\n",
            "  [-0.287109 -0.574219 -0.0605469 ... -1.02344 -0.367188 -1]\n",
            "  [-0.503906 -0.644531 -0.412109 ... -1.04688 -0.135742 -0.546875]\n",
            "  [-0.251953 -0.257812 0.196289 ... -1.17188 -0.714844 -0.566406]]]\n",
            "attn_output=[[[-0.141602 -0.402344 -1.13281 ... -0.294922 0.0717773 -0.972656]\n",
            "  [-0.141602 -0.402344 -1.13281 ... -0.294922 0.0717773 -0.972656]\n",
            "  [-0.165039 -0.392578 -1.13281 ... -0.298828 0.0673828 -0.96875]\n",
            "  ...\n",
            "  [-0.539062 0.0571289 -1.55469 ... 0.0830078 0.777344 0.65625]\n",
            "  [0.192383 -1 -1.75781 ... 0.578125 1.04688 -0.0751953]\n",
            "  [-1.33594 -0.816406 -1.40625 ... -0.359375 -0.138672 -1.21875]]\n",
            "\n",
            " [[-0.12793 0.941406 -0.59375 ... -0.376953 -0.980469 -0.0761719]\n",
            "  [0.761719 0.859375 -0.921875 ... -0.285156 0.308594 0.742188]\n",
            "  [-0.160156 0.574219 -1.32812 ... -0.133789 -0.0849609 -1.1875]\n",
            "  ...\n",
            "  [1.39062 2.23438 -3.20312 ... -0.239258 0.0810547 0.322266]\n",
            "  [-1.55469 0.4375 -1.89844 ... -0.71875 -0.746094 -0.714844]\n",
            "  [-1.95312 0.515625 -1.34375 ... -0.980469 -0.145508 -0.980469]]\n",
            "\n",
            " [[-0.141602 -0.402344 -1.13281 ... -0.294922 0.0717773 -0.972656]\n",
            "  [-0.141602 -0.402344 -1.13281 ... -0.294922 0.0717773 -0.972656]\n",
            "  [-0.165039 -0.392578 -1.13281 ... -0.298828 0.0673828 -0.96875]\n",
            "  ...\n",
            "  [0.121582 -1.17188 -2.73438 ... 0.482422 -1.86719 0.125]\n",
            "  [-1.46875 -1.25781 -1.63281 ... -0.204102 -0.8125 -0.890625]\n",
            "  [-1.04688 -1.48438 -1.83594 ... -0.644531 -0.691406 -0.582031]]\n",
            "\n",
            " [[-0.141602 -0.402344 -1.13281 ... -0.294922 0.0717773 -0.972656]\n",
            "  [-0.141602 -0.402344 -1.13281 ... -0.294922 0.0717773 -0.972656]\n",
            "  [-0.165039 -0.392578 -1.13281 ... -0.298828 0.0673828 -0.96875]\n",
            "  ...\n",
            "  [-0.523438 0.546875 -1.27344 ... 0.103027 -0.671875 -0.746094]\n",
            "  [-0.523438 0.059082 -2.375 ... 0.154297 -0.449219 -1.04688]\n",
            "  [-1.03906 -1.32031 -1.89844 ... -0.621094 -0.570312 -0.789062]]\n",
            "\n",
            " [[-0.141602 -0.402344 -1.13281 ... -0.294922 0.0717773 -0.972656]\n",
            "  [0.625 0.5625 -1.38281 ... -0.201172 0.472656 -0.224609]\n",
            "  [0.125977 0.0227051 -1.57031 ... -0.151367 0.130859 -1.20312]\n",
            "  ...\n",
            "  [-0.757812 0.585938 -2.48438 ... 0.605469 -0.0539551 0.318359]\n",
            "  [-0.361328 -0.419922 -3.4375 ... 1.14844 0.640625 -0.503906]\n",
            "  [-1.21875 -0.96875 -2.0625 ... -0.625 -0.742188 -1.16406]]]\n",
            "next_layer_addition_dropped_out=[[[-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.949219 -3.48438 -6.59375 ... -2.65625 1.52344 -5.46875]\n",
            "  ...\n",
            "  [-2.42188 -0.546875 -7.5625 ... -0.363281 3.71875 3]\n",
            "  [0.574219 -5.03125 -7.4375 ... 1.35156 4.59375 -0.820312]\n",
            "  [-8.75 -5.96875 -9.1875 ... -2.21875 -2.42188 -8.5625]]\n",
            "\n",
            " [[-1.78125 5.3125 -3.96875 ... -2.1875 -4.1875 -0.217773]\n",
            "  [3.48438 4.3125 -4.59375 ... -1.35938 2.46875 3.20312]\n",
            "  [-0.546875 2.625 -6 ... -0.117188 -0.597656 -4.6875]\n",
            "  ...\n",
            "  [4.40625 7.625 -14 ... -0.683594 1.1875 2.1875]\n",
            "  [-9.75 0.796875 -12.1875 ... -6.0625 -5.09375 -3.9375]\n",
            "  [-11.75 1.80469 -8.8125 ... -4.75 -1.39844 -4.125]]\n",
            "\n",
            " [[-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.949219 -3.48438 -6.59375 ... -2.65625 1.52344 -5.46875]\n",
            "  ...\n",
            "  [-0.621094 -6.46875 -12.0625 ... 1.76562 -10.375 -0.449219]\n",
            "  [-8.375 -6.71875 -8.75 ... -2.10938 -5.09375 -5.46875]\n",
            "  [-8.125 -10.875 -13.0625 ... -4.5 -7.03125 -4.96875]]\n",
            "\n",
            " [[-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.949219 -3.48438 -6.59375 ... -2.65625 1.52344 -5.46875]\n",
            "  ...\n",
            "  [-1.38281 2.35938 -4.5 ... 0.253906 -3.0625 -2.28125]\n",
            "  [-2.65625 0.957031 -10.0625 ... -0.242188 -2.17188 -5]\n",
            "  [-8.5 -9.125 -13.9375 ... -4.25 -6.375 -6.625]]\n",
            "\n",
            " [[-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [3.03125 1.89062 -6.21875 ... -0.738281 2.39062 -1.42969]\n",
            "  [0.550781 0.216797 -6.78125 ... -0.804688 0.177734 -6.1875]\n",
            "  ...\n",
            "  [-3.67188 2.75 -9.875 ... 3.03125 -0.859375 1.21875]\n",
            "  [-1.69531 -1.38281 -13.25 ... 4.28125 2.78125 -1.35938]\n",
            "  [-9.3125 -6.0625 -13.0625 ... -4.3125 -7.125 -8.1875]]]\n",
            "inputs=[[[-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.949219 -3.48438 -6.59375 ... -2.65625 1.52344 -5.46875]\n",
            "  ...\n",
            "  [-2.42188 -0.546875 -7.5625 ... -0.363281 3.71875 3]\n",
            "  [0.574219 -5.03125 -7.4375 ... 1.35156 4.59375 -0.820312]\n",
            "  [-8.75 -5.96875 -9.1875 ... -2.21875 -2.42188 -8.5625]]\n",
            "\n",
            " [[-1.78125 5.3125 -3.96875 ... -2.1875 -4.1875 -0.217773]\n",
            "  [3.48438 4.3125 -4.59375 ... -1.35938 2.46875 3.20312]\n",
            "  [-0.546875 2.625 -6 ... -0.117188 -0.597656 -4.6875]\n",
            "  ...\n",
            "  [4.40625 7.625 -14 ... -0.683594 1.1875 2.1875]\n",
            "  [-9.75 0.796875 -12.1875 ... -6.0625 -5.09375 -3.9375]\n",
            "  [-11.75 1.80469 -8.8125 ... -4.75 -1.39844 -4.125]]\n",
            "\n",
            " [[-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.949219 -3.48438 -6.59375 ... -2.65625 1.52344 -5.46875]\n",
            "  ...\n",
            "  [-0.621094 -6.46875 -12.0625 ... 1.76562 -10.375 -0.449219]\n",
            "  [-8.375 -6.71875 -8.75 ... -2.10938 -5.09375 -5.46875]\n",
            "  [-8.125 -10.875 -13.0625 ... -4.5 -7.03125 -4.96875]]\n",
            "\n",
            " [[-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [-0.949219 -3.48438 -6.59375 ... -2.65625 1.52344 -5.46875]\n",
            "  ...\n",
            "  [-1.38281 2.35938 -4.5 ... 0.253906 -3.0625 -2.28125]\n",
            "  [-2.65625 0.957031 -10.0625 ... -0.242188 -2.17188 -5]\n",
            "  [-8.5 -9.125 -13.9375 ... -4.25 -6.375 -6.625]]\n",
            "\n",
            " [[-0.800781 -3.53125 -6.59375 ... -2.65625 1.53125 -5.46875]\n",
            "  [3.03125 1.89062 -6.21875 ... -0.738281 2.39062 -1.42969]\n",
            "  [0.550781 0.216797 -6.78125 ... -0.804688 0.177734 -6.1875]\n",
            "  ...\n",
            "  [-3.67188 2.75 -9.875 ... 3.03125 -0.859375 1.21875]\n",
            "  [-1.69531 -1.38281 -13.25 ... 4.28125 2.78125 -1.35938]\n",
            "  [-9.3125 -6.0625 -13.0625 ... -4.3125 -7.125 -8.1875]]]\n",
            "lnx=[[[-0.142578 -0.628906 -1.17188 ... -0.472656 0.273438 -0.972656]\n",
            "  [-0.142578 -0.628906 -1.17188 ... -0.472656 0.273438 -0.972656]\n",
            "  [-0.168945 -0.621094 -1.17188 ... -0.472656 0.271484 -0.972656]\n",
            "  ...\n",
            "  [-0.535156 -0.121094 -1.67188 ... -0.0805664 0.824219 0.664062]\n",
            "  [0.132812 -1.16406 -1.71875 ... 0.3125 1.0625 -0.189453]\n",
            "  [-1.38281 -0.941406 -1.45312 ... -0.349609 -0.382812 -1.35156]]\n",
            "\n",
            " [[-0.349609 1.03906 -0.777344 ... -0.427734 -0.820312 -0.0427246]\n",
            "  [0.792969 0.984375 -1.04688 ... -0.310547 0.5625 0.730469]\n",
            "  [-0.12793 0.613281 -1.40625 ... -0.0274658 -0.139648 -1.10156]\n",
            "  ...\n",
            "  [1.15625 1.99219 -3.65625 ... -0.178711 0.310547 0.574219]\n",
            "  [-1.57031 0.128906 -1.96875 ... -0.976562 -0.820312 -0.636719]\n",
            "  [-2.20312 0.337891 -1.64844 ... -0.890625 -0.261719 -0.773438]]\n",
            "\n",
            " [[-0.142578 -0.628906 -1.17188 ... -0.472656 0.273438 -0.972656]\n",
            "  [-0.142578 -0.628906 -1.17188 ... -0.472656 0.273438 -0.972656]\n",
            "  [-0.168945 -0.621094 -1.17188 ... -0.472656 0.271484 -0.972656]\n",
            "  ...\n",
            "  [-0.121094 -1.26562 -2.35938 ... 0.34375 -2.03125 -0.0874023]\n",
            "  [-1.52344 -1.21875 -1.58594 ... -0.382812 -0.925781 -0.992188]\n",
            "  [-1.07031 -1.42969 -1.71875 ... -0.589844 -0.925781 -0.652344]]\n",
            "\n",
            " [[-0.142578 -0.628906 -1.17188 ... -0.472656 0.273438 -0.972656]\n",
            "  [-0.142578 -0.628906 -1.17188 ... -0.472656 0.273438 -0.972656]\n",
            "  [-0.168945 -0.621094 -1.17188 ... -0.472656 0.271484 -0.972656]\n",
            "  ...\n",
            "  [-0.326172 0.558594 -1.0625 ... 0.0600586 -0.722656 -0.539062]\n",
            "  [-0.589844 0.212891 -2.23438 ... -0.0537109 -0.482422 -1.10938]\n",
            "  [-1.15625 -1.24219 -1.89844 ... -0.578125 -0.867188 -0.902344]]\n",
            "\n",
            " [[-0.142578 -0.628906 -1.17188 ... -0.472656 0.273438 -0.972656]\n",
            "  [0.636719 0.396484 -1.30469 ... -0.155273 0.503906 -0.300781]\n",
            "  [0.121094 0.0476074 -1.49219 ... -0.176758 0.0390625 -1.35938]\n",
            "  ...\n",
            "  [-0.925781 0.695312 -2.48438 ... 0.765625 -0.216797 0.306641]\n",
            "  [-0.443359 -0.361328 -3.46875 ... 1.125 0.726562 -0.355469]\n",
            "  [-1.4375 -0.933594 -2.01562 ... -0.664062 -1.10156 -1.25781]]]\n",
            "attention_lnx=[[[0.648438 -1.96094 -0.408203 ... 0.386719 -2.42188 0.960938]\n",
            "  [0.648438 -1.96094 -0.408203 ... 0.386719 -2.42188 0.960938]\n",
            "  [0.644531 -1.96094 -0.408203 ... 0.378906 -2.42188 0.96875]\n",
            "  ...\n",
            "  [0.0269775 -1.84375 -0.408203 ... -0.097168 -2.40625 1.53125]\n",
            "  [0.00811768 -1.89062 -0.365234 ... -0.100586 -2.35938 1.54688]\n",
            "  [-0.0284424 -2.10938 -0.359375 ... 0.00613403 -2.42188 1.39844]]\n",
            "\n",
            " [[0.109863 -1.42188 0.523438 ... -0.116699 -0.165039 0.181641]\n",
            "  [-0.84375 -0.910156 -0.0957031 ... -0.214844 -0.291016 1.01562]\n",
            "  [-1.25781 -1.23438 -0.0571289 ... -0.34375 0.146484 0.890625]\n",
            "  ...\n",
            "  [-0.40625 -0.820312 -0.664062 ... -0.222656 -0.625 1.85156]\n",
            "  [0.304688 -1.07812 -0.933594 ... -0.320312 -0.855469 1.82812]\n",
            "  [0.230469 -0.925781 -0.820312 ... -0.119629 -0.984375 1.89844]]\n",
            "\n",
            " [[0.648438 -1.96094 -0.408203 ... 0.386719 -2.42188 0.960938]\n",
            "  [0.648438 -1.96094 -0.408203 ... 0.386719 -2.42188 0.960938]\n",
            "  [0.644531 -1.96094 -0.408203 ... 0.378906 -2.42188 0.96875]\n",
            "  ...\n",
            "  [0.0581055 -1.9375 -0.211914 ... 0.298828 -1.64062 1.6875]\n",
            "  [-0.207031 -2.125 -0.175781 ... 0.289062 -1.625 1.92969]\n",
            "  [-0.138672 -1.86719 -0.24707 ... 0.199219 -1.76562 1.75]]\n",
            "\n",
            " [[0.648438 -1.96094 -0.408203 ... 0.386719 -2.42188 0.960938]\n",
            "  [0.648438 -1.96094 -0.408203 ... 0.386719 -2.42188 0.960938]\n",
            "  [0.644531 -1.96094 -0.408203 ... 0.378906 -2.42188 0.96875]\n",
            "  ...\n",
            "  [-0.057373 -1.64844 0.0534668 ... 0.129883 -1.49219 1.58594]\n",
            "  [-0.155273 -1.70312 -0.00156403 ... 0.24707 -1.50781 1.61719]\n",
            "  [-0.318359 -1.64062 -0.185547 ... 0.335938 -1.5625 1.70312]]\n",
            "\n",
            " [[0.648438 -1.96094 -0.408203 ... 0.386719 -2.42188 0.960938]\n",
            "  [0.617188 -1.77344 -0.138672 ... 0.306641 -2.09375 1.34375]\n",
            "  [0.316406 -2.15625 -0.0339355 ... 0.119629 -1.67969 0.867188]\n",
            "  ...\n",
            "  [-0.75 -1.71875 0.0708008 ... 0.259766 -1.16406 2.14062]\n",
            "  [-0.808594 -1.74219 0.171875 ... 0.0927734 -1.39062 2.39062]\n",
            "  [-0.679688 -1.4375 -0.18457 ... 0.129883 -1.34375 2.01562]]]\n",
            "attn_output=[[[-0.0262451 -0.945312 -1.21094 ... -0.390625 -0.15332 -0.777344]\n",
            "  [-0.0262451 -0.945312 -1.21094 ... -0.390625 -0.15332 -0.777344]\n",
            "  [-0.0524902 -0.9375 -1.21094 ... -0.392578 -0.155273 -0.777344]\n",
            "  ...\n",
            "  [-0.515625 -0.515625 -1.71094 ... -0.0991211 0.283203 0.972656]\n",
            "  [0.130859 -1.54688 -1.75 ... 0.279297 0.5 0.163086]\n",
            "  [-1.34375 -1.24219 -1.46094 ... -0.339844 -0.742188 -1.10156]]\n",
            "\n",
            " [[-0.322266 0.75 -0.664062 ... -0.445312 -0.839844 -0.00698853]\n",
            "  [0.589844 0.761719 -1.04688 ... -0.351562 0.486328 0.941406]\n",
            "  [-0.416016 0.320312 -1.39844 ... -0.106445 -0.104004 -0.875]\n",
            "  ...\n",
            "  [1.02344 1.73438 -3.73438 ... -0.231445 0.143555 1.03125]\n",
            "  [-1.49219 -0.0444336 -2.0625 ... -1.00781 -0.9375 -0.332031]\n",
            "  [-2.09375 0.160156 -1.75781 ... -0.886719 -0.433594 -0.40625]]\n",
            "\n",
            " [[-0.0262451 -0.945312 -1.21094 ... -0.390625 -0.15332 -0.777344]\n",
            "  [-0.0262451 -0.945312 -1.21094 ... -0.390625 -0.15332 -0.777344]\n",
            "  [-0.0524902 -0.9375 -1.21094 ... -0.392578 -0.155273 -0.777344]\n",
            "  ...\n",
            "  [-0.107422 -1.60156 -2.34375 ... 0.392578 -2.28125 0.236328]\n",
            "  [-1.51562 -1.5625 -1.57812 ... -0.322266 -1.1875 -0.625]\n",
            "  [-1.0625 -1.63281 -1.71094 ... -0.550781 -1.13281 -0.414062]]\n",
            "\n",
            " [[-0.0262451 -0.945312 -1.21094 ... -0.390625 -0.15332 -0.777344]\n",
            "  [-0.0262451 -0.945312 -1.21094 ... -0.390625 -0.15332 -0.777344]\n",
            "  [-0.0524902 -0.9375 -1.21094 ... -0.392578 -0.155273 -0.777344]\n",
            "  ...\n",
            "  [-0.333984 0.164062 -1.03125 ... 0.0888672 -1.05469 -0.161133]\n",
            "  [-0.609375 -0.161133 -2.17188 ... 0.00105286 -0.796875 -0.730469]\n",
            "  [-1.17188 -1.42969 -1.88281 ... -0.519531 -1.05469 -0.65625]]\n",
            "\n",
            " [[-0.0262451 -0.945312 -1.21094 ... -0.390625 -0.15332 -0.777344]\n",
            "  [0.742188 0.0239258 -1.29688 ... -0.0878906 0.0605469 -0.0174561]\n",
            "  [0.185547 -0.416016 -1.46094 ... -0.146484 -0.322266 -1.14062]\n",
            "  ...\n",
            "  [-1.08594 0.253906 -2.40625 ... 0.808594 -0.496094 0.824219]\n",
            "  [-0.640625 -0.800781 -3.34375 ... 1.11719 0.355469 0.263672]\n",
            "  [-1.5 -1.125 -1.99219 ... -0.628906 -1.27344 -0.925781]]]\n",
            "next_layer_addition_dropped_out=[[[0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.109375 -7.53125 -7.875 ... -4.125 -0.660156 -4.5625]\n",
            "  ...\n",
            "  [-2.17188 -4.25 -7.8125 ... -1.04688 1.60938 5.15625]\n",
            "  [1.10938 -8.375 -8.0625 ... 0.800781 3.03125 -0.0703125]\n",
            "  [-9.8125 -9.8125 -10.375 ... -4.75 -5.75 -7.34375]]\n",
            "\n",
            " [[-2.5625 3.21875 -4.75 ... -3.90625 -4.25 -1.8125]\n",
            "  [2.1875 2.60938 -4.375 ... -1.27344 1.96875 4.375]\n",
            "  [-1.40625 0.488281 -6.21875 ... -0.726562 -0.625 -3.75]\n",
            "  ...\n",
            "  [4.03125 6.84375 -14.625 ... -0.515625 0.177734 3.15625]\n",
            "  [-10.4375 -1.03125 -13.5625 ... -8.625 -6.03125 -3.32812]\n",
            "  [-14.0625 0.0625 -9.75 ... -6.71875 -2.875 -3.65625]]\n",
            "\n",
            " [[0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.109375 -7.53125 -7.875 ... -4.125 -0.660156 -4.5625]\n",
            "  ...\n",
            "  [0.398438 -9.25 -11.6875 ... 0.171875 -11.9375 0.171875]\n",
            "  [-10.3125 -11.375 -9.1875 ... -4.96875 -6.84375 -4.1875]\n",
            "  [-9.625 -14.125 -13.8125 ... -7.75 -10.375 -3.40625]]\n",
            "\n",
            " [[0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.109375 -7.53125 -7.875 ... -4.125 -0.660156 -4.5625]\n",
            "  ...\n",
            "  [-1.73438 -0.875 -4.5625 ... 0.19043 -5.1875 -0.929688]\n",
            "  [-2.09375 -1.875 -9.9375 ... -0.847656 -4.3125 -3.26562]\n",
            "  [-10.4375 -12.375 -14.75 ... -7.1875 -9.4375 -5.6875]]\n",
            "\n",
            " [[0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [2.8125 -1.60156 -7.0625 ... -2.57812 1.40625 0.263672]\n",
            "  [0.9375 -2.8125 -6.375 ... -2.09375 -1.33594 -5]\n",
            "  ...\n",
            "  [-4.25 0.359375 -8.875 ... 3.76562 -0.679688 2.125]\n",
            "  [-2.625 -4.25 -11.3125 ... 3.8125 1.73438 -0.132812]\n",
            "  [-11.25 -9.125 -12.125 ... -6.625 -10.1875 -7.15625]]]\n",
            "inputs=[[[0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.109375 -7.53125 -7.875 ... -4.125 -0.660156 -4.5625]\n",
            "  ...\n",
            "  [-2.17188 -4.25 -7.8125 ... -1.04688 1.60938 5.15625]\n",
            "  [1.10938 -8.375 -8.0625 ... 0.800781 3.03125 -0.0703125]\n",
            "  [-9.8125 -9.8125 -10.375 ... -4.75 -5.75 -7.34375]]\n",
            "\n",
            " [[-2.5625 3.21875 -4.75 ... -3.90625 -4.25 -1.8125]\n",
            "  [2.1875 2.60938 -4.375 ... -1.27344 1.96875 4.375]\n",
            "  [-1.40625 0.488281 -6.21875 ... -0.726562 -0.625 -3.75]\n",
            "  ...\n",
            "  [4.03125 6.84375 -14.625 ... -0.515625 0.177734 3.15625]\n",
            "  [-10.4375 -1.03125 -13.5625 ... -8.625 -6.03125 -3.32812]\n",
            "  [-14.0625 0.0625 -9.75 ... -6.71875 -2.875 -3.65625]]\n",
            "\n",
            " [[0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.109375 -7.53125 -7.875 ... -4.125 -0.660156 -4.5625]\n",
            "  ...\n",
            "  [0.398438 -9.25 -11.6875 ... 0.171875 -11.9375 0.171875]\n",
            "  [-10.3125 -11.375 -9.1875 ... -4.96875 -6.84375 -4.1875]\n",
            "  [-9.625 -14.125 -13.8125 ... -7.75 -10.375 -3.40625]]\n",
            "\n",
            " [[0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [0.109375 -7.53125 -7.875 ... -4.125 -0.660156 -4.5625]\n",
            "  ...\n",
            "  [-1.73438 -0.875 -4.5625 ... 0.19043 -5.1875 -0.929688]\n",
            "  [-2.09375 -1.875 -9.9375 ... -0.847656 -4.3125 -3.26562]\n",
            "  [-10.4375 -12.375 -14.75 ... -7.1875 -9.4375 -5.6875]]\n",
            "\n",
            " [[0.248047 -7.59375 -7.875 ... -4.09375 -0.660156 -4.5625]\n",
            "  [2.8125 -1.60156 -7.0625 ... -2.57812 1.40625 0.263672]\n",
            "  [0.9375 -2.8125 -6.375 ... -2.09375 -1.33594 -5]\n",
            "  ...\n",
            "  [-4.25 0.359375 -8.875 ... 3.76562 -0.679688 2.125]\n",
            "  [-2.625 -4.25 -11.3125 ... 3.8125 1.73438 -0.132812]\n",
            "  [-11.25 -9.125 -12.125 ... -6.625 -10.1875 -7.15625]]]\n",
            "lnx=[[[0.0407715 -1.25 -1.29688 ... -0.671875 -0.108398 -0.75]\n",
            "  [0.0407715 -1.25 -1.29688 ... -0.671875 -0.108398 -0.75]\n",
            "  [0.0179443 -1.23438 -1.29688 ... -0.675781 -0.108398 -0.75]\n",
            "  ...\n",
            "  [-0.455078 -0.890625 -1.64062 ... -0.219727 0.337891 1.07812]\n",
            "  [0.241211 -1.82031 -1.75 ... 0.173828 0.660156 -0.0152588]\n",
            "  [-1.36719 -1.36719 -1.44531 ... -0.660156 -0.800781 -1.02344]]\n",
            "\n",
            " [[-0.484375 0.609375 -0.898438 ... -0.738281 -0.804688 -0.341797]\n",
            "  [0.484375 0.578125 -0.96875 ... -0.28125 0.435547 0.96875]\n",
            "  [-0.322266 0.111816 -1.42188 ... -0.166016 -0.142578 -0.859375]\n",
            "  ...\n",
            "  [1 1.70312 -3.64062 ... -0.12793 0.0441895 0.785156]\n",
            "  [-1.44531 -0.142578 -1.88281 ... -1.19531 -0.835938 -0.460938]\n",
            "  [-2.28125 0.0101318 -1.58594 ... -1.09375 -0.466797 -0.59375]]\n",
            "\n",
            " [[0.0407715 -1.25 -1.29688 ... -0.671875 -0.108398 -0.75]\n",
            "  [0.0407715 -1.25 -1.29688 ... -0.671875 -0.108398 -0.75]\n",
            "  [0.0179443 -1.23438 -1.29688 ... -0.675781 -0.108398 -0.75]\n",
            "  ...\n",
            "  [0.0703125 -1.625 -2.0625 ... 0.0302734 -2.09375 0.0302734]\n",
            "  [-1.64062 -1.80469 -1.46094 ... -0.789062 -1.08594 -0.664062]\n",
            "  [-1.10156 -1.60938 -1.57812 ... -0.886719 -1.1875 -0.388672]]\n",
            "\n",
            " [[0.0407715 -1.25 -1.29688 ... -0.671875 -0.108398 -0.75]\n",
            "  [0.0407715 -1.25 -1.29688 ... -0.671875 -0.108398 -0.75]\n",
            "  [0.0179443 -1.23438 -1.29688 ... -0.675781 -0.108398 -0.75]\n",
            "  ...\n",
            "  [-0.388672 -0.196289 -1.02344 ... 0.0427246 -1.16406 -0.208984]\n",
            "  [-0.427734 -0.382812 -2.03125 ... -0.172852 -0.878906 -0.667969]\n",
            "  [-1.23438 -1.46094 -1.74219 ... -0.851562 -1.11719 -0.671875]]\n",
            "\n",
            " [[0.0407715 -1.25 -1.29688 ... -0.671875 -0.108398 -0.75]\n",
            "  [0.554688 -0.314453 -1.39062 ... -0.507812 0.277344 0.0517578]\n",
            "  [0.195312 -0.585938 -1.32812 ... -0.435547 -0.277344 -1.03906]\n",
            "  ...\n",
            "  [-1.00781 0.0854492 -2.10938 ... 0.894531 -0.161133 0.503906]\n",
            "  [-0.660156 -1.0625 -2.84375 ... 0.957031 0.435547 -0.0332031]\n",
            "  [-1.5 -1.21094 -1.61719 ... -0.882812 -1.35938 -0.953125]]]\n",
            "attention_lnx=[[[-1.35156 -0.710938 -0.490234 ... 0.065918 -1.99219 -1.60938]\n",
            "  [-1.35156 -0.710938 -0.490234 ... 0.065918 -1.99219 -1.60938]\n",
            "  [-1.35156 -0.714844 -0.490234 ... 0.0644531 -1.99219 -1.60938]\n",
            "  ...\n",
            "  [-1.25 -0.65625 -0.460938 ... -0.304688 -1.74219 -1.42969]\n",
            "  [-1.11719 -0.730469 -0.332031 ... -0.296875 -1.76562 -1.54688]\n",
            "  [-1.14062 -0.617188 -0.34375 ... -0.722656 -1.55469 -1.65625]]\n",
            "\n",
            " [[-1.07812 -2.60938 0.570312 ... 1.17188 0.0854492 -2.0625]\n",
            "  [-0.761719 -1.40625 0.4375 ... 0.808594 0.601562 -1.55469]\n",
            "  [-0.917969 -1.02344 0.671875 ... 0.902344 0.129883 -1.51562]\n",
            "  ...\n",
            "  [-0.163086 -1.28125 0.287109 ... -0.425781 -0.271484 -2.23438]\n",
            "  [-0.25 -1.38281 0.116699 ... -0.921875 0.0825195 -2.46875]\n",
            "  [-0.259766 -1.17188 0.230469 ... -0.65625 0.164062 -1.91406]]\n",
            "\n",
            " [[-1.35156 -0.710938 -0.490234 ... 0.065918 -1.99219 -1.60938]\n",
            "  [-1.35156 -0.710938 -0.490234 ... 0.065918 -1.99219 -1.60938]\n",
            "  [-1.35156 -0.714844 -0.490234 ... 0.0644531 -1.99219 -1.60938]\n",
            "  ...\n",
            "  [-0.875 -0.960938 -0.458984 ... -0.871094 -1.29688 -1.92188]\n",
            "  [-0.597656 -1.14844 -0.695312 ... -1.10938 -1.20312 -1.99219]\n",
            "  [-0.933594 -1.14844 -0.773438 ... -1.38281 -1.15625 -1.89844]]\n",
            "\n",
            " [[-1.35156 -0.710938 -0.490234 ... 0.065918 -1.99219 -1.60938]\n",
            "  [-1.35156 -0.710938 -0.490234 ... 0.065918 -1.99219 -1.60938]\n",
            "  [-1.35156 -0.714844 -0.490234 ... 0.0644531 -1.99219 -1.60938]\n",
            "  ...\n",
            "  [-0.464844 -0.667969 -0.246094 ... -0.941406 -1.0625 -1.5625]\n",
            "  [-0.625 -0.863281 -0.349609 ... -0.960938 -1.28125 -1.80469]\n",
            "  [-0.65625 -0.925781 -0.523438 ... -1.28125 -1.1875 -1.85156]]\n",
            "\n",
            " [[-1.35156 -0.710938 -0.490234 ... 0.065918 -1.99219 -1.60938]\n",
            "  [-1.28125 -1.40625 -0.144531 ... 0.527344 -1.6875 -2.3125]\n",
            "  [-1.25 -1.24219 -0.324219 ... 0.304688 -1.60938 -2.15625]\n",
            "  ...\n",
            "  [0.00909424 -0.753906 -0.675781 ... -1.04688 -1.02344 -2.17188]\n",
            "  [-0.0786133 -0.90625 -0.734375 ... -1.08594 -0.886719 -2.01562]\n",
            "  [-0.5625 -1.07031 -0.910156 ... -1.32812 -0.78125 -1.75781]]]\n",
            "attn_output=[[[-0.177734 -1.34375 -1.35156 ... -0.648438 -0.427734 -0.996094]\n",
            "  [-0.177734 -1.34375 -1.35156 ... -0.648438 -0.427734 -0.996094]\n",
            "  [-0.200195 -1.32812 -1.35156 ... -0.65625 -0.427734 -0.996094]\n",
            "  ...\n",
            "  [-0.703125 -1.00781 -1.70312 ... -0.277344 -0.0273438 0.765625]\n",
            "  [-0.00167084 -1.94531 -1.78906 ... 0.107422 0.269531 -0.345703]\n",
            "  [-1.49219 -1.42188 -1.46094 ... -0.746094 -0.996094 -1.22656]]\n",
            "\n",
            " [[-0.667969 0.112305 -0.769531 ... -0.503906 -0.765625 -0.710938]\n",
            "  [0.306641 0.259766 -0.847656 ... -0.100098 0.554688 0.605469]\n",
            "  [-0.519531 -0.119629 -1.24219 ... 0.0393066 -0.11084 -1.17969]\n",
            "  ...\n",
            "  [0.945312 1.35938 -3.51562 ... -0.230469 -0.0229492 0.225586]\n",
            "  [-1.44531 -0.326172 -1.8125 ... -1.28906 -0.804688 -0.78125]\n",
            "  [-2.26562 -0.174805 -1.5 ... -1.16406 -0.427734 -0.878906]]\n",
            "\n",
            " [[-0.177734 -1.34375 -1.35156 ... -0.648438 -0.427734 -0.996094]\n",
            "  [-0.177734 -1.34375 -1.35156 ... -0.648438 -0.427734 -0.996094]\n",
            "  [-0.200195 -1.32812 -1.35156 ... -0.65625 -0.427734 -0.996094]\n",
            "  ...\n",
            "  [-0.081543 -1.75 -2.07812 ... -0.120117 -2.26562 -0.300781]\n",
            "  [-1.6875 -1.9375 -1.53125 ... -0.941406 -1.24219 -0.957031]\n",
            "  [-1.17969 -1.70312 -1.625 ... -1.01562 -1.28906 -0.59375]]\n",
            "\n",
            " [[-0.177734 -1.34375 -1.35156 ... -0.648438 -0.427734 -0.996094]\n",
            "  [-0.177734 -1.34375 -1.35156 ... -0.648438 -0.427734 -0.996094]\n",
            "  [-0.200195 -1.32812 -1.35156 ... -0.65625 -0.427734 -0.996094]\n",
            "  ...\n",
            "  [-0.484375 -0.339844 -1.0625 ... -0.165039 -1.375 -0.550781]\n",
            "  [-0.539062 -0.542969 -2.04688 ... -0.359375 -1.10938 -1.00781]\n",
            "  [-1.28125 -1.53906 -1.76562 ... -0.980469 -1.22656 -0.871094]]\n",
            "\n",
            " [[-0.177734 -1.34375 -1.35156 ... -0.648438 -0.427734 -0.996094]\n",
            "  [0.294922 -0.582031 -1.39062 ... -0.396484 -0.0541992 -0.396484]\n",
            "  [-0.0639648 -0.828125 -1.36719 ... -0.365234 -0.601562 -1.46094]\n",
            "  ...\n",
            "  [-0.984375 -0.0913086 -2.21875 ... 0.628906 -0.394531 -0.0108643]\n",
            "  [-0.667969 -1.27344 -2.96875 ... 0.671875 0.208984 -0.53125]\n",
            "  [-1.53906 -1.32812 -1.69531 ... -1.03125 -1.42969 -1.15625]]]\n",
            "next_layer_addition_dropped_out=[[[-1.41406 -8.1875 -8.8125 ... -3.90625 -2.3125 -6.84375]\n",
            "  [-1.41406 -8.1875 -8.8125 ... -3.90625 -2.3125 -6.84375]\n",
            "  [-1.54688 -8.125 -8.875 ... -3.95312 -2.28125 -6.84375]\n",
            "  ...\n",
            "  [-2.46875 -5.3125 -8.6875 ... -0.425781 -0.289062 3.28125]\n",
            "  [0.0776367 -9.125 -8.4375 ... 1.75 2.40625 -2.3125]\n",
            "  [-9.5625 -10.1875 -12.0625 ... -6.25 -6.5 -9.875]]\n",
            "\n",
            " [[-3.04688 0.9375 -4.3125 ... -4 -3.64062 -4.875]\n",
            "  [1.08594 1.73438 -4.1875 ... -0.617188 2.78125 2.54688]\n",
            "  [-2.39062 -0.625 -5.28125 ... -0.464844 -0.138672 -5.34375]\n",
            "  ...\n",
            "  [2.51562 6.34375 -13.4375 ... -2.10938 -0.103027 1.17969]\n",
            "  [-9.5625 -2.3125 -15.375 ... -11.375 -7.09375 -6.6875]\n",
            "  [-14.625 -0.5625 -9.0625 ... -9.375 -2.89062 -4.625]]\n",
            "\n",
            " [[-1.41406 -8.1875 -8.8125 ... -3.90625 -2.3125 -6.84375]\n",
            "  [-1.41406 -8.1875 -8.8125 ... -3.90625 -2.3125 -6.84375]\n",
            "  [-1.54688 -8.125 -8.875 ... -3.95312 -2.28125 -6.84375]\n",
            "  ...\n",
            "  [-0.152344 -10.0625 -12.8125 ... -0.828125 -12.5625 -3.46875]\n",
            "  [-9.8125 -11.3125 -10.25 ... -6.96875 -9 -7.75]\n",
            "  [-9.375 -14.75 -15.75 ... -10.875 -11.4375 -6.53125]]\n",
            "\n",
            " [[-1.41406 -8.1875 -8.8125 ... -3.90625 -2.3125 -6.84375]\n",
            "  [-1.41406 -8.1875 -8.8125 ... -3.90625 -2.3125 -6.84375]\n",
            "  [-1.54688 -8.125 -8.875 ... -3.95312 -2.28125 -6.84375]\n",
            "  ...\n",
            "  [-1.83594 -1.48438 -4.78125 ... -0.953125 -5.8125 -3.65625]\n",
            "  [-1.67188 -3.4375 -11.375 ... -1.98438 -5.3125 -4.34375]\n",
            "  [-10.125 -12.75 -16 ... -9.875 -10.1875 -8.375]]\n",
            "\n",
            " [[-1.41406 -8.1875 -8.8125 ... -3.90625 -2.3125 -6.84375]\n",
            "  [2.5625 -2.15625 -7.65625 ... -1.25 0 -3.28125]\n",
            "  [-0.460938 -4.09375 -7.46875 ... -0.28125 -2.98438 -6.625]\n",
            "  ...\n",
            "  [-4.875 -0.482422 -10.1875 ... 2.85938 -2.57812 -0.357422]\n",
            "  [-4.0625 -5.9375 -12.1875 ... 1.61719 0.585938 -1.94531]\n",
            "  [-11.5 -9.5 -13.375 ... -10.0625 -11.0625 -8.9375]]]\n",
            "inputs=[[[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]]\n",
            "lnx=[[[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]]\n",
            "attention_lnx=[[[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]]\n",
            "attn_output=[[[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]]\n",
            "next_layer_addition_dropped_out=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "inputs=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "lnx=[[[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]]\n",
            "attention_lnx=[[[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]]\n",
            "attn_output=[[[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]]\n",
            "next_layer_addition_dropped_out=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "inputs=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "lnx=[[[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]]\n",
            "attention_lnx=[[[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]]\n",
            "attn_output=[[[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]]\n",
            "next_layer_addition_dropped_out=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "inputs=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "lnx=[[[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]]\n",
            "attention_lnx=[[[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]]\n",
            "attn_output=[[[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]]\n",
            "next_layer_addition_dropped_out=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "inputs=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "lnx=[[[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]]\n",
            "attention_lnx=[[[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]]\n",
            "attn_output=[[[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]]\n",
            "next_layer_addition_dropped_out=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "inputs=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "lnx=[[[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]]\n",
            "attention_lnx=[[[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]]\n",
            "attn_output=[[[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]]\n",
            "next_layer_addition_dropped_out=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "inputs=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "lnx=[[[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]]\n",
            "attention_lnx=[[[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]]\n",
            "attn_output=[[[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]]\n",
            "next_layer_addition_dropped_out=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "inputs=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "lnx=[[[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]]\n",
            "attention_lnx=[[[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]]\n",
            "attn_output=[[[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]]\n",
            "next_layer_addition_dropped_out=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "inputs=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "lnx=[[[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]]\n",
            "attention_lnx=[[[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]]\n",
            "attn_output=[[[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]]\n",
            "next_layer_addition_dropped_out=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "inputs=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "lnx=[[[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]]\n",
            "attention_lnx=[[[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]]\n",
            "attn_output=[[[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]]\n",
            "next_layer_addition_dropped_out=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "inputs=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "lnx=[[[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]]\n",
            "attention_lnx=[[[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]]\n",
            "attn_output=[[[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]]\n",
            "next_layer_addition_dropped_out=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "inputs=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "lnx=[[[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]]\n",
            "attention_lnx=[[[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]]\n",
            "attn_output=[[[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]]\n",
            "next_layer_addition_dropped_out=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "inputs=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "lnx=[[[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]]\n",
            "attention_lnx=[[[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]]\n",
            "attn_output=[[[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]]\n",
            "next_layer_addition_dropped_out=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "inputs=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "lnx=[[[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]]\n",
            "attention_lnx=[[[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]]\n",
            "attn_output=[[[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]]\n",
            "next_layer_addition_dropped_out=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "inputs=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "lnx=[[[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]]\n",
            "attention_lnx=[[[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]]\n",
            "attn_output=[[[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]]\n",
            "next_layer_addition_dropped_out=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "inputs=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "lnx=[[[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]]\n",
            "attention_lnx=[[[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]]\n",
            "attn_output=[[[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]]\n",
            "next_layer_addition_dropped_out=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "inputs=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "lnx=[[[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]]\n",
            "attention_lnx=[[[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]]\n",
            "attn_output=[[[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]]\n",
            "next_layer_addition_dropped_out=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "inputs=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "lnx=[[[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]]\n",
            "attention_lnx=[[[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]]\n",
            "attn_output=[[[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]]\n",
            "next_layer_addition_dropped_out=[[[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]]\n",
            "inputs=[[[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]]\n",
            "lnx=[[[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]]\n",
            "attention_lnx=[[[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]]\n",
            "attn_output=[[[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]]\n",
            "next_layer_addition_dropped_out=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "inputs=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "lnx=[[[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]]\n",
            "attention_lnx=[[[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]]\n",
            "attn_output=[[[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]]\n",
            "next_layer_addition_dropped_out=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "inputs=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "lnx=[[[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]]\n",
            "attention_lnx=[[[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]]\n",
            "attn_output=[[[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]]\n",
            "next_layer_addition_dropped_out=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "inputs=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "lnx=[[[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]]\n",
            "attention_lnx=[[[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]]\n",
            "attn_output=[[[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]]\n",
            "next_layer_addition_dropped_out=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "inputs=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "lnx=[[[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]]\n",
            "attention_lnx=[[[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]]\n",
            "attn_output=[[[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]]\n",
            "next_layer_addition_dropped_out=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "inputs=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "lnx=[[[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]]\n",
            "attention_lnx=[[[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]]\n",
            "attn_output=[[[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]]\n",
            "next_layer_addition_dropped_out=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "inputs=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "lnx=[[[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]]\n",
            "attention_lnx=[[[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]]\n",
            "attn_output=[[[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]]\n",
            "next_layer_addition_dropped_out=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "inputs=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "lnx=[[[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]]\n",
            "attention_lnx=[[[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]]\n",
            "attn_output=[[[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]]\n",
            "next_layer_addition_dropped_out=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "inputs=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "lnx=[[[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]]\n",
            "attention_lnx=[[[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]]\n",
            "attn_output=[[[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]]\n",
            "next_layer_addition_dropped_out=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "inputs=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "lnx=[[[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]]\n",
            "attention_lnx=[[[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]]\n",
            "attn_output=[[[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]]\n",
            "next_layer_addition_dropped_out=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "inputs=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "lnx=[[[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]]\n",
            "attention_lnx=[[[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]]\n",
            "attn_output=[[[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]]\n",
            "next_layer_addition_dropped_out=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "inputs=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "lnx=[[[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]]\n",
            "attention_lnx=[[[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]]\n",
            "attn_output=[[[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]]\n",
            "next_layer_addition_dropped_out=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "inputs=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "lnx=[[[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]]\n",
            "attention_lnx=[[[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]]\n",
            "attn_output=[[[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]]\n",
            "next_layer_addition_dropped_out=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "inputs=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "lnx=[[[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]]\n",
            "attention_lnx=[[[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]]\n",
            "attn_output=[[[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]]\n",
            "next_layer_addition_dropped_out=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "inputs=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "lnx=[[[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]]\n",
            "attention_lnx=[[[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]]\n",
            "attn_output=[[[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]]\n",
            "next_layer_addition_dropped_out=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "inputs=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "lnx=[[[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]]\n",
            "attention_lnx=[[[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]]\n",
            "attn_output=[[[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]]\n",
            "next_layer_addition_dropped_out=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "inputs=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "lnx=[[[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]]\n",
            "attention_lnx=[[[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]]\n",
            "attn_output=[[[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]]\n",
            "next_layer_addition_dropped_out=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "inputs=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "lnx=[[[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]]\n",
            "attention_lnx=[[[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]]\n",
            "attn_output=[[[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]]\n",
            "next_layer_addition_dropped_out=[[[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]]\n",
            "inputs=[[[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]]\n",
            "lnx=[[[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]]\n",
            "attention_lnx=[[[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]]\n",
            "attn_output=[[[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]]\n",
            "next_layer_addition_dropped_out=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "inputs=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "lnx=[[[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]]\n",
            "attention_lnx=[[[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]]\n",
            "attn_output=[[[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]]\n",
            "next_layer_addition_dropped_out=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "inputs=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "lnx=[[[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]]\n",
            "attention_lnx=[[[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]]\n",
            "attn_output=[[[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]]\n",
            "next_layer_addition_dropped_out=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "inputs=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "lnx=[[[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]]\n",
            "attention_lnx=[[[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]]\n",
            "attn_output=[[[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]]\n",
            "next_layer_addition_dropped_out=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "inputs=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "lnx=[[[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]]\n",
            "attention_lnx=[[[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]]\n",
            "attn_output=[[[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]]\n",
            "next_layer_addition_dropped_out=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "inputs=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "lnx=[[[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]]\n",
            "attention_lnx=[[[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]]\n",
            "attn_output=[[[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]]\n",
            "next_layer_addition_dropped_out=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "inputs=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "lnx=[[[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]]\n",
            "attention_lnx=[[[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]]\n",
            "attn_output=[[[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]]\n",
            "next_layer_addition_dropped_out=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "inputs=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "lnx=[[[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]]\n",
            "attention_lnx=[[[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]]\n",
            "attn_output=[[[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]]\n",
            "next_layer_addition_dropped_out=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "inputs=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "lnx=[[[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]]\n",
            "attention_lnx=[[[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]]\n",
            "attn_output=[[[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]]\n",
            "next_layer_addition_dropped_out=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "inputs=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "lnx=[[[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]]\n",
            "attention_lnx=[[[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]]\n",
            "attn_output=[[[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]]\n",
            "next_layer_addition_dropped_out=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "inputs=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "lnx=[[[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]]\n",
            "attention_lnx=[[[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]]\n",
            "attn_output=[[[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]]\n",
            "next_layer_addition_dropped_out=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "inputs=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "lnx=[[[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]]\n",
            "attention_lnx=[[[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]]\n",
            "attn_output=[[[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]]\n",
            "next_layer_addition_dropped_out=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "inputs=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "lnx=[[[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]]\n",
            "attention_lnx=[[[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]]\n",
            "attn_output=[[[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]]\n",
            "next_layer_addition_dropped_out=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "inputs=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "lnx=[[[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]]\n",
            "attention_lnx=[[[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]]\n",
            "attn_output=[[[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]]\n",
            "next_layer_addition_dropped_out=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "inputs=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "lnx=[[[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]]\n",
            "attention_lnx=[[[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]]\n",
            "attn_output=[[[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]]\n",
            "next_layer_addition_dropped_out=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "inputs=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "lnx=[[[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]]\n",
            "attention_lnx=[[[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]]\n",
            "attn_output=[[[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]]\n",
            "next_layer_addition_dropped_out=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "inputs=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "lnx=[[[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]]\n",
            "attention_lnx=[[[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]]\n",
            "attn_output=[[[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]]\n",
            "next_layer_addition_dropped_out=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "inputs=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "lnx=[[[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]]\n",
            "attention_lnx=[[[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]]\n",
            "attn_output=[[[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]]\n",
            "next_layer_addition_dropped_out=[[[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]]\n",
            "inputs=[[[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]]\n",
            "lnx=[[[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]]\n",
            "attention_lnx=[[[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]]\n",
            "attn_output=[[[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]]\n",
            "next_layer_addition_dropped_out=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "inputs=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "lnx=[[[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]]\n",
            "attention_lnx=[[[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]]\n",
            "attn_output=[[[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]]\n",
            "next_layer_addition_dropped_out=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "inputs=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "lnx=[[[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]]\n",
            "attention_lnx=[[[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]]\n",
            "attn_output=[[[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]]\n",
            "next_layer_addition_dropped_out=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "inputs=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "lnx=[[[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]]\n",
            "attention_lnx=[[[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]]\n",
            "attn_output=[[[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]]\n",
            "next_layer_addition_dropped_out=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "inputs=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "lnx=[[[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]]\n",
            "attention_lnx=[[[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]]\n",
            "attn_output=[[[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]]\n",
            "next_layer_addition_dropped_out=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "inputs=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "lnx=[[[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]]\n",
            "attention_lnx=[[[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]]\n",
            "attn_output=[[[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]]\n",
            "next_layer_addition_dropped_out=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "inputs=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "lnx=[[[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]]\n",
            "attention_lnx=[[[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]]\n",
            "attn_output=[[[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]]\n",
            "next_layer_addition_dropped_out=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "inputs=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "lnx=[[[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]]\n",
            "attention_lnx=[[[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]]\n",
            "attn_output=[[[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]]\n",
            "next_layer_addition_dropped_out=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "inputs=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "lnx=[[[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]]\n",
            "attention_lnx=[[[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]]\n",
            "attn_output=[[[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]]\n",
            "next_layer_addition_dropped_out=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "inputs=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "lnx=[[[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]]\n",
            "attention_lnx=[[[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]]\n",
            "attn_output=[[[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]]\n",
            "next_layer_addition_dropped_out=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "inputs=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "lnx=[[[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]]\n",
            "attention_lnx=[[[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]]\n",
            "attn_output=[[[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]]\n",
            "next_layer_addition_dropped_out=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "inputs=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "lnx=[[[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]]\n",
            "attention_lnx=[[[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]]\n",
            "attn_output=[[[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]]\n",
            "next_layer_addition_dropped_out=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "inputs=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "lnx=[[[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]]\n",
            "attention_lnx=[[[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]]\n",
            "attn_output=[[[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]]\n",
            "next_layer_addition_dropped_out=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "inputs=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "lnx=[[[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]]\n",
            "attention_lnx=[[[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]]\n",
            "attn_output=[[[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]]\n",
            "next_layer_addition_dropped_out=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "inputs=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "lnx=[[[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]]\n",
            "attention_lnx=[[[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]]\n",
            "attn_output=[[[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]]\n",
            "next_layer_addition_dropped_out=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "inputs=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "lnx=[[[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]]\n",
            "attention_lnx=[[[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]]\n",
            "attn_output=[[[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]]\n",
            "next_layer_addition_dropped_out=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "inputs=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "lnx=[[[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]]\n",
            "attention_lnx=[[[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]]\n",
            "attn_output=[[[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]]\n",
            "next_layer_addition_dropped_out=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "inputs=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "lnx=[[[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]]\n",
            "attention_lnx=[[[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]]\n",
            "attn_output=[[[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]]\n",
            "next_layer_addition_dropped_out=[[[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]]\n",
            "inputs=[[[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]]\n",
            "lnx=[[[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]]\n",
            "attention_lnx=[[[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]]\n",
            "attn_output=[[[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]]\n",
            "next_layer_addition_dropped_out=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "inputs=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "lnx=[[[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]]\n",
            "attention_lnx=[[[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]]\n",
            "attn_output=[[[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]]\n",
            "next_layer_addition_dropped_out=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "inputs=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "lnx=[[[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]]\n",
            "attention_lnx=[[[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]]\n",
            "attn_output=[[[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]]\n",
            "next_layer_addition_dropped_out=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "inputs=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "lnx=[[[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]]\n",
            "attention_lnx=[[[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]]\n",
            "attn_output=[[[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]]\n",
            "next_layer_addition_dropped_out=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "inputs=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "lnx=[[[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]]\n",
            "attention_lnx=[[[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]]\n",
            "attn_output=[[[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]]\n",
            "next_layer_addition_dropped_out=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "inputs=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "lnx=[[[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]]\n",
            "attention_lnx=[[[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]]\n",
            "attn_output=[[[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]]\n",
            "next_layer_addition_dropped_out=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "inputs=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "lnx=[[[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]]\n",
            "attention_lnx=[[[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]]\n",
            "attn_output=[[[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]]\n",
            "next_layer_addition_dropped_out=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "inputs=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "lnx=[[[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]]\n",
            "attention_lnx=[[[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]]\n",
            "attn_output=[[[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]]\n",
            "next_layer_addition_dropped_out=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "inputs=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "lnx=[[[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]]\n",
            "attention_lnx=[[[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]]\n",
            "attn_output=[[[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]]\n",
            "next_layer_addition_dropped_out=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "inputs=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "lnx=[[[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]]\n",
            "attention_lnx=[[[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]]\n",
            "attn_output=[[[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]]\n",
            "next_layer_addition_dropped_out=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "inputs=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "lnx=[[[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]]\n",
            "attention_lnx=[[[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]]\n",
            "attn_output=[[[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]]\n",
            "next_layer_addition_dropped_out=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "inputs=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "lnx=[[[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]]\n",
            "attention_lnx=[[[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]]\n",
            "attn_output=[[[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]]\n",
            "next_layer_addition_dropped_out=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "inputs=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "lnx=[[[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]]\n",
            "attention_lnx=[[[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]]\n",
            "attn_output=[[[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]]\n",
            "next_layer_addition_dropped_out=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "inputs=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "lnx=[[[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]]\n",
            "attention_lnx=[[[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]]\n",
            "attn_output=[[[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]]\n",
            "next_layer_addition_dropped_out=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "inputs=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "lnx=[[[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]]\n",
            "attention_lnx=[[[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]]\n",
            "attn_output=[[[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]]\n",
            "next_layer_addition_dropped_out=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "inputs=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "lnx=[[[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]]\n",
            "attention_lnx=[[[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]]\n",
            "attn_output=[[[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]]\n",
            "next_layer_addition_dropped_out=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "inputs=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "lnx=[[[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]]\n",
            "attention_lnx=[[[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]]\n",
            "attn_output=[[[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]]\n",
            "next_layer_addition_dropped_out=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "inputs=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "lnx=[[[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]]\n",
            "attention_lnx=[[[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]]\n",
            "attn_output=[[[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]]\n",
            "next_layer_addition_dropped_out=[[[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]]\n",
            "inputs=[[[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]]\n",
            "lnx=[[[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]]\n",
            "attention_lnx=[[[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]]\n",
            "attn_output=[[[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]]\n",
            "next_layer_addition_dropped_out=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "inputs=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "lnx=[[[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]]\n",
            "attention_lnx=[[[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]]\n",
            "attn_output=[[[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]]\n",
            "next_layer_addition_dropped_out=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "inputs=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "lnx=[[[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]]\n",
            "attention_lnx=[[[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]]\n",
            "attn_output=[[[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]]\n",
            "next_layer_addition_dropped_out=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "inputs=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "lnx=[[[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]]\n",
            "attention_lnx=[[[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]]\n",
            "attn_output=[[[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]]\n",
            "next_layer_addition_dropped_out=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "inputs=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "lnx=[[[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]]\n",
            "attention_lnx=[[[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]]\n",
            "attn_output=[[[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]]\n",
            "next_layer_addition_dropped_out=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "inputs=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "lnx=[[[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]]\n",
            "attention_lnx=[[[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]]\n",
            "attn_output=[[[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]]\n",
            "next_layer_addition_dropped_out=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "inputs=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "lnx=[[[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]]\n",
            "attention_lnx=[[[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]]\n",
            "attn_output=[[[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]]\n",
            "next_layer_addition_dropped_out=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "inputs=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "lnx=[[[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]]\n",
            "attention_lnx=[[[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]]\n",
            "attn_output=[[[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]]\n",
            "next_layer_addition_dropped_out=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "inputs=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "lnx=[[[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]]\n",
            "attention_lnx=[[[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]]\n",
            "attn_output=[[[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]]\n",
            "next_layer_addition_dropped_out=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "inputs=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "lnx=[[[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]]\n",
            "attention_lnx=[[[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]]\n",
            "attn_output=[[[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]]\n",
            "next_layer_addition_dropped_out=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "inputs=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "lnx=[[[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]]\n",
            "attention_lnx=[[[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]]\n",
            "attn_output=[[[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]]\n",
            "next_layer_addition_dropped_out=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "inputs=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "lnx=[[[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]]\n",
            "attention_lnx=[[[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]]\n",
            "attn_output=[[[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]]\n",
            "next_layer_addition_dropped_out=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "inputs=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "lnx=[[[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]]\n",
            "attention_lnx=[[[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]]\n",
            "attn_output=[[[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]]\n",
            "next_layer_addition_dropped_out=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "inputs=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "lnx=[[[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]]\n",
            "attention_lnx=[[[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]]\n",
            "attn_output=[[[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]]\n",
            "next_layer_addition_dropped_out=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "inputs=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "lnx=[[[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]]\n",
            "attention_lnx=[[[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]]\n",
            "attn_output=[[[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]]\n",
            "next_layer_addition_dropped_out=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "inputs=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "lnx=[[[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]]\n",
            "attention_lnx=[[[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]]\n",
            "attn_output=[[[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]]\n",
            "next_layer_addition_dropped_out=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "inputs=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "lnx=[[[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]]\n",
            "attention_lnx=[[[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]]\n",
            "attn_output=[[[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]]\n",
            "next_layer_addition_dropped_out=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "inputs=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "lnx=[[[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]]\n",
            "attention_lnx=[[[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]]\n",
            "attn_output=[[[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]]\n",
            "next_layer_addition_dropped_out=[[[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]]\n",
            "inputs=[[[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]]\n",
            "lnx=[[[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]]\n",
            "attention_lnx=[[[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]]\n",
            "attn_output=[[[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]]\n",
            "next_layer_addition_dropped_out=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "inputs=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "lnx=[[[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]]\n",
            "attention_lnx=[[[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]]\n",
            "attn_output=[[[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]]\n",
            "next_layer_addition_dropped_out=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "inputs=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "lnx=[[[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]]\n",
            "attention_lnx=[[[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]]\n",
            "attn_output=[[[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]]\n",
            "next_layer_addition_dropped_out=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "inputs=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "lnx=[[[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]]\n",
            "attention_lnx=[[[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]]\n",
            "attn_output=[[[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]]\n",
            "next_layer_addition_dropped_out=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "inputs=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "lnx=[[[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]]\n",
            "attention_lnx=[[[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]]\n",
            "attn_output=[[[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]]\n",
            "next_layer_addition_dropped_out=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "inputs=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "lnx=[[[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]]\n",
            "attention_lnx=[[[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]]\n",
            "attn_output=[[[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]]\n",
            "next_layer_addition_dropped_out=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "inputs=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "lnx=[[[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]]\n",
            "attention_lnx=[[[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]]\n",
            "attn_output=[[[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]]\n",
            "next_layer_addition_dropped_out=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "inputs=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "lnx=[[[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]]\n",
            "attention_lnx=[[[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]]\n",
            "attn_output=[[[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]]\n",
            "next_layer_addition_dropped_out=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "inputs=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "lnx=[[[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]]\n",
            "attention_lnx=[[[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]]\n",
            "attn_output=[[[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]]\n",
            "next_layer_addition_dropped_out=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "inputs=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "lnx=[[[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]]\n",
            "attention_lnx=[[[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]]\n",
            "attn_output=[[[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]]\n",
            "next_layer_addition_dropped_out=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "inputs=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "lnx=[[[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]]\n",
            "attention_lnx=[[[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]]\n",
            "attn_output=[[[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]]\n",
            "next_layer_addition_dropped_out=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "inputs=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "lnx=[[[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]]\n",
            "attention_lnx=[[[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]]\n",
            "attn_output=[[[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]]\n",
            "next_layer_addition_dropped_out=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "inputs=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "lnx=[[[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]]\n",
            "attention_lnx=[[[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]]\n",
            "attn_output=[[[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]]\n",
            "next_layer_addition_dropped_out=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "inputs=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "lnx=[[[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]]\n",
            "attention_lnx=[[[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]]\n",
            "attn_output=[[[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]]\n",
            "next_layer_addition_dropped_out=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "inputs=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "lnx=[[[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]]\n",
            "attention_lnx=[[[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]]\n",
            "attn_output=[[[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]]\n",
            "next_layer_addition_dropped_out=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "inputs=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "lnx=[[[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]]\n",
            "attention_lnx=[[[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]]\n",
            "attn_output=[[[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]]\n",
            "next_layer_addition_dropped_out=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "inputs=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "lnx=[[[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]]\n",
            "attention_lnx=[[[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]]\n",
            "attn_output=[[[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]]\n",
            "next_layer_addition_dropped_out=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "inputs=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "lnx=[[[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]]\n",
            "attention_lnx=[[[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]]\n",
            "attn_output=[[[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]]\n",
            "next_layer_addition_dropped_out=[[[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]]\n",
            "inputs=[[[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]]\n",
            "lnx=[[[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]]\n",
            "attention_lnx=[[[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]]\n",
            "attn_output=[[[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]]\n",
            "next_layer_addition_dropped_out=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "inputs=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "lnx=[[[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]]\n",
            "attention_lnx=[[[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]]\n",
            "attn_output=[[[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]]\n",
            "next_layer_addition_dropped_out=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "inputs=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "lnx=[[[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]]\n",
            "attention_lnx=[[[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]]\n",
            "attn_output=[[[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]]\n",
            "next_layer_addition_dropped_out=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "inputs=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "lnx=[[[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]]\n",
            "attention_lnx=[[[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]]\n",
            "attn_output=[[[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]]\n",
            "next_layer_addition_dropped_out=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "inputs=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "lnx=[[[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]]\n",
            "attention_lnx=[[[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]]\n",
            "attn_output=[[[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]]\n",
            "next_layer_addition_dropped_out=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "inputs=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "lnx=[[[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]]\n",
            "attention_lnx=[[[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]]\n",
            "attn_output=[[[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]]\n",
            "next_layer_addition_dropped_out=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "inputs=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "lnx=[[[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]]\n",
            "attention_lnx=[[[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]]\n",
            "attn_output=[[[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]]\n",
            "next_layer_addition_dropped_out=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "inputs=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "lnx=[[[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]]\n",
            "attention_lnx=[[[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]]\n",
            "attn_output=[[[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]]\n",
            "next_layer_addition_dropped_out=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "inputs=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "lnx=[[[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]]\n",
            "attention_lnx=[[[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]]\n",
            "attn_output=[[[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]]\n",
            "next_layer_addition_dropped_out=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "inputs=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "lnx=[[[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]]\n",
            "attention_lnx=[[[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]]\n",
            "attn_output=[[[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]]\n",
            "next_layer_addition_dropped_out=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "inputs=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "lnx=[[[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]]\n",
            "attention_lnx=[[[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]]\n",
            "attn_output=[[[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]]\n",
            "next_layer_addition_dropped_out=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "inputs=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "lnx=[[[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]]\n",
            "attention_lnx=[[[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]]\n",
            "attn_output=[[[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]]\n",
            "next_layer_addition_dropped_out=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "inputs=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "lnx=[[[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]]\n",
            "attention_lnx=[[[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]]\n",
            "attn_output=[[[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]]\n",
            "next_layer_addition_dropped_out=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "inputs=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "lnx=[[[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]]\n",
            "attention_lnx=[[[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]]\n",
            "attn_output=[[[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]]\n",
            "next_layer_addition_dropped_out=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "inputs=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "lnx=[[[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]]\n",
            "attention_lnx=[[[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]]\n",
            "attn_output=[[[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]]\n",
            "next_layer_addition_dropped_out=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "inputs=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "lnx=[[[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]]\n",
            "attention_lnx=[[[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]]\n",
            "attn_output=[[[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]]\n",
            "next_layer_addition_dropped_out=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "inputs=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "lnx=[[[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]]\n",
            "attention_lnx=[[[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]]\n",
            "attn_output=[[[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]]\n",
            "next_layer_addition_dropped_out=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "inputs=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "lnx=[[[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]]\n",
            "attention_lnx=[[[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]]\n",
            "attn_output=[[[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]]\n",
            "next_layer_addition_dropped_out=[[[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]]\n",
            "inputs=[[[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]]\n",
            "lnx=[[[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]]\n",
            "attention_lnx=[[[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]]\n",
            "attn_output=[[[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]]\n",
            "next_layer_addition_dropped_out=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "inputs=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "lnx=[[[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]]\n",
            "attention_lnx=[[[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]]\n",
            "attn_output=[[[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]]\n",
            "next_layer_addition_dropped_out=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "inputs=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "lnx=[[[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]]\n",
            "attention_lnx=[[[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]]\n",
            "attn_output=[[[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]]\n",
            "next_layer_addition_dropped_out=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "inputs=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "lnx=[[[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]]\n",
            "attention_lnx=[[[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]]\n",
            "attn_output=[[[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]]\n",
            "next_layer_addition_dropped_out=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "inputs=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "lnx=[[[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]]\n",
            "attention_lnx=[[[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]]\n",
            "attn_output=[[[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]]\n",
            "next_layer_addition_dropped_out=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "inputs=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "lnx=[[[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]]\n",
            "attention_lnx=[[[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]]\n",
            "attn_output=[[[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]]\n",
            "next_layer_addition_dropped_out=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "inputs=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "lnx=[[[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]]\n",
            "attention_lnx=[[[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]]\n",
            "attn_output=[[[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]]\n",
            "next_layer_addition_dropped_out=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "inputs=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "lnx=[[[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]]\n",
            "attention_lnx=[[[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]]\n",
            "attn_output=[[[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]]\n",
            "next_layer_addition_dropped_out=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "inputs=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "lnx=[[[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]]\n",
            "attention_lnx=[[[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]]\n",
            "attn_output=[[[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]]\n",
            "next_layer_addition_dropped_out=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "inputs=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "lnx=[[[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]]\n",
            "attention_lnx=[[[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]]\n",
            "attn_output=[[[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]]\n",
            "next_layer_addition_dropped_out=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "inputs=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "lnx=[[[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]]\n",
            "attention_lnx=[[[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]]\n",
            "attn_output=[[[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]]\n",
            "next_layer_addition_dropped_out=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "inputs=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "lnx=[[[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]]\n",
            "attention_lnx=[[[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]]\n",
            "attn_output=[[[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]]\n",
            "next_layer_addition_dropped_out=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "inputs=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "lnx=[[[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]]\n",
            "attention_lnx=[[[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]]\n",
            "attn_output=[[[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]]\n",
            "next_layer_addition_dropped_out=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "inputs=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "lnx=[[[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]]\n",
            "attention_lnx=[[[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]]\n",
            "attn_output=[[[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]]\n",
            "next_layer_addition_dropped_out=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "inputs=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "lnx=[[[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]]\n",
            "attention_lnx=[[[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]]\n",
            "attn_output=[[[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]]\n",
            "next_layer_addition_dropped_out=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "inputs=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "lnx=[[[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]]\n",
            "attention_lnx=[[[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]]\n",
            "attn_output=[[[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]]\n",
            "next_layer_addition_dropped_out=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "inputs=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "lnx=[[[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]]\n",
            "attention_lnx=[[[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]]\n",
            "attn_output=[[[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]]\n",
            "next_layer_addition_dropped_out=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "inputs=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "lnx=[[[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]]\n",
            "attention_lnx=[[[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]]\n",
            "attn_output=[[[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]]\n",
            "next_layer_addition_dropped_out=[[[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]]\n",
            "inputs=[[[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]\n",
            "\n",
            " [[-0.132812 -1 -0.292969 ... -0.192383 1.01562 1.28125]]]\n",
            "lnx=[[[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]\n",
            "\n",
            " [[-0.131836 -0.992188 -0.291016 ... -0.19043 1.00781 1.27344]]]\n",
            "attention_lnx=[[[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]\n",
            "\n",
            " [[-1.39062 -2.125 -1.19531 ... 1.45312 -0.878906 0.404297]]]\n",
            "attn_output=[[[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]\n",
            "\n",
            " [[-1.07031 -2.20312 -1.04688 ... 0.886719 0.0961914 1.1875]]]\n",
            "next_layer_addition_dropped_out=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "inputs=[[[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]\n",
            "\n",
            " [[-2.10938 -3.875 -0.792969 ... 1.3125 0.59375 1.23438]]]\n",
            "lnx=[[[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]\n",
            "\n",
            " [[-1.35938 -2.5 -0.511719 ... 0.84375 0.382812 0.796875]]]\n",
            "attention_lnx=[[[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]\n",
            "\n",
            " [[1.05469 -0.0299072 0.867188 ... 0.777344 -0.455078 -0.996094]]]\n",
            "attn_output=[[[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]\n",
            "\n",
            " [[-0.554688 -2.04688 0.0388184 ... 1.09375 0.0727539 0.125]]]\n",
            "next_layer_addition_dropped_out=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "inputs=[[[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]\n",
            "\n",
            " [[-1.21094 -3.3125 0.628906 ... 1.73438 0.78125 -0.152344]]]\n",
            "lnx=[[[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]\n",
            "\n",
            " [[-0.601562 -1.64844 0.3125 ... 0.863281 0.388672 -0.0756836]]]\n",
            "attention_lnx=[[[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]\n",
            "\n",
            " [[0.859375 -0.570312 -1.40625 ... -0.103516 0.131836 0.890625]]]\n",
            "attn_output=[[[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]\n",
            "\n",
            " [[-0.155273 -1.71094 -0.341797 ... 0.71875 0.402344 0.326172]]]\n",
            "next_layer_addition_dropped_out=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "inputs=[[[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]\n",
            "\n",
            " [[-0.65625 -4.75 -1.11719 ... 2.10938 0.0273438 -0.0664062]]]\n",
            "lnx=[[[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]\n",
            "\n",
            " [[-0.277344 -2.01562 -0.472656 ... 0.890625 0.0115356 -0.0280762]]]\n",
            "attention_lnx=[[[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]\n",
            "\n",
            " [[0.101074 1.66406 -1.05469 ... 1.04688 0.355469 -0.279297]]]\n",
            "attn_output=[[[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]\n",
            "\n",
            " [[-0.214844 -1.19531 -0.839844 ... 1.21875 0.148438 -0.133789]]]\n",
            "next_layer_addition_dropped_out=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "inputs=[[[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]\n",
            "\n",
            " [[-0.75 -4.03125 -1.72656 ... 3.67188 -0.0136719 0.0175781]]]\n",
            "lnx=[[[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]\n",
            "\n",
            " [[-0.28125 -1.51562 -0.648438 ... 1.375 -0.00512695 0.0065918]]]\n",
            "attention_lnx=[[[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]\n",
            "\n",
            " [[0.310547 -1.69531 0.341797 ... -1.33594 -0.18457 0.53125]]]\n",
            "attn_output=[[[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]\n",
            "\n",
            " [[-0.15332 -2 -0.484375 ... 0.816406 -0.0693359 0.191406]]]\n",
            "next_layer_addition_dropped_out=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "inputs=[[[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]\n",
            "\n",
            " [[-0.291016 -5.1875 -1.46875 ... 3.21875 -1.30469 1.07812]]]\n",
            "lnx=[[[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]\n",
            "\n",
            " [[-0.100098 -1.78906 -0.507812 ... 1.10938 -0.449219 0.371094]]]\n",
            "attention_lnx=[[[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]\n",
            "\n",
            " [[-0.441406 -0.84375 -0.96875 ... -0.392578 -1.19531 0.326172]]]\n",
            "attn_output=[[[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]\n",
            "\n",
            " [[-0.239258 -1.96875 -0.796875 ... 0.921875 -0.816406 0.458984]]]\n",
            "next_layer_addition_dropped_out=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "inputs=[[[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]\n",
            "\n",
            " [[-1.14062 -6.09375 -2.125 ... 3.0625 -2.75 2.01562]]]\n",
            "lnx=[[[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]\n",
            "\n",
            " [[-0.363281 -1.9375 -0.675781 ... 0.972656 -0.875 0.640625]]]\n",
            "attention_lnx=[[[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]\n",
            "\n",
            " [[-0.496094 -1.20312 -1.27344 ... -0.376953 -0.423828 -0.355469]]]\n",
            "attn_output=[[[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]\n",
            "\n",
            " [[-0.490234 -2.1875 -1.02344 ... 0.804688 -0.953125 0.498047]]]\n",
            "next_layer_addition_dropped_out=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "inputs=[[[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]\n",
            "\n",
            " [[-2.53125 -6.34375 -3.17188 ... 2.57812 -2.92188 2.6875]]]\n",
            "lnx=[[[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]\n",
            "\n",
            " [[-0.746094 -1.875 -0.9375 ... 0.761719 -0.863281 0.792969]]]\n",
            "attention_lnx=[[[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]\n",
            "\n",
            " [[0.235352 1.01562 -1.75 ... 0.204102 -1.24219 0.0771484]]]\n",
            "attn_output=[[[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]\n",
            "\n",
            " [[-0.640625 -1.49219 -1.375 ... 0.777344 -1.16406 0.773438]]]\n",
            "next_layer_addition_dropped_out=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "inputs=[[[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]\n",
            "\n",
            " [[-1.54688 -4.9375 -5.28125 ... 1.75781 -4.78125 2.53125]]]\n",
            "lnx=[[[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]\n",
            "\n",
            " [[-0.425781 -1.35938 -1.45312 ... 0.482422 -1.3125 0.695312]]]\n",
            "attention_lnx=[[[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]\n",
            "\n",
            " [[-0.0149536 0.02771 0.145508 ... -0.570312 0.910156 1.57812]]]\n",
            "attn_output=[[[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]\n",
            "\n",
            " [[-0.412109 -1.28906 -1.35156 ... 0.3125 -1.01562 1.07812]]]\n",
            "next_layer_addition_dropped_out=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "inputs=[[[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]\n",
            "\n",
            " [[-0.644531 -5.03125 -5.1875 ... 0.859375 -3.57812 5.03125]]]\n",
            "lnx=[[[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]\n",
            "\n",
            " [[-0.166992 -1.30469 -1.34375 ... 0.223633 -0.929688 1.30469]]]\n",
            "attention_lnx=[[[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]\n",
            "\n",
            " [[2.125 1.59375 -1.07812 ... -1.28906 -1.09375 -1.66406]]]\n",
            "attn_output=[[[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]\n",
            "\n",
            " [[0.373047 -0.867188 -1.57812 ... -0.108398 -1.17969 0.851562]]]\n",
            "next_layer_addition_dropped_out=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "inputs=[[[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]\n",
            "\n",
            " [[2.04688 -4.5625 -6.34375 ... 0.433594 -4.375 4.375]]]\n",
            "lnx=[[[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]\n",
            "\n",
            " [[0.507812 -1.13281 -1.57812 ... 0.10791 -1.08594 1.08594]]]\n",
            "attention_lnx=[[[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]\n",
            "\n",
            " [[-0.808594 0.0551758 -0.253906 ... 0.894531 -0.917969 0.878906]]]\n",
            "attn_output=[[[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]\n",
            "\n",
            " [[0.296875 -1.07812 -1.57812 ... 0.318359 -1.26562 1.25781]]]\n",
            "next_layer_addition_dropped_out=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "inputs=[[[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]\n",
            "\n",
            " [[1.07812 -5.21875 -7.75 ... 0.578125 -4.625 5.78125]]]\n",
            "lnx=[[[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]\n",
            "\n",
            " [[0.253906 -1.23438 -1.82812 ... 0.136719 -1.09375 1.36719]]]\n",
            "attention_lnx=[[[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]\n",
            "\n",
            " [[-0.120605 0.539062 -0.298828 ... 0.375 0.123535 -0.123047]]]\n",
            "attn_output=[[[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]\n",
            "\n",
            " [[0.220703 -1.07812 -1.85938 ... 0.219727 -1.03906 1.30469]]]\n",
            "next_layer_addition_dropped_out=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "inputs=[[[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]\n",
            "\n",
            " [[1.50781 -4.59375 -7.9375 ... 1.38281 -3.84375 5.9375]]]\n",
            "lnx=[[[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]\n",
            "\n",
            " [[0.341797 -1.04688 -1.80469 ... 0.314453 -0.871094 1.35156]]]\n",
            "attention_lnx=[[[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]\n",
            "\n",
            " [[0.222656 0.0830078 0.898438 ... 0.605469 -1.57031 1.33594]]]\n",
            "attn_output=[[[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]\n",
            "\n",
            " [[0.382812 -0.996094 -1.55469 ... 0.439453 -1.19531 1.60156]]]\n",
            "next_layer_addition_dropped_out=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "inputs=[[[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]\n",
            "\n",
            " [[1.60938 -4.21875 -6.34375 ... 2.70312 -6.34375 6.8125]]]\n",
            "lnx=[[[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]\n",
            "\n",
            " [[0.351562 -0.917969 -1.38281 ... 0.589844 -1.38281 1.48438]]]\n",
            "attention_lnx=[[[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]\n",
            "\n",
            " [[-0.582031 1.125 -0.519531 ... 2.01562 0.628906 0.126953]]]\n",
            "attn_output=[[[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]\n",
            "\n",
            " [[0.217773 -0.65625 -1.45312 ... 1 -1.21094 1.47656]]]\n",
            "next_layer_addition_dropped_out=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "inputs=[[[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]\n",
            "\n",
            " [[0.75 -2.09375 -6 ... 5.21875 -5.75 6.4375]]]\n",
            "lnx=[[[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]\n",
            "\n",
            " [[0.157227 -0.439453 -1.25781 ... 1.09375 -1.20312 1.35156]]]\n",
            "attention_lnx=[[[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]\n",
            "\n",
            " [[-0.324219 -1.35156 -0.734375 ... 0.192383 -0.730469 1.11719]]]\n",
            "attn_output=[[[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]\n",
            "\n",
            " [[0.0869141 -0.703125 -1.375 ... 1.10938 -1.32812 1.54688]]]\n",
            "next_layer_addition_dropped_out=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "inputs=[[[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]\n",
            "\n",
            " [[0.408203 -2.95312 -5.875 ... 6.21875 -6.71875 7.84375]]]\n",
            "lnx=[[[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]\n",
            "\n",
            " [[0.0825195 -0.59375 -1.1875 ... 1.25 -1.35156 1.57812]]]\n",
            "attention_lnx=[[[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]\n",
            "\n",
            " [[-0.386719 0.339844 -2.04688 ... -0.59375 1.88281 -0.0888672]]]\n",
            "attn_output=[[[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]\n",
            "\n",
            " [[0.00421143 -0.511719 -1.55469 ... 1.10156 -0.949219 1.52344]]]\n",
            "next_layer_addition_dropped_out=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "inputs=[[[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]\n",
            "\n",
            " [[-0.507812 -2.67188 -7.71875 ... 6 -5.78125 7.53125]]]\n",
            "lnx=[[[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]\n",
            "\n",
            " [[-0.0981445 -0.515625 -1.49219 ... 1.16406 -1.11719 1.46094]]]\n",
            "attention_lnx=[[[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]\n",
            "\n",
            " [[0.03125 -0.102539 0.878906 ... -1.5 -0.0288086 1.41406]]]\n",
            "attn_output=[[[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]\n",
            "\n",
            " [[-0.090332 -0.527344 -1.29688 ... 0.851562 -1.10156 1.69531]]]\n",
            "next_layer_addition_dropped_out=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "inputs=[[[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]\n",
            "\n",
            " [[0.78125 -2.6875 -6.65625 ... 6.40625 -5.625 8.6875]]]\n",
            "lnx=[[[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]\n",
            "\n",
            " [[0.145508 -0.503906 -1.24219 ... 1.19531 -1.04688 1.625]]]\n",
            "attention_lnx=[[[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]\n",
            "\n",
            " [[1.42969 0.867188 0.144531 ... -0.542969 -1.36719 1.75]]]\n",
            "attn_output=[[[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]\n",
            "\n",
            " [[0.404297 -0.332031 -1.1875 ... 1.07031 -1.27344 1.90625]]]\n",
            "next_layer_addition_dropped_out=[[[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]\n",
            "\n",
            " [[2.4375 -1.95312 -6 ... 6.8125 -6.90625 11.3125]]]\n",
            "token_buffers={token_buffers}\n",
            "----------------------\n",
            "Prompt:\n",
            "The color of the sky is \n",
            "\n",
            "Output:\n",
            "The color of the sky is \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Tokens:\n",
            "[     2    651   2881    576    573   8203    603 235248    108    108\n",
            "    108    108    108    108    108    108    108    108    108]\n",
            "----------------------\n",
            "Prompt:\n",
            "Translate this into French:\n",
            "Hello, my name is Morgane.\n",
            "\n",
            "Output:\n",
            "Translate this into French:\n",
            "Hello, my name is Morgane.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Tokens:\n",
            "[     2  49688    736   1280   6987 235292    108   4521 235269    970\n",
            "   1503    603  20189 235249 235265    108    108    108    108    108\n",
            "    108    108    108    108    108    108]\n",
            "----------------------\n",
            "Prompt:\n",
            "Translate this into French:\n",
            "This dish is delicious!\n",
            "\n",
            "Output:\n",
            "Translate this into French:\n",
            "This dish is delicious!\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Tokens:\n",
            "[     2  49688    736   1280   6987 235292    108   1596  15862    603\n",
            "  15855 235341    108    108    108    108    108    108    108    108\n",
            "    108    108    108]\n",
            "----------------------\n",
            "Prompt:\n",
            "Translate this into French:\n",
            "I am a student.\n",
            "\n",
            "Output:\n",
            "Translate this into French:\n",
            "I am a student.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Tokens:\n",
            "[     2  49688    736   1280   6987 235292    108 235285   1144    476\n",
            "   5913 235265    108    108    108    108    108    108    108    108\n",
            "    108    108    108]\n",
            "----------------------\n",
            "Prompt:\n",
            "Translate this into French:\n",
            "How's the weather today?\n",
            "\n",
            "Output:\n",
            "Translate this into French:\n",
            "How's the weather today?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Tokens:\n",
            "[     2  49688    736   1280   6987 235292    108   2299 235303 235256\n",
            "    573   8957   3646 235336    108    108    108    108    108    108\n",
            "    108    108    108    108    108]\n"
          ]
        }
      ],
      "source": [
        "gemma_tokenizer = data_lib.GemmaTokenizer(\n",
        "    os.path.join(kaggle_ckpt_path, \"tokenizer.model\")\n",
        ")\n",
        "\n",
        "from MaxText.input_pipeline import _input_pipeline_utils\n",
        "from MaxText.globals import PKG_DIR\n",
        "\n",
        "# gemma_tokenizer = _input_pipeline_utils.get_tokenizer(\n",
        "#         os.path.join(os.path.dirname(PKG_DIR), \"assets\", \"tokenizer_llama3.tiktoken\"),\n",
        "#         \"tiktoken\",\n",
        "#         add_bos=True,\n",
        "#         add_eos=False,\n",
        "#     )\n",
        "#     )\n",
        "# gemma_tokenizer = data_lib.GemmaTokenizer(\n",
        "# )\n",
        "\n",
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=gemma,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")\n",
        "\n",
        "input_batch = [\n",
        "    \"The color of the sky is \\n\",\n",
        "    \"Translate this into French:\\nHello, my name is Morgane.\\n\",\n",
        "    \"Translate this into French:\\nThis dish is delicious!\\n\",\n",
        "    \"Translate this into French:\\nI am a student.\\n\",\n",
        "    \"Translate this into French:\\nHow's the weather today?\\n\",\n",
        "]\n",
        "\n",
        "out_data = sampler(\n",
        "    input_strings=input_batch,\n",
        "    total_generation_steps=10,  # The number of steps performed when generating a response.\n",
        "    echo=True,  # Whether to echo the input string in the output.\n",
        ")\n",
        "\n",
        "for input_string, out_string, tokens in zip(input_batch, out_data.text, out_data.tokens):\n",
        "  print(f\"----------------------\")\n",
        "  print(f\"Prompt:\\n{input_string}\")\n",
        "  print(f\"Output:\\n{out_string}\")\n",
        "  print(f\"Tokens:\\n{tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'?\\n'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gemma_tokenizer.encode(\"The color\\n\")\n",
        "gemma_tokenizer.decode([235336    , 108 ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "State({\n",
              "  'base': {\n",
              "    'decoder': {\n",
              "      'decoder_norm': {\n",
              "        'scale': Param( # 2,048 (4.1 KB)\n",
              "          value=Array([1, 1, 1, ..., 1, 1, 1], dtype=bfloat16),\n",
              "          mesh=None,\n",
              "          sharding=('norm',),\n",
              "          sharding_rules=None,\n",
              "          linen_meta_type=LogicallyPartitioned\n",
              "        )\n",
              "      },\n",
              "      'layers': {\n",
              "        'mlp': {\n",
              "          'wi_0': {\n",
              "            'kernel': Param( # 603,979,776 (1.2 GB)\n",
              "              value=Array([[[-0.0349121, 0.019165, 0.0178223, ..., -0.00421143, 0.0117188,\n",
              "                       -0.00521851],\n",
              "                      [-0.0375977, 0.0284424, -0.00469971, ..., 0.0314941, -0.017334,\n",
              "                       0],\n",
              "                      [0.0032196, 0.0111694, 0.00750732, ..., 0.0314941, -0.0148315,\n",
              "                       -0.0197754],\n",
              "                      ...,\n",
              "                      [0.00723267, -0.046875, 0.0303955, ..., 0.0441895, 0.00370789,\n",
              "                       -0.0090332],\n",
              "                      [0.00723267, 0.00799561, 0.00521851, ..., -0.00370789,\n",
              "                       0.00671387, -0.000984192],\n",
              "                      [-0.0375977, -0.0324707, -0.000492096, ..., -0.046875,\n",
              "                       0.00854492, 0.00421143]],\n",
              "              \n",
              "                     [[0.0154419, -0.0148315, -0.0375977, ..., 0.00469971, 0.0214844,\n",
              "                       -0.0375977],\n",
              "                      [-0.0090332, -0.0245361, 0.0402832, ..., 0.00271606, -0.012146,\n",
              "                       -0.000492096],\n",
              "                      [0.036377, -0.000984192, -0.02771, ..., 0.0195312, -0.00622559,\n",
              "                       0.0159912],\n",
              "                      ...,\n",
              "                      [0.0117188, -0.0214844, -0.00234985, ..., -0.0197754,\n",
              "                       -0.00765991, 0.0178223],\n",
              "                      [0.000492096, -0.0065918, -0.0238037, ..., 0.036377, 0.0111694,\n",
              "                       0.00622559],\n",
              "                      [0.00958252, -0.00135803, 0.0122681, ..., -0.013855, -0.0197754,\n",
              "                       -0.00811768]],\n",
              "              \n",
              "                     [[0.0117188, 0.0402832, -0.0284424, ..., 0.0245361, -0.0303955,\n",
              "                       0.00570679],\n",
              "                      [-0.0361328, -0.0132446, -0.0294189, ..., -0.0106201,\n",
              "                       0.000492096, -0.0361328],\n",
              "                      [-0.00958252, 0.0230713, -0.0253906, ..., -0.0106201,\n",
              "                       -0.0267334, 0.00671387],\n",
              "                      ...,\n",
              "                      [0.0303955, 0.0238037, 0.0142212, ..., -0.00334167, 0.0388184,\n",
              "                       -0.00370789],\n",
              "                      [0.0270996, 0.00671387, -0.0201416, ..., 0.0128784, -0.0361328,\n",
              "                       0.0101318],\n",
              "                      [-0.00811768, -0.0349121, 0.00421143, ..., 0.00247192,\n",
              "                       0.00958252, 0.00570679]],\n",
              "              \n",
              "                     ...,\n",
              "              \n",
              "                     [[-0.0238037, 0.00854492, 0.00723267, ..., -0.00622559,\n",
              "                       -0.0132446, -0.0253906],\n",
              "                      [-0.019043, -0.0111694, 0.017334, ..., 0.0388184, -0.0201416,\n",
              "                       -0.0294189],\n",
              "                      [-0.0361328, 0.0032196, -0.0336914, ..., -0.00421143, 0.0349121,\n",
              "                       0.0184326],\n",
              "                      ...,\n",
              "                      [0.0375977, 0.00799561, -0.0375977, ..., 0.0441895, 0.0422363,\n",
              "                       0.0274658],\n",
              "                      [-0.00282288, 0.000492096, -0.0223389, ..., 0.0245361,\n",
              "                       0.00671387, -0.0267334],\n",
              "                      [-0.0336914, 0.00469971, 0.0128784, ..., -0.0375977,\n",
              "                       -0.00421143, -0.0349121]],\n",
              "              \n",
              "                     [[-0.020874, -0.0284424, -0.00811768, ..., 0.00196838, 0.0284424,\n",
              "                       -0.00811768],\n",
              "                      [0.00135803, -0.019043, -0.000984192, ..., 0.0214844,\n",
              "                       0.00196838, 0.0303955],\n",
              "                      [0.0032196, 0.0263672, 0.0117188, ..., -0.0500488, 0,\n",
              "                       0.00521851],\n",
              "                      ...,\n",
              "                      [0.00723267, -0.0197754, 0.0131836, ..., -0.00521851,\n",
              "                       -0.0101318, 0.0201416],\n",
              "                      [-0.019043, 0.0223389, 0.0336914, ..., 0.0314941, -0.0253906,\n",
              "                       0.0159912],\n",
              "                      [-0.0230713, -0.0101318, 0.0136719, ..., 0.00135803, -0.0441895,\n",
              "                       -0.00622559]],\n",
              "              \n",
              "                     [[-0.0159912, 0.0136719, -0.0101318, ..., -0.0407715, 0.0167236,\n",
              "                       0.00854492],\n",
              "                      [0.0159912, 0.0128784, 0.0294189, ..., 0.0090332, 0.0195312,\n",
              "                       -0.0132446],\n",
              "                      [0.00469971, 0.0375977, 0.0111694, ..., 0.00196838, 0.0122681,\n",
              "                       -0.00469971],\n",
              "                      ...,\n",
              "                      [-0.000984192, 0.0349121, 0.0131836, ..., 0.0148315, 0.0184326,\n",
              "                       0.0195312],\n",
              "                      [-0.020874, 0.00421143, -0.0336914, ..., 0.0245361, 0.0294189,\n",
              "                       0.0314941],\n",
              "                      [0.00421143, 0.0184326, 0.0230713, ..., 0.000984192, 0.00570679,\n",
              "                       0.046875]]], dtype=bfloat16),\n",
              "              mesh=None,\n",
              "              sharding=('embed', 'layers', 'mlp'),\n",
              "              sharding_rules=None,\n",
              "              linen_meta_type=LogicallyPartitioned\n",
              "            )\n",
              "          },\n",
              "          'wi_1': {\n",
              "            'kernel': Param( # 603,979,776 (1.2 GB)\n",
              "              value=Array([[[0.00799561, -0.0324707, 0.00570679, ..., -0.0388184,\n",
              "                       0.00799561, 0.0106201],\n",
              "                      [0.00521851, -0.0441895, 0.0178223, ..., -0.0238037, 0.0106201,\n",
              "                       0.019165],\n",
              "                      [0.0142212, -0.0258789, 0.0090332, ..., -0.0090332, 0.0184326,\n",
              "                       -0.0324707],\n",
              "                      ...,\n",
              "                      [-0.0253906, -0.0500488, -0.0388184, ..., -0.0375977,\n",
              "                       0.00421143, -0.0336914],\n",
              "                      [-0.0230713, -0.0201416, 0.0154419, ..., -0.0294189, -0.013855,\n",
              "                       -0.00185394],\n",
              "                      [0.00271606, -0.00135803, 0.0274658, ..., -0.0126953,\n",
              "                       0.00469971, 0.00750732]],\n",
              "              \n",
              "                     [[-0.0336914, 0.00271606, 0.00521851, ..., 0.00521851, 0.0270996,\n",
              "                       0.00570679],\n",
              "                      [-0.00866699, -0.00866699, 0.00521851, ..., 0.0349121,\n",
              "                       0.0142212, 0],\n",
              "                      [-0.00622559, 0.019165, 0.019165, ..., 0.00723267, 0.017334,\n",
              "                       0.00196838],\n",
              "                      ...,\n",
              "                      [-0.0090332, 0.00370789, 0.0303955, ..., -0.0267334, 0.00854492,\n",
              "                       0.0159912],\n",
              "                      [0.020874, -0.00622559, -0.017334, ..., 0.0131836, 0.0245361,\n",
              "                       -0.00521851],\n",
              "                      [-0.0253906, -0.0144653, 0, ..., -0.0111694, 0.0274658,\n",
              "                       0.0274658]],\n",
              "              \n",
              "                     [[0.00799561, 0.0131836, -0.0324707, ..., 0.0131836, -0.0407715,\n",
              "                       0.0111694],\n",
              "                      [-0.012146, -0.012146, 0.0238037, ..., 0.046875, -0.0238037,\n",
              "                       -0.0314941],\n",
              "                      [-0.0159912, -0.0126953, -0.0201416, ..., 0.0128784, 0.0223389,\n",
              "                       0.00247192],\n",
              "                      ...,\n",
              "                      [-0.0294189, -0.0441895, -0.0167236, ..., -0.000492096,\n",
              "                       0.0136719, 0.0201416],\n",
              "                      [-0.0201416, -0.00469971, -0.00958252, ..., 0.0154419,\n",
              "                       0.0422363, -0.0375977],\n",
              "                      [-0.0238037, -0.0144653, 0.00370789, ..., -0.0375977, -0.046875,\n",
              "                       -0.00469971]],\n",
              "              \n",
              "                     ...,\n",
              "              \n",
              "                     [[-0.0132446, -0.00334167, -0.00521851, ..., 0.00469971,\n",
              "                       -0.00282288, 0.00196838],\n",
              "                      [-0.0167236, -0.0294189, -0.0294189, ..., 0.0178223, -0.0284424,\n",
              "                       0.0253906],\n",
              "                      [-0.0303955, -0.00958252, -0.0303955, ..., 0.0441895,\n",
              "                       0.00196838, -0.0090332],\n",
              "                      ...,\n",
              "                      [0.00671387, -0.0197754, -0.00135803, ..., 0.0178223, 0.0245361,\n",
              "                       0.020874],\n",
              "                      [-0.0154419, -0.00185394, 0.0245361, ..., 0.0159912, 0.0284424,\n",
              "                       0.0245361],\n",
              "                      [0.0245361, -0.012146, 0.0195312, ..., 0.00135803, 0.0230713,\n",
              "                       -0.000984192]],\n",
              "              \n",
              "                     [[-0.00234985, -0.020874, -0.02771, ..., -0.0178223, -0.00811768,\n",
              "                       0.0274658],\n",
              "                      [-0.00185394, -0.000984192, 0.00370789, ..., -0.0324707,\n",
              "                       -0.0214844, -0.00370789],\n",
              "                      [-0.02771, -0.00622559, 0.0230713, ..., -0.0238037, -0.0388184,\n",
              "                       0.0245361],\n",
              "                      ...,\n",
              "                      [0.0136719, -0.0407715, -0.0407715, ..., -0.00370789,\n",
              "                       -0.00622559, -0.0090332],\n",
              "                      [-0.00765991, -0.0361328, -0.00185394, ..., -0.0154419,\n",
              "                       0.0117188, 0.0117188],\n",
              "                      [0.0101318, 0.00247192, 0.00421143, ..., -0.0117188, 0.0294189,\n",
              "                       -0.0167236]],\n",
              "              \n",
              "                     [[0.0117188, 0.036377, 0, ..., -0.00521851, 0.0375977,\n",
              "                       -0.0427246],\n",
              "                      [0.0184326, -0.0375977, -0.0303955, ..., -0.0407715,\n",
              "                       0.000492096, 0.0032196],\n",
              "                      [0.0422363, -0.00958252, 0.0154419, ..., -0.00135803,\n",
              "                       -0.0230713, 0.0422363],\n",
              "                      ...,\n",
              "                      [0.0388184, -0.00469971, 0.0238037, ..., -0.019043, 0.0375977,\n",
              "                       0.0245361],\n",
              "                      [-0.00421143, -0.00958252, 0.00196838, ..., -0.0407715,\n",
              "                       -0.00135803, -0.0111694],\n",
              "                      [0.0032196, -0.0154419, -0.00135803, ..., -0.0238037,\n",
              "                       -0.0178223, -0.0349121]]], dtype=bfloat16),\n",
              "              mesh=None,\n",
              "              sharding=('embed', 'layers', 'mlp'),\n",
              "              sharding_rules=None,\n",
              "              linen_meta_type=LogicallyPartitioned\n",
              "            )\n",
              "          },\n",
              "          'wo': {\n",
              "            'kernel': Param( # 603,979,776 (1.2 GB)\n",
              "              value=Array([[[0.00415039, 0.015625, 0.00958252, ..., -0.000999451,\n",
              "                       -0.0133057, 0.00338745],\n",
              "                      [0.00415039, -0.00714111, 0, ..., -0.00946045, 0.000173569,\n",
              "                       -0.015625],\n",
              "                      [-0.0108032, 0, -0.010437, ..., 0.00283813, 0.0133667,\n",
              "                       0.00817871],\n",
              "                      ...,\n",
              "                      [-0.00650024, -0.0119629, -0.0133057, ..., 0.00320435,\n",
              "                       -0.0137939, 0.00592041],\n",
              "                      [-0.00219727, -0.00271606, -0.0128174, ..., -0.0128174,\n",
              "                       -0.00118256, -0.00921631],\n",
              "                      [0.00485229, -0.00946045, -0.00674438, ..., -0.00184631,\n",
              "                       -0.000831604, 0.00692749]],\n",
              "              \n",
              "                     [[0.00897217, -0.0137939, -0.00674438, ..., -0.000480652,\n",
              "                       -0.000173569, -0.0108032],\n",
              "                      [0.000694275, 0.00396729, -0.0108032, ..., 0.0114746,\n",
              "                       0.00338745, -0.00524902],\n",
              "                      [-0.0137939, 0.00283813, -0.0111084, ..., -0.00271606,\n",
              "                       -0.00698853, 0.010437],\n",
              "                      ...,\n",
              "                      [-0.0128174, 0.0114746, -0.00567627, ..., -0.00738525,\n",
              "                       0.0111084, -0.00592041],\n",
              "                      [0.00148773, -0.00872803, 0.0067749, ..., -0.0177002,\n",
              "                       -0.00698853, 0.00762939],\n",
              "                      [-0.00166321, -0.00201416, -0.00897217, ..., -0.000999451,\n",
              "                       0.00219727, 0.000347137]],\n",
              "              \n",
              "                     [[-0.00738525, 0.00238037, 0.00320435, ..., -0.00430298, 0,\n",
              "                       -0.00184631],\n",
              "                      [-0.00131226, 0.00184631, -0.00271606, ..., -0.0108032,\n",
              "                       -0.0144653, 0.00933838],\n",
              "                      [-0.015625, -0.00793457, -0.0133057, ..., 0.00113678,\n",
              "                       -0.00286865, 0.00148773],\n",
              "                      ...,\n",
              "                      [-0.00396729, -0.00396729, -0.0108032, ..., 0.00238037,\n",
              "                       -0.00306702, 0.00454712],\n",
              "                      [0.00302124, 0.000347137, 0.00466919, ..., 0.000347137,\n",
              "                       0.00338745, 0.000873566],\n",
              "                      [-0.00148773, -0.0128174, -0.00650024, ..., 0.015625,\n",
              "                       0.00131226, -0.000656128]],\n",
              "              \n",
              "                     ...,\n",
              "              \n",
              "                     [[0.000873566, -0.0151978, -0.00396729, ..., 0.00933838,\n",
              "                       -0.000480652, 0.00793457],\n",
              "                      [-0.00817871, -0.00738525, -0.0025177, ..., -0.00306702,\n",
              "                       -0.00271606, -0.0111084],\n",
              "                      [-0.00396729, -0.00148773, -0.00233459, ..., -0.000999451,\n",
              "                       -0.0144653, -0.0119629],\n",
              "                      ...,\n",
              "                      [-0.0144653, -0.00166321, 0.00267029, ..., -0.00946045,\n",
              "                       -0.00921631, -0.00546265],\n",
              "                      [-0.00491333, -0.0177002, 0.00131226, ..., -0.00118256,\n",
              "                       0.000173569, 0.00592041],\n",
              "                      [-0.00448608, 0.00148773, 0.00466919, ..., 0.0100708,\n",
              "                       -0.00430298, -0.00219727]],\n",
              "              \n",
              "                     [[0.00320435, 0.0050354, -0.00166321, ..., -0.00233459,\n",
              "                       0.00738525, -0.000173569],\n",
              "                      [-0.00491333, -0.00201416, -0.00184631, ..., -0.00762939,\n",
              "                       -0.0114746, 0.0142822],\n",
              "                      [-0.00184631, -0.00946045, 0.00692749, ..., 0.000694275,\n",
              "                       -0.0144653, -0.00650024],\n",
              "                      ...,\n",
              "                      [-0.00131226, -0.00286865, 0.00933838, ..., 0.0149536,\n",
              "                       0.000347137, 0.00485229],\n",
              "                      [0.00320435, 0, -0.00650024, ..., -0.00921631, -0.00219727,\n",
              "                       0.00897217],\n",
              "                      [0.00454712, -0.00491333, 0.00375366, ..., 0.00166321,\n",
              "                       0.00219727, -0.00714111]],\n",
              "              \n",
              "                     [[-0.00592041, 0.0166016, 0.0123901, ..., 0.00436401, 0.00454712,\n",
              "                       0.00338745],\n",
              "                      [0.0128784, 0.0128784, 0.000480652, ..., 0.010437, 0.00113678,\n",
              "                       0.000694275],\n",
              "                      [0.0123901, -0.00219727, -0.00921631, ..., 0.000694275,\n",
              "                       -0.0151978, -0.00415039],\n",
              "                      ...,\n",
              "                      [-0.00469971, 0.0133667, -0.00338745, ..., 0.00762939,\n",
              "                       0.00219727, 0.000873566],\n",
              "                      [0.00396729, 0.00976562, 0.00634766, ..., 0, -0.00430298,\n",
              "                       -0.000656128],\n",
              "                      [0.00466919, 0.0114746, 0.00238037, ..., -0.0100708,\n",
              "                       -0.00271606, -0.000480652]]], dtype=bfloat16),\n",
              "              mesh=None,\n",
              "              sharding=('mlp', 'layers', 'embed'),\n",
              "              sharding_rules=None,\n",
              "              linen_meta_type=LogicallyPartitioned\n",
              "            )\n",
              "          }\n",
              "        },\n",
              "        'pre_ffw_norm': {\n",
              "          'scale': Param( # 36,864 (73.7 KB)\n",
              "            value=Array([[1, 1, 1, ..., 1, 1, 1],\n",
              "                   [1, 1, 1, ..., 1, 1, 1],\n",
              "                   [1, 1, 1, ..., 1, 1, 1],\n",
              "                   ...,\n",
              "                   [1, 1, 1, ..., 1, 1, 1],\n",
              "                   [1, 1, 1, ..., 1, 1, 1],\n",
              "                   [1, 1, 1, ..., 1, 1, 1]], dtype=bfloat16),\n",
              "            mesh=None,\n",
              "            sharding=('norm', 'layers'),\n",
              "            sharding_rules=None,\n",
              "            linen_meta_type=LogicallyPartitioned\n",
              "          )\n",
              "        },\n",
              "        'pre_self_attention_norm': {\n",
              "          'scale': Param( # 36,864 (73.7 KB)\n",
              "            value=Array([[1, 1, 1, ..., 1, 1, 1],\n",
              "                   [1, 1, 1, ..., 1, 1, 1],\n",
              "                   [1, 1, 1, ..., 1, 1, 1],\n",
              "                   ...,\n",
              "                   [1, 1, 1, ..., 1, 1, 1],\n",
              "                   [1, 1, 1, ..., 1, 1, 1],\n",
              "                   [1, 1, 1, ..., 1, 1, 1]], dtype=bfloat16),\n",
              "            mesh=None,\n",
              "            sharding=('norm', 'layers'),\n",
              "            sharding_rules=None,\n",
              "            linen_meta_type=LogicallyPartitioned\n",
              "          )\n",
              "        },\n",
              "        'self_attention': {\n",
              "          'key': {\n",
              "            'kernel': Param( # 9,437,184 (18.9 MB)\n",
              "              value=Array([[[[0.000107765, 0.0317383, 0.0114136, ..., 0.019165, -0.0466309,\n",
              "                        -0.0220947]],\n",
              "              \n",
              "                      [[-0.0116577, 0.0249023, 0.0167236, ..., 0.000972748,\n",
              "                        -0.00292969, 0.032959]],\n",
              "              \n",
              "                      [[-0.0269775, -0.020752, -0.00469971, ..., -0.0383301,\n",
              "                        -0.00970459, 0.0375977]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[-0.0432129, 0.015564, 0.048584, ..., 0.00854492, 0.0203857,\n",
              "                        -0.0116577]],\n",
              "              \n",
              "                      [[-0.0334473, -0.0269775, 0.0203857, ..., 0.00946045,\n",
              "                        0.00576782, -0.00205994]],\n",
              "              \n",
              "                      [[-0.0126953, -0.0405273, 0.010437, ..., 0.019165, 0.00314331,\n",
              "                        -0.0175781]]],\n",
              "              \n",
              "              \n",
              "                     [[[0.032959, -0.0299072, -0.0405273, ..., 0.0341797, 0.0197754,\n",
              "                        -0.0228271]],\n",
              "              \n",
              "                      [[-0.0158691, -0.00738525, -0.0288086, ..., 0.0375977,\n",
              "                        -0.026123, 0.0203857]],\n",
              "              \n",
              "                      [[0.00534058, -0.0349121, -0.0234375, ..., -0.0142212,\n",
              "                        -0.0251465, 0.0444336]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.0292969, 0.048584, -0.027832, ..., -0.0349121, -0.00970459,\n",
              "                        -0.00830078]],\n",
              "              \n",
              "                      [[-0.00601196, 0.00854492, 0.0255127, ..., 0.017334, 0.0240479,\n",
              "                        0.0395508]],\n",
              "              \n",
              "                      [[-0.0111084, -0.00830078, -0.0220947, ..., -0.0181885,\n",
              "                        0.0249023, 0.00576782]]],\n",
              "              \n",
              "              \n",
              "                     [[[0.0249023, -0.0432129, -0.0214844, ..., 0.0167236, 0.032959,\n",
              "                        0.00759888]],\n",
              "              \n",
              "                      [[-0.0106812, -0.0639648, 0.00402832, ..., -0.0106812,\n",
              "                        -0.000759125, -0.024292]],\n",
              "              \n",
              "                      [[-0.0169678, 0.0129395, 0.0358887, ..., -0.0515137, 0.0129395,\n",
              "                        -0.0078125]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[-0.0164795, -0.00646973, -0.0234375, ..., -0.0142212,\n",
              "                        0.0071106, 0.0071106]],\n",
              "              \n",
              "                      [[0.0556641, -0.0288086, 0.0240479, ..., -0.000324249,\n",
              "                        0.0556641, -0.0299072]],\n",
              "              \n",
              "                      [[0.019165, 0.0071106, 0.00183105, ..., -0.0126953, 0.019165,\n",
              "                        0.010437]]],\n",
              "              \n",
              "              \n",
              "                     ...,\n",
              "              \n",
              "              \n",
              "                     [[[0.0375977, 0.00358582, -0.0383301, ..., 0.0230713,\n",
              "                        -0.0214844, 0.0129395]],\n",
              "              \n",
              "                      [[0.0124512, -0.00379944, -0.0383301, ..., 0.0240479,\n",
              "                        -0.0142212, 0.0255127]],\n",
              "              \n",
              "                      [[-0.0334473, -0.0101929, -0.00738525, ..., -0.0349121,\n",
              "                        -0.000759125, 0.032959]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.0444336, 0.0283203, -0.0432129, ..., -0.012146, 0.00140381,\n",
              "                        -0.0220947]],\n",
              "              \n",
              "                      [[-0.0228271, 0.0395508, 0.0145264, ..., -0.00738525,\n",
              "                        -0.0116577, 0.00759888]],\n",
              "              \n",
              "                      [[0.0203857, -0.0106812, 0.0230713, ..., -0.0111084,\n",
              "                        0.00491333, 0.0230713]]],\n",
              "              \n",
              "              \n",
              "                     [[[0.00946045, -0.00205994, 0.0119019, ..., 0.00759888,\n",
              "                        0.019165, -0.0147705]],\n",
              "              \n",
              "                      [[-0.0111084, -0.00646973, -0.0078125, ..., -0.00337219,\n",
              "                        -0.0405273, 0.0240479]],\n",
              "              \n",
              "                      [[0.0179443, -0.00469971, -0.0158691, ..., -0.0349121,\n",
              "                        0.0556641, 0.010437]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.0317383, 0.0230713, -0.0175781, ..., -0.0116577, 0.0119019,\n",
              "                        -0.0181885]],\n",
              "              \n",
              "                      [[-0.0164795, 0.00314331, 0.0139771, ..., 0.0556641,\n",
              "                        -0.0175781, 0.000107765]],\n",
              "              \n",
              "                      [[0.0292969, 0.0124512, -0.00878906, ..., -0.0187988,\n",
              "                        -0.0234375, 0.00271606]]],\n",
              "              \n",
              "              \n",
              "                     [[[-0.0466309, 0.0124512, 0.0249023, ..., -0.0269775,\n",
              "                        -0.00379944, 0.0071106]],\n",
              "              \n",
              "                      [[0.0305176, -0.00921631, 0.000972748, ..., 0.0197754,\n",
              "                        0.0129395, 0.00227356]],\n",
              "              \n",
              "                      [[-0.0220947, 0.0375977, -0.00921631, ..., 0.0444336,\n",
              "                        -0.0366211, 0.00227356]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.00140381, -0.0078125, -0.00970459, ..., 0.0444336,\n",
              "                        -0.0152588, -0.0158691]],\n",
              "              \n",
              "                      [[-0.0269775, 0.000541687, -0.00248718, ..., -0.00424194,\n",
              "                        0.0224609, 0.0217285]],\n",
              "              \n",
              "                      [[-0.00248718, 0.0444336, -0.0251465, ..., 0.010437,\n",
              "                        -0.00119019, 0.00227356]]]], dtype=bfloat16),\n",
              "              mesh=None,\n",
              "              sharding=('embed', 'layers', 'kv_heads', 'kv_head_dim'),\n",
              "              sharding_rules=None,\n",
              "              linen_meta_type=LogicallyPartitioned\n",
              "            )\n",
              "          },\n",
              "          'out': {\n",
              "            'kernel': Param( # 75,497,472 (151.0 MB)\n",
              "              value=Array([[[[0.0114136, 0.0129395, 0.00854492, ..., 0.00759888,\n",
              "                        -0.0142212, 0.010437],\n",
              "                       [-0.0334473, -0.0288086, 0.019165, ..., -0.00970459, 0.032959,\n",
              "                        0.0161133],\n",
              "                       [0.00491333, 0.0071106, -0.0187988, ..., 0.0185547,\n",
              "                        -0.0311279, 0.017334],\n",
              "                       ...,\n",
              "                       [0.000972748, -0.0194092, 0.019165, ..., 0.048584, -0.0152588,\n",
              "                        -0.0169678],\n",
              "                       [-0.0269775, 0.00854492, 0.0145264, ..., -0.0169678,\n",
              "                        -0.0269775, -0.00738525],\n",
              "                       [0.0139771, -0.0111084, 0.0556641, ..., -0.020752, 0.00854492,\n",
              "                        0.00271606]],\n",
              "              \n",
              "                      [[-0.0251465, -0.0142212, -0.027832, ..., -0.0142212,\n",
              "                        -0.00921631, 0.0224609],\n",
              "                       [0.00140381, -0.00689697, -0.0201416, ..., -0.0106812,\n",
              "                        0.0292969, -0.0311279],\n",
              "                       [-0.00921631, 0.017334, 0.0283203, ..., -0.00738525,\n",
              "                        -0.0194092, 0.00314331],\n",
              "                       ...,\n",
              "                       [-0.0194092, -0.00337219, -0.0299072, ..., 0.048584,\n",
              "                        -0.024292, 0.00674438],\n",
              "                       [0.0185547, -0.012146, -0.00469971, ..., -0.00512695,\n",
              "                        -0.0169678, 0.0217285],\n",
              "                       [-0.020752, -0.0334473, 0.0161133, ..., 0.0197754, 0.0240479,\n",
              "                        -0.00292969]],\n",
              "              \n",
              "                      [[0.0224609, 0.0263672, 0.0358887, ..., -0.00248718, 0.041748,\n",
              "                        -0.0175781],\n",
              "                       [-0.00970459, 0.0109253, 0.00805664, ..., -0.0228271,\n",
              "                        0.0185547, -0.00646973],\n",
              "                       [0.010437, -0.012146, 0.0179443, ..., -0.0288086, -0.0405273,\n",
              "                        0.0129395],\n",
              "                       ...,\n",
              "                       [-0.0288086, 0.000541687, 0.00491333, ..., -0.026123,\n",
              "                        0.0305176, -0.0322266],\n",
              "                       [0.0556641, -0.0234375, 0.0071106, ..., -0.0515137,\n",
              "                        -0.00512695, 0.00271606],\n",
              "                       [-0.0639648, -0.0142212, -0.0142212, ..., -0.0201416,\n",
              "                        -0.0131836, -0.00689697]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.00227356, 0.00445557, 0.00227356, ..., -0.0164795,\n",
              "                        0.0150146, 0.017334],\n",
              "                       [0.0230713, 0.0255127, 0.0119019, ..., 0.0240479, 0.00271606,\n",
              "                        0.00674438],\n",
              "                       [0.000107765, -0.0201416, 0.0263672, ..., 0.0341797,\n",
              "                        -0.00601196, 0.000541687],\n",
              "                       ...,\n",
              "                       [-0.0269775, -0.0111084, 0.0263672, ..., -0.00119019,\n",
              "                        -0.00379944, 0.0255127],\n",
              "                       [0.000107765, 0.0375977, -0.0131836, ..., -0.0111084,\n",
              "                        0.0167236, -0.0126953],\n",
              "                       [-0.00248718, -0.0405273, 0.000541687, ..., 0.0292969,\n",
              "                        0.017334, 0.00140381]],\n",
              "              \n",
              "                      [[-0.00205994, -0.0201416, 0.0556641, ..., -0.00689697,\n",
              "                        -0.0078125, -0.00292969],\n",
              "                       [-0.00689697, 0.0119019, -0.0466309, ..., -0.00689697,\n",
              "                        0.0317383, -0.0131836],\n",
              "                       [-0.020752, 0.0358887, -0.0142212, ..., 0.000107765,\n",
              "                        0.0292969, 0.041748],\n",
              "                       ...,\n",
              "                       [0.00805664, -0.0288086, -0.0169678, ..., -0.0147705,\n",
              "                        -0.0187988, 0.032959],\n",
              "                       [0.0090332, 0.0341797, 0.032959, ..., 0.048584, -0.024292,\n",
              "                        0.00534058],\n",
              "                       [-0.0152588, 0.0167236, -0.0366211, ..., -0.0214844,\n",
              "                        -0.00921631, -0.0639648]],\n",
              "              \n",
              "                      [[0.00314331, -0.0164795, -0.0169678, ..., -0.0152588,\n",
              "                        0.00402832, -0.00921631],\n",
              "                       [-0.00830078, -0.0101929, 0.000107765, ..., -0.00878906,\n",
              "                        -0.0288086, -0.00469971],\n",
              "                       [0.0119019, 0.0240479, 0.00854492, ..., -0.0116577,\n",
              "                        0.00491333, 0.0292969],\n",
              "                       ...,\n",
              "                       [-0.026123, 0.000972748, -0.0147705, ..., -0.0251465,\n",
              "                        -0.0169678, -0.0311279],\n",
              "                       [-0.024292, -0.0349121, 0.00805664, ..., -0.0164795,\n",
              "                        -0.0101929, 0.00183105],\n",
              "                       [-0.0515137, -0.00646973, -0.0220947, ..., -0.0175781,\n",
              "                        -0.0322266, 0.00491333]]],\n",
              "              \n",
              "              \n",
              "                     [[[0.00534058, 0.00994873, 0.000107765, ..., -0.012146,\n",
              "                        -0.0214844, -0.0201416],\n",
              "                       [-0.00379944, 0.0062561, 0.0129395, ..., -0.0366211,\n",
              "                        0.00674438, -0.0515137],\n",
              "                       [-0.020752, -0.0055542, -0.0181885, ..., -0.0078125,\n",
              "                        0.00576782, -0.0126953],\n",
              "                       ...,\n",
              "                       [-0.012146, -0.00337219, 0.00183105, ..., -0.00878906,\n",
              "                        -0.0116577, 0.00946045],\n",
              "                       [-0.0322266, -0.00424194, -0.0169678, ..., 0.0341797,\n",
              "                        -0.000324249, 0.0240479],\n",
              "                       [0.0129395, -0.0515137, 0.032959, ..., 0.00759888, 0.0305176,\n",
              "                        0.00271606]],\n",
              "              \n",
              "                      [[0.0090332, 0.00314331, -0.0432129, ..., -0.020752,\n",
              "                        0.00314331, -0.0515137],\n",
              "                       [0.0129395, -0.0299072, 0.0341797, ..., -0.000759125,\n",
              "                        0.0305176, 0.00674438],\n",
              "                       [-0.0131836, -0.0234375, 0.0145264, ..., -0.0234375,\n",
              "                        0.00491333, -0.000324249],\n",
              "                       ...,\n",
              "                       [0.00805664, -0.0269775, -0.027832, ..., 0.0283203,\n",
              "                        -0.00921631, -0.00292969],\n",
              "                       [-0.0311279, 0.0129395, 0.0444336, ..., -0.0175781,\n",
              "                        -0.00248718, 0.00759888],\n",
              "                       [-0.00379944, -0.00878906, -0.0515137, ..., 0.000541687,\n",
              "                        -0.0078125, 0.00805664]],\n",
              "              \n",
              "                      [[-0.0126953, 0.00805664, 0.010437, ..., -0.0366211,\n",
              "                        -0.0116577, -0.00512695],\n",
              "                       [0.0211182, -0.0106812, -0.00738525, ..., -0.0405273,\n",
              "                        -0.00878906, -0.0383301],\n",
              "                       [0.00183105, -0.0234375, 0.0255127, ..., 0.000972748,\n",
              "                        0.0341797, 0.00946045],\n",
              "                       ...,\n",
              "                       [0.0119019, 0.00759888, 0.0230713, ..., -0.0288086,\n",
              "                        0.00358582, 0.0249023],\n",
              "                       [-0.00292969, -0.0269775, 0.0145264, ..., 0.0203857,\n",
              "                        0.00854492, -0.0147705],\n",
              "                       [-0.0169678, 0.032959, -0.0078125, ..., 0.00227356,\n",
              "                        -0.0322266, -0.0106812]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[-0.0269775, 0.0114136, -0.00601196, ..., 0.00576782,\n",
              "                        -0.0111084, 0.048584],\n",
              "                       [-0.00424194, -0.00738525, 0.0150146, ..., 0.0090332,\n",
              "                        -0.0152588, 0.0203857],\n",
              "                       [0.0062561, 0.00534058, -0.012146, ..., 0.00854492,\n",
              "                        -0.000759125, 0.0305176],\n",
              "                       ...,\n",
              "                       [0.00227356, 0.0556641, 0.0124512, ..., 0.0119019,\n",
              "                        -0.00119019, 0.0197754],\n",
              "                       [0.0145264, -0.0288086, 0.00358582, ..., 0.0139771,\n",
              "                        -0.0311279, -0.0152588],\n",
              "                       [0.00491333, -0.0169678, 0.0305176, ..., -0.00337219,\n",
              "                        0.010437, -0.00469971]],\n",
              "              \n",
              "                      [[-0.00689697, -0.00292969, -0.0220947, ..., -0.020752,\n",
              "                        -0.00689697, -0.00469971],\n",
              "                       [0.0375977, -0.0116577, -0.0152588, ..., 0.017334, 0.00759888,\n",
              "                        -0.0106812],\n",
              "                       [-0.0366211, 0.0150146, -0.0234375, ..., 0.0341797, 0.017334,\n",
              "                        0.0203857],\n",
              "                       ...,\n",
              "                       [-0.0116577, -0.0101929, -0.0366211, ..., 0.0124512,\n",
              "                        0.00491333, 0.0240479],\n",
              "                       [-0.0228271, 0.0217285, -0.0101929, ..., 0.0249023,\n",
              "                        -0.0366211, -0.000759125],\n",
              "                       [-0.00878906, 0.00854492, 0.0134888, ..., -0.00601196,\n",
              "                        -0.00878906, 0.000972748]],\n",
              "              \n",
              "                      [[0.0114136, 0.000107765, 0.00358582, ..., -0.0181885,\n",
              "                        0.041748, -0.0142212],\n",
              "                       [0.00445557, -0.027832, 0.0185547, ..., 0.0185547,\n",
              "                        -0.000324249, 0.0556641],\n",
              "                       [0.0185547, 0.00534058, 0.015564, ..., 0.048584, -0.0137329,\n",
              "                        0.0139771],\n",
              "                       ...,\n",
              "                       [-0.0111084, -0.00424194, 0.00183105, ..., 0.0185547,\n",
              "                        0.0134888, -0.0405273],\n",
              "                       [0.0395508, 0.00445557, -0.0194092, ..., -0.0214844,\n",
              "                        0.00358582, 0.00994873],\n",
              "                       [0.0161133, 0.0375977, -0.0137329, ..., -0.0175781,\n",
              "                        0.00140381, 0.0161133]]],\n",
              "              \n",
              "              \n",
              "                     [[[0.0249023, 0.000541687, -0.00878906, ..., -0.0164795,\n",
              "                        0.017334, -0.0175781],\n",
              "                       [-0.0251465, 0.0249023, 0.00183105, ..., 0.00227356,\n",
              "                        -0.024292, 0.0341797],\n",
              "                       [0.0062561, 0.0114136, 0.0071106, ..., -0.0383301, 0.00271606,\n",
              "                        0.0317383],\n",
              "                       ...,\n",
              "                       [-0.0366211, 0.0145264, -0.0366211, ..., 0.00183105,\n",
              "                        0.0249023, -0.0639648],\n",
              "                       [0.00576782, 0.0305176, 0.000541687, ..., 0.00491333,\n",
              "                        0.048584, -0.00469971],\n",
              "                       [0.0444336, 0.0444336, 0.00946045, ..., -0.0432129, 0.0119019,\n",
              "                        -0.00337219]],\n",
              "              \n",
              "                      [[-0.027832, -0.0405273, 0.0109253, ..., -0.0116577, 0.0134888,\n",
              "                        0.0139771],\n",
              "                       [0.0129395, -0.0116577, 0.0395508, ..., 0.0283203,\n",
              "                        0.000972748, 0.0305176],\n",
              "                       [-0.0158691, 0.0197754, 0.048584, ..., 0.0129395, -0.0220947,\n",
              "                        0.015564],\n",
              "                       ...,\n",
              "                       [0.00271606, -0.0311279, 0.0283203, ..., -0.00970459,\n",
              "                        -0.00163269, -0.0101929],\n",
              "                       [-0.0405273, -0.00830078, -0.0106812, ..., -0.00292969,\n",
              "                        -0.00292969, 0.000972748],\n",
              "                       [0.0341797, -0.0234375, 0.0395508, ..., -0.0432129, 0.048584,\n",
              "                        -0.0228271]],\n",
              "              \n",
              "                      [[-0.0137329, -0.0515137, -0.00292969, ..., -0.0194092,\n",
              "                        -0.000759125, 0.00946045],\n",
              "                       [-0.0181885, -0.0432129, 0.000541687, ..., 0.0139771,\n",
              "                        0.0109253, -0.00424194],\n",
              "                       [-0.0432129, -0.0234375, 0.000107765, ..., -0.00337219,\n",
              "                        0.00534058, 0.017334],\n",
              "                       ...,\n",
              "                       [0.00576782, 0.0255127, 0.0341797, ..., 0.0161133, 0.032959,\n",
              "                        0.0375977],\n",
              "                       [0.0145264, 0.015564, -0.0515137, ..., 0.019165, -0.00424194,\n",
              "                        -0.00119019],\n",
              "                       [0.048584, -0.027832, -0.0322266, ..., 0.041748, -0.0164795,\n",
              "                        0.0114136]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.00946045, -0.0214844, -0.0111084, ..., -0.0164795,\n",
              "                        -0.0349121, 0.0274658],\n",
              "                       [-0.000324249, -0.0055542, 0.0090332, ..., 0.010437,\n",
              "                        0.0090332, 0.00805664],\n",
              "                       [0.0358887, 0.0230713, 0.0185547, ..., -0.000324249,\n",
              "                        0.00759888, -0.00337219],\n",
              "                       ...,\n",
              "                       [-0.00512695, 0.0341797, 0.00805664, ..., 0.000972748,\n",
              "                        0.0217285, -0.0194092],\n",
              "                       [-0.0078125, -0.0220947, -0.0299072, ..., 0.0062561,\n",
              "                        0.0211182, -0.0228271],\n",
              "                       [0.0230713, -0.00646973, -0.0334473, ..., -0.00205994,\n",
              "                        -0.0466309, 0.0129395]],\n",
              "              \n",
              "                      [[0.041748, 0.00227356, 0.00140381, ..., -0.027832, -0.0078125,\n",
              "                        -0.00205994],\n",
              "                       [0.00854492, -0.0181885, -0.0234375, ..., 0.0274658,\n",
              "                        -0.00205994, -0.0169678],\n",
              "                       [0.0185547, 0.0109253, -0.0288086, ..., -0.0234375,\n",
              "                        0.000541687, 0.00759888],\n",
              "                       ...,\n",
              "                       [0.0255127, -0.0432129, -0.012146, ..., -0.0194092, -0.020752,\n",
              "                        0.00759888],\n",
              "                       [0.0217285, 0.0317383, 0.00534058, ..., -0.0055542,\n",
              "                        0.00445557, 0.0167236],\n",
              "                       [0.0109253, -0.0405273, 0.017334, ..., -0.0466309, -0.0334473,\n",
              "                        0.0119019]],\n",
              "              \n",
              "                      [[-0.00970459, 0.0317383, 0.0119019, ..., 0.017334, -0.0158691,\n",
              "                        0.00759888],\n",
              "                       [0.0185547, -0.0220947, -0.0126953, ..., -0.0383301,\n",
              "                        0.0395508, -0.00646973],\n",
              "                       [0.00534058, -0.0288086, -0.026123, ..., -0.026123, 0.0249023,\n",
              "                        0.032959],\n",
              "                       ...,\n",
              "                       [-0.00379944, 0.0375977, -0.0152588, ..., 0.032959, 0.0358887,\n",
              "                        -0.00689697],\n",
              "                       [-0.00970459, -0.0366211, 0.0341797, ..., 0.00271606,\n",
              "                        0.032959, -0.00379944],\n",
              "                       [-0.0126953, 0.000107765, 0.00491333, ..., -0.000759125,\n",
              "                        -0.027832, 0.0203857]]],\n",
              "              \n",
              "              \n",
              "                     ...,\n",
              "              \n",
              "              \n",
              "                     [[[0.0556641, -0.0220947, 0.00759888, ..., -0.0383301,\n",
              "                        -0.00878906, -0.00248718],\n",
              "                       [0.0556641, 0.0197754, -0.0322266, ..., -0.00163269,\n",
              "                        0.0274658, -0.0466309],\n",
              "                       [0.0283203, -0.00646973, -0.026123, ..., -0.024292, 0.0139771,\n",
              "                        -0.0175781],\n",
              "                       ...,\n",
              "                       [-0.0432129, -0.027832, -0.00921631, ..., -0.00512695,\n",
              "                        -0.00119019, 0.0283203],\n",
              "                       [0.00491333, -0.00337219, 0.0292969, ..., -0.0311279,\n",
              "                        0.00314331, 0.00445557],\n",
              "                       [-0.00248718, -0.00292969, -0.0220947, ..., 0.00183105,\n",
              "                        0.00445557, 0.00854492]],\n",
              "              \n",
              "                      [[-0.0515137, -0.0366211, 0.0114136, ..., -0.0078125, 0.010437,\n",
              "                        0.00946045],\n",
              "                       [-0.0126953, -0.0101929, 0.0556641, ..., -0.00292969,\n",
              "                        0.010437, -0.00689697],\n",
              "                       [-0.0187988, -0.00601196, -0.00601196, ..., 0.0211182,\n",
              "                        0.0167236, -0.0366211],\n",
              "                       ...,\n",
              "                       [-0.0131836, -0.0366211, -0.0181885, ..., -0.0175781,\n",
              "                        0.00759888, -0.0126953],\n",
              "                       [-0.00337219, -0.00379944, 0.0249023, ..., 0.0161133,\n",
              "                        0.0062561, 0.00994873],\n",
              "                       [-0.0269775, -0.00830078, 0.0230713, ..., -0.0078125,\n",
              "                        -0.00379944, -0.0055542]],\n",
              "              \n",
              "                      [[-0.0131836, -0.0432129, -0.0078125, ..., -0.00601196,\n",
              "                        -0.0201416, 0.0150146],\n",
              "                       [0.017334, 0.0358887, -0.0515137, ..., 0.00674438, -0.0106812,\n",
              "                        0.048584],\n",
              "                       [-0.0466309, 0.00140381, -0.0131836, ..., -0.0152588,\n",
              "                        0.048584, -0.0311279],\n",
              "                       ...,\n",
              "                       [0.00140381, -0.00379944, -0.0639648, ..., 0.000541687,\n",
              "                        -0.0106812, 0.00491333],\n",
              "                       [0.0071106, -0.00689697, -0.00163269, ..., -0.0405273,\n",
              "                        -0.00163269, -0.000759125],\n",
              "                       [0.0109253, 0.0185547, 0.032959, ..., 0.0185547, 0.000972748,\n",
              "                        0.0375977]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.0139771, -0.0078125, -0.0334473, ..., 0.000541687,\n",
              "                        0.0062561, 0.019165],\n",
              "                       [-0.0299072, 0.00674438, -0.0078125, ..., 0.048584, 0.017334,\n",
              "                        0.00994873],\n",
              "                       [0.0119019, -0.0126953, 0.0211182, ..., 0.017334, -0.0142212,\n",
              "                        -0.0220947],\n",
              "                       ...,\n",
              "                       [0.0375977, -0.00248718, -0.00921631, ..., 0.000972748,\n",
              "                        -0.0405273, 0.000107765],\n",
              "                       [0.0263672, -0.0106812, 0.015564, ..., -0.0131836, 0.0167236,\n",
              "                        0.0230713],\n",
              "                       [-0.0194092, 0.00314331, 0.0185547, ..., 0.0161133,\n",
              "                        0.00445557, 0.0139771]],\n",
              "              \n",
              "                      [[0.00445557, -0.00337219, -0.0158691, ..., 0.00576782,\n",
              "                        -0.0234375, -0.0349121],\n",
              "                       [-0.0101929, -0.00337219, 0.019165, ..., -0.0251465,\n",
              "                        0.0134888, -0.0432129],\n",
              "                       [0.0197754, 0.048584, 0.0119019, ..., -0.00248718, 0.00183105,\n",
              "                        0.0211182],\n",
              "                       ...,\n",
              "                       [0.0129395, 0.0071106, 0.0197754, ..., 0.0556641, 0.0114136,\n",
              "                        -0.0126953],\n",
              "                       [0.00183105, -0.00512695, -0.026123, ..., 0.000541687,\n",
              "                        0.0124512, 0.0109253],\n",
              "                       [-0.00512695, -0.00970459, 0.0129395, ..., 0.00534058,\n",
              "                        -0.00337219, -0.0164795]],\n",
              "              \n",
              "                      [[-0.0101929, 0.00994873, -0.0383301, ..., 0.0134888,\n",
              "                        -0.0116577, 0.000972748],\n",
              "                       [0.00674438, 0.0139771, -0.0269775, ..., -0.00738525,\n",
              "                        -0.00646973, -0.0187988],\n",
              "                       [-0.0158691, 0.0444336, 0.0224609, ..., 0.00140381,\n",
              "                        -0.0175781, -0.0288086],\n",
              "                       ...,\n",
              "                       [0.0114136, -0.0214844, 0.0217285, ..., 0.017334, 0.00271606,\n",
              "                        -0.00646973],\n",
              "                       [-0.0366211, 0.0395508, 0.0255127, ..., 0.00491333,\n",
              "                        -0.0158691, -0.00646973],\n",
              "                       [0.0230713, -0.0288086, 0.0185547, ..., 0.00854492,\n",
              "                        -0.00205994, -0.0639648]]],\n",
              "              \n",
              "              \n",
              "                     [[[0.0395508, 0.00445557, -0.0055542, ..., -0.00337219,\n",
              "                        -0.0126953, -0.0152588],\n",
              "                       [-0.0299072, -0.00163269, 0.0358887, ..., 0.0255127,\n",
              "                        0.00183105, 0.00445557],\n",
              "                       [0.000541687, -0.0299072, 0.00759888, ..., -0.0334473,\n",
              "                        -0.0269775, -0.00646973],\n",
              "                       ...,\n",
              "                       [-0.00205994, 0.0145264, 0.0292969, ..., -0.0322266,\n",
              "                        -0.00424194, -0.00830078],\n",
              "                       [0.0224609, 0.0129395, -0.0228271, ..., 0.00994873,\n",
              "                        0.00314331, -0.0349121],\n",
              "                       [0.0375977, 0.0161133, 0.0179443, ..., -0.0383301, -0.0169678,\n",
              "                        -0.0137329]],\n",
              "              \n",
              "                      [[-0.00646973, 0.0090332, -0.0142212, ..., 0.00227356,\n",
              "                        -0.0639648, -0.027832],\n",
              "                       [-0.027832, -0.00205994, 0.0274658, ..., -0.0164795,\n",
              "                        -0.0299072, -0.0405273],\n",
              "                       [0.00534058, 0.00358582, 0.0119019, ..., -0.0078125,\n",
              "                        -0.00512695, -0.0322266],\n",
              "                       ...,\n",
              "                       [0.0114136, 0.0150146, 0.0317383, ..., -0.00469971, 0.0283203,\n",
              "                        -0.0131836],\n",
              "                       [-0.0111084, 0.0109253, -0.00646973, ..., 0.0230713, 0.041748,\n",
              "                        0.000107765],\n",
              "                       [0.00358582, -0.00337219, 0.0317383, ..., 0.00227356,\n",
              "                        0.0305176, 0.015564]],\n",
              "              \n",
              "                      [[0.032959, -0.0126953, -0.00119019, ..., -0.00512695,\n",
              "                        -0.00119019, 0.0119019],\n",
              "                       [0.0071106, -0.027832, -0.020752, ..., -0.0131836,\n",
              "                        -0.00248718, 0.017334],\n",
              "                       [-0.00205994, 0.00674438, -0.00646973, ..., 0.019165,\n",
              "                        -0.00512695, -0.0432129],\n",
              "                       ...,\n",
              "                       [-0.0116577, -0.0126953, -0.00205994, ..., -0.0466309,\n",
              "                        0.019165, 0.00994873],\n",
              "                       [0.0317383, -0.0515137, -0.0137329, ..., -0.0175781,\n",
              "                        0.0139771, 0.00271606],\n",
              "                       [0.0129395, 0.015564, -0.0101929, ..., 0.00759888,\n",
              "                        -0.00163269, -0.0131836]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.0341797, -0.00248718, -0.00738525, ..., 0.00183105,\n",
              "                        -0.0466309, -0.0201416],\n",
              "                       [0.017334, -0.024292, 0.048584, ..., -0.027832, 0.0230713,\n",
              "                        -0.0137329],\n",
              "                       [0.00271606, -0.00248718, 0.00994873, ..., 0.0179443,\n",
              "                        0.00491333, -0.0126953],\n",
              "                       ...,\n",
              "                       [-0.0101929, 0.0224609, -0.00292969, ..., -0.00738525,\n",
              "                        -0.0269775, -0.00379944],\n",
              "                       [0.00227356, 0.0145264, 0.00445557, ..., 0.0283203,\n",
              "                        0.00227356, 0.00674438],\n",
              "                       [0.0224609, 0.0240479, 0.00854492, ..., 0.0179443, -0.0137329,\n",
              "                        0.00358582]],\n",
              "              \n",
              "                      [[0.0217285, -0.00379944, -0.00469971, ..., 0.0203857,\n",
              "                        0.0114136, 0.010437],\n",
              "                       [-0.00830078, -0.0142212, 0.041748, ..., 0.0305176,\n",
              "                        -0.0055542, -0.0288086],\n",
              "                       [0.0197754, 0.0134888, -0.0220947, ..., 0.0556641, 0.015564,\n",
              "                        0.017334],\n",
              "                       ...,\n",
              "                       [-0.027832, 0.00140381, 0.00227356, ..., 0.0179443,\n",
              "                        0.00314331, -0.00970459],\n",
              "                       [-0.00921631, -0.0349121, -0.00163269, ..., -0.00738525,\n",
              "                        -0.0322266, -0.0126953],\n",
              "                       [0.00402832, 0.0203857, -0.0515137, ..., 0.0139771, 0.0217285,\n",
              "                        -0.00379944]],\n",
              "              \n",
              "                      [[0.00227356, 0.0224609, -0.00921631, ..., 0.0217285,\n",
              "                        0.0358887, -0.0175781],\n",
              "                       [-0.0106812, 0.0124512, -0.0311279, ..., 0.0139771,\n",
              "                        -0.00163269, -0.0187988],\n",
              "                       [0.032959, 0.019165, 0.0240479, ..., 0.0341797, -0.00970459,\n",
              "                        0.0274658],\n",
              "                       ...,\n",
              "                       [0.0358887, 0.0255127, -0.0164795, ..., 0.015564, 0.0263672,\n",
              "                        0.019165],\n",
              "                       [0.000107765, -0.00878906, -0.0334473, ..., 0.0167236,\n",
              "                        0.00183105, 0.0134888],\n",
              "                       [-0.0078125, 0.00402832, 0.0114136, ..., -0.0299072,\n",
              "                        0.0263672, -0.0078125]]],\n",
              "              \n",
              "              \n",
              "                     [[[0.0129395, -0.000324249, 0.00445557, ..., 0.0556641,\n",
              "                        -0.00646973, -0.00878906],\n",
              "                       [0.015564, -0.0147705, 0.0224609, ..., 0.0071106, -0.012146,\n",
              "                        -0.000759125],\n",
              "                       [-0.0078125, 0.00994873, 0.0274658, ..., 0.00358582,\n",
              "                        0.0224609, -0.00119019],\n",
              "                       ...,\n",
              "                       [0.0249023, 0.00674438, 0.00491333, ..., 0.00314331,\n",
              "                        -0.0131836, -0.00163269],\n",
              "                       [-0.00921631, -0.00512695, 0.00183105, ..., 0.0109253,\n",
              "                        -0.0515137, 0.0203857],\n",
              "                       [-0.00248718, 0.0292969, 0.0341797, ..., -0.0055542,\n",
              "                        0.0071106, -0.00163269]],\n",
              "              \n",
              "                      [[-0.00292969, -0.0432129, -0.0101929, ..., -0.0311279,\n",
              "                        -0.00424194, 0.000107765],\n",
              "                       [-0.0111084, 0.0119019, 0.0240479, ..., 0.017334, 0.00402832,\n",
              "                        -0.0234375],\n",
              "                       [-0.0111084, -0.0078125, -0.024292, ..., -0.0055542,\n",
              "                        -0.0147705, -0.00689697],\n",
              "                       ...,\n",
              "                       [0.0134888, -0.00646973, -0.000759125, ..., -0.0214844,\n",
              "                        -0.0101929, -0.0228271],\n",
              "                       [-0.0078125, -0.024292, 0.041748, ..., 0.0444336, -0.024292,\n",
              "                        0.00140381],\n",
              "                       [0.032959, 0.0444336, 0.0358887, ..., 0.00576782, -0.024292,\n",
              "                        0.0197754]],\n",
              "              \n",
              "                      [[-0.000324249, 0.0167236, -0.0169678, ..., 0.00271606,\n",
              "                        0.00759888, -0.0116577],\n",
              "                       [0.00491333, 0.0292969, 0.0341797, ..., 0.0263672, -0.0366211,\n",
              "                        0.00140381],\n",
              "                       [-0.00424194, 0.0150146, -0.00379944, ..., -0.00469971,\n",
              "                        0.041748, 0.00534058],\n",
              "                       ...,\n",
              "                       [-0.0131836, -0.00512695, 0.0134888, ..., -0.0220947,\n",
              "                        0.0217285, -0.0334473],\n",
              "                       [0.00534058, 0.0255127, -0.0055542, ..., -0.0106812,\n",
              "                        0.0062561, 0.0255127],\n",
              "                       [-0.00830078, 0.0124512, -0.0234375, ..., 0.00402832,\n",
              "                        -0.0175781, 0.0185547]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.00674438, -0.0142212, 0.0185547, ..., 0.0249023,\n",
              "                        -0.0405273, -0.0515137],\n",
              "                       [-0.0181885, 0.0375977, -0.0466309, ..., 0.0283203,\n",
              "                        -0.0288086, -0.00424194],\n",
              "                       [-0.00424194, 0.0167236, 0.00674438, ..., 0.015564,\n",
              "                        -0.00424194, 0.019165],\n",
              "                       ...,\n",
              "                       [0.0249023, 0.0119019, 0.0317383, ..., 0.019165, -0.0432129,\n",
              "                        -0.00689697],\n",
              "                       [-0.0055542, -0.00738525, -0.00970459, ..., 0.00576782,\n",
              "                        -0.00646973, 0.00445557],\n",
              "                       [0.000107765, -0.0147705, 0.0263672, ..., -0.00830078,\n",
              "                        0.00759888, -0.0152588]],\n",
              "              \n",
              "                      [[0.00314331, -0.00248718, 0.0129395, ..., -0.0137329,\n",
              "                        0.000541687, -0.00970459],\n",
              "                       [-0.00337219, -0.0201416, 0.0185547, ..., -0.00469971,\n",
              "                        0.0114136, -0.0299072],\n",
              "                       [0.0134888, 0.032959, 0.0556641, ..., -0.0349121, 0.00946045,\n",
              "                        0.041748],\n",
              "                       ...,\n",
              "                       [0.00402832, -0.0269775, 0.0062561, ..., -0.0334473,\n",
              "                        -0.0269775, 0.0139771],\n",
              "                       [0.000107765, -0.00205994, 0.0134888, ..., 0.000541687,\n",
              "                        -0.0220947, 0.00445557],\n",
              "                       [-0.027832, -0.0251465, -0.020752, ..., -0.0116577,\n",
              "                        -0.0175781, 0.019165]],\n",
              "              \n",
              "                      [[0.0124512, -0.0322266, 0.0071106, ..., 0.0109253, 0.010437,\n",
              "                        0.0255127],\n",
              "                       [-0.0175781, -0.0137329, -0.0515137, ..., 0.0255127, 0.032959,\n",
              "                        -0.00248718],\n",
              "                       [-0.00469971, -0.0383301, -0.00970459, ..., -0.00424194,\n",
              "                        0.00140381, 0.0185547],\n",
              "                       ...,\n",
              "                       [-0.000324249, 0.0114136, 0.0240479, ..., -0.0055542,\n",
              "                        0.0197754, 0.0444336],\n",
              "                       [0.0249023, -0.0322266, -0.0142212, ..., -0.0137329,\n",
              "                        0.000107765, 0.0203857],\n",
              "                       [0.048584, 0.00271606, -0.0152588, ..., -0.0131836, 0.015564,\n",
              "                        0.00314331]]]], dtype=bfloat16),\n",
              "              mesh=None,\n",
              "              sharding=('heads', 'layers', 'kv', 'embed'),\n",
              "              sharding_rules=None,\n",
              "              linen_meta_type=LogicallyPartitioned\n",
              "            )\n",
              "          },\n",
              "          'query': {\n",
              "            'kernel': Param( # 75,497,472 (151.0 MB)\n",
              "              value=Array([[[[0.000169754, 0.000873566, 0.00135803, ..., -0.000759125,\n",
              "                        0.000682831, -0.000823975],\n",
              "                       [-0.000858307, -0.000375748, 0.000652313, ..., -0.000858307,\n",
              "                        -0.00109863, -0.000823975],\n",
              "                       [0.000907898, -0.000667572, 0.000360489, ..., -0.00138092,\n",
              "                        0.000142097, 0.0015564],\n",
              "                       ...,\n",
              "                       [0.000682831, 0.000421524, 6.73532e-06, ..., 0.00234985,\n",
              "                        -0.00146484, 0.000114441],\n",
              "                       [0.000360489, -0.0032196, 0.00234985, ..., -0.000953674,\n",
              "                        -0.00109863, 0.00047493],\n",
              "                       [-0.00201416, -0.0018692, 0.00159454, ..., 0.000169754,\n",
              "                        -0.00106049, -0.000923157]],\n",
              "              \n",
              "                      [[-0.00209045, 0.000778198, 0.00104523, ..., 6.73532e-06,\n",
              "                        0.00198364, -0.00157166],\n",
              "                       [0.000307083, 0.000972748, 0.00171661, ..., 0.000938416,\n",
              "                        -7.43866e-05, -0.000155449],\n",
              "                       [-0.000265121, -0.00109863, -7.43866e-05, ..., -0.0014267,\n",
              "                        -0.000759125, 0.00123596],\n",
              "                       ...,\n",
              "                       [-0.00163269, 0.00224304, -0.0032196, ..., -0.000265121,\n",
              "                        -0.00109863, -0.000606537],\n",
              "                       [-0.00239563, 0.00260925, -0.0018692, ..., -0.000293732,\n",
              "                        -0.001297, -0.00209045],\n",
              "                       [0.000743866, -4.74453e-05, -0.000404358, ..., 0.000713348,\n",
              "                        6.07967e-05, -0.000461578]],\n",
              "              \n",
              "                      [[0.00112152, -2.02656e-05, -0.000637054, ..., -7.43866e-05,\n",
              "                        -0.000102043, 0.0027771],\n",
              "                       [-0.000461578, -0.000102043, 0.00047493, ..., 0.00213623,\n",
              "                        -0.000237465, 0.00247192],\n",
              "                       [-0.000320435, -0.000953674, -0.0019455, ..., -0.000858307,\n",
              "                        0.00144196, -0.000102043],\n",
              "                       ...,\n",
              "                       [0.00213623, 0.000972748, 0.00260925, ..., -0.000320435,\n",
              "                        -0.000637054, 0.000333786],\n",
              "                       [-0.0032196, -0.0016861, -0.000759125, ..., -0.000210762,\n",
              "                        6.07967e-05, -0.000549316],\n",
              "                       [-0.00228882, 0.000652313, -0.00109863, ..., -0.000265121,\n",
              "                        -0.00109863, 0.000307083]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[-0.000431061, 0.000278473, -0.00228882, ..., 0.00119781,\n",
              "                        -0.00138092, 0.000621796],\n",
              "                       [0.000534058, -0.000694275, -0.00134277, ..., 0.000333786,\n",
              "                        -0.000888824, -0.00291443],\n",
              "                       [-0.000404358, -0.0018692, 0.000224113, ..., -7.43866e-05,\n",
              "                        -0.000667572, 0.000278473],\n",
              "                       ...,\n",
              "                       [-0.00239563, 0.000534058, 0.000621796, ..., 0.00183105,\n",
              "                        -0.00239563, 0.00213623],\n",
              "                       [8.7738e-05, 0.00177002, 0.000808716, ..., -0.0016861,\n",
              "                        0.000843048, -0.000858307],\n",
              "                       [-0.000375748, 0.00150299, -0.000793457, ..., -2.02656e-05,\n",
              "                        0.00190735, -0.000728607]],\n",
              "              \n",
              "                      [[0.00260925, -0.000102043, 0.000778198, ..., 0.000621796,\n",
              "                        -0.000694275, -0.00228882],\n",
              "                       [-0.00106049, 0.00144196, 0.000224113, ..., -0.00146484,\n",
              "                        0.00131989, 0.000333786],\n",
              "                       [-0.000637054, -0.000694275, 0.00159454, ..., -0.000991821,\n",
              "                        0.00198364, 0.00159454],\n",
              "                       ...,\n",
              "                       [-0.00291443, -0.00109863, -0.00253296, ..., -0.000210762,\n",
              "                        -0.00121307, -0.0032196],\n",
              "                       [-0.000576019, 0.00190735, 0.00127411, ..., 0.00213623,\n",
              "                        -0.000375748, 0.000873566],\n",
              "                       [-0.000667572, 0.00205994, -0.000606537, ..., 0.00177002,\n",
              "                        0.000307083, 0.000591278]],\n",
              "              \n",
              "                      [[0.000843048, -0.000793457, -0.000128746, ..., 0.000333786,\n",
              "                        -0.000576019, -0.001297],\n",
              "                       [0.00127411, -0.000102043, 0.00144196, ..., 0.00131989,\n",
              "                        0.00100708, 0.000278473],\n",
              "                       [-0.00228882, 6.07967e-05, 0.00119781, ..., -0.000728607,\n",
              "                        0.000421524, -0.00157166],\n",
              "                       ...,\n",
              "                       [-0.00102997, 0.0015564, 0.000169754, ..., 0.000114441,\n",
              "                        -0.0018692, 0.00131989],\n",
              "                       [3.38554e-05, -0.001297, 8.7738e-05, ..., 0.00213623,\n",
              "                        0.000682831, 0.000224113],\n",
              "                       [0.000142097, -0.00113678, -0.00157166, ..., -0.0018692,\n",
              "                        -0.00218201, -0.00134277]]],\n",
              "              \n",
              "              \n",
              "                     [[[-0.0014267, -0.000953674, 0.00050354, ..., 0.00100708,\n",
              "                        0.00140381, 0.000591278],\n",
              "                       [0.00112152, 0.00050354, -0.000576019, ..., -0.000375748,\n",
              "                        -0.00163269, -0.000293732],\n",
              "                       [-0.000667572, -0.000461578, 0.00183105, ..., 0.000591278,\n",
              "                        -0.00218201, -0.000823975],\n",
              "                       ...,\n",
              "                       [-7.43866e-05, -0.000759125, -0.00228882, ..., 0.000652313,\n",
              "                        0.000114441, 0.000360489],\n",
              "                       [-0.00146484, -0.00218201, -0.000923157, ..., 0.00213623,\n",
              "                        -0.000953674, -0.00180054],\n",
              "                       [0.00104523, -0.000694275, 0.000591278, ..., 0.00190735,\n",
              "                        -0.00209045, 0.000307083]],\n",
              "              \n",
              "                      [[-0.000858307, -0.00228882, 0.00047493, ..., 0.000333786,\n",
              "                        -0.000728607, -0.000461578],\n",
              "                       [-0.0014267, -0.000858307, -0.00253296, ..., -0.0019455,\n",
              "                        -0.000375748, -0.00134277],\n",
              "                       [0.00108337, -0.00106049, 0.000142097, ..., 0.003479,\n",
              "                        -0.00180054, 0.000621796],\n",
              "                       ...,\n",
              "                       [-0.000210762, -0.00180054, -0.00291443, ..., 0.000421524,\n",
              "                        -0.000991821, 0.000564575],\n",
              "                       [-7.43866e-05, 0.000564575, -0.000293732, ..., 0.00140381,\n",
              "                        0.00190735, 0.000196457],\n",
              "                       [0.000360489, -2.02656e-05, 0.000278473, ..., -0.0017395,\n",
              "                        -0.00121307, 0.00171661]],\n",
              "              \n",
              "                      [[0.000360489, -0.00253296, 0.00025177, ..., 0.00234985,\n",
              "                        -0.00134277, -0.00102997],\n",
              "                       [0.00127411, 0.000713348, 0.00108337, ..., 0.000873566,\n",
              "                        -0.000728607, 6.07967e-05],\n",
              "                       [-0.00151825, -0.000431061, -0.000637054, ..., 0.000534058,\n",
              "                        0.000534058, -0.00180054],\n",
              "                       ...,\n",
              "                       [-0.00134277, 0.00100708, 0.000843048, ..., 0.000391006,\n",
              "                        -0.00163269, -0.00209045],\n",
              "                       [-0.00157166, -0.000637054, 0.000196457, ..., -4.74453e-05,\n",
              "                        -0.000667572, -0.0032196],\n",
              "                       [0.000778198, 6.73532e-06, 0.003479, ..., -0.00209045,\n",
              "                        -0.000461578, 0.000808716]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.0015564, -0.00163269, -0.00146484, ..., -0.0017395,\n",
              "                        -0.0039978, 0.000534058],\n",
              "                       [-0.000637054, 0.000808716, -0.00239563, ..., -0.000606537,\n",
              "                        -0.000347137, -0.0014267],\n",
              "                       [-0.000953674, 0.000907898, 0.000591278, ..., 0.000652313,\n",
              "                        0.000169754, -0.000858307],\n",
              "                       ...,\n",
              "                       [0.000169754, -0.000128746, -2.02656e-05, ..., 0.000778198,\n",
              "                        -0.000183105, 0.00104523],\n",
              "                       [-0.000210762, 0.00115967, 0.000444412, ..., -0.000549316,\n",
              "                        0.000778198, -0.000667572],\n",
              "                       [-0.00253296, 0.003479, 0.00140381, ..., 0.00150299,\n",
              "                        -0.0032196, 0.000391006]],\n",
              "              \n",
              "                      [[-0.0016861, 0.000333786, -0.0018692, ..., 0.00108337,\n",
              "                        0.003479, 0.0027771],\n",
              "                       [-0.00113678, 0.000360489, -0.000155449, ..., -0.0019455,\n",
              "                        -0.000576019, 0.000972748],\n",
              "                       [-0.0016861, 0.0027771, 0.0027771, ..., -0.00163269,\n",
              "                        0.000421524, -0.000549316],\n",
              "                       ...,\n",
              "                       [0.000169754, -0.0018692, -0.0019455, ..., -0.00157166,\n",
              "                        -0.000759125, 6.07967e-05],\n",
              "                       [0.000444412, 0.0027771, 0.000907898, ..., -0.000431061,\n",
              "                        -0.000888824, 0.00104523],\n",
              "                       [-0.000637054, -0.0014267, 0.000907898, ..., 0.0027771,\n",
              "                        0.000224113, 0.000360489]],\n",
              "              \n",
              "                      [[0.00234985, 0.00198364, 0.00260925, ..., 0.00234985,\n",
              "                        0.00050354, 0.00140381],\n",
              "                       [-0.000923157, -0.000210762, 6.73532e-06, ..., -0.000237465,\n",
              "                        -0.000991821, -0.000728607],\n",
              "                       [-0.0018692, -0.00228882, 0.00234985, ..., -0.00138092,\n",
              "                        0.003479, -0.000991821],\n",
              "                       ...,\n",
              "                       [-0.000347137, -0.0014267, 0.000391006, ..., 0.000591278,\n",
              "                        0.000808716, -0.000375748],\n",
              "                       [8.7738e-05, -0.00163269, -0.0016861, ..., -0.000488281,\n",
              "                        0.00183105, 0.000142097],\n",
              "                       [-0.00218201, -0.000155449, -0.0014267, ..., 0.00140381,\n",
              "                        0.00224304, -0.000823975]]],\n",
              "              \n",
              "              \n",
              "                     [[[0.000743866, 0.000713348, 0.00108337, ..., 0.00198364,\n",
              "                        -0.000888824, 0.000169754],\n",
              "                       [0.00115967, -0.00117493, -0.000183105, ..., -0.0017395,\n",
              "                        0.000972748, 0.000391006],\n",
              "                       [0.00119781, 0.000307083, -0.000694275, ..., 0.00260925,\n",
              "                        0.00190735, 0.00135803],\n",
              "                       ...,\n",
              "                       [0.000713348, 0.000333786, 0.00164795, ..., 0.00025177,\n",
              "                        0.00025177, 0.000196457],\n",
              "                       [0.00213623, 0.00144196, -0.000991821, ..., -0.00121307,\n",
              "                        -0.00228882, 0.0015564],\n",
              "                       [0.00047493, 0.00177002, 0.00190735, ..., 0.000682831,\n",
              "                        -0.00138092, -0.000793457]],\n",
              "              \n",
              "                      [[0.000873566, -0.00134277, -0.001297, ..., 0.00025177,\n",
              "                        0.00183105, -0.00138092],\n",
              "                       [0.00108337, -0.000549316, 0.000142097, ..., -0.00113678,\n",
              "                        -0.000265121, 0.00112152],\n",
              "                       [-0.0018692, -0.00209045, -0.000637054, ..., 0.000591278,\n",
              "                        0.00213623, 0.00224304],\n",
              "                       ...,\n",
              "                       [-0.00138092, -0.000667572, 0.000591278, ..., 0.00119781,\n",
              "                        -0.000320435, 0.00119781],\n",
              "                       [0.000444412, 0.00224304, -0.00138092, ..., -0.00209045,\n",
              "                        0.00115967, 0.0015564],\n",
              "                       [0.0030365, -0.000347137, 0.00247192, ..., -0.0017395,\n",
              "                        -0.000293732, -0.000210762]],\n",
              "              \n",
              "                      [[-0.0014267, 0.00247192, 0.000196457, ..., -0.00291443,\n",
              "                        -0.000347137, 0.000142097],\n",
              "                       [0.000169754, -0.000759125, 0.0015564, ..., 0.000778198,\n",
              "                        -0.000991821, 0.000621796],\n",
              "                       [0.00131989, -0.000183105, 0.00164795, ..., 0.00164795,\n",
              "                        -0.000488281, -0.000210762],\n",
              "                       ...,\n",
              "                       [0.000843048, -0.000102043, -0.00201416, ..., -0.000347137,\n",
              "                        0.000391006, 0.000224113],\n",
              "                       [-0.00239563, 0.00234985, 0.00050354, ..., -0.000606537,\n",
              "                        -0.000606537, 0.000278473],\n",
              "                       [-0.000102043, 0.000564575, -0.00138092, ..., -0.00201416,\n",
              "                        -2.02656e-05, -0.000823975]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.00159454, -0.000488281, 0.000307083, ..., 0.00198364,\n",
              "                        0.000391006, -0.00117493],\n",
              "                       [-0.000637054, -0.00113678, 0.00140381, ..., -0.000759125,\n",
              "                        -0.000183105, -0.000265121],\n",
              "                       [0.000591278, -0.000518799, -0.000888824, ..., -0.000888824,\n",
              "                        0.003479, -0.00146484],\n",
              "                       ...,\n",
              "                       [0.00135803, -0.00121307, 0.00112152, ..., -0.000518799,\n",
              "                        -4.74453e-05, 0.00164795],\n",
              "                       [0.000169754, 0.00047493, 0.00047493, ..., -0.0019455,\n",
              "                        -0.000923157, -0.00228882],\n",
              "                       [-0.00239563, 0.000843048, 0.00171661, ..., 0.000114441,\n",
              "                        -0.00134277, -0.0019455]],\n",
              "              \n",
              "                      [[-0.00270081, -0.000293732, 0.000534058, ..., -0.0039978,\n",
              "                        -0.00121307, -4.74453e-05],\n",
              "                       [-0.00253296, -0.000667572, 0.00140381, ..., 0.00150299,\n",
              "                        0.000360489, -0.000953674],\n",
              "                       [0.00213623, -0.00134277, -0.000128746, ..., -0.00180054,\n",
              "                        0.000778198, -0.000576019],\n",
              "                       ...,\n",
              "                       [6.73532e-06, 0.000621796, 0.000743866, ..., -0.000237465,\n",
              "                        -0.000823975, 0.00224304],\n",
              "                       [0.00159454, -0.000694275, 0.00213623, ..., 0.00127411,\n",
              "                        -0.000637054, 0.000682831],\n",
              "                       [0.000652313, -0.000923157, -0.000667572, ..., 0.000360489,\n",
              "                        -0.00253296, -0.000576019]],\n",
              "              \n",
              "                      [[-0.000347137, -0.0018692, 0.00150299, ..., 0.00127411,\n",
              "                        -0.000461578, 0.00234985],\n",
              "                       [0.000444412, 0.00205994, -0.00253296, ..., -0.00117493,\n",
              "                        -0.001297, -0.00117493],\n",
              "                       [0.000224113, -0.000728607, -0.000128746, ..., -0.0018692,\n",
              "                        -0.000293732, -0.00121307],\n",
              "                       ...,\n",
              "                       [0.000421524, 0.0027771, 0.000360489, ..., 0.000360489,\n",
              "                        -0.00117493, 0.00247192],\n",
              "                       [-0.00291443, 0.000196457, -0.0017395, ..., -7.43866e-05,\n",
              "                        -0.00253296, -0.00134277],\n",
              "                       [0.0027771, 0.00104523, -0.00134277, ..., -0.000375748,\n",
              "                        -0.000320435, -0.00163269]]],\n",
              "              \n",
              "              \n",
              "                     ...,\n",
              "              \n",
              "              \n",
              "                     [[[-0.00125885, 0.00205994, -0.000293732, ..., -0.00180054,\n",
              "                        0.00047493, 0.000713348],\n",
              "                       [0.00205994, -0.00209045, -0.000155449, ..., -0.000488281,\n",
              "                        0.00177002, 3.38554e-05],\n",
              "                       [0.000142097, -0.001297, -0.000576019, ..., -4.74453e-05,\n",
              "                        -0.000793457, 0.000534058],\n",
              "                       ...,\n",
              "                       [0.00127411, -0.00151825, -0.0014267, ..., 0.000743866,\n",
              "                        -0.000102043, -0.000858307],\n",
              "                       [-7.43866e-05, -0.00291443, 0.00159454, ..., 0.0030365,\n",
              "                        -0.00209045, 0.000713348],\n",
              "                       [0.00213623, 3.38554e-05, 0.00119781, ..., 0.000621796,\n",
              "                        0.000224113, 0.00260925]],\n",
              "              \n",
              "                      [[0.00260925, -0.000375748, 0.00050354, ..., -0.00146484,\n",
              "                        0.000682831, 0.0027771],\n",
              "                       [-2.02656e-05, 6.07967e-05, -0.000265121, ..., 0.000444412,\n",
              "                        0.000307083, -0.00228882],\n",
              "                       [-0.0018692, -0.000320435, -0.00151825, ..., 0.000114441,\n",
              "                        -0.00138092, 0.000278473],\n",
              "                       ...,\n",
              "                       [0.000713348, 0.000938416, 0.000333786, ..., -0.000991821,\n",
              "                        0.0015564, 0.00047493],\n",
              "                       [0.00144196, -0.000488281, 0.00150299, ..., -0.000347137,\n",
              "                        0.000713348, -0.000637054],\n",
              "                       [-0.00270081, -0.000128746, 0.000591278, ..., -0.000888824,\n",
              "                        -0.000858307, -0.00125885]],\n",
              "              \n",
              "                      [[-0.000793457, 0.00190735, 8.7738e-05, ..., -2.02656e-05,\n",
              "                        0.000196457, -0.00270081],\n",
              "                       [-0.000606537, -0.001297, 0.000972748, ..., -0.000576019,\n",
              "                        0.000142097, 0.000682831],\n",
              "                       [-0.0016861, -0.000858307, -0.000823975, ..., -0.0014267,\n",
              "                        0.00159454, -0.000461578],\n",
              "                       ...,\n",
              "                       [0.00164795, -0.000667572, -0.0014267, ..., 8.7738e-05,\n",
              "                        -0.000606537, -0.000375748],\n",
              "                       [-0.00253296, -0.00291443, -0.00109863, ..., -0.00138092,\n",
              "                        0.00183105, 0.00115967],\n",
              "                       [-0.00109863, 8.7738e-05, -0.000210762, ..., -0.00151825,\n",
              "                        -0.00102997, -0.00117493]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.00100708, -0.000293732, -0.0019455, ..., -0.001297,\n",
              "                        0.00144196, 0.000360489],\n",
              "                       [0.000307083, -0.00239563, -0.00102997, ..., 0.000196457,\n",
              "                        0.00047493, -0.00109863],\n",
              "                       [0.00224304, 0.000621796, -4.74453e-05, ..., -0.0017395,\n",
              "                        -0.00209045, 0.000114441],\n",
              "                       ...,\n",
              "                       [-0.000991821, 0.00198364, 0.000224113, ..., 0.000196457,\n",
              "                        -0.00134277, 0.000682831],\n",
              "                       [0.00171661, -0.0032196, -0.000320435, ..., 0.000196457,\n",
              "                        0.000778198, -0.0032196],\n",
              "                       [0.00119781, -0.000320435, 0.0030365, ..., 0.000169754,\n",
              "                        -0.00151825, -0.00151825]],\n",
              "              \n",
              "                      [[0.000843048, -0.000793457, -0.000128746, ..., 0.000278473,\n",
              "                        -0.000375748, -0.000888824],\n",
              "                       [0.003479, -0.000102043, 0.00119781, ..., -0.00239563,\n",
              "                        0.00119781, -0.00291443],\n",
              "                       [-0.000102043, 3.38554e-05, -0.0016861, ..., 0.000444412,\n",
              "                        0.0015564, 0.000907898],\n",
              "                       ...,\n",
              "                       [-0.0039978, -0.00138092, 0.00100708, ..., -0.000404358,\n",
              "                        -0.00209045, -0.0039978],\n",
              "                       [-0.000549316, -0.00121307, -0.000183105, ..., -0.00125885,\n",
              "                        0.000682831, 0.000907898],\n",
              "                       [0.000778198, 0.00047493, -0.000404358, ..., 0.000114441,\n",
              "                        -0.0014267, -0.000488281]],\n",
              "              \n",
              "                      [[-0.000265121, 0.00112152, 0.000444412, ..., -0.00180054,\n",
              "                        3.38554e-05, -0.0018692],\n",
              "                       [0.00144196, 0.00234985, -0.00253296, ..., -0.000953674,\n",
              "                        -7.43866e-05, -0.000694275],\n",
              "                       [-0.000793457, 0.00144196, -0.000461578, ..., 0.000682831,\n",
              "                        -0.0039978, -0.000183105],\n",
              "                       ...,\n",
              "                       [-0.00138092, -0.000375748, -0.000404358, ..., 0.000972748,\n",
              "                        -0.000953674, -0.00125885],\n",
              "                       [-0.00151825, -0.000375748, -0.000183105, ..., 0.00164795,\n",
              "                        0.00171661, -0.000265121],\n",
              "                       [-0.00134277, -0.000488281, 0.00205994, ..., 0.00177002,\n",
              "                        -0.000320435, -0.00102997]]],\n",
              "              \n",
              "              \n",
              "                     [[[0.00205994, 0.00131989, 0.0027771, ..., 0.000843048,\n",
              "                        0.00224304, -0.000461578],\n",
              "                       [-0.00291443, -0.000694275, -0.000431061, ..., 0.000224113,\n",
              "                        -0.000858307, 0.000444412],\n",
              "                       [-0.000576019, 0.000534058, 0.000682831, ..., -0.00201416,\n",
              "                        -0.0039978, 6.73532e-06],\n",
              "                       ...,\n",
              "                       [0.00104523, -0.000759125, 0.000621796, ..., -0.00113678,\n",
              "                        -0.0032196, 0.000972748],\n",
              "                       [0.000621796, 0.000907898, 6.07967e-05, ..., 0.00050354,\n",
              "                        -0.000858307, 0.000114441],\n",
              "                       [-0.000375748, -0.000347137, 0.00047493, ..., -0.000320435,\n",
              "                        0.00025177, -0.000953674]],\n",
              "              \n",
              "                      [[0.00177002, -0.000265121, -0.0014267, ..., -0.00239563,\n",
              "                        0.00205994, 0.00047493],\n",
              "                       [0.00260925, -0.00291443, 0.00025177, ..., -0.00228882,\n",
              "                        -0.00151825, -7.43866e-05],\n",
              "                       [-0.000375748, -0.00134277, 0.000360489, ..., 0.00150299,\n",
              "                        0.00183105, -0.00117493],\n",
              "                       ...,\n",
              "                       [-0.00253296, 0.00050354, -0.000128746, ..., -0.00163269,\n",
              "                        -0.000953674, 0.000142097],\n",
              "                       [-0.00239563, 0.000743866, -0.000210762, ..., 0.00112152,\n",
              "                        -0.00201416, 0.000444412],\n",
              "                       [0.00050354, 0.00047493, 0.00198364, ..., -0.0032196,\n",
              "                        0.00150299, 0.000360489]],\n",
              "              \n",
              "                      [[0.00144196, 0.00025177, 0.0015564, ..., -0.00125885,\n",
              "                        0.000591278, 0.000444412],\n",
              "                       [0.000938416, 0.000743866, 0.00025177, ..., -0.000858307,\n",
              "                        0.00100708, -0.000155449],\n",
              "                       [0.00183105, -0.000237465, 0.00131989, ..., -0.0014267,\n",
              "                        -0.000637054, -0.000858307],\n",
              "                       ...,\n",
              "                       [0.00112152, 0.000564575, -0.000488281, ..., -0.000549316,\n",
              "                        -0.00253296, 0.00123596],\n",
              "                       [-0.000953674, -0.000461578, -0.00146484, ..., 0.000534058,\n",
              "                        0.00104523, 0.00025177],\n",
              "                       [0.003479, -0.00125885, -0.0019455, ..., -0.00228882,\n",
              "                        -0.000155449, -0.0039978]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[-0.0016861, 0.00025177, 0.000421524, ..., 0.00112152,\n",
              "                        -0.0019455, -0.000155449],\n",
              "                       [-0.00218201, -0.000518799, 0.00104523, ..., 0.000196457,\n",
              "                        -0.000518799, 0.003479],\n",
              "                       [-0.000210762, 0.00205994, 0.00159454, ..., -0.00228882,\n",
              "                        -0.00102997, -0.00239563],\n",
              "                       ...,\n",
              "                       [-2.02656e-05, -0.000637054, -0.000759125, ..., 0.00144196,\n",
              "                        -0.001297, -0.000549316],\n",
              "                       [-0.00106049, 0.000534058, -0.000293732, ..., 0.00205994,\n",
              "                        0.00171661, 0.000142097],\n",
              "                       [0.000873566, 0.00047493, -0.00117493, ..., -0.00270081,\n",
              "                        -0.00201416, -0.000576019]],\n",
              "              \n",
              "                      [[-0.000461578, -0.0018692, -0.000759125, ..., -7.43866e-05,\n",
              "                        -0.000858307, 0.000682831],\n",
              "                       [-7.43866e-05, 0.00123596, -0.000265121, ..., -0.00157166,\n",
              "                        -0.00106049, -0.000320435],\n",
              "                       [0.00213623, -0.00117493, -0.000155449, ..., 0.000591278,\n",
              "                        0.00183105, -0.0019455],\n",
              "                       ...,\n",
              "                       [0.000938416, 0.000421524, -0.0014267, ..., 0.000972748,\n",
              "                        0.000778198, 0.0030365],\n",
              "                       [0.000142097, -0.0019455, -0.000576019, ..., 0.00224304,\n",
              "                        -0.000549316, -0.0017395],\n",
              "                       [-0.000128746, 0.00171661, -0.00102997, ..., 0.000873566,\n",
              "                        -0.00291443, -0.000320435]],\n",
              "              \n",
              "                      [[-7.43866e-05, 0.00047493, -0.000128746, ..., 0.000444412,\n",
              "                        0.00115967, -0.00201416],\n",
              "                       [0.000682831, 0.00183105, 0.00131989, ..., -0.000183105,\n",
              "                        0.000743866, -0.00134277],\n",
              "                       [-0.00102997, -0.00228882, -2.02656e-05, ..., 0.00198364,\n",
              "                        0.000444412, -0.00134277],\n",
              "                       ...,\n",
              "                       [0.00123596, 0.000169754, -0.00134277, ..., -0.00109863,\n",
              "                        8.7738e-05, 0.000843048],\n",
              "                       [-0.00106049, -0.000518799, -0.000728607, ..., 0.000972748,\n",
              "                        0.00224304, -0.00138092],\n",
              "                       [-0.00146484, -0.000320435, -0.000488281, ..., 0.00224304,\n",
              "                        0.00190735, 0.000743866]]],\n",
              "              \n",
              "              \n",
              "                     [[[-0.00218201, -0.00117493, 0.00050354, ..., -0.00109863,\n",
              "                        0.00104523, -4.74453e-05],\n",
              "                       [0.00164795, -0.000728607, -0.000549316, ..., -0.000461578,\n",
              "                        -0.00291443, -0.000759125],\n",
              "                       [0.00159454, 0.00183105, -0.00270081, ..., 0.00247192,\n",
              "                        8.7738e-05, 0.00260925],\n",
              "                       ...,\n",
              "                       [0.00100708, -0.000953674, 0.000564575, ..., 0.00213623,\n",
              "                        0.00123596, -0.000637054],\n",
              "                       [0.0027771, -0.00270081, 0.0030365, ..., 0.000534058,\n",
              "                        -0.000320435, -0.000320435],\n",
              "                       [0.000391006, -0.000991821, -0.000183105, ..., -0.00228882,\n",
              "                        -0.00218201, -0.00138092]],\n",
              "              \n",
              "                      [[0.00224304, 0.00108337, -0.00121307, ..., 0.00131989,\n",
              "                        -0.00146484, 0.000682831],\n",
              "                       [-0.000759125, 0.000224113, -0.0018692, ..., 0.000907898,\n",
              "                        -0.000667572, -0.000102043],\n",
              "                       [0.000873566, -0.000488281, -0.000923157, ..., 0.00190735,\n",
              "                        0.000682831, 0.000778198],\n",
              "                       ...,\n",
              "                       [-0.00253296, -0.000823975, -0.000549316, ..., 0.000972748,\n",
              "                        0.003479, -0.000183105],\n",
              "                       [-0.000728607, -0.000128746, 0.003479, ..., -4.74453e-05,\n",
              "                        0.00224304, -0.00109863],\n",
              "                       [0.000169754, 0.003479, -0.00125885, ..., 0.000360489,\n",
              "                        0.000907898, -0.00218201]],\n",
              "              \n",
              "                      [[-0.0019455, 0.000778198, -0.000375748, ..., 0.00183105,\n",
              "                        -0.000858307, -0.000793457],\n",
              "                       [-0.000404358, -0.000155449, -0.000404358, ..., 0.00205994,\n",
              "                        -0.000576019, -0.000923157],\n",
              "                       [0.000713348, 0.000682831, -0.000237465, ..., -2.02656e-05,\n",
              "                        -0.000728607, 0.00127411],\n",
              "                       ...,\n",
              "                       [0.00047493, 6.07967e-05, 0.000621796, ..., 0.000652313,\n",
              "                        0.00260925, -7.43866e-05],\n",
              "                       [-0.000991821, -0.00209045, 0.00119781, ..., 0.000114441,\n",
              "                        -0.00109863, 0.00183105],\n",
              "                       [0.000360489, -0.0039978, -0.000461578, ..., -0.0014267,\n",
              "                        0.00115967, -0.00134277]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.00260925, -0.000923157, -0.00121307, ..., 0.000444412,\n",
              "                        -0.001297, 0.0015564],\n",
              "                       [-0.000183105, -0.00109863, 0.000743866, ..., -0.000793457,\n",
              "                        -0.0014267, -0.000461578],\n",
              "                       [0.000682831, 0.00213623, 0.000196457, ..., -0.000576019,\n",
              "                        -0.00163269, 0.00127411],\n",
              "                       ...,\n",
              "                       [-0.000375748, 0.00112152, -0.000823975, ..., -0.000210762,\n",
              "                        -0.00109863, -0.000431061],\n",
              "                       [6.73532e-06, 0.000778198, -0.0017395, ..., -0.000320435,\n",
              "                        -0.000293732, 0.00144196],\n",
              "                       [-0.00151825, 0.000444412, 0.000444412, ..., -0.000576019,\n",
              "                        -0.000549316, -0.00180054]],\n",
              "              \n",
              "                      [[0.00213623, 0.000972748, -0.00113678, ..., 0.00112152,\n",
              "                        -0.000404358, 0.00183105],\n",
              "                       [0.00025177, 0.00171661, -0.00228882, ..., 0.000333786,\n",
              "                        0.000808716, -0.000858307],\n",
              "                       [0.003479, 0.00159454, -0.000576019, ..., -0.0017395,\n",
              "                        0.0015564, 6.07967e-05],\n",
              "                       ...,\n",
              "                       [0.000621796, -0.000431061, 0.000682831, ..., 0.003479,\n",
              "                        0.00213623, -0.0017395],\n",
              "                       [-0.000728607, 0.00177002, 0.000743866, ..., -0.000375748,\n",
              "                        0.000278473, -0.00291443],\n",
              "                       [-0.00239563, 0.00025177, 0.00140381, ..., 0.00119781,\n",
              "                        -0.000102043, -0.000265121]],\n",
              "              \n",
              "                      [[-0.00121307, 0.000743866, 0.00047493, ..., 0.00205994,\n",
              "                        -0.00291443, 0.00025177],\n",
              "                       [8.7738e-05, 0.00224304, 0.000778198, ..., -0.000320435,\n",
              "                        0.00177002, -7.43866e-05],\n",
              "                       [-0.000694275, 0.00177002, 0.000114441, ..., 0.00131989,\n",
              "                        -0.000210762, -0.000237465],\n",
              "                       ...,\n",
              "                       [0.000564575, -0.000210762, 0.000938416, ..., 0.000564575,\n",
              "                        3.38554e-05, 0.00224304],\n",
              "                       [-2.02656e-05, -0.000102043, -2.02656e-05, ..., -0.0018692,\n",
              "                        0.00150299, 0.00108337],\n",
              "                       [0.000196457, 0.00150299, 0.00183105, ..., -0.000461578,\n",
              "                        0.00159454, 0.0027771]]]], dtype=bfloat16),\n",
              "              mesh=None,\n",
              "              sharding=('embed', 'layers', 'q_heads', 'kv'),\n",
              "              sharding_rules=None,\n",
              "              linen_meta_type=LogicallyPartitioned\n",
              "            )\n",
              "          },\n",
              "          'value': {\n",
              "            'kernel': Param( # 9,437,184 (18.9 MB)\n",
              "              value=Array([[[[0.0150146, 0.015564, -0.00921631, ..., 0.000541687,\n",
              "                        0.0263672, 0.0263672]],\n",
              "              \n",
              "                      [[-0.0214844, 0.00994873, -0.0137329, ..., 0.00183105,\n",
              "                        0.00674438, -0.027832]],\n",
              "              \n",
              "                      [[0.0556641, -0.0383301, -0.0466309, ..., -0.0432129,\n",
              "                        -0.0111084, 0.00674438]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[-0.0181885, 0.0114136, 0.00674438, ..., 0.0444336, 0.0375977,\n",
              "                        -0.0078125]],\n",
              "              \n",
              "                      [[-0.0126953, -0.0187988, 0.015564, ..., -0.0152588,\n",
              "                        -0.00379944, -0.0299072]],\n",
              "              \n",
              "                      [[0.0197754, 0.010437, 0.0129395, ..., 0.0150146, -0.00248718,\n",
              "                        -0.00205994]]],\n",
              "              \n",
              "              \n",
              "                     [[[-0.00921631, -0.0126953, -0.0158691, ..., 0.0179443,\n",
              "                        0.017334, 0.0395508]],\n",
              "              \n",
              "                      [[0.0556641, 0.0283203, -0.0251465, ..., 0.0203857, 0.0161133,\n",
              "                        0.010437]],\n",
              "              \n",
              "                      [[0.0139771, -0.00119019, -0.012146, ..., -0.00119019,\n",
              "                        -0.00830078, -0.020752]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.0124512, -0.00601196, 0.032959, ..., 0.0274658, -0.0111084,\n",
              "                        -0.00119019]],\n",
              "              \n",
              "                      [[0.0134888, -0.0152588, -0.0639648, ..., -0.0137329,\n",
              "                        -0.0055542, 0.0341797]],\n",
              "              \n",
              "                      [[0.0358887, 0.0145264, 0.048584, ..., 0.0203857, -0.0116577,\n",
              "                        -0.0220947]]],\n",
              "              \n",
              "              \n",
              "                     [[[-0.000324249, -0.0152588, -0.00205994, ..., 0.0145264,\n",
              "                        -0.0175781, 0.00805664]],\n",
              "              \n",
              "                      [[0.0167236, 0.00271606, -0.00646973, ..., -0.00970459,\n",
              "                        -0.00646973, 0.0240479]],\n",
              "              \n",
              "                      [[0.000541687, -0.00119019, 0.0071106, ..., -0.0142212,\n",
              "                        -0.0228271, -0.00119019]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[-0.000759125, -0.0269775, 0.000972748, ..., -0.00970459,\n",
              "                        -0.00830078, -0.00646973]],\n",
              "              \n",
              "                      [[0.00183105, -0.012146, -0.0322266, ..., -0.0111084,\n",
              "                        0.00759888, -0.0126953]],\n",
              "              \n",
              "                      [[0.0395508, -0.00921631, 0.00854492, ..., 0.0179443,\n",
              "                        0.0217285, -0.00163269]]],\n",
              "              \n",
              "              \n",
              "                     ...,\n",
              "              \n",
              "              \n",
              "                     [[[0.032959, 0.000107765, -0.00830078, ..., 0.0305176,\n",
              "                        0.0274658, -0.0299072]],\n",
              "              \n",
              "                      [[-0.0158691, 0.0283203, -0.0164795, ..., 0.00854492,\n",
              "                        -0.0349121, 0.0185547]],\n",
              "              \n",
              "                      [[0.0114136, -0.0106812, -0.0366211, ..., 0.0358887,\n",
              "                        -0.0515137, 0.0305176]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.0129395, -0.00337219, 0.000972748, ..., -0.0126953,\n",
              "                        0.0341797, 0.0129395]],\n",
              "              \n",
              "                      [[0.0358887, 0.0283203, -0.00205994, ..., 0.0150146,\n",
              "                        0.00805664, 0.00576782]],\n",
              "              \n",
              "                      [[-0.0164795, 0.00674438, 0.010437, ..., 0.0185547, 0.019165,\n",
              "                        0.00183105]]],\n",
              "              \n",
              "              \n",
              "                     [[[0.0240479, 0.000972748, -0.0158691, ..., -0.00205994,\n",
              "                        0.000107765, -0.0334473]],\n",
              "              \n",
              "                      [[0.010437, 0.0167236, 0.00183105, ..., -0.0101929, 0.0124512,\n",
              "                        -0.0466309]],\n",
              "              \n",
              "                      [[0.00358582, 0.00402832, 0.000107765, ..., 0.0292969,\n",
              "                        0.010437, 0.0274658]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[-0.0187988, 0.0211182, -0.0078125, ..., -0.00878906,\n",
              "                        -0.00689697, 0.019165]],\n",
              "              \n",
              "                      [[-0.0299072, 0.0161133, 0.00140381, ..., 0.00314331,\n",
              "                        0.0263672, 0.0197754]],\n",
              "              \n",
              "                      [[-0.00512695, -0.00738525, -0.0220947, ..., -0.0131836,\n",
              "                        0.0211182, 0.0145264]]],\n",
              "              \n",
              "              \n",
              "                     [[[0.0263672, -0.0187988, 0.0255127, ..., 0.0109253, 0.0134888,\n",
              "                        -0.027832]],\n",
              "              \n",
              "                      [[-0.00738525, 0.0395508, 0.00227356, ..., 0.0124512,\n",
              "                        -0.012146, -0.026123]],\n",
              "              \n",
              "                      [[0.0129395, -0.00163269, -0.00379944, ..., 0.0179443,\n",
              "                        0.0197754, -0.00601196]],\n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "                      [[0.019165, -0.0251465, 0.0317383, ..., -0.0214844,\n",
              "                        -0.00119019, -0.0169678]],\n",
              "              \n",
              "                      [[0.0185547, 0.00183105, 0.0119019, ..., -0.0366211,\n",
              "                        -0.0152588, -0.026123]],\n",
              "              \n",
              "                      [[0.00445557, -0.012146, -0.0234375, ..., 0.00854492,\n",
              "                        -0.0111084, -0.0126953]]]], dtype=bfloat16),\n",
              "              mesh=None,\n",
              "              sharding=('embed', 'layers', 'kv_heads', 'kv_head_dim'),\n",
              "              sharding_rules=None,\n",
              "              linen_meta_type=LogicallyPartitioned\n",
              "            )\n",
              "          }\n",
              "        }\n",
              "      },\n",
              "      'to_nnx__rngs': {\n",
              "        'dropout': {\n",
              "          'count': RngCount( # 1 (4 B)\n",
              "            value=Array(2, dtype=uint32),\n",
              "            tag='dropout'\n",
              "          ),\n",
              "          'key': RngKey( # 1 (8 B)\n",
              "            value=Array((), dtype=key<fry>) overlaying:\n",
              "            [ 507451445 1853169794],\n",
              "            tag='dropout'\n",
              "          )\n",
              "        },\n",
              "        'params': {\n",
              "          'count': RngCount( # 1 (4 B)\n",
              "            value=Array(2, dtype=uint32),\n",
              "            tag='params'\n",
              "          ),\n",
              "          'key': RngKey( # 1 (8 B)\n",
              "            value=Array((), dtype=key<fry>) overlaying:\n",
              "            [ 928981903 3453687069],\n",
              "            tag='params'\n",
              "          )\n",
              "        }\n",
              "      }\n",
              "    },\n",
              "    'token_embedder': {\n",
              "      'embedding': Param( # 524,550,144 (1.0 GB)\n",
              "        value=Array([[1.15625, -0.353516, 1.4375, ..., -1.57812, 0.0245361, -0.644531],\n",
              "               [1.78906, 0.703125, 1.24219, ..., 1.04688, 0.0439453, 1.38281],\n",
              "               [-0.550781, 1.32812, 0.304688, ..., -0.691406, 0.921875, -1.73438],\n",
              "               ...,\n",
              "               [-0.972656, -0.851562, -1, ..., 0.0439453, -1.73438, 1.4375],\n",
              "               [0.00488281, 0.304688, 0.65625, ..., -1.83594, -0.192383,\n",
              "                0.162109],\n",
              "               [-0.251953, -1.65625, 0.730469, ..., -1.25781, -0.271484,\n",
              "                -0.878906]], dtype=bfloat16),\n",
              "        sharding=('vocab', 'embed')\n",
              "      )\n",
              "    }\n",
              "  }\n",
              "})"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sampler.transformer_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdxn7DQYT3Uv"
      },
      "source": [
        "## Apply LoRA/QLoRA to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "X3t-7leAT3Uv"
      },
      "outputs": [],
      "source": [
        "def get_lora_model(base_model, mesh):\n",
        "  lora_provider = lora.LoraProvider(\n",
        "      module_path=\".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj\",\n",
        "      rank=RANK,\n",
        "      alpha=ALPHA,\n",
        "      # comment the two args below for LoRA (w/o quantisation).\n",
        "      weight_qtype=\"nf4\",\n",
        "      tile_size=256,\n",
        "  )\n",
        "\n",
        "  model_input = base_model.get_model_input()\n",
        "  lora_model = lora.apply_lora_to_model(\n",
        "      base_model, lora_provider, **model_input\n",
        "  )\n",
        "\n",
        "  with mesh:\n",
        "    state = nnx.state(lora_model)\n",
        "    pspecs = nnx.get_partition_spec(state)\n",
        "    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
        "    nnx.update(lora_model, sharded_state)\n",
        "\n",
        "  return lora_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MfmodoqrT3Uv"
      },
      "outputs": [],
      "source": [
        "# # LoRA model\n",
        "# lora_gemma = get_lora_model(gemma, mesh=mesh)\n",
        "# nnx.display(lora_gemma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD6-iU0PT3Uv"
      },
      "source": [
        "## Load Datasets for SFT Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6T_7Cik8T3Uv"
      },
      "outputs": [],
      "source": [
        "# Loads the training and validation datasets\n",
        "train_ds, validation_ds = data_lib.create_datasets(\n",
        "    dataset_name='mtnt/en-fr',\n",
        "    # Uncomment the line below to use a Hugging Face dataset.\n",
        "    # Note that this requires upgrading the 'datasets' package and restarting\n",
        "    # the Colab runtime.\n",
        "    # dataset_name='Helsinki-NLP/opus-100',\n",
        "    global_batch_size=BATCH_SIZE,\n",
        "    max_target_length=256,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    tokenizer=gemma_tokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "def gen_model_input_fn(x: peft_trainer.TrainingInput):\n",
        "  pad_mask = x.input_tokens != gemma_tokenizer.pad_id()\n",
        "  positions = gemma_lib.build_positions_from_mask(pad_mask)\n",
        "  attention_mask = gemma_lib.make_causal_attn_mask(pad_mask)\n",
        "  return {\n",
        "      'input_tokens': x.input_tokens,\n",
        "      'input_mask': x.input_mask,\n",
        "      'positions': positions,\n",
        "      'attention_mask': attention_mask,\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w0PHlVBT3Uv"
      },
      "source": [
        "## SFT Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh1xPieRT3Uv"
      },
      "source": [
        "### Training with full weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-oYR9JKNT3Uv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Private W&B dashboard, no account required\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Invalid choice\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Invalid choice\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Private W&B dashboard, no account required'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/mazumdera_google_com/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manony-mouse-863749125460230603\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/mazumdera_google_com/tunix/examples/wandb/run-20250801_231357-t6xeiq7h</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/anony-mouse-863749125460230603/tunix/runs/t6xeiq7h?apiKey=b4d00f0db746a7c14cc080c4b44a878fb3515769' target=\"_blank\">2025-08-01_23-13-09</a></strong> to <a href='https://wandb.ai/anony-mouse-863749125460230603/tunix?apiKey=b4d00f0db746a7c14cc080c4b44a878fb3515769' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/anony-mouse-863749125460230603/tunix?apiKey=b4d00f0db746a7c14cc080c4b44a878fb3515769' target=\"_blank\">https://wandb.ai/anony-mouse-863749125460230603/tunix?apiKey=b4d00f0db746a7c14cc080c4b44a878fb3515769</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/anony-mouse-863749125460230603/tunix/runs/t6xeiq7h?apiKey=b4d00f0db746a7c14cc080c4b44a878fb3515769' target=\"_blank\">https://wandb.ai/anony-mouse-863749125460230603/tunix/runs/t6xeiq7h?apiKey=b4d00f0db746a7c14cc080c4b44a878fb3515769</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Do NOT share these links with anyone. They can be used to claim your runs."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "ValueError",
          "evalue": "Resource axis: norm of PartitionSpec('norm',) is not found in mesh: ('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive').",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m jax.profiler.trace(os.path.join(PROFILING_DIR, \u001b[33m\"\u001b[39m\u001b[33mfull_training\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m     13\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m mesh:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_ds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/tunix/sft/peft_trainer.py:443\u001b[39m, in \u001b[36mPeftTrainer.train\u001b[39m\u001b[34m(self, train_ds, eval_ds, skip_jit)\u001b[39m\n\u001b[32m    440\u001b[39m mesh = pxla.thread_resources.env.physical_mesh\n\u001b[32m    441\u001b[39m logging.info(\u001b[33m\"\u001b[39m\u001b[33mTraining with mesh: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, mesh)\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m train_step, eval_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjit_train_and_eval_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mskip_jit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.max_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pbar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    446\u001b[39m   \u001b[38;5;28mself\u001b[39m._pbar = progress_bar.ProgressBar(\n\u001b[32m    447\u001b[39m       metrics_logger=\u001b[38;5;28mself\u001b[39m.metrics_logger,\n\u001b[32m    448\u001b[39m       initial_steps=\u001b[38;5;28mself\u001b[39m._train_steps,\n\u001b[32m    449\u001b[39m       max_steps=\u001b[38;5;28mself\u001b[39m.config.max_steps,\n\u001b[32m    450\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/tunix/sft/peft_trainer.py:346\u001b[39m, in \u001b[36mPeftTrainer.jit_train_and_eval_step\u001b[39m\u001b[34m(self, skip_jit)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jitted_train_step_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    345\u001b[39m   mesh = pxla.thread_resources.env.physical_mesh\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_shard_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m   \u001b[38;5;28mself\u001b[39m._jitted_train_step_fn = nnx.jit(\n\u001b[32m    348\u001b[39m       train_step, donate_argnames=(\u001b[33m\"\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m\"\u001b[39m,)\n\u001b[32m    349\u001b[39m   )\n\u001b[32m    350\u001b[39m   \u001b[38;5;28mself\u001b[39m._jitted_eval_step_fn = nnx.jit(\n\u001b[32m    351\u001b[39m       eval_step, donate_argnames=(\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m,)\n\u001b[32m    352\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/tunix/sft/peft_trainer.py:322\u001b[39m, in \u001b[36mPeftTrainer._shard_optimizer\u001b[39m\u001b[34m(self, mesh)\u001b[39m\n\u001b[32m    318\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    319\u001b[39m optimizer_state = nnx.state(\n\u001b[32m    320\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer, nnx.optimizer.OptState\n\u001b[32m    321\u001b[39m )  \u001b[38;5;66;03m# select only the optimizer state\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m optimizer_shardings = \u001b[43mnnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_named_sharding\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m optimizer_sharded_state = jax.lax.with_sharding_constraint(\n\u001b[32m    324\u001b[39m     optimizer_state, optimizer_shardings\n\u001b[32m    325\u001b[39m )\n\u001b[32m    326\u001b[39m nnx.update(\u001b[38;5;28mself\u001b[39m.optimizer, optimizer_sharded_state)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/flax/nnx/spmd.py:151\u001b[39m, in \u001b[36mget_named_sharding\u001b[39m\u001b[34m(tree, mesh)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_named_sharding\u001b[39m(tree: A, mesh: jax.sharding.Mesh) -> A:\n\u001b[32m    150\u001b[39m   spec = get_partition_spec(tree)\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m   sharding = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43msharding\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNamedSharding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m sharding\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/jax/_src/tree.py:155\u001b[39m, in \u001b[36mmap\u001b[39m\u001b[34m(f, tree, is_leaf, *rest)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmap\u001b[39m(f: Callable[..., Any],\n\u001b[32m    116\u001b[39m         tree: Any,\n\u001b[32m    117\u001b[39m         *rest: Any,\n\u001b[32m    118\u001b[39m         is_leaf: Callable[[Any], \u001b[38;5;28mbool\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> Any:\n\u001b[32m    119\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Maps a multi-input function over pytree args to produce a new pytree.\u001b[39;00m\n\u001b[32m    120\u001b[39m \n\u001b[32m    121\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    153\u001b[39m \u001b[33;03m    - :func:`jax.tree.reduce`\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree_util\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/jax/_src/tree_util.py:362\u001b[39m, in \u001b[36mtree_map\u001b[39m\u001b[34m(f, tree, is_leaf, *rest)\u001b[39m\n\u001b[32m    360\u001b[39m leaves, treedef = tree_flatten(tree, is_leaf)\n\u001b[32m    361\u001b[39m all_leaves = [leaves] + [treedef.flatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreedef\u001b[49m\u001b[43m.\u001b[49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_leaves\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/jax/_src/tree_util.py:362\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    360\u001b[39m leaves, treedef = tree_flatten(tree, is_leaf)\n\u001b[32m    361\u001b[39m all_leaves = [leaves] + [treedef.flatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m treedef.unflatten(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(*all_leaves))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/flax/nnx/spmd.py:151\u001b[39m, in \u001b[36mget_named_sharding.<locals>.<lambda>\u001b[39m\u001b[34m(p)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_named_sharding\u001b[39m(tree: A, mesh: jax.sharding.Mesh) -> A:\n\u001b[32m    150\u001b[39m   spec = get_partition_spec(tree)\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m   sharding = jax.tree.map(\u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43msharding\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNamedSharding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m, spec)\n\u001b[32m    152\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m sharding\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/jax/_src/util.py:299\u001b[39m, in \u001b[36mcache.<locals>.wrap.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.check_tracer_leaks.value:\n\u001b[32m    298\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m f(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrace_context_in_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_ignore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m              \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/jax/_src/util.py:293\u001b[39m, in \u001b[36mcache.<locals>.wrap.<locals>.cached\u001b[39m\u001b[34m(_, *args, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache(max_size)\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached\u001b[39m(_, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/jax/_src/named_sharding.py:461\u001b[39m, in \u001b[36mcheck_pspec\u001b[39m\u001b[34m(mesh, spec, _manual_axes)\u001b[39m\n\u001b[32m    458\u001b[39m \u001b[38;5;129m@cache\u001b[39m(max_size=\u001b[32m128\u001b[39m, trace_context_in_key=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_pspec\u001b[39m(mesh, spec, _manual_axes=\u001b[38;5;28mfrozenset\u001b[39m()):\n\u001b[32m    460\u001b[39m   _check_unique_resources(spec, \u001b[33m\"\u001b[39m\u001b[33mNamedSharding spec\u001b[39m\u001b[33m\"\u001b[39m, mesh)\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m   \u001b[43m_check_mesh_resource_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    462\u001b[39m   _check_mesh_unreduced(mesh, spec)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/jax/_src/named_sharding.py:503\u001b[39m, in \u001b[36m_check_mesh_resource_axis\u001b[39m\u001b[34m(mesh, pspec)\u001b[39m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m p:\n\u001b[32m    502\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mesh.axis_names:\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    504\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResource axis: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpspec\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis not found in mesh: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(mesh.shape.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(mesh._name_to_type[p[\u001b[32m0\u001b[39m]] == mesh._name_to_type[r] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m p):\n\u001b[32m    507\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    508\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mAxisTypes should be the same in a tuple subset of PartitionSpec:\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    509\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpspec\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Got subset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with axis\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    510\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m types: (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(mesh._name_to_type[r])\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mr\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mp)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mValueError\u001b[39m: Resource axis: norm of PartitionSpec('norm',) is not found in mesh: ('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive')."
          ]
        }
      ],
      "source": [
        "logging_option = metrics_logger.MetricsLoggerOptions(\n",
        "    log_dir=\"/tmp/tensorboard/full\", flush_every_n_steps=20\n",
        ")\n",
        "training_config = peft_trainer.TrainingConfig(\n",
        "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "    max_steps=MAX_STEPS,\n",
        "    metrics_logging_options=logging_option,\n",
        ")\n",
        "trainer = peft_trainer.PeftTrainer(gemma, optax.adamw(1e-5), training_config)\n",
        "trainer = trainer.with_gen_model_input_fn(gen_model_input_fn)\n",
        "\n",
        "with jax.profiler.trace(os.path.join(PROFILING_DIR, \"full_training\")):\n",
        "  with mesh:\n",
        "    trainer.train(train_ds, validation_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gd-66SRT3Uv"
      },
      "source": [
        "### Training with LoRA/QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZVp0SYMT3Uv"
      },
      "outputs": [],
      "source": [
        "# Restart Colab runtime.\n",
        "\n",
        "training_config = peft_trainer.TrainingConfig(\n",
        "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "    max_steps=MAX_STEPS,\n",
        "    checkpoint_root_directory=CKPT_DIR,\n",
        ")\n",
        "lora_trainer = peft_trainer.PeftTrainer(\n",
        "    lora_gemma, optax.adamw(1e-3), training_config\n",
        ").with_gen_model_input_fn(gen_model_input_fn)\n",
        "\n",
        "with jax.profiler.trace(os.path.join(PROFILING_DIR, \"peft\")):\n",
        "  with mesh:\n",
        "    lora_trainer.train(train_ds, validation_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXUzmyVIT3Uv"
      },
      "source": [
        "### Compare profile results of different training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIw2rIKmT3Uv"
      },
      "source": [
        "<font size=3>Setup<font>           | <font size=3>Train Step Time<font> | <font size=3>Peak Memory Usage<font>\n",
        "---------------------------------- | ---------------------------------- | ------------------------------\n",
        "<font size=3>Full weights<font>        |   <font size=3>~1.22 s<font>     |   <font size=3>43.26 GiB<font>\n",
        "<font size=3>QLoRA<font>        |   <font size=3>~1.19 s<font>     |   <font size=3>28.14 GiB<font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QICLDHTkT3Uv"
      },
      "source": [
        "## Generate with the LoRA/QLoRA model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTsmUCK6T3Uv"
      },
      "source": [
        "The QLoRA model still cannot do English-to-French translation properly since we\n",
        "only trained for 100 steps. If you train it for longer, you will see better\n",
        "results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WohZCuY0T3Uw"
      },
      "outputs": [],
      "source": [
        "gemma_tokenizer = data_lib.GemmaTokenizer(\n",
        "    os.path.join(kaggle_ckpt_path, \"tokenizer.model\")\n",
        ")\n",
        "\n",
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_gemma,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")\n",
        "\n",
        "input_batch = [\n",
        "    \"Translate this into French:\\nHello, my name is Morgane.\\n\",\n",
        "    \"Translate this into French:\\nThis dish is delicious!\\n\",\n",
        "    \"Translate this into French:\\nI am a student.\\n\",\n",
        "    \"Translate this into French:\\nHow's the weather today?\\n\",\n",
        "]\n",
        "\n",
        "out_data = sampler(\n",
        "    input_strings=input_batch,\n",
        "    total_generation_steps=10,  # The number of steps performed when generating a response.\n",
        ")\n",
        "\n",
        "for input_string, out_string in zip(input_batch, out_data.text):\n",
        "  print(f\"----------------------\")\n",
        "  print(f\"Prompt:\\n{input_string}\")\n",
        "  print(f\"Output:\\n{out_string}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//third_party/py/tunix/google/examples/qlora_gemma:qlora_demo_colab",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv-py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
