{
  "cells": [
    {
      "metadata": {
        "id": "H-gq1gcpyCpT"
      },
      "cell_type": "markdown",
      "source": [
        "\u003ca href=\"https://colab.research.google.com/github/google/tunix/blob/main/examples/qlora_demo.ipynb\" \u003e\u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/\u003e\u003c/a\u003e\n",
        "\n",
        "## Install necessary librarie"
      ]
    },
    {
      "metadata": {
        "id": "RTlz7JP7yCpT"
      },
      "cell_type": "code",
      "source": [
        "!pip install -q kagglehub\n",
        "\n",
        "!pip install -q tensorflow\n",
        "!pip install -q tensorboardX\n",
        "!pip install -q grain\n",
        "!pip install -q git+https://github.com/google/tunix\n",
        "!pip install -q git+https://github.com/google/qwix\n",
        "\n",
        "!pip uninstall -q -y flax\n",
        "!pip install -q git+https://github.com/google/flax.git"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "YEjoS0-JyCpT"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "id": "LshEMmSzx6W6"
      },
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "import time\n",
        "\n",
        "from flax import nnx\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import kagglehub\n",
        "import optax\n",
        "from orbax import checkpoint as ocp\n",
        "from qwix import lora\n",
        "from tunix.examples.gemma_libs import data as data_lib\n",
        "from tunix.examples.gemma_libs import gemma as gemma_lib\n",
        "from tunix.examples.gemma_libs import params as params_lib\n",
        "from tunix.examples.gemma_libs import sampler as sampler_lib\n",
        "from tunix.sft import metrics_logger\n",
        "from tunix.sft import peft_trainer"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "VnEZ_jXwypn-"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "QBcMaL22T3Uu"
      },
      "cell_type": "code",
      "source": [
        "# Data\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Model\n",
        "MESH = [(1, 8), (\"fsdp\", \"tp\")]\n",
        "# LoRA\n",
        "RANK = 16\n",
        "ALPHA = 2.0\n",
        "\n",
        "# Train\n",
        "MAX_STEPS = 100\n",
        "EVAL_EVERY_N_STEPS = 20\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "\n",
        "# Checkpoint saving\n",
        "INTERMEDIATE_CKPT_DIR = \"/content/intermediate_ckpt/\"\n",
        "CKPT_DIR = \"/content/ckpts/\"\n",
        "PROFILING_DIR = \"/content/profiling/\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "r3s5Qg6xT3Uu"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Gemma 2B\n",
        "\n",
        "To load the model, you need to be on [Kaggle](https://www.kaggle.com/) and need\n",
        "to have agreed to the Gemma license\n",
        "[here](https://www.kaggle.com/models/google/gemma/flax/)."
      ]
    },
    {
      "metadata": {
        "id": "o_7Sk8d7T3Uu"
      },
      "cell_type": "code",
      "source": [
        "# Log in\n",
        "if \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n",
        "  kagglehub.login()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "oJC3Hfh9T3Uv"
      },
      "cell_type": "code",
      "source": [
        "kaggle_ckpt_path = kagglehub.model_download(\"google/gemma/flax/2b\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "r_5FWrq-T3Uv"
      },
      "cell_type": "code",
      "source": [
        "# This is a workaround. The checkpoints on Kaggle don't work with NNX. So, we\n",
        "# load the model, save the checkpoint locally, and then reload the model\n",
        "# (sharded).\n",
        "params = params_lib.load_and_format_params(os.path.join(kaggle_ckpt_path, \"2b\"))\n",
        "gemma = gemma_lib.Transformer.from_params(params, version=\"2b\")\n",
        "checkpointer = ocp.StandardCheckpointer()\n",
        "_, state = nnx.split(gemma)\n",
        "checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "_ertAr6FT3Uv"
      },
      "cell_type": "code",
      "source": [
        "# Wait for the ckpt to save successfully.\n",
        "time.sleep(60)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "nGYKuLyFT3Uv"
      },
      "cell_type": "code",
      "source": [
        "# Delete the intermediate model to save memory.\n",
        "del params\n",
        "del gemma\n",
        "del state\n",
        "gc.collect()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "HFKUZT10T3Uv"
      },
      "cell_type": "code",
      "source": [
        "def get_base_model(ckpt_path):\n",
        "\n",
        "  mesh = jax.make_mesh(*MESH)\n",
        "  abs_gemma: nnx.Module = nnx.eval_shape(\n",
        "      lambda: gemma_lib.Transformer(\n",
        "          gemma_lib.TransformerConfig.gemma_2b(), rngs=nnx.Rngs(params=0)\n",
        "      )\n",
        "  )\n",
        "  abs_state = nnx.state(abs_gemma)\n",
        "  abs_state = jax.tree.map(\n",
        "      lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.float32, sharding=s),\n",
        "      abs_state,\n",
        "      nnx.get_named_sharding(abs_state, mesh),\n",
        "  )\n",
        "  checkpointer = ocp.StandardCheckpointer()\n",
        "  restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n",
        "\n",
        "  graph_def, _ = nnx.split(abs_gemma)\n",
        "  gemma = nnx.merge(graph_def, restored_params)\n",
        "  return gemma, mesh"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "KCPPEEi3T3Uv"
      },
      "cell_type": "code",
      "source": [
        "# Base model\n",
        "gemma, mesh = get_base_model(\n",
        "    ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\n",
        ")\n",
        "nnx.display(gemma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "d2iDYbnsT3Uv"
      },
      "cell_type": "markdown",
      "source": [
        "## Prompt the model\n",
        "\n",
        "Let's see how the model performs on the English-French translation task."
      ]
    },
    {
      "metadata": {
        "id": "iH82cHpAT3Uv"
      },
      "cell_type": "code",
      "source": [
        "gemma_tokenizer = data_lib.GemmaTokenizer(\n",
        "    os.path.join(kaggle_ckpt_path, \"tokenizer.model\")\n",
        ")\n",
        "\n",
        "sampler = sampler_lib.Sampler(transformer=gemma, vocab=gemma_tokenizer.vocab)\n",
        "\n",
        "input_batch = [\n",
        "    \"Translate this into French:\\nHello, my name is Morgane.\\n\",\n",
        "    \"Translate this into French:\\nThis dish is delicious!\\n\",\n",
        "    \"Translate this into French:\\nI am a student.\\n\",\n",
        "    \"Translate this into French:\\nHow's the weather today?\\n\",\n",
        "]\n",
        "\n",
        "out_data = sampler(\n",
        "    input_strings=input_batch,\n",
        "    total_generation_steps=10,  # The number of steps performed when generating a response.\n",
        ")\n",
        "\n",
        "for input_string, out_string in zip(input_batch, out_data.text):\n",
        "  print(f\"----------------------\")\n",
        "  print(f\"Prompt:\\n{input_string}\")\n",
        "  print(f\"Output:\\n{out_string}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Xdxn7DQYT3Uv"
      },
      "cell_type": "markdown",
      "source": [
        "## Apply LoRA/QLoRA to the model"
      ]
    },
    {
      "metadata": {
        "id": "X3t-7leAT3Uv"
      },
      "cell_type": "code",
      "source": [
        "def get_lora_model(base_model, mesh):\n",
        "  lora_provider = lora.LoraProvider(\n",
        "      module_path=\".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj\",\n",
        "      rank=RANK,\n",
        "      alpha=ALPHA,\n",
        "      # comment the two args below for LoRA (w/o quantisation).\n",
        "      weight_qtype=\"nf4\",\n",
        "      tile_size=256,\n",
        "  )\n",
        "\n",
        "  model_input = base_model.get_model_input()\n",
        "  lora_model = lora.apply_lora_to_model(\n",
        "      base_model, lora_provider, **model_input\n",
        "  )\n",
        "\n",
        "  with mesh:\n",
        "    state = nnx.state(lora_model)\n",
        "    pspecs = nnx.get_partition_spec(state)\n",
        "    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
        "    nnx.update(lora_model, sharded_state)\n",
        "\n",
        "  return lora_model"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "MfmodoqrT3Uv"
      },
      "cell_type": "code",
      "source": [
        "# LoRA model\n",
        "lora_gemma = get_lora_model(gemma, mesh=mesh)\n",
        "nnx.display(lora_gemma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "HD6-iU0PT3Uv"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Datasets for SFT Training"
      ]
    },
    {
      "metadata": {
        "id": "6T_7Cik8T3Uv"
      },
      "cell_type": "code",
      "source": [
        "# Loads the training and validation datasets\n",
        "train_ds, validation_ds = data_lib.create_datasets(\n",
        "    dataset_name='mtnt/en-fr',\n",
        "    global_batch_size=BATCH_SIZE,\n",
        "    max_target_length=256,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    tokenizer=gemma_tokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "def gen_model_input_fn(x: peft_trainer.TrainingInput):\n",
        "  pad_mask = x.input_tokens != gemma_tokenizer.pad_id\n",
        "  positions = gemma_lib.build_positions_from_mask(pad_mask)\n",
        "  attention_mask = gemma_lib.make_causal_attn_mask(pad_mask)\n",
        "  return {\n",
        "      'input_tokens': x.input_tokens,\n",
        "      'input_mask': x.input_mask,\n",
        "      'positions': positions,\n",
        "      'attention_mask': attention_mask,\n",
        "  }"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "-w0PHlVBT3Uv"
      },
      "cell_type": "markdown",
      "source": [
        "## SFT Training"
      ]
    },
    {
      "metadata": {
        "id": "qh1xPieRT3Uv"
      },
      "cell_type": "markdown",
      "source": [
        "### Training with full weights"
      ]
    },
    {
      "metadata": {
        "id": "-oYR9JKNT3Uv"
      },
      "cell_type": "code",
      "source": [
        "logging_option = metrics_logger.MetricsLoggerOptions(\n",
        "    log_dir=\"/tmp/tensorboard/full\", flush_every_n_steps=20\n",
        ")\n",
        "training_config = peft_trainer.TrainingConfig(\n",
        "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "    max_steps=MAX_STEPS,\n",
        "    metrics_logging_options=logging_option,\n",
        ")\n",
        "trainer = peft_trainer.PeftTrainer(gemma, optax.adamw(1e-5), training_config)\n",
        "trainer = trainer.with_gen_model_input_fn(gen_model_input_fn)\n",
        "\n",
        "with jax.profiler.trace(os.path.join(PROFILING_DIR, \"full_training\")):\n",
        "  with mesh:\n",
        "    trainer.train(train_ds, validation_ds)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "-Gd-66SRT3Uv"
      },
      "cell_type": "markdown",
      "source": [
        "### Training with LoRA/QLoRA"
      ]
    },
    {
      "metadata": {
        "id": "oZVp0SYMT3Uv"
      },
      "cell_type": "code",
      "source": [
        "# Restart Colab runtime.\n",
        "\n",
        "training_config = peft_trainer.TrainingConfig(\n",
        "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "    max_steps=MAX_STEPS,\n",
        "    checkpoint_root_directory=CKPT_DIR,\n",
        ")\n",
        "lora_trainer = peft_trainer.PeftTrainer(\n",
        "    lora_gemma, optax.adamw(1e-3), training_config\n",
        ").with_gen_model_input_fn(gen_model_input_fn)\n",
        "\n",
        "with jax.profiler.trace(os.path.join(PROFILING_DIR, \"peft\")):\n",
        "  with mesh:\n",
        "    lora_trainer.train(train_ds, validation_ds)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "sXUzmyVIT3Uv"
      },
      "cell_type": "markdown",
      "source": [
        "### Compare profile results of different training"
      ]
    },
    {
      "metadata": {
        "id": "eIw2rIKmT3Uv"
      },
      "cell_type": "markdown",
      "source": [
        "\u003cfont size=3\u003eSetup\u003cfont\u003e           | \u003cfont size=3\u003eTrain Step Time\u003cfont\u003e | \u003cfont size=3\u003ePeak Memory Usage\u003cfont\u003e\n",
        "---------------------------------- | ---------------------------------- | ------------------------------\n",
        "\u003cfont size=3\u003eFull weights\u003cfont\u003e        |   \u003cfont size=3\u003e~1.22 s\u003cfont\u003e     |   \u003cfont size=3\u003e43.26 GiB\u003cfont\u003e\n",
        "\u003cfont size=3\u003eQLoRA\u003cfont\u003e        |   \u003cfont size=3\u003e~1.19 s\u003cfont\u003e     |   \u003cfont size=3\u003e28.14 GiB\u003cfont\u003e"
      ]
    },
    {
      "metadata": {
        "id": "QICLDHTkT3Uv"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate with the LoRA/QLoRA model"
      ]
    },
    {
      "metadata": {
        "id": "oTsmUCK6T3Uv"
      },
      "cell_type": "markdown",
      "source": [
        "The QLoRA model still cannot do English-to-French translation properly since we\n",
        "only trained for 100 steps. If you train it for longer, you will see better\n",
        "results."
      ]
    },
    {
      "metadata": {
        "id": "WohZCuY0T3Uw"
      },
      "cell_type": "code",
      "source": [
        "gemma_tokenizer = data_lib.GemmaTokenizer(\n",
        "    os.path.join(kaggle_ckpt_path, \"tokenizer.model\")\n",
        ")\n",
        "\n",
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_gemma, vocab=gemma_tokenizer.vocab\n",
        ")\n",
        "\n",
        "input_batch = [\n",
        "    \"Translate this into French:\\nHello, my name is Morgane.\\n\",\n",
        "    \"Translate this into French:\\nThis dish is delicious!\\n\",\n",
        "    \"Translate this into French:\\nI am a student.\\n\",\n",
        "    \"Translate this into French:\\nHow's the weather today?\\n\",\n",
        "]\n",
        "\n",
        "out_data = sampler(\n",
        "    input_strings=input_batch,\n",
        "    total_generation_steps=10,  # The number of steps performed when generating a response.\n",
        ")\n",
        "\n",
        "for input_string, out_string in zip(input_batch, out_data.text):\n",
        "  print(f\"----------------------\")\n",
        "  print(f\"Prompt:\\n{input_string}\")\n",
        "  print(f\"Output:\\n{out_string}\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//third_party/py/tunix/google/examples/qlora_gemma:qlora_demo_colab",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/third_party/py/tunix/google/examples/qlora_gemma/qlora_demo.ipynb",
          "timestamp": 1745566103252
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
