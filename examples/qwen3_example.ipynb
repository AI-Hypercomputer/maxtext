{
  "cells": [
    {
      "metadata": {
        "id": "OeEzmwEz-Mls"
      },
      "cell_type": "markdown",
      "source": [
        "\u003ca href=\"https://colab.research.google.com/github/google/tunix/blob/main/examples/qwen3_example.ipynb\" \u003e\u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/\u003e\u003c/a\u003e\n",
        "\n",
        "## Install necessary libraries\n",
        "\n",
        "## Download pre-trained Qwen3 model\n",
        "If not exist in local dir, download from https://www.kaggle.com/models/qwen-lm/qwen-3/transformers/\n"
      ]
    },
    {
      "metadata": {
        "id": "6f-FF3V6b_y9"
      },
      "cell_type": "code",
      "source": [
        "from tunix.models.qwen3 import params\n",
        "from tunix.models.qwen3 import model\n",
        "from flax import nnx\n",
        "\n",
        "MODEL_CP_PATH = '\u003cyour model download dir\u003e'\n",
        "\n",
        "config = model.ModelConfig.qwen3_0_6_b()  # pick correponding config based on model version\n",
        "qwen3 = params.create_model_from_safe_tensors(MODEL_CP_PATH, config)\n",
        "nnx.display(qwen3)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "bHoIcCFVfLgv"
      },
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Qwen3ForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CP_PATH)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "IY_BGK_OfWuX"
      },
      "cell_type": "code",
      "source": [
        "from tunix.generate import sampler\n",
        "\n",
        "def templatize(prompts):\n",
        "  out = []\n",
        "  for p in prompts:\n",
        "    out.append(\n",
        "        tokenizer.apply_chat_template(\n",
        "          [\n",
        "              {\"role\": \"user\", \"content\": p},\n",
        "          ],\n",
        "          tokenize=False,\n",
        "          add_generation_prompt=True,\n",
        "          enable_thinking=True,\n",
        "      )\n",
        "    )\n",
        "  return out\n",
        "\n",
        "inputs = templatize(\n",
        "    [\n",
        "        \"which is larger 9.9 or 9.11?\",\n",
        "        \"讲几句人话\",\n",
        "        \"tell me your name, respond in Chinese\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "sampler = sampler.Sampler(qwen3, tokenizer, sampler.CacheConfig(cache_size=256, num_layers=28, num_kv_heads=8, head_dim=128))\n",
        "out = sampler(inputs, total_generation_steps=128, echo=True)\n",
        "\n",
        "for t in out.text:\n",
        "  print(t)\n",
        "  print('*' * 30)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Vr_9ULDVA2-Y"
      },
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
