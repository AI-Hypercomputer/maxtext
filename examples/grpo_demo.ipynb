{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "abdhOBYHqYz6",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/tunix/blob/main/examples/grpo_demo.ipynb\" ><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This tutorial demonstrates training the Gemma 2 2B-IT model on the GSM8K math\n",
        "reasoning benchmark using Group Relative Policy Optimization (GRPO). GRPO can\n",
        "enhance your model's problem-solving skills on mathematical word problems,\n",
        "coding problems, etc.\n",
        "\n",
        "GRPO is an RL algorithm designed to enhance the reasoning abilities of LLMs. It\n",
        "is a variant of Proximal Policy Optimization (PPO) that reduces memory usage by\n",
        "eliminating the need for a separate value function model. GRPO works by\n",
        "generating multiple responses for a given prompt, evaluating these responses\n",
        "using a reward model, and then calculating a relative advantage based on the\n",
        "group's performance to update the policy.\n",
        "\n",
        "In this tutorial we use Colab's `v2-8` TPU. Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afofSj37qYz6",
      "metadata": {},
      "source": [
        "## Install necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z03GnyApTn1j",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kagglehub\n",
        "\n",
        "!pip install -q tensorflow\n",
        "!pip install -q tensorboardX\n",
        "!pip install -q grain\n",
        "!pip install -q git+https://github.com/google/tunix\n",
        "!pip install -q git+https://github.com/google/qwix\n",
        "\n",
        "!pip uninstall -q -y flax\n",
        "!pip install -q git+https://github.com/google/flax.git\n",
        "\n",
        "!pip install -q tensorflow-datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f95eb96c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ipywidgets\n",
            "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: comm>=0.1.3 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipywidgets) (9.3.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
            "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
            "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
            "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: decorator in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
            "Requirement already satisfied: ipython-pygments-lexers in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: stack_data in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
            "Requirement already satisfied: typing_extensions>=4.6 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.14.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: executing>=1.2.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
            "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
            "Successfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LnF9ZACiTn1k",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "McTNo_r8Tn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "import functools\n",
        "import gc\n",
        "import os\n",
        "from pprint import pprint\n",
        "import re\n",
        "import time\n",
        "\n",
        "from flax import nnx\n",
        "import grain\n",
        "import humanize\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "# import kagglehub\n",
        "import optax\n",
        "from orbax import checkpoint as ocp\n",
        "from qwix import lora\n",
        "import tensorflow_datasets as tfds\n",
        "from tqdm.auto import tqdm\n",
        "from tunix.generate import sampler as sampler_lib\n",
        "from tunix.models.gemma import data as data_lib\n",
        "from tunix.models.gemma import gemma as gemma_lib\n",
        "from tunix.models.gemma import params as params_lib\n",
        "from tunix.rl import rl_cluster as rl_cluster_lib\n",
        "from tunix.rl.rollout import base_rollout\n",
        "from tunix.rl.grpo.grpo_learner import GrpoConfig, GrpoLearner\n",
        "from tunix.sft import metrics_logger"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Eu_NI9nHTn1k",
      "metadata": {},
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Let's define the configuration we are going to use. Note that this is by no\n",
        "means a \"perfect\" set of hyperparameters. To get good results, you might have\n",
        "to train the model for longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ZPPKme47Tn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====== Data ======\n",
        "TRAIN_DATA_DIR = \"./data/train\"\n",
        "TEST_DATA_DIR = \"./data/test\"\n",
        "TRAIN_FRACTION = 1.0\n",
        "\n",
        "# ====== LoRA ======\n",
        "RANK = 64\n",
        "ALPHA = 64.0\n",
        "\n",
        "# ====== Sharding ======\n",
        "MESH = [(1, 4), (\"fsdp\", \"tp\")]\n",
        "\n",
        "# ====== GRPO ======\n",
        "# === Generation during GRPO training ===\n",
        "MAX_PROMPT_LENGTH = 256\n",
        "TOTAL_GENERATION_STEPS = 768\n",
        "# Important to keep a high-ish temperature for varied, diverse responses during\n",
        "# training.\n",
        "TEMPERATURE = 0.9\n",
        "TOP_P = 1.0\n",
        "TOP_K = 50\n",
        "# The number of times the policy generates multiple responses for a given prompt\n",
        "# within a single training step. This corresponds to `G` in Algorithm 1 in the\n",
        "# paper. The \"group\" in GRPO comes from here.\n",
        "NUM_GENERATIONS = 2\n",
        "\n",
        "# === other GRPO configs ===\n",
        "# The number of iterations per batch (𝜇 in GRPO algo 1).\n",
        "NUM_ITERATIONS = 1\n",
        "# The coefficient for the KL divergence penalty (𝛽) in the GRPO loss function.\n",
        "# Important to keep a high enough value for this, otherwise, the KL divergence\n",
        "# can increase unchecked.\n",
        "BETA = 0.08\n",
        "# Epsilon value for clipping (𝜀 in GRPO loss in paper). Similar to PPO, for\n",
        "# stable updates.\n",
        "EPSILON = 0.2\n",
        "\n",
        "# ====== Training ======\n",
        "BATCH_SIZE = 1\n",
        "# Increase `NUM_BATCHES` and `MAX_STEPS` for better results.\n",
        "NUM_BATCHES = 3738\n",
        "# Keep `NUM_TEST_BATCHES` low so that evaluation runs quickly. It can be\n",
        "# increased to a max. of 330 (if batch size is 4).\n",
        "NUM_TEST_BATCHES = 100\n",
        "\n",
        "EVAL_EVERY_N_STEPS = 10  # this doesn't matter if `TRAIN_FRACTION = 1.0`.\n",
        "NUM_EPOCHS = 1  # can potentially train for more epochs\n",
        "\n",
        "# Number of training steps.\n",
        "MAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\n",
        "\n",
        "# === AdamW, warmup, cosine scheduler ===\n",
        "LEARNING_RATE = 3e-6\n",
        "B1 = 0.9\n",
        "B2 = 0.99\n",
        "WEIGHT_DECAY = 0.1\n",
        "# == Cosine decay with warmup scheduler ==\n",
        "# Linearly increase learning rate from 0. to 5e-6 in the first 10% training\n",
        "# steps, and then gradually decrease the learning rate to 0 using cosine\n",
        "# scheduler.\n",
        "WARMUP_STEPS = 0.1 * MAX_STEPS\n",
        "# == Grad clipping ==\n",
        "# Grad clipping to prevent large gradients. Found this\n",
        "# important to keep KL divergence in check.\n",
        "MAX_GRAD_NORM = 0.1\n",
        "\n",
        "# Checkpoint saving\n",
        "INTERMEDIATE_CKPT_DIR = \"/content/intermediate_ckpt/\"\n",
        "CKPT_DIR = \"/content/ckpts/\"\n",
        "SAVE_INTERVAL_STEPS = 500\n",
        "MAX_TO_KEEP = 4\n",
        "\n",
        "# ====== Inference ======\n",
        "GENERATION_CONFIGS = {\n",
        "    # greedy search\n",
        "    \"greedy\": {\"temperature\": 1e-4, \"top_k\": 1, \"top_p\": 1.0},\n",
        "    # some randomness\n",
        "    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
        "    # liberal\n",
        "    \"liberal\": {\"temperature\": 0.85, \"top_k\": 2000, \"top_p\": 1.0},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ngjtE-63Tn1k",
      "metadata": {},
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "wjMFOr7aTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_hbm_usage():\n",
        "  \"\"\"Displays memory usage per device.\"\"\"\n",
        "  fmt_size = functools.partial(humanize.naturalsize, binary=True)\n",
        "\n",
        "  for d in jax.local_devices():\n",
        "    stats = d.memory_stats()\n",
        "    used = stats[\"bytes_in_use\"]\n",
        "    limit = stats[\"bytes_limit\"]\n",
        "    print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:%}) on {d}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6BtpYMlaTn1k",
      "metadata": {},
      "source": [
        "## Data preprocessing\n",
        "\n",
        "First, let's define some special tokens. We instruct the model to first reason\n",
        "between the `<reasoning>` and `</reasoning>` tokens. After\n",
        "reasoning, we expect it to provide the answer between the `<answer>` and\n",
        "`</answer>` tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "h6RGv1kSTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "reasoning_start = \"<reasoning>\"\n",
        "reasoning_end = \"</reasoning>\"\n",
        "solution_start = \"<answer>\"\n",
        "solution_end = \"</answer>\"\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT = f\"\"\"You are given a problem. Think about the problem and \\\n",
        "provide your reasoning. Place it between {reasoning_start} and \\\n",
        "{reasoning_end}. Then, provide the final answer (i.e., just one numerical \\\n",
        "value) between {solution_start} and {solution_end}.\"\"\"\n",
        "\n",
        "TEMPLATE = \"\"\"<start_of_turn>user\n",
        "{system_prompt}\n",
        "\n",
        "{question}<end_of_turn>\n",
        "<start_of_turn>model\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WASP9N5JTn1k",
      "metadata": {},
      "source": [
        "We use OpenAI's GSM8K dataset. GSM8K comprises grade school math word problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "gTGjcSMNTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_hash_answer(text: str) -> str | None:\n",
        "  if \"####\" not in text:\n",
        "    return None\n",
        "  return text.split(\"####\")[1].strip()\n",
        "\n",
        "\n",
        "def get_dataset(data_dir, split=\"train\") -> grain.MapDataset:\n",
        "  # Download data\n",
        "  if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "  data = tfds.data_source(\n",
        "      \"gsm8k\",\n",
        "      split=split,\n",
        "      data_dir=data_dir,\n",
        "      builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n",
        "      download=True,\n",
        "  )\n",
        "\n",
        "  dataset = (\n",
        "      grain.MapDataset.source(data)\n",
        "      .shuffle(seed=42)\n",
        "      .map(\n",
        "          lambda x: {\n",
        "              # passed to model forward pass\n",
        "              \"prompts\": TEMPLATE.format(\n",
        "                  system_prompt=SYSTEM_PROMPT,\n",
        "                  question=x[\"question\"].decode(\"utf-8\"),\n",
        "              ),\n",
        "              # passed to reward functions\n",
        "              \"question\": x[\"question\"].decode(\"utf-8\"),\n",
        "              # passed to reward functions\n",
        "              \"answer\": extract_hash_answer(x[\"answer\"].decode(\"utf-8\")),\n",
        "          }\n",
        "      )\n",
        "  )\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "KXhOL6GyTn1k",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Variant folder data/train/gsm8k/1.0.0 has no dataset_info.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to data/train/gsm8k/1.0.0...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Dl Completed...: 0 url [00:00, ? url/s]\n",
            "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
            "Dl Completed...:   0%|          | 0/2 [00:00<?, ? url/s]\n",
            "Dl Completed...:   0%|          | 0/3 [00:00<?, ? url/s]\n",
            "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
            "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
            "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
            "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:00,  3.77 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  7.26 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.45 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.35 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.28 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.22 url/s]\n",
            "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  4.74 url/s]\n",
            "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  4.55 url/s]\n",
            "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  4.44 url/s]\n",
            "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  4.37 url/s]\n",
            "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  4.30 url/s]\n",
            "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  4.23 url/s]\n",
            "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  4.17 url/s]\n",
            "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00,  5.52 url/s]\n",
            "Extraction completed...: 0 file [00:00, ? file/s]\n",
            "Dl Size...: 8 MiB [00:00, 11.01 MiB/s]\n",
            "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00,  5.49 url/s]\n",
            "WARNING:absl:Variant folder data/test/gsm8k/1.0.0 has no dataset_info.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDataset gsm8k downloaded and prepared to data/train/gsm8k/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
            "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to data/test/gsm8k/1.0.0...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Dl Completed...: 0 url [00:00, ? url/s]\n",
            "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
            "Dl Completed...:   0%|          | 0/2 [00:00<?, ? url/s]\n",
            "Dl Completed...:   0%|          | 0/3 [00:00<?, ? url/s]\n",
            "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
            "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
            "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
            "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
            "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
            "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:00, 19.26 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00, 33.60 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00, 29.94 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00, 28.58 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00, 24.06 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00, 22.97 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00, 20.46 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00, 19.56 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00, 17.97 url/s]\n",
            "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00, 25.80 url/s]\n",
            "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00, 24.04 url/s]\n",
            "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00, 31.20 url/s]\n",
            "Extraction completed...: 0 file [00:00, ? file/s]\n",
            "Dl Size...: 8 MiB [00:00, 61.37 MiB/s]\n",
            "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00, 30.37 url/s]\n",
            "                                                                        "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDataset gsm8k downloaded and prepared to data/test/gsm8k/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(3738, 0, 100)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = get_dataset(TRAIN_DATA_DIR, \"train\").batch(BATCH_SIZE)[:NUM_BATCHES]\n",
        "\n",
        "if TRAIN_FRACTION == 1.0:\n",
        "  train_dataset = dataset.repeat(NUM_EPOCHS)\n",
        "  val_dataset = None\n",
        "else:\n",
        "  train_dataset = dataset[: int(len(dataset) * TRAIN_FRACTION)]\n",
        "  train_dataset = train_dataset.repeat(NUM_EPOCHS)\n",
        "\n",
        "  val_dataset = dataset[int(len(dataset) * TRAIN_FRACTION) :].repeat(NUM_EPOCHS)\n",
        "\n",
        "test_dataset = get_dataset(TEST_DATA_DIR, \"test\").batch(BATCH_SIZE)[\n",
        "    :NUM_TEST_BATCHES\n",
        "]\n",
        "\n",
        "len(train_dataset), len(val_dataset) if val_dataset is not None else 0, len(\n",
        "    test_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k7n8L0VzTn1k",
      "metadata": {},
      "source": [
        "Let's see how one batch of the dataset looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5TF-wNQ2Tn1k",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'answer': array(['13'], dtype='<U2'),\n",
            " 'prompts': array(['<start_of_turn>user\\nYou are given a problem. Think about the problem and provide your reasoning. Place it between <reasoning> and </reasoning>. Then, provide the final answer (i.e., just one numerical value) between <answer> and </answer>.\\n\\nJane is painting her fingernails. She applies a base coat that takes 2 minutes to dry, two color coats that take 3 minutes each to dry, and a clear top coat that takes 5 minutes to dry. How many minutes total does Jane spend waiting for her nail polish to dry?<end_of_turn>\\n<start_of_turn>model'],\n",
            "      dtype='<U535'),\n",
            " 'question': array(['Jane is painting her fingernails. She applies a base coat that takes 2 minutes to dry, two color coats that take 3 minutes each to dry, and a clear top coat that takes 5 minutes to dry. How many minutes total does Jane spend waiting for her nail polish to dry?'],\n",
            "      dtype='<U260')}\n"
          ]
        }
      ],
      "source": [
        "for ele in train_dataset[:1]:\n",
        "  pprint(ele)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BZxBR7Y_Tn1k",
      "metadata": {},
      "source": [
        "## Load the policy model and the reference model\n",
        "\n",
        "The policy model is the model which is actually trained and whose weights are\n",
        "updated. The reference model is the model with which we compute KL divergence.\n",
        "This is to ensure that the policy updates are not huge and that it does not\n",
        "deviate too much from the reference model.\n",
        "\n",
        "Typically, the reference model is the base model, and the policy model is the\n",
        "same base model, but with LoRA parameters. Only the LoRA parameters are updated.\n",
        "\n",
        "Note: We perform full precision (fp32) training. You can, however, leverage\n",
        "Qwix for QAT.\n",
        "\n",
        "To load the model, you need to be on [Kaggle](https://www.kaggle.com/) and need\n",
        "to have agreed to the Gemma license\n",
        "[here](https://www.kaggle.com/models/google/gemma/flax/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "thp6hhqfTn1k",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da8771d2503046b99f92d9a5a7b7e77a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Log in\n",
        "if \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n",
        "  kagglehub.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "srH2s_jzTn1k",
      "metadata": {},
      "outputs": [
        {
          "ename": "KaggleApiHTTPError",
          "evalue": "403 Client Error.\n\nYou don't have permission to access resource at URL: https://www.kaggle.com/models/google/gemma-2/flax/gemma2-2b-it/1. The server reported the following issues: Permission denied on resource (or it may not exists).\nPlease make sure you are authenticated if you are trying to access a private resource or a resource requiring consent.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/exceptions.py:66\u001b[39m, in \u001b[36mkaggle_api_raise_for_status\u001b[39m\u001b[34m(response, resource_handle)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[31mHTTPError\u001b[39m: 403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/models/google/gemma-2/flax/gemma2-2b-it/1/files?page_size=25",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mKaggleApiHTTPError\u001b[39m                        Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m kaggle_ckpt_path = \u001b[43mkagglehub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_download\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoogle/gemma-2/flax/gemma2-2b-it\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/models.py:35\u001b[39m, in \u001b[36mmodel_download\u001b[39m\u001b[34m(handle, path, force_download)\u001b[39m\n\u001b[32m     33\u001b[39m h = parse_model_handle(handle)\n\u001b[32m     34\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloading Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh.to_url()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m, extra={**EXTRA_CONSOLE_BLOCK})\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m path, _ = \u001b[43mregistry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/registry.py:28\u001b[39m, in \u001b[36mMultiImplRegistry.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m._impls):\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m impl.is_supported(*args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     30\u001b[39m         fails.append(\u001b[38;5;28mtype\u001b[39m(impl).\u001b[34m__name__\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/resolver.py:29\u001b[39m, in \u001b[36mResolver.__call__\u001b[39m\u001b[34m(self, handle, path, force_download)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mself\u001b[39m, handle: T, path: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m, *, force_download: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     17\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[32m     18\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Resolves a handle into a path with the requested file(s) and the resource's version number.\u001b[39;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m \u001b[33;03m        Some cases where version number might be missing: Competition datasource, API-based models.\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     path, version = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_resolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# Note handles are immutable, so _resolve() could not have altered our reference\u001b[39;00m\n\u001b[32m     32\u001b[39m     register_datasource_access(handle, version)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/http_resolver.py:172\u001b[39m, in \u001b[36mModelHttpResolver._resolve\u001b[39m\u001b[34m(self, h, path, force_download)\u001b[39m\n\u001b[32m    167\u001b[39m     api_client.download_file(url_path, out_path, h, extract_auto_compressed_file=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# List the files and decide how to download them:\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;66;03m# - <= 25 files: Download files in parallel\u001b[39;00m\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# > 25 files: Download the archive and uncompress\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     (files, has_more) = \u001b[43m_list_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m has_more:\n\u001b[32m    174\u001b[39m         \u001b[38;5;66;03m# Downloading the full archived bundle.\u001b[39;00m\n\u001b[32m    175\u001b[39m         archive_path = get_cached_archive_path(h)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/http_resolver.py:311\u001b[39m, in \u001b[36m_list_files\u001b[39m\u001b[34m(api_client, h)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_list_files\u001b[39m(api_client: KaggleApiV1Client, h: ModelHandle) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     json_response = \u001b[43mapi_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_build_list_model_instance_version_files_url_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mfiles\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m json_response:\n\u001b[32m    313\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33mInvalid ListModelInstanceVersionFiles API response. Expected to include a \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfiles\u001b[39m\u001b[33m'\u001b[39m\u001b[33m field\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/clients.py:139\u001b[39m, in \u001b[36mKaggleApiV1Client.get\u001b[39m\u001b[34m(self, path, resource_handle)\u001b[39m\n\u001b[32m    132\u001b[39m url = \u001b[38;5;28mself\u001b[39m._build_url(path)\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m requests.get(\n\u001b[32m    134\u001b[39m     url,\n\u001b[32m    135\u001b[39m     headers={\u001b[33m\"\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m\"\u001b[39m: get_user_agent()},\n\u001b[32m    136\u001b[39m     auth=\u001b[38;5;28mself\u001b[39m._get_auth(),\n\u001b[32m    137\u001b[39m     timeout=(DEFAULT_CONNECT_TIMEOUT, DEFAULT_READ_TIMEOUT),\n\u001b[32m    138\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[43mkaggle_api_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_handle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_for_version_update(response)\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/exceptions.py:106\u001b[39m, in \u001b[36mkaggle_api_raise_for_status\u001b[39m\u001b[34m(response, resource_handle)\u001b[39m\n\u001b[32m     97\u001b[39m     message = (\n\u001b[32m     98\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     99\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    102\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease make sure you specified the correct resource identifiers.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    103\u001b[39m     )\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# Default handling\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m KaggleApiHTTPError(message, response=response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
            "\u001b[31mKaggleApiHTTPError\u001b[39m: 403 Client Error.\n\nYou don't have permission to access resource at URL: https://www.kaggle.com/models/google/gemma-2/flax/gemma2-2b-it/1. The server reported the following issues: Permission denied on resource (or it may not exists).\nPlease make sure you are authenticated if you are trying to access a private resource or a resource requiring consent."
          ]
        }
      ],
      "source": [
        "kaggle_ckpt_path = kagglehub.model_download(\"google/gemma-2/flax/gemma2-2b-it\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cIFAxgVOTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is a workaround. The checkpoints on Kaggle don't work with NNX. So, we\n",
        "# load the model, save the checkpoint locally, and then reload the model\n",
        "# (sharded).\n",
        "params = params_lib.load_and_format_params(\n",
        "    os.path.join(kaggle_ckpt_path, \"gemma2-2b-it\")\n",
        ")\n",
        "gemma = gemma_lib.Transformer.from_params(params, version=\"2-2b-it\")\n",
        "checkpointer = ocp.StandardCheckpointer()\n",
        "_, state = nnx.split(gemma)\n",
        "checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JSz-XmQpTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wait for the ckpt to save successfully.\n",
        "time.sleep(60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_w8kav8sTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete the intermediate model to save memory.\n",
        "del params\n",
        "del gemma\n",
        "del state\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52b18135",
      "metadata": {},
      "source": [
        "### Load MaxText model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6aa758c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mlperf-logging@ git+https://github.com/mlperf/logging.git (from -r ../../maxtext/requirements.txt (line 39))\n",
            "  Cloning https://github.com/mlperf/logging.git to /tmp/pip-install-j1qaesax/mlperf-logging_ea2e287b6f8343a3abb99cda9a59c33f\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/mlperf/logging.git /tmp/pip-install-j1qaesax/mlperf-logging_ea2e287b6f8343a3abb99cda9a59c33f\n",
            "  Resolved https://github.com/mlperf/logging.git to commit 44b4810e65e8c0a7d9e4e207c60e51d9458a3fb8\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting google-jetstream@ git+https://github.com/AI-Hypercomputer/JetStream.git (from -r ../../maxtext/requirements.txt (line 40))\n",
            "  Cloning https://github.com/AI-Hypercomputer/JetStream.git to /tmp/pip-install-j1qaesax/google-jetstream_409f8bda9d064e9284dc18ac4e114fe5\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI-Hypercomputer/JetStream.git /tmp/pip-install-j1qaesax/google-jetstream_409f8bda9d064e9284dc18ac4e114fe5\n",
            "  Resolved https://github.com/AI-Hypercomputer/JetStream.git to commit 29329e8e73820993f77cfc8efe34eb2a73f5de98\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: jax>=0.4.30 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from -r ../../maxtext/requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: jaxlib>=0.4.30 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from -r ../../maxtext/requirements.txt (line 2)) (0.6.2)\n",
            "Requirement already satisfied: orbax-checkpoint>=0.5.12 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from -r ../../maxtext/requirements.txt (line 3)) (0.11.16)\n",
            "Requirement already satisfied: absl-py in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from -r ../../maxtext/requirements.txt (line 4)) (2.3.0)\n",
            "Requirement already satisfied: array-record in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from -r ../../maxtext/requirements.txt (line 5)) (0.7.2)\n",
            "Collecting aqtp (from -r ../../maxtext/requirements.txt (line 6))\n",
            "  Using cached aqtp-0.8.4-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting cloud-accelerator-diagnostics (from -r ../../maxtext/requirements.txt (line 7))\n",
            "  Using cached cloud_accelerator_diagnostics-0.1.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting cloud-tpu-diagnostics (from -r ../../maxtext/requirements.txt (line 8))\n",
            "  Using cached cloud_tpu_diagnostics-0.1.5-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: datasets in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from -r ../../maxtext/requirements.txt (line 9)) (3.6.0)\n",
            "Collecting gcsfs (from -r ../../maxtext/requirements.txt (line 10))\n",
            "  Using cached gcsfs-2025.5.1-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting google-cloud-aiplatform==1.61.0 (from -r ../../maxtext/requirements.txt (line 11))\n",
            "  Using cached google_cloud_aiplatform-1.61.0-py2.py3-none-any.whl.metadata (31 kB)\n",
            "Collecting google-cloud-storage (from -r ../../maxtext/requirements.txt (line 12))\n",
            "  Using cached google_cloud_storage-3.1.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting google-cloud-monitoring (from -r ../../maxtext/requirements.txt (line 13))\n",
            "  Using cached google_cloud_monitoring-2.27.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting google-api-core (from -r ../../maxtext/requirements.txt (line 14))\n",
            "  Using cached google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting google-api-python-client (from -r ../../maxtext/requirements.txt (line 15))\n",
            "  Using cached google_api_python_client-2.174.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: grain>=0.2.6 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from grain[parquet]>=0.2.6->-r ../../maxtext/requirements.txt (line 16)) (0.2.10)\n",
            "Requirement already satisfied: huggingface_hub in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from -r ../../maxtext/requirements.txt (line 17)) (0.33.1)\n",
            "Requirement already satisfied: flax>=0.10.6 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from -r ../../maxtext/requirements.txt (line 18)) (0.10.6)\n",
            "Requirement already satisfied: jaxtyping in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from -r ../../maxtext/requirements.txt (line 19)) (0.3.2)\n",
            "Collecting ml-collections (from -r ../../maxtext/requirements.txt (line 20))\n",
            "  Using cached ml_collections-1.1.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting ml-goodput-measurement==0.0.10 (from -r ../../maxtext/requirements.txt (line 21))\n",
            "  Using cached ml_goodput_measurement-0.0.10-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: numpy in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from -r ../../maxtext/requirements.txt (line 22)) (2.1.3)\n",
            "Requirement already satisfied: optax in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from -r ../../maxtext/requirements.txt (line 23)) (0.2.5)\n",
            "Collecting protobuf==3.20.3 (from -r ../../maxtext/requirements.txt (line 24))\n",
            "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Collecting pylint (from -r ../../maxtext/requirements.txt (line 25))\n",
            "  Using cached pylint-3.3.7-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pytest (from -r ../../maxtext/requirements.txt (line 26))\n",
            "  Using cached pytest-8.4.1-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting pyink (from -r ../../maxtext/requirements.txt (line 27))\n",
            "  Using cached pyink-24.10.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pre-commit (from -r ../../maxtext/requirements.txt (line 28))\n",
            "  Using cached pre_commit-4.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting pytype (from -r ../../maxtext/requirements.txt (line 29))\n",
            "  Using cached pytype-2024.10.11-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
            "Collecting pillow>=11.1.0 (from -r ../../maxtext/requirements.txt (line 30))\n",
            "  Using cached pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: sentencepiece==0.2.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from -r ../../maxtext/requirements.txt (line 31)) (0.2.0)\n",
            "Collecting tensorflow-text>=2.13.0 (from -r ../../maxtext/requirements.txt (line 32))\n",
            "  Using cached tensorflow_text-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: tensorflow>=2.13.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from -r ../../maxtext/requirements.txt (line 33)) (2.19.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from -r ../../maxtext/requirements.txt (line 34)) (4.9.9)\n",
            "Requirement already satisfied: tensorboardx>=2.6.2.2 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from -r ../../maxtext/requirements.txt (line 35)) (2.6.4)\n",
            "Collecting tensorboard-plugin-profile (from -r ../../maxtext/requirements.txt (line 36))\n",
            "  Using cached tensorboard_plugin_profile-2.20.1-cp311-none-manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting tiktoken (from -r ../../maxtext/requirements.txt (line 37))\n",
            "  Using cached tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting transformers (from -r ../../maxtext/requirements.txt (line 38))\n",
            "  Using cached transformers-4.53.0-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting jsonlines (from -r ../../maxtext/requirements.txt (line 41))\n",
            "  Using cached jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pathwaysutils==0.1.1 (from -r ../../maxtext/requirements.txt (line 42))\n",
            "  Using cached pathwaysutils-0.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting omegaconf (from -r ../../maxtext/requirements.txt (line 43))\n",
            "  Using cached omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting google-auth<3.0.0dev,>=2.14.1 (from google-cloud-aiplatform==1.61.0->-r ../../maxtext/requirements.txt (line 11))\n",
            "  Using cached google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-cloud-aiplatform==1.61.0->-r ../../maxtext/requirements.txt (line 11))\n",
            "  Using cached proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: packaging>=14.3 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from google-cloud-aiplatform==1.61.0->-r ../../maxtext/requirements.txt (line 11)) (25.0)\n",
            "Collecting google-cloud-storage (from -r ../../maxtext/requirements.txt (line 12))\n",
            "  Using cached google_cloud_storage-2.19.0-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 (from google-cloud-aiplatform==1.61.0->-r ../../maxtext/requirements.txt (line 11))\n",
            "  Using cached google_cloud_bigquery-3.34.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform==1.61.0->-r ../../maxtext/requirements.txt (line 11))\n",
            "  Using cached google_cloud_resource_manager-1.14.2-py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting shapely<3.0.0dev (from google-cloud-aiplatform==1.61.0->-r ../../maxtext/requirements.txt (line 11))\n",
            "  Using cached shapely-2.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting pydantic<3 (from google-cloud-aiplatform==1.61.0->-r ../../maxtext/requirements.txt (line 11))\n",
            "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
            "Requirement already satisfied: docstring-parser<1 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from google-cloud-aiplatform==1.61.0->-r ../../maxtext/requirements.txt (line 11)) (0.16)\n",
            "Collecting google-cloud-logging>=3.5.0 (from ml-goodput-measurement==0.0.10->-r ../../maxtext/requirements.txt (line 21))\n",
            "  Using cached google_cloud_logging-3.12.1-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: requests in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ml-goodput-measurement==0.0.10->-r ../../maxtext/requirements.txt (line 21)) (2.32.4)\n",
            "Requirement already satisfied: scipy in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ml-goodput-measurement==0.0.10->-r ../../maxtext/requirements.txt (line 21)) (1.16.0)\n",
            "Requirement already satisfied: urllib3 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ml-goodput-measurement==0.0.10->-r ../../maxtext/requirements.txt (line 21)) (2.5.0)\n",
            "Collecting fastapi (from pathwaysutils==0.1.1->-r ../../maxtext/requirements.txt (line 42))\n",
            "  Using cached fastapi-0.115.14-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn (from pathwaysutils==0.1.1->-r ../../maxtext/requirements.txt (line 42))\n",
            "  Using cached uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from jax>=0.4.30->-r ../../maxtext/requirements.txt (line 1)) (0.5.1)\n",
            "Requirement already satisfied: opt_einsum in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from jax>=0.4.30->-r ../../maxtext/requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: etils[epath,epy] in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint>=0.5.12->-r ../../maxtext/requirements.txt (line 3)) (1.12.2)\n",
            "Requirement already satisfied: typing_extensions in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint>=0.5.12->-r ../../maxtext/requirements.txt (line 3)) (4.14.0)\n",
            "Requirement already satisfied: msgpack in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint>=0.5.12->-r ../../maxtext/requirements.txt (line 3)) (1.1.1)\n",
            "Requirement already satisfied: pyyaml in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint>=0.5.12->-r ../../maxtext/requirements.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: tensorstore>=0.1.71 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint>=0.5.12->-r ../../maxtext/requirements.txt (line 3)) (0.1.75)\n",
            "Requirement already satisfied: nest_asyncio in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint>=0.5.12->-r ../../maxtext/requirements.txt (line 3)) (1.6.0)\n",
            "Requirement already satisfied: humanize in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint>=0.5.12->-r ../../maxtext/requirements.txt (line 3)) (4.12.3)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint>=0.5.12->-r ../../maxtext/requirements.txt (line 3)) (3.20.1)\n",
            "Requirement already satisfied: filelock in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from datasets->-r ../../maxtext/requirements.txt (line 9)) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from datasets->-r ../../maxtext/requirements.txt (line 9)) (20.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from datasets->-r ../../maxtext/requirements.txt (line 9)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from datasets->-r ../../maxtext/requirements.txt (line 9)) (2.3.0)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from datasets->-r ../../maxtext/requirements.txt (line 9)) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from datasets->-r ../../maxtext/requirements.txt (line 9)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from datasets->-r ../../maxtext/requirements.txt (line 9)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r ../../maxtext/requirements.txt (line 9)) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from gcsfs->-r ../../maxtext/requirements.txt (line 10)) (3.12.13)\n",
            "Requirement already satisfied: decorator>4.1.2 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from gcsfs->-r ../../maxtext/requirements.txt (line 10)) (5.2.1)\n",
            "INFO: pip is looking at multiple versions of gcsfs to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting gcsfs (from -r ../../maxtext/requirements.txt (line 10))\n",
            "  Using cached gcsfs-2025.5.0.post1-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "  Using cached gcsfs-2025.5.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "  Using cached gcsfs-2025.3.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "  Using cached gcsfs-2025.3.1-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "  Using cached gcsfs-2025.3.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting google-auth-oauthlib (from gcsfs->-r ../../maxtext/requirements.txt (line 10))\n",
            "  Using cached google_auth_oauthlib-1.2.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting google-cloud-core<3.0dev,>=2.3.0 (from google-cloud-storage->-r ../../maxtext/requirements.txt (line 12))\n",
            "  Using cached google_cloud_core-2.4.3-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting google-resumable-media>=2.7.2 (from google-cloud-storage->-r ../../maxtext/requirements.txt (line 12))\n",
            "  Using cached google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage->-r ../../maxtext/requirements.txt (line 12))\n",
            "  Using cached google_crc32c-1.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from google-api-core->-r ../../maxtext/requirements.txt (line 14)) (1.70.0)\n",
            "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->-r ../../maxtext/requirements.txt (line 15))\n",
            "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->-r ../../maxtext/requirements.txt (line 15))\n",
            "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->-r ../../maxtext/requirements.txt (line 15))\n",
            "  Using cached uritemplate-4.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: cloudpickle in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from grain>=0.2.6->grain[parquet]>=0.2.6->-r ../../maxtext/requirements.txt (line 16)) (3.1.1)\n",
            "Requirement already satisfied: dm-tree in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from grain>=0.2.6->grain[parquet]>=0.2.6->-r ../../maxtext/requirements.txt (line 16)) (0.1.9)\n",
            "Requirement already satisfied: more-itertools>=9.1.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from grain>=0.2.6->grain[parquet]>=0.2.6->-r ../../maxtext/requirements.txt (line 16)) (10.7.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from huggingface_hub->-r ../../maxtext/requirements.txt (line 17)) (1.1.5)\n",
            "Requirement already satisfied: rich>=11.1 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from flax>=0.10.6->-r ../../maxtext/requirements.txt (line 18)) (14.0.0)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from flax>=0.10.6->-r ../../maxtext/requirements.txt (line 18)) (0.1.9)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.3 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from jaxtyping->-r ../../maxtext/requirements.txt (line 19)) (0.1.7)\n",
            "Requirement already satisfied: chex>=0.1.87 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from optax->-r ../../maxtext/requirements.txt (line 23)) (0.1.89)\n",
            "Collecting astroid<=3.4.0.dev0,>=3.3.8 (from pylint->-r ../../maxtext/requirements.txt (line 25))\n",
            "  Using cached astroid-3.3.10-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting isort!=5.13,<7,>=4.2.5 (from pylint->-r ../../maxtext/requirements.txt (line 25))\n",
            "  Using cached isort-6.0.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mccabe<0.8,>=0.6 (from pylint->-r ../../maxtext/requirements.txt (line 25))\n",
            "  Using cached mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: platformdirs>=2.2 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from pylint->-r ../../maxtext/requirements.txt (line 25)) (4.3.8)\n",
            "Collecting tomlkit>=0.10.1 (from pylint->-r ../../maxtext/requirements.txt (line 25))\n",
            "  Using cached tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting iniconfig>=1 (from pytest->-r ../../maxtext/requirements.txt (line 26))\n",
            "  Using cached iniconfig-2.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting pluggy<2,>=1.5 (from pytest->-r ../../maxtext/requirements.txt (line 26))\n",
            "  Using cached pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: pygments>=2.7.2 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from pytest->-r ../../maxtext/requirements.txt (line 26)) (2.19.2)\n",
            "Collecting black==24.10.0 (from pyink->-r ../../maxtext/requirements.txt (line 27))\n",
            "  Using cached black-24.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
            "Collecting click>=8.0.0 (from pyink->-r ../../maxtext/requirements.txt (line 27))\n",
            "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting mypy-extensions>=0.4.3 (from pyink->-r ../../maxtext/requirements.txt (line 27))\n",
            "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from pyink->-r ../../maxtext/requirements.txt (line 27))\n",
            "  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit->-r ../../maxtext/requirements.txt (line 28))\n",
            "  Using cached cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit->-r ../../maxtext/requirements.txt (line 28))\n",
            "  Using cached identify-2.6.12-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting nodeenv>=0.11.1 (from pre-commit->-r ../../maxtext/requirements.txt (line 28))\n",
            "  Using cached nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit->-r ../../maxtext/requirements.txt (line 28))\n",
            "  Using cached virtualenv-20.31.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: attrs>=21.4.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from pytype->-r ../../maxtext/requirements.txt (line 29)) (25.3.0)\n",
            "Collecting importlab>=0.8 (from pytype->-r ../../maxtext/requirements.txt (line 29))\n",
            "  Using cached importlab-0.8.1-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: immutabledict>=4.1.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from pytype->-r ../../maxtext/requirements.txt (line 29)) (4.2.1)\n",
            "Collecting jinja2>=3.1.2 (from pytype->-r ../../maxtext/requirements.txt (line 29))\n",
            "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting libcst>=1.0.1 (from pytype->-r ../../maxtext/requirements.txt (line 29))\n",
            "  Using cached libcst-1.8.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (15 kB)\n",
            "Collecting msgspec>=0.18.6 (from pytype->-r ../../maxtext/requirements.txt (line 29))\n",
            "  Using cached msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting networkx>=2.8 (from pytype->-r ../../maxtext/requirements.txt (line 29))\n",
            "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting ninja>=1.10.0.post2 (from pytype->-r ../../maxtext/requirements.txt (line 29))\n",
            "  Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting pycnite>=2024.07.31 (from pytype->-r ../../maxtext/requirements.txt (line 29))\n",
            "  Using cached pycnite-2024.7.31-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting pydot>=1.4.2 (from pytype->-r ../../maxtext/requirements.txt (line 29))\n",
            "  Using cached pydot-4.0.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting tabulate>=0.8.10 (from pytype->-r ../../maxtext/requirements.txt (line 29))\n",
            "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: toml>=0.10.2 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from pytype->-r ../../maxtext/requirements.txt (line 29)) (0.10.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from tensorflow>=2.13.0->-r ../../maxtext/requirements.txt (line 33)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from tensorflow>=2.13.0->-r ../../maxtext/requirements.txt (line 33)) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from tensorflow>=2.13.0->-r ../../maxtext/requirements.txt (line 33)) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from tensorflow>=2.13.0->-r ../../maxtext/requirements.txt (line 33)) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from tensorflow>=2.13.0->-r ../../maxtext/requirements.txt (line 33)) (18.1.1)\n",
            "Requirement already satisfied: setuptools in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from tensorflow>=2.13.0->-r ../../maxtext/requirements.txt (line 33)) (65.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from tensorflow>=2.13.0->-r ../../maxtext/requirements.txt (line 33)) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from tensorflow>=2.13.0->-r ../../maxtext/requirements.txt (line 33)) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from tensorflow>=2.13.0->-r ../../maxtext/requirements.txt (line 33)) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from tensorflow>=2.13.0->-r ../../maxtext/requirements.txt (line 33)) (1.73.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from tensorflow>=2.13.0->-r ../../maxtext/requirements.txt (line 33)) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from tensorflow>=2.13.0->-r ../../maxtext/requirements.txt (line 33)) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from tensorflow>=2.13.0->-r ../../maxtext/requirements.txt (line 33)) (3.14.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from tensorflow>=2.13.0->-r ../../maxtext/requirements.txt (line 33)) (0.37.1)\n",
            "Requirement already satisfied: promise in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets->-r ../../maxtext/requirements.txt (line 34)) (2.3)\n",
            "Requirement already satisfied: psutil in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets->-r ../../maxtext/requirements.txt (line 34)) (7.0.0)\n",
            "Requirement already satisfied: simple_parsing in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets->-r ../../maxtext/requirements.txt (line 34)) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from tensorflow-datasets->-r ../../maxtext/requirements.txt (line 34)) (1.17.2)\n",
            "Collecting xprof==2.20.1 (from tensorboard-plugin-profile->-r ../../maxtext/requirements.txt (line 36))\n",
            "  Using cached xprof-2.20.1-cp311-none-manylinux2014_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting gviz_api>=1.9.0 (from xprof==2.20.1->tensorboard-plugin-profile->-r ../../maxtext/requirements.txt (line 36))\n",
            "  Using cached gviz_api-1.10.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from xprof==2.20.1->tensorboard-plugin-profile->-r ../../maxtext/requirements.txt (line 36)) (3.1.3)\n",
            "Collecting cheroot>=10.0.1 (from xprof==2.20.1->tensorboard-plugin-profile->-r ../../maxtext/requirements.txt (line 36))\n",
            "  Using cached cheroot-10.0.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting regex>=2022.1.18 (from tiktoken->-r ../../maxtext/requirements.txt (line 37))\n",
            "  Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers->-r ../../maxtext/requirements.txt (line 38))\n",
            "  Using cached tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers->-r ../../maxtext/requirements.txt (line 38))\n",
            "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting coverage (from google-jetstream@ git+https://github.com/AI-Hypercomputer/JetStream.git->-r ../../maxtext/requirements.txt (line 40))\n",
            "  Using cached coverage-7.9.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.9 kB)\n",
            "Collecting portpicker (from google-jetstream@ git+https://github.com/AI-Hypercomputer/JetStream.git->-r ../../maxtext/requirements.txt (line 40))\n",
            "  Using cached portpicker-1.6.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting prometheus-client (from google-jetstream@ git+https://github.com/AI-Hypercomputer/JetStream.git->-r ../../maxtext/requirements.txt (line 40))\n",
            "  Using cached prometheus_client-0.22.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting seqio (from google-jetstream@ git+https://github.com/AI-Hypercomputer/JetStream.git->-r ../../maxtext/requirements.txt (line 40))\n",
            "  Using cached seqio-0.0.19-py3-none-any.whl.metadata (51 kB)\n",
            "Collecting blobfile (from google-jetstream@ git+https://github.com/AI-Hypercomputer/JetStream.git->-r ../../maxtext/requirements.txt (line 40))\n",
            "  Using cached blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting parameterized (from google-jetstream@ git+https://github.com/AI-Hypercomputer/JetStream.git->-r ../../maxtext/requirements.txt (line 40))\n",
            "  Using cached parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Collecting shortuuid (from google-jetstream@ git+https://github.com/AI-Hypercomputer/JetStream.git->-r ../../maxtext/requirements.txt (line 40))\n",
            "  Using cached shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting sympy (from google-jetstream@ git+https://github.com/AI-Hypercomputer/JetStream.git->-r ../../maxtext/requirements.txt (line 40))\n",
            "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting nltk (from google-jetstream@ git+https://github.com/AI-Hypercomputer/JetStream.git->-r ../../maxtext/requirements.txt (line 40))\n",
            "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting evaluate (from google-jetstream@ git+https://github.com/AI-Hypercomputer/JetStream.git->-r ../../maxtext/requirements.txt (line 40))\n",
            "  Using cached evaluate-0.4.4-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->-r ../../maxtext/requirements.txt (line 43))\n",
            "  Using cached antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r ../../maxtext/requirements.txt (line 10)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r ../../maxtext/requirements.txt (line 10)) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r ../../maxtext/requirements.txt (line 10)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r ../../maxtext/requirements.txt (line 10)) (6.6.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r ../../maxtext/requirements.txt (line 10)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r ../../maxtext/requirements.txt (line 10)) (1.20.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow>=2.13.0->-r ../../maxtext/requirements.txt (line 33)) (0.45.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from chex>=0.1.87->optax->-r ../../maxtext/requirements.txt (line 23)) (1.0.0)\n",
            "Requirement already satisfied: einops in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->-r ../../maxtext/requirements.txt (line 34)) (0.8.1)\n",
            "Requirement already satisfied: importlib_resources in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->-r ../../maxtext/requirements.txt (line 34)) (6.5.2)\n",
            "Requirement already satisfied: zipp in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->-r ../../maxtext/requirements.txt (line 34)) (3.23.0)\n",
            "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.61.0->-r ../../maxtext/requirements.txt (line 11))\n",
            "  Using cached grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform==1.61.0->-r ../../maxtext/requirements.txt (line 11))\n",
            "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform==1.61.0->-r ../../maxtext/requirements.txt (line 11))\n",
            "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform==1.61.0->-r ../../maxtext/requirements.txt (line 11))\n",
            "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.61.0->-r ../../maxtext/requirements.txt (line 11)) (2.9.0.post0)\n",
            "Collecting google-cloud-appengine-logging<2.0.0,>=0.1.3 (from google-cloud-logging>=3.5.0->ml-goodput-measurement==0.0.10->-r ../../maxtext/requirements.txt (line 21))\n",
            "  Using cached google_cloud_appengine_logging-1.6.2-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting google-cloud-audit-log<1.0.0,>=0.3.1 (from google-cloud-logging>=3.5.0->ml-goodput-measurement==0.0.10->-r ../../maxtext/requirements.txt (line 21))\n",
            "  Using cached google_cloud_audit_log-0.3.2-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting grpc-google-iam-v1<1.0.0,>=0.12.4 (from google-cloud-logging>=3.5.0->ml-goodput-measurement==0.0.10->-r ../../maxtext/requirements.txt (line 21))\n",
            "  Using cached grpc_google_iam_v1-0.14.2-py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting opentelemetry-api>=1.9.0 (from google-cloud-logging>=3.5.0->ml-goodput-measurement==0.0.10->-r ../../maxtext/requirements.txt (line 21))\n",
            "  Using cached opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.0.0,>=0.19.0->google-api-python-client->-r ../../maxtext/requirements.txt (line 15))\n",
            "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from jinja2>=3.1.2->pytype->-r ../../maxtext/requirements.txt (line 29)) (3.0.2)\n",
            "Requirement already satisfied: namex in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow>=2.13.0->-r ../../maxtext/requirements.txt (line 33)) (0.1.0)\n",
            "Requirement already satisfied: optree in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow>=2.13.0->-r ../../maxtext/requirements.txt (line 33)) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from pandas->datasets->-r ../../maxtext/requirements.txt (line 9)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from pandas->datasets->-r ../../maxtext/requirements.txt (line 9)) (2025.2)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3->google-cloud-aiplatform==1.61.0->-r ../../maxtext/requirements.txt (line 11))\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.2 (from pydantic<3->google-cloud-aiplatform==1.61.0->-r ../../maxtext/requirements.txt (line 11))\n",
            "  Using cached pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic<3->google-cloud-aiplatform==1.61.0->-r ../../maxtext/requirements.txt (line 11))\n",
            "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from requests->ml-goodput-measurement==0.0.10->-r ../../maxtext/requirements.txt (line 21)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from requests->ml-goodput-measurement==0.0.10->-r ../../maxtext/requirements.txt (line 21)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from requests->ml-goodput-measurement==0.0.10->-r ../../maxtext/requirements.txt (line 21)) (2025.6.15)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from rich>=11.1->flax>=0.10.6->-r ../../maxtext/requirements.txt (line 18)) (3.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow>=2.13.0->-r ../../maxtext/requirements.txt (line 33)) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow>=2.13.0->-r ../../maxtext/requirements.txt (line 33)) (0.7.2)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->-r ../../maxtext/requirements.txt (line 28))\n",
            "  Using cached distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting pycryptodomex>=3.8 (from blobfile->google-jetstream@ git+https://github.com/AI-Hypercomputer/JetStream.git->-r ../../maxtext/requirements.txt (line 40))\n",
            "  Using cached pycryptodomex-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting lxml>=4.9 (from blobfile->google-jetstream@ git+https://github.com/AI-Hypercomputer/JetStream.git->-r ../../maxtext/requirements.txt (line 40))\n",
            "  Using cached lxml-6.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi->pathwaysutils==0.1.1->-r ../../maxtext/requirements.txt (line 42))\n",
            "  Using cached starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib->gcsfs->-r ../../maxtext/requirements.txt (line 10))\n",
            "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting werkzeug>=0.11.15 (from xprof==2.20.1->tensorboard-plugin-profile->-r ../../maxtext/requirements.txt (line 36))\n",
            "  Using cached Werkzeug-2.0.3-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting joblib (from nltk->google-jetstream@ git+https://github.com/AI-Hypercomputer/JetStream.git->-r ../../maxtext/requirements.txt (line 40))\n",
            "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting clu (from seqio->google-jetstream@ git+https://github.com/AI-Hypercomputer/JetStream.git->-r ../../maxtext/requirements.txt (line 40))\n",
            "  Using cached clu-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting editdistance (from seqio->google-jetstream@ git+https://github.com/AI-Hypercomputer/JetStream.git->-r ../../maxtext/requirements.txt (line 40))\n",
            "  Using cached editdistance-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Collecting pyglove (from seqio->google-jetstream@ git+https://github.com/AI-Hypercomputer/JetStream.git->-r ../../maxtext/requirements.txt (line 40))\n",
            "  Using cached pyglove-0.4.4-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting tfds-nightly==4.9.2.dev202308090034 (from seqio->google-jetstream@ git+https://github.com/AI-Hypercomputer/JetStream.git->-r ../../maxtext/requirements.txt (line 40))\n",
            "  Using cached tfds_nightly-4.9.2.dev202308090034-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->google-jetstream@ git+https://github.com/AI-Hypercomputer/JetStream.git->-r ../../maxtext/requirements.txt (line 40))\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "INFO: pip is looking at multiple versions of tensorflow-metadata to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow-metadata (from tensorflow-datasets->-r ../../maxtext/requirements.txt (line 34))\n",
            "  Using cached tensorflow_metadata-1.17.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached tensorflow_metadata-1.17.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached tensorflow_metadata-1.16.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Using cached tensorflow_metadata-1.16.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Using cached tensorflow_metadata-1.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Using cached tensorflow_metadata-1.14.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting absl-py (from -r ../../maxtext/requirements.txt (line 4))\n",
            "  Using cached absl_py-1.4.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting h11>=0.8 (from uvicorn->pathwaysutils==0.1.1->-r ../../maxtext/requirements.txt (line 42))\n",
            "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting jaraco.functools (from cheroot>=10.0.1->xprof==2.20.1->tensorboard-plugin-profile->-r ../../maxtext/requirements.txt (line 36))\n",
            "  Using cached jaraco_functools-4.2.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.61.0->-r ../../maxtext/requirements.txt (line 11))\n",
            "  Using cached grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.70.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.69.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached grpcio_status-1.68.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.62.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.62.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.62.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.61.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.60.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.60.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.60.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.59.5-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.59.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.59.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.59.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.58.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.58.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.57.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "  Using cached grpcio_status-1.56.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.56.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.55.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.54.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.54.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.54.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.53.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.53.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.53.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.51.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.51.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.50.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached grpcio_status-1.49.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting grpc-google-iam-v1<1.0.0,>=0.12.4 (from google-cloud-logging>=3.5.0->ml-goodput-measurement==0.0.10->-r ../../maxtext/requirements.txt (line 21))\n",
            "  Using cached grpc_google_iam_v1-0.14.1-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "  Using cached grpc_google_iam_v1-0.14.0-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting google-cloud-audit-log<1.0.0,>=0.3.1 (from google-cloud-logging>=3.5.0->ml-goodput-measurement==0.0.10->-r ../../maxtext/requirements.txt (line 21))\n",
            "  Using cached google_cloud_audit_log-0.3.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting google-cloud-appengine-logging<2.0.0,>=0.1.3 (from google-cloud-logging>=3.5.0->ml-goodput-measurement==0.0.10->-r ../../maxtext/requirements.txt (line 21))\n",
            "  Using cached google_cloud_appengine_logging-1.6.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "  Using cached google_cloud_appengine_logging-1.6.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "  Using cached google_cloud_appengine_logging-1.5.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "  Using cached google_cloud_appengine_logging-1.4.5-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "  Using cached google_cloud_appengine_logging-1.4.4-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "  Using cached google_cloud_appengine_logging-1.4.3-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "  Using cached google_cloud_appengine_logging-1.4.2-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "  Using cached google_cloud_appengine_logging-1.4.1-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "  Using cached google_cloud_appengine_logging-1.4.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "  Using cached google_cloud_appengine_logging-1.3.2-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "  Using cached google_cloud_appengine_logging-1.3.1-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "  Using cached google_cloud_appengine_logging-1.3.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "  Using cached google_cloud_appengine_logging-1.2.0-py2.py3-none-any.whl.metadata (5.1 kB)\n",
            "  Using cached google_cloud_appengine_logging-1.1.6-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached google_cloud_appengine_logging-1.1.5-py2.py3-none-any.whl.metadata (4.8 kB)\n",
            "  Using cached google_cloud_appengine_logging-1.1.4-py2.py3-none-any.whl.metadata (4.8 kB)\n",
            "  Using cached google_cloud_appengine_logging-1.1.3-py2.py3-none-any.whl.metadata (4.8 kB)\n",
            "  Using cached google_cloud_appengine_logging-1.1.2-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "  Using cached google_cloud_appengine_logging-1.1.1-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "  Using cached google_cloud_appengine_logging-1.1.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "INFO: pip is still looking at multiple versions of tensorflow-metadata to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow-metadata (from tensorflow-datasets->-r ../../maxtext/requirements.txt (line 34))\n",
            "  Using cached tensorflow_metadata-1.13.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached tensorflow_metadata-1.13.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "  Using cached tensorflow_metadata-1.12.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-1.11.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-1.10.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-1.9.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-1.8.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-1.7.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-1.6.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-1.5.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-1.4.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-1.2.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-1.0.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-0.30.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-0.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-0.28.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-0.27.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-0.26.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-0.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-0.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-0.23.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-0.22.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-0.22.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-0.22.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-0.21.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-0.21.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-0.21.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-0.15.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-0.15.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-0.15.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-0.14.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-0.13.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Using cached tensorflow_metadata-0.12.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting seqio (from google-jetstream@ git+https://github.com/AI-Hypercomputer/JetStream.git->-r ../../maxtext/requirements.txt (line 40))\n",
            "  Using cached seqio-0.0.18-py3-none-any.whl.metadata (51 kB)\n",
            "Collecting tfds-nightly (from seqio->google-jetstream@ git+https://github.com/AI-Hypercomputer/JetStream.git->-r ../../maxtext/requirements.txt (line 40))\n",
            "  Using cached tfds_nightly-4.9.9.dev202506300045-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached tfds_nightly-4.9.9.dev202506290045-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting seqio (from google-jetstream@ git+https://github.com/AI-Hypercomputer/JetStream.git->-r ../../maxtext/requirements.txt (line 40))\n",
            "  Using cached seqio-0.0.17-py3-none-any.whl.metadata (51 kB)\n",
            "  Using cached seqio-0.0.16-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/mazumdera/venv-py311/bin/pip\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/home/mazumdera/venv-py311/lib/python3.11/site-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mazumdera/venv-py311/lib/python3.11/site-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mazumdera/venv-py311/lib/python3.11/site-packages/pip/_internal/cli/base_command.py\", line 236, in _main\n",
            "    self.handle_pip_version_check(options)\n",
            "  File \"/home/mazumdera/venv-py311/lib/python3.11/site-packages/pip/_internal/cli/req_command.py\", line 188, in handle_pip_version_check\n",
            "    pip_self_version_check(session, options)\n",
            "  File \"/home/mazumdera/venv-py311/lib/python3.11/site-packages/pip/_internal/self_outdated_check.py\", line 231, in pip_self_version_check\n",
            "    installed_dist = get_default_environment().get_distribution(\"pip\")\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mazumdera/venv-py311/lib/python3.11/site-packages/pip/_internal/metadata/importlib/_envs.py\", line 189, in get_distribution\n",
            "    return next(matches, None)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mazumdera/venv-py311/lib/python3.11/site-packages/pip/_internal/metadata/importlib/_envs.py\", line 184, in <genexpr>\n",
            "    matches = (\n",
            "              ^\n",
            "  File \"/home/mazumdera/venv-py311/lib/python3.11/site-packages/pip/_internal/metadata/base.py\", line 626, in iter_all_distributions\n",
            "    for dist in self._iter_distributions():\n",
            "  File \"/home/mazumdera/venv-py311/lib/python3.11/site-packages/pip/_internal/metadata/importlib/_envs.py\", line 176, in _iter_distributions\n",
            "    yield from finder.find(location)\n",
            "  File \"/home/mazumdera/venv-py311/lib/python3.11/site-packages/pip/_internal/metadata/importlib/_envs.py\", line 79, in find\n",
            "    for dist, info_location in self._find_impl(location):\n",
            "  File \"/home/mazumdera/venv-py311/lib/python3.11/site-packages/pip/_internal/metadata/importlib/_envs.py\", line 64, in _find_impl\n",
            "    raw_name = get_dist_name(dist)\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mazumdera/venv-py311/lib/python3.11/site-packages/pip/_internal/metadata/importlib/_compat.py\", line 52, in get_dist_name\n",
            "    name = cast(Any, dist).name\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/metadata/__init__.py\", line 622, in name\n",
            "    return self.metadata['Name']\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/metadata/__init__.py\", line 617, in metadata\n",
            "    return _adapters.Message(email.message_from_string(text))\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/email/__init__.py\", line 37, in message_from_string\n",
            "    return Parser(*args, **kws).parsestr(s)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/email/parser.py\", line 67, in parsestr\n",
            "    return self.parse(StringIO(text), headersonly=headersonly)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/email/parser.py\", line 56, in parse\n",
            "    feedparser.feed(data)\n",
            "  File \"/usr/lib/python3.11/email/feedparser.py\", line 174, in feed\n",
            "    self._call_parse()\n",
            "  File \"/usr/lib/python3.11/email/feedparser.py\", line 178, in _call_parse\n",
            "    self._parse()\n",
            "  File \"/usr/lib/python3.11/email/feedparser.py\", line 238, in _parsegen\n",
            "    self._parse_headers(headers)\n",
            "  File \"/usr/lib/python3.11/email/feedparser.py\", line 486, in _parse_headers\n",
            "    self._cur.set_raw(*self.policy.header_source_parse(lastvalue))\n",
            "  File \"/usr/lib/python3.11/email/message.py\", line 507, in set_raw\n",
            "    def set_raw(self, name, value):\n",
            "\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'aqt'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33m pip install -r ../../maxtext/requirements.txt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# now this will work, assuming MaxText is a proper Python package (i.e. has __init__.py)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMaxText\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/maxtext/MaxText/__init__.py:28\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# maxtext/__init__.py\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMaxText\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m maxtext_utils\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMaxText\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_utils\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMaxText\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pyconfig\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMaxText\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m models\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/maxtext/MaxText/train_utils.py:21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[33;03m\"\"\" Utils that are only interesting for training in MaxText. \"\"\"\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMaxText\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quantizations\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMaxText\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMaxText\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optimizers\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/maxtext/MaxText/layers/quantizations.py:23\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tuple, Sequence\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maqt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config \u001b[38;5;28;01mas\u001b[39;00m aqt_config\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maqt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m aqt_tensor\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maqt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mflax\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m aqt_flax\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'aqt'"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# add the parent directory (one level up) to sys.path\n",
        "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../../maxtext')))\n",
        "\n",
        "# ! pip install -r ../../maxtext/requirements.txt\n",
        "\n",
        "import MaxText as mt\n",
        "from MaxText import pyconfig\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58b42bcb",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# 4) Load your pretrained model and extract its mesh\n",
        "\n",
        "model = mt.from_pretrained(config)\n",
        "mesh  = model.mesh\n",
        "\n",
        "# 5) Split model into policy and reference copies\n",
        "policy_model    = model\n",
        "policy_params   = model.params\n",
        "# Fresh copy for reference (so GRPO’s KL target stays fixed)\n",
        "reference_model  = mt.from_pretrained(config)\n",
        "reference_params = reference_model.params"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eee062b3",
      "metadata": {},
      "source": [
        "#### Convert MaxText model to nnx (use a commit from MaxText repo prior to )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05ad3dfe",
      "metadata": {},
      "outputs": [],
      "source": [
        "from flax import nnx\n",
        "from flax import linen as nn\n",
        "from flax.nnx import bridge\n",
        "\n",
        "model_maxtext_nnx = nnx.bridge.ToNNX(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m2KD-nmbTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_ref_maxtext_model():\n",
        "\n",
        "  #python3 -m MaxText.train MaxText/configs/base.yml base_output_directory=${BASE_OUTPUT_DIRECTORY} dataset_path=${DATASET_PATH} tokenizer_path=assets/tokenizer.gemma load_parameters_path=${CONVERTED_CHECKPOINT} per_device_batch_size=1 run_name=${FINETUNE_RUN_NAME} max_target_length=8192 steps=10 async_checkpointing=false model_name=gemma-2b checkpoint_period=5\n",
        "\n",
        "  #TODO: @mazumdera: change this to use Gemma2-2b-it\n",
        "  config = pyconfig.initialize(\n",
        "      [\"\", \"../configs/base.yml\"], #TODO: @mazumdera: why decode.py?\n",
        "      base_output_directory=\"gs://dummy_output_dir\",  # This is not used in Tunix.\n",
        "      run_name=\"test-tunix-maxtext-gemma-2b\",\n",
        "      # dataset_path=we use Tunix's dataset\n",
        "      load_parameters_path=\"gs://maxtext-gemma/2b/\",\n",
        "      tokenizer_path=\"../../maxtext/assets/tokenizer.gemma\",\n",
        "      per_device_batch_size=1,\n",
        "      max_target_length=8192,\n",
        "      steps=10,\n",
        "      async_checkpointing=\"false\",\n",
        "      model_name=\"gemma-2b\",\n",
        "      checkpoint_period=5,\n",
        "\n",
        "  )\n",
        "  model = mt.from_pretrained(config)\n",
        "  mesh  = model.mesh\n",
        "  \n",
        "  # We can continue to use Tunix's model_config\n",
        "  model_config = gemma_lib.TransformerConfig.gemma2_2b()\n",
        "  \n",
        "  return model, mesh, model_config\n",
        "\n",
        "def get_ref_model(ckpt_path):\n",
        "  mesh = jax.make_mesh(*MESH)\n",
        "  model_config = gemma_lib.TransformerConfig.gemma2_2b()\n",
        "  abs_gemma: nnx.Module = nnx.eval_shape(\n",
        "      lambda: gemma_lib.Transformer(model_config, rngs=nnx.Rngs(params=0))\n",
        "  )\n",
        "  abs_state = nnx.state(abs_gemma)\n",
        "  abs_state = jax.tree.map(\n",
        "      lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.float32, sharding=s),\n",
        "      abs_state,\n",
        "      nnx.get_named_sharding(abs_state, mesh),\n",
        "  )\n",
        "  checkpointer = ocp.StandardCheckpointer()\n",
        "  restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n",
        "\n",
        "  graph_def, _ = nnx.split(abs_gemma)\n",
        "  gemma = nnx.merge(graph_def, restored_params)\n",
        "  return gemma, mesh, model_config\n",
        "\n",
        "\n",
        "def get_lora_model(base_model, mesh):\n",
        "  lora_provider = lora.LoraProvider(\n",
        "      module_path=(\n",
        "          \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
        "          \".*attn_vec_einsum\"\n",
        "      ),\n",
        "      rank=RANK,\n",
        "      alpha=ALPHA,\n",
        "  )\n",
        "\n",
        "  model_input = base_model.get_model_input()\n",
        "  lora_model = lora.apply_lora_to_model(\n",
        "      base_model, lora_provider, **model_input\n",
        "  )\n",
        "\n",
        "  with mesh:\n",
        "    state = nnx.state(lora_model)\n",
        "    pspecs = nnx.get_partition_spec(state)\n",
        "    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
        "    nnx.update(lora_model, sharded_state)\n",
        "\n",
        "  return lora_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kSdZ7aGhTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reference model\n",
        "# gemma, mesh, model_config = get_ref_model(\n",
        "#     ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\n",
        "# )\n",
        "gemma, mesh, model_config = get_ref_maxtext_model()\n",
        "nnx.display(gemma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4i3CfJ1gTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Policy model\n",
        "# This can remain unchanged from default Tunix's colab\n",
        "lora_gemma = get_lora_model(gemma, mesh=mesh)\n",
        "nnx.display(lora_gemma)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zLzR1tJfTn1k",
      "metadata": {},
      "source": [
        "## Define reward functions\n",
        "\n",
        "We define four reward functions:\n",
        "\n",
        "- reward if the format of the output exactly matches the instruction given in\n",
        "`TEMPLATE`;\n",
        "- reward if the format of the output approximately matches the instruction given\n",
        "in `TEMPLATE`;\n",
        "- reward if the answer is correct/partially correct;\n",
        "- Sometimes, the text between `<answer>`, `</answer>` might not be one\n",
        "  number. So, extract the number, and reward the model if the answer is correct.\n",
        "\n",
        "The reward functions are inspired from\n",
        "[here](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb).\n",
        "\n",
        "First off, let's define a RegEx for checking whether the format matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C7Beft8wTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "match_format = re.compile(\n",
        "    rf\"^[\\s]{{0,}}\"\n",
        "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\n",
        "    rf\"{solution_start}(.+?){solution_end}\"\n",
        "    rf\"[\\s]{{0,}}$\",\n",
        "    flags=re.MULTILINE | re.DOTALL,\n",
        ")\n",
        "\n",
        "match_format.search(\n",
        "    f\"{reasoning_start}Let me\"\n",
        "    f\" think!{reasoning_end}{solution_start}2{solution_end}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fe1rF15zTn1k",
      "metadata": {},
      "source": [
        "Give the model a reward of 3 points if the format matches exactly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_fhQ6pY2Tn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def match_format_exactly(prompts, completions, **kargs):\n",
        "  scores = []\n",
        "  for completion in completions:\n",
        "    score = 0\n",
        "    response = completion\n",
        "    # Match if format is seen exactly!\n",
        "    if match_format.search(response) is not None:\n",
        "      score += 3.0\n",
        "    scores.append(score)\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sWdAdUHuTn1k",
      "metadata": {},
      "source": [
        "We also reward the model if the format of the output matches partially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uOhO4f3-Tn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def match_format_approximately(prompts, completions, **kargs):\n",
        "  scores = []\n",
        "\n",
        "  for completion in completions:\n",
        "    score = 0\n",
        "    response = completion\n",
        "    # Count how many keywords are seen - we penalize if too many!\n",
        "    # If we see 1, then plus some points!\n",
        "    score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n",
        "    score += 0.5 if response.count(reasoning_end) == 1 else -0.5\n",
        "    score += 0.5 if response.count(solution_start) == 1 else -0.5\n",
        "    score += 0.5 if response.count(solution_end) == 1 else -0.5\n",
        "    scores.append(score)\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A2fNZDgTTn1k",
      "metadata": {},
      "source": [
        "Reward the model if the answer is correct. A reward is also given if the answer\n",
        "does not match exactly, i.e., based on how close the answer is to the correct\n",
        "value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S8zcWsmhTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_answer(prompts, completions, answer, **kargs):\n",
        "  responses = completions\n",
        "\n",
        "  extracted_responses = [\n",
        "      guess.group(1) if (guess := match_format.search(r)) is not None else None\n",
        "      for r in responses\n",
        "  ]\n",
        "\n",
        "  scores = []\n",
        "  for guess, true_answer in zip(extracted_responses, answer):\n",
        "    score = 0\n",
        "    if guess is None:\n",
        "      scores.append(0)\n",
        "      continue\n",
        "    # Correct answer gets 3 points!\n",
        "    if guess == true_answer:\n",
        "      score += 3.0\n",
        "    # Match if spaces are seen\n",
        "    elif guess.strip() == true_answer.strip():\n",
        "      score += 1.5\n",
        "    else:\n",
        "      # We also reward it if the answer is close via ratios!\n",
        "      # Ie if the answer is within some range, reward it!\n",
        "      try:\n",
        "        ratio = float(guess) / float(true_answer)\n",
        "        if ratio >= 0.9 and ratio <= 1.1:\n",
        "          score += 0.5\n",
        "        elif ratio >= 0.8 and ratio <= 1.2:\n",
        "          score += 0.25\n",
        "        else:\n",
        "          score -= 1.0  # Penalize wrong answers\n",
        "      except:\n",
        "        score -= 0.5  # Penalize\n",
        "    scores.append(score)\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nIpOVv78Tn1k",
      "metadata": {},
      "source": [
        "Sometimes, the text between `<answer>` and `</answer>` might not be one\n",
        "number; it can be a sentence. So, we extract the number and compare the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NXvRtbk8Tn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "match_numbers = re.compile(\n",
        "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\", flags=re.MULTILINE | re.DOTALL\n",
        ")\n",
        "match_numbers.findall(f\"{solution_start}  0.34  {solution_end}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oxZQAFKOTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_numbers(prompts, completions, answer, **kargs):\n",
        "  question = kargs[\"question\"]\n",
        "  responses = completions\n",
        "\n",
        "  extracted_responses = [\n",
        "      guess.group(1) if (guess := match_numbers.search(r)) is not None else None\n",
        "      for r in responses\n",
        "  ]\n",
        "\n",
        "  scores = []\n",
        "  print(\"START ============================\")\n",
        "  print(f\"Question: {question[0]}\")\n",
        "  print(f\"Answer: {answer[0]}\")\n",
        "  print(f\"Response: {responses[0]}\")\n",
        "  print(f\"Extracted: {extracted_responses[0]}\")\n",
        "  print(\"END ==============================\")\n",
        "  for guess, true_answer in zip(extracted_responses, answer):\n",
        "    if guess is None:\n",
        "      scores.append(0)\n",
        "      continue\n",
        "    # Convert to numbers\n",
        "    try:\n",
        "      true_answer = float(true_answer.strip())\n",
        "      guess = float(guess.strip())\n",
        "      scores.append(1.5 if guess == true_answer else 0.0)\n",
        "    except:\n",
        "      scores.append(0)\n",
        "      continue\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AaiYMJxFTn1k",
      "metadata": {},
      "source": [
        "## Evaluate\n",
        "\n",
        "\n",
        "Before we train the model, let's evaluate the model on the test set so we can\n",
        "see the improvement post training.\n",
        "\n",
        "We evaluate it in two ways:\n",
        "\n",
        "**Quantitative**\n",
        "\n",
        "* **Answer Accuracy**: percentage of samples for which the model predicts the\n",
        "correct final numerical answer  \n",
        "* **Answer (Partial) Accuracy**: percentage of samples for which the model\n",
        "predicts a final numerical answer such that the \\`model answer / answer\\`\n",
        "ratio lies between 0.9 and 1.1.  \n",
        "* **Format Accuracy**: percentage of samples for which the model outputs the\n",
        "correct format, i.e., reasoning between the reasoning special tokens, and the\n",
        "final answer between the \\`\\<start\\_answer\\>\\`, \\`\\<end\\_answer\\>\\` tokens.\n",
        "\n",
        "**Qualitative**\n",
        "\n",
        "We'll also print outputs for a few given questions so that we can compare the generated output later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_k58bOicUHJy",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate(\n",
        "    question, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None\n",
        "):\n",
        "  \"\"\"Given prompt, generates text.\"\"\"\n",
        "\n",
        "  if isinstance(question, str):\n",
        "    input_batch = [\n",
        "        TEMPLATE.format(\n",
        "            system_prompt=SYSTEM_PROMPT,\n",
        "            question=question,\n",
        "        ),\n",
        "    ]\n",
        "  else:\n",
        "    input_batch = [\n",
        "        TEMPLATE.format(\n",
        "            system_prompt=SYSTEM_PROMPT,\n",
        "            question=q,\n",
        "        )\n",
        "        for q in question\n",
        "    ]\n",
        "\n",
        "  out_data = sampler(\n",
        "      input_strings=input_batch,\n",
        "      total_generation_steps=768,\n",
        "      temperature=temperature,\n",
        "      top_k=top_k,\n",
        "      top_p=top_p,\n",
        "      echo=False,\n",
        "      seed=seed if seed is not None else None,\n",
        "  )\n",
        "\n",
        "  output = out_data.text\n",
        "  if isinstance(question, str):\n",
        "    return output[0]\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yJo2nuKB-wlw",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    dataset,\n",
        "    sampler,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    num_passes=1,\n",
        "    corr_lst=False,\n",
        "    make_lst=False,\n",
        "):\n",
        "  \"\"\"Computes accuracy and percentage of outputs matching the format.\"\"\"\n",
        "\n",
        "  response_lst = []\n",
        "  corr = 0\n",
        "  partially_corr = 0\n",
        "  corr_format = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch in tqdm(dataset):\n",
        "    answers = batch[\"answer\"]\n",
        "    questions = batch[\"question\"]\n",
        "\n",
        "    multiple_call_responses = [[] for _ in range(len(questions))]\n",
        "    for p in range(num_passes):\n",
        "      responses = generate(\n",
        "          questions, sampler, temperature, top_k, top_p, seed=p\n",
        "      )\n",
        "      for idx, response in enumerate(responses):\n",
        "        multiple_call_responses[idx].append(response)\n",
        "\n",
        "    for question, multiple_call_response, answer in zip(\n",
        "        questions, multiple_call_responses, answers\n",
        "    ):\n",
        "      # check answer\n",
        "      corr_ctr_per_question = 0\n",
        "      partially_corr_per_question = 0\n",
        "      corr_format_per_question = 0\n",
        "      for response in multiple_call_response:\n",
        "        extracted_response = (\n",
        "            guess.group(1)\n",
        "            if (guess := match_numbers.search(response)) is not None\n",
        "            else \"-1000000\"\n",
        "        )\n",
        "        try:\n",
        "          if float(extracted_response.strip()) == float(answer.strip()):\n",
        "            corr_ctr_per_question += 1\n",
        "\n",
        "          ratio = float(extracted_response.strip()) / float(answer.strip())\n",
        "          if ratio >= 0.9 and ratio <= 1.1:\n",
        "            partially_corr_per_question += 1\n",
        "        except:\n",
        "          print(\"SKIPPED\")\n",
        "\n",
        "        # check format\n",
        "        if match_format.search(response) is not None:\n",
        "          corr_format_per_question += 1\n",
        "\n",
        "        if (\n",
        "            corr_ctr_per_question > 0\n",
        "            and partially_corr_per_question > 0\n",
        "            and corr_format_per_question > 0\n",
        "        ):\n",
        "          break\n",
        "\n",
        "      if corr_ctr_per_question > 0:\n",
        "        corr += 1\n",
        "        if corr_lst and make_lst:\n",
        "          response_lst.append((question, answer, multiple_call_response))\n",
        "      else:\n",
        "        if not corr_lst and make_lst:\n",
        "          response_lst.append((question, answer, multiple_call_response))\n",
        "      if partially_corr_per_question > 0:\n",
        "        partially_corr += 1\n",
        "      if corr_format_per_question > 0:\n",
        "        corr_format += 1\n",
        "\n",
        "      total += 1\n",
        "      if total % 10 == 0:\n",
        "        print(\n",
        "            f\"===> {corr=}, {total=}, {corr / total * 100=}, \"\n",
        "            f\"{partially_corr / total * 100=}, {corr_format / total * 100=}\"\n",
        "        )\n",
        "\n",
        "  to_return = (\n",
        "      corr,\n",
        "      total,\n",
        "      corr / total * 100,\n",
        "      partially_corr / total * 100,\n",
        "      corr_format / total * 100,\n",
        "  )\n",
        "  if make_lst:\n",
        "    return to_return, response_lst\n",
        "  return to_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HZMO-KflTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "gemma_tokenizer = data_lib.GemmaTokenizer()\n",
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_gemma,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YQM-tzXWUmoE",
      "metadata": {},
      "outputs": [],
      "source": [
        "(corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n",
        "    test_dataset,\n",
        "    sampler,\n",
        "    **GENERATION_CONFIGS[\"greedy\"],\n",
        ")\n",
        "print(\n",
        "    f\"{corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n",
        "    f\" {format_accuracy=}%\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PJV6wNGY-3PG",
      "metadata": {},
      "outputs": [],
      "source": [
        "# for eval_example in QUALITATIVE_EVAL_EXAMPLES:\n",
        "#   question = eval_example[\"question\"]\n",
        "#   answer = eval_example[\"answer\"]\n",
        "#   response = generate(\n",
        "#       question,\n",
        "#       sampler,\n",
        "#       temperature=INFERENCE_TEMPERATURE,\n",
        "#       top_k=INFERENCE_TOP_K,\n",
        "#       top_p=INFERENCE_TOP_P,\n",
        "#   )\n",
        "\n",
        "#   print(f\"Question:\\n{question}\")\n",
        "#   print(f\"Answer:\\n{answer}\")\n",
        "#   print(f\"Response:\\n{response}\")\n",
        "#   print(\"===============\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-CmB2ZT9Tn1l",
      "metadata": {},
      "source": [
        "## Train\n",
        "\n",
        "Let's set up all the configs first - checkpointing, metric logging and training.\n",
        "We then train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mHzdsYsGTn1l",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ckpt saving\n",
        "checkpointing_options = ocp.CheckpointManagerOptions(\n",
        "    save_interval_steps=SAVE_INTERVAL_STEPS, max_to_keep=MAX_TO_KEEP\n",
        ")\n",
        "\n",
        "# Metrics logger\n",
        "metrics_logging_options = metrics_logger.MetricsLoggerOptions(\n",
        "    log_dir=\"/content/tmp/tensorboard/grpo\", flush_every_n_steps=20\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u1Sc1fNC_CJ7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logs\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/tmp/tensorboard/grpo --port=0"
      ]
    },
    {
      "metadata": {},
      "id": "_6VxFW1ZTn1l",
      "cell_type": "code",
      "execution_count": null,
      "id": "_6VxFW1ZTn1l",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training config\n",
        "training_config = GrpoTrainingConfig(\n",
        "    max_prompt_length=MAX_PROMPT_LENGTH,\n",
        "    total_generation_steps=TOTAL_GENERATION_STEPS,\n",
        "    num_generations=NUM_GENERATIONS,\n",
        "    num_iterations=NUM_ITERATIONS,\n",
        "    beta=BETA,\n",
        "    epsilon=EPSILON,\n",
        "    temperature=TEMPERATURE,\n",
        "    top_p=TOP_P,\n",
        "    top_k=TOP_K,\n",
        "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "    max_steps=MAX_STEPS,\n",
        "    # metrics logging\n",
        "    metrics_logging_options=metrics_logging_options,\n",
        "    # checkpoint saving\n",
        "    checkpoint_root_directory=CKPT_DIR,\n",
        "    checkpointing_options=checkpointing_options,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "id": "OIe1lO08Tn1l",
      "cell_type": "code",
      "source": [
        "# Sampler\n",
        "gemma_tokenizer = data_lib.GemmaTokenizer()\n",
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_gemma,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Optimizer, learning rate scheduler, gradient clipping\n",
        "optimizer = optax.adamw(\n",
        "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
        "        init_value=0.0,\n",
        "        peak_value=LEARNING_RATE,\n",
        "        warmup_steps=WARMUP_STEPS,\n",
        "        decay_steps=MAX_STEPS,\n",
        "        end_value=0.0,\n",
        "    ),\n",
        "    b1=B1,\n",
        "    b2=B2,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        ")\n",
        "if MAX_GRAD_NORM is not None:\n",
        "  optimizer = optax.chain(\n",
        "      optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n",
        "      optimizer,\n",
        "  )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "id": "_6VxFW1ZTn1l",
      "cell_type": "code",
      "source": [
        "# Training config\n",
        "cluster_config = rl_cluster_lib.ClusterConfig(\n",
        "    role_to_mesh={\n",
        "        rl_cluster_lib.Role.ACTOR: mesh,\n",
        "        rl_cluster_lib.Role.REFERENCE: mesh,\n",
        "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
        "    },\n",
        "    rollout_engine='vanilla',\n",
        "    offload_to_cpu=False,\n",
        "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
        "        actor_optimizer=optimizer,\n",
        "        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "        max_steps=MAX_STEPS,\n",
        "        gradient_accumulation_steps=1,\n",
        "        # metrics logging\n",
        "        metrics_logging_options=metrics_logging_options,\n",
        "        # checkpoint saving\n",
        "        checkpoint_root_directory=CKPT_DIR,\n",
        "        checkpointing_options=checkpointing_options,\n",
        "    ),\n",
        "    rollout_config=base_rollout.RolloutConfig(\n",
        "        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n",
        "        max_prompt_length=MAX_PROMPT_LENGTH,\n",
        "        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_p=TOP_P,\n",
        "        top_k=TOP_K,\n",
        "    ),\n",
        ")\n",
        "\n",
        "grpo_config = GrpoConfig(\n",
        "    num_generations=NUM_GENERATIONS,\n",
        "    num_iterations=NUM_ITERATIONS,\n",
        "    beta=BETA,\n",
        "    epsilon=EPSILON,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "id": "OIe1lO08Tn1l",
      "cell_type": "code",
      "source": [
        "# RL cluster\n",
        "rl_cluster = rl_cluster_lib.RLCluster(\n",
        "    actor=lora_gemma,\n",
        "    reference=gemma,\n",
        "    tokenizer=data_lib.GemmaTokenizer(),\n",
        "    cluster_config=cluster_config,\n",
        ")\n",
        "\n",
        "# GRPO Trainer\n",
        "grpo_trainer = GrpoLearner(\n",
        "    rl_cluster=rl_cluster,\n",
        "    reward_fns=[\n",
        "        match_format_exactly,\n",
        "        match_format_approximately,\n",
        "        check_answer,\n",
        "        check_numbers,\n",
        "    ],\n",
        "    grpo_config=grpo_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S27XDebYTn1l",
      "metadata": {},
      "outputs": [],
      "source": [
        "with mesh:\n",
        "  grpo_trainer.train(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FzIP8glkTn1l",
      "metadata": {},
      "source": [
        "## Evaluate\n",
        "\n",
        "Let's evaluate our model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V-73HfP1Tn1l",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load checkpoint first.\n",
        "\n",
        "trained_ckpt_path = os.path.join(CKPT_DIR, str(MAX_STEPS), \"model_params\")\n",
        "\n",
        "abs_params = jax.tree.map(\n",
        "    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
        "    nnx.state(lora_gemma, nnx.LoRAParam),\n",
        ")\n",
        "checkpointer = ocp.StandardCheckpointer()\n",
        "trained_lora_params = checkpointer.restore(trained_ckpt_path, target=abs_params)\n",
        "\n",
        "nnx.update(\n",
        "    lora_gemma,\n",
        "    jax.tree.map(\n",
        "        lambda a, b: b,\n",
        "        nnx.state(lora_gemma, nnx.LoRAParam),\n",
        "        trained_lora_params,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1vY9kl-ITn1l",
      "metadata": {},
      "outputs": [],
      "source": [
        "gemma_tokenizer = data_lib.GemmaTokenizer()\n",
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_gemma,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nz0q_gGHqYz6",
      "metadata": {},
      "outputs": [],
      "source": [
        "(corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n",
        "    test_dataset,\n",
        "    sampler,\n",
        "    **GENERATION_CONFIGS[\"greedy\"],\n",
        ")\n",
        "print(\n",
        "    f\"{corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n",
        "    f\" {format_accuracy=}%\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jd9gpYVpUd3_",
      "metadata": {},
      "outputs": [],
      "source": [
        "# for eval_example in QUALITATIVE_EVAL_EXAMPLES:\n",
        "#   question = eval_example[\"question\"]\n",
        "#   answer = eval_example[\"answer\"]\n",
        "#   response = generate(\n",
        "#       question,\n",
        "#       sampler,\n",
        "#       temperature=INFERENCE_TEMPERATURE,\n",
        "#       top_k=INFERENCE_TOP_K,\n",
        "#       top_p=INFERENCE_TOP_P,\n",
        "#   )\n",
        "\n",
        "#   print(f\"Question:\\n{question}\")\n",
        "#   print(f\"Answer:\\n{answer}\")\n",
        "#   print(f\"Response:\\n{response}\")\n",
        "#   print(\"===============\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RrE_Rvaz_8CV",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
