{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "abdhOBYHqYz6",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/tunix/blob/main/examples/grpo_demo.ipynb\" ><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This tutorial demonstrates training the Gemma 2 2B-IT model on the GSM8K math\n",
        "reasoning benchmark using Group Relative Policy Optimization (GRPO). GRPO can\n",
        "enhance your model's problem-solving skills on mathematical word problems,\n",
        "coding problems, etc.\n",
        "\n",
        "GRPO is an RL algorithm designed to enhance the reasoning abilities of LLMs. It\n",
        "is a variant of Proximal Policy Optimization (PPO) that reduces memory usage by\n",
        "eliminating the need for a separate value function model. GRPO works by\n",
        "generating multiple responses for a given prompt, evaluating these responses\n",
        "using a reward model, and then calculating a relative advantage based on the\n",
        "group's performance to update the policy.\n",
        "\n",
        "In this tutorial we use Colab's `v2-8` TPU. Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afofSj37qYz6",
      "metadata": {},
      "source": [
        "## Install necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z03GnyApTn1j",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kagglehub\n",
        "\n",
        "!pip install -q tensorflow\n",
        "!pip install -q tensorboardX\n",
        "!pip install -q grain\n",
        "!pip install -q git+https://github.com/google/tunix\n",
        "!pip install -q git+https://github.com/google/qwix\n",
        "\n",
        "!pip uninstall -q -y flax\n",
        "!pip install -q git+https://github.com/google/flax.git\n",
        "\n",
        "!pip install -q tensorflow-datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f95eb96c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ipywidgets\n",
            "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: comm>=0.1.3 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipywidgets) (9.3.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
            "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
            "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
            "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: decorator in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
            "Requirement already satisfied: ipython-pygments-lexers in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: stack_data in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
            "Requirement already satisfied: typing_extensions>=4.6 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.14.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: executing>=1.2.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in /home/mazumdera/venv-py311/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
            "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m216.6/216.6 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
            "Successfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LnF9ZACiTn1k",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "McTNo_r8Tn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "import functools\n",
        "import gc\n",
        "import os\n",
        "from pprint import pprint\n",
        "import re\n",
        "import time\n",
        "\n",
        "from flax import nnx\n",
        "import grain\n",
        "import humanize\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "# import kagglehub\n",
        "import optax\n",
        "from orbax import checkpoint as ocp\n",
        "from qwix import lora\n",
        "import tensorflow_datasets as tfds\n",
        "from tqdm.auto import tqdm\n",
        "from tunix.generate import sampler as sampler_lib\n",
        "from tunix.models.gemma import data as data_lib\n",
        "from tunix.models.gemma import gemma as gemma_lib\n",
        "from tunix.models.gemma import params as params_lib\n",
        "from tunix.rl import rl_cluster as rl_cluster_lib\n",
        "from tunix.rl.rollout import base_rollout\n",
        "from tunix.rl.grpo.grpo_learner import GrpoConfig, GrpoLearner\n",
        "from tunix.sft import metrics_logger"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Eu_NI9nHTn1k",
      "metadata": {},
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Let's define the configuration we are going to use. Note that this is by no\n",
        "means a \"perfect\" set of hyperparameters. To get good results, you might have\n",
        "to train the model for longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ZPPKme47Tn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====== Data ======\n",
        "TRAIN_DATA_DIR = \"./data/train\"\n",
        "TEST_DATA_DIR = \"./data/test\"\n",
        "TRAIN_FRACTION = 1.0\n",
        "\n",
        "# ====== LoRA ======\n",
        "RANK = 64\n",
        "ALPHA = 64.0\n",
        "\n",
        "# ====== Sharding ======\n",
        "MESH = [(1, 4), (\"fsdp\", \"tp\")]\n",
        "\n",
        "# ====== GRPO ======\n",
        "# === Generation during GRPO training ===\n",
        "MAX_PROMPT_LENGTH = 256\n",
        "TOTAL_GENERATION_STEPS = 768\n",
        "# Important to keep a high-ish temperature for varied, diverse responses during\n",
        "# training.\n",
        "TEMPERATURE = 0.9\n",
        "TOP_P = 1.0\n",
        "TOP_K = 50\n",
        "# The number of times the policy generates multiple responses for a given prompt\n",
        "# within a single training step. This corresponds to `G` in Algorithm 1 in the\n",
        "# paper. The \"group\" in GRPO comes from here.\n",
        "NUM_GENERATIONS = 2\n",
        "\n",
        "# === other GRPO configs ===\n",
        "# The number of iterations per batch (ùúá in GRPO algo 1).\n",
        "NUM_ITERATIONS = 1\n",
        "# The coefficient for the KL divergence penalty (ùõΩ) in the GRPO loss function.\n",
        "# Important to keep a high enough value for this, otherwise, the KL divergence\n",
        "# can increase unchecked.\n",
        "BETA = 0.08\n",
        "# Epsilon value for clipping (ùúÄ in GRPO loss in paper). Similar to PPO, for\n",
        "# stable updates.\n",
        "EPSILON = 0.2\n",
        "\n",
        "# ====== Training ======\n",
        "BATCH_SIZE = 1\n",
        "# Increase `NUM_BATCHES` and `MAX_STEPS` for better results.\n",
        "NUM_BATCHES = 3738\n",
        "# Keep `NUM_TEST_BATCHES` low so that evaluation runs quickly. It can be\n",
        "# increased to a max. of 330 (if batch size is 4).\n",
        "NUM_TEST_BATCHES = 100\n",
        "\n",
        "EVAL_EVERY_N_STEPS = 10  # this doesn't matter if `TRAIN_FRACTION = 1.0`.\n",
        "NUM_EPOCHS = 1  # can potentially train for more epochs\n",
        "\n",
        "# Number of training steps.\n",
        "MAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\n",
        "\n",
        "# === AdamW, warmup, cosine scheduler ===\n",
        "LEARNING_RATE = 3e-6\n",
        "B1 = 0.9\n",
        "B2 = 0.99\n",
        "WEIGHT_DECAY = 0.1\n",
        "# == Cosine decay with warmup scheduler ==\n",
        "# Linearly increase learning rate from 0. to 5e-6 in the first 10% training\n",
        "# steps, and then gradually decrease the learning rate to 0 using cosine\n",
        "# scheduler.\n",
        "WARMUP_STEPS = 0.1 * MAX_STEPS\n",
        "# == Grad clipping ==\n",
        "# Grad clipping to prevent large gradients. Found this\n",
        "# important to keep KL divergence in check.\n",
        "MAX_GRAD_NORM = 0.1\n",
        "\n",
        "# Checkpoint saving\n",
        "INTERMEDIATE_CKPT_DIR = \"/content/intermediate_ckpt/\"\n",
        "CKPT_DIR = \"/content/ckpts/\"\n",
        "SAVE_INTERVAL_STEPS = 500\n",
        "MAX_TO_KEEP = 4\n",
        "\n",
        "# ====== Inference ======\n",
        "GENERATION_CONFIGS = {\n",
        "    # greedy search\n",
        "    \"greedy\": {\"temperature\": 1e-4, \"top_k\": 1, \"top_p\": 1.0},\n",
        "    # some randomness\n",
        "    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
        "    # liberal\n",
        "    \"liberal\": {\"temperature\": 0.85, \"top_k\": 2000, \"top_p\": 1.0},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ngjtE-63Tn1k",
      "metadata": {},
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "wjMFOr7aTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_hbm_usage():\n",
        "  \"\"\"Displays memory usage per device.\"\"\"\n",
        "  fmt_size = functools.partial(humanize.naturalsize, binary=True)\n",
        "\n",
        "  for d in jax.local_devices():\n",
        "    stats = d.memory_stats()\n",
        "    used = stats[\"bytes_in_use\"]\n",
        "    limit = stats[\"bytes_limit\"]\n",
        "    print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:%}) on {d}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6BtpYMlaTn1k",
      "metadata": {},
      "source": [
        "## Data preprocessing\n",
        "\n",
        "First, let's define some special tokens. We instruct the model to first reason\n",
        "between the `<reasoning>` and `</reasoning>` tokens. After\n",
        "reasoning, we expect it to provide the answer between the `<answer>` and\n",
        "`</answer>` tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "h6RGv1kSTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "reasoning_start = \"<reasoning>\"\n",
        "reasoning_end = \"</reasoning>\"\n",
        "solution_start = \"<answer>\"\n",
        "solution_end = \"</answer>\"\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT = f\"\"\"You are given a problem. Think about the problem and \\\n",
        "provide your reasoning. Place it between {reasoning_start} and \\\n",
        "{reasoning_end}. Then, provide the final answer (i.e., just one numerical \\\n",
        "value) between {solution_start} and {solution_end}.\"\"\"\n",
        "\n",
        "TEMPLATE = \"\"\"<start_of_turn>user\n",
        "{system_prompt}\n",
        "\n",
        "{question}<end_of_turn>\n",
        "<start_of_turn>model\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WASP9N5JTn1k",
      "metadata": {},
      "source": [
        "We use OpenAI's GSM8K dataset. GSM8K comprises grade school math word problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "gTGjcSMNTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_hash_answer(text: str) -> str | None:\n",
        "  if \"####\" not in text:\n",
        "    return None\n",
        "  return text.split(\"####\")[1].strip()\n",
        "\n",
        "\n",
        "def get_dataset(data_dir, split=\"train\") -> grain.MapDataset:\n",
        "  # Download data\n",
        "  if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "  data = tfds.data_source(\n",
        "      \"gsm8k\",\n",
        "      split=split,\n",
        "      data_dir=data_dir,\n",
        "      builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n",
        "      download=True,\n",
        "  )\n",
        "\n",
        "  dataset = (\n",
        "      grain.MapDataset.source(data)\n",
        "      .shuffle(seed=42)\n",
        "      .map(\n",
        "          lambda x: {\n",
        "              # passed to model forward pass\n",
        "              \"prompts\": TEMPLATE.format(\n",
        "                  system_prompt=SYSTEM_PROMPT,\n",
        "                  question=x[\"question\"].decode(\"utf-8\"),\n",
        "              ),\n",
        "              # passed to reward functions\n",
        "              \"question\": x[\"question\"].decode(\"utf-8\"),\n",
        "              # passed to reward functions\n",
        "              \"answer\": extract_hash_answer(x[\"answer\"].decode(\"utf-8\")),\n",
        "          }\n",
        "      )\n",
        "  )\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "KXhOL6GyTn1k",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3738, 0, 100)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = get_dataset(TRAIN_DATA_DIR, \"train\").batch(BATCH_SIZE)[:NUM_BATCHES]\n",
        "\n",
        "if TRAIN_FRACTION == 1.0:\n",
        "  train_dataset = dataset.repeat(NUM_EPOCHS)\n",
        "  val_dataset = None\n",
        "else:\n",
        "  train_dataset = dataset[: int(len(dataset) * TRAIN_FRACTION)]\n",
        "  train_dataset = train_dataset.repeat(NUM_EPOCHS)\n",
        "\n",
        "  val_dataset = dataset[int(len(dataset) * TRAIN_FRACTION) :].repeat(NUM_EPOCHS)\n",
        "\n",
        "test_dataset = get_dataset(TEST_DATA_DIR, \"test\").batch(BATCH_SIZE)[\n",
        "    :NUM_TEST_BATCHES\n",
        "]\n",
        "\n",
        "len(train_dataset), len(val_dataset) if val_dataset is not None else 0, len(\n",
        "    test_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k7n8L0VzTn1k",
      "metadata": {},
      "source": [
        "Let's see how one batch of the dataset looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5TF-wNQ2Tn1k",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'answer': array(['13'], dtype='<U2'),\n",
            " 'prompts': array(['<start_of_turn>user\\nYou are given a problem. Think about the problem and provide your reasoning. Place it between <reasoning> and </reasoning>. Then, provide the final answer (i.e., just one numerical value) between <answer> and </answer>.\\n\\nJane is painting her fingernails. She applies a base coat that takes 2 minutes to dry, two color coats that take 3 minutes each to dry, and a clear top coat that takes 5 minutes to dry. How many minutes total does Jane spend waiting for her nail polish to dry?<end_of_turn>\\n<start_of_turn>model'],\n",
            "      dtype='<U535'),\n",
            " 'question': array(['Jane is painting her fingernails. She applies a base coat that takes 2 minutes to dry, two color coats that take 3 minutes each to dry, and a clear top coat that takes 5 minutes to dry. How many minutes total does Jane spend waiting for her nail polish to dry?'],\n",
            "      dtype='<U260')}\n"
          ]
        }
      ],
      "source": [
        "for ele in train_dataset[:1]:\n",
        "  pprint(ele)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BZxBR7Y_Tn1k",
      "metadata": {},
      "source": [
        "## Load the policy model and the reference model\n",
        "\n",
        "The policy model is the model which is actually trained and whose weights are\n",
        "updated. The reference model is the model with which we compute KL divergence.\n",
        "This is to ensure that the policy updates are not huge and that it does not\n",
        "deviate too much from the reference model.\n",
        "\n",
        "Typically, the reference model is the base model, and the policy model is the\n",
        "same base model, but with LoRA parameters. Only the LoRA parameters are updated.\n",
        "\n",
        "Note: We perform full precision (fp32) training. You can, however, leverage\n",
        "Qwix for QAT.\n",
        "\n",
        "To load the model, you need to be on [Kaggle](https://www.kaggle.com/) and need\n",
        "to have agreed to the Gemma license\n",
        "[here](https://www.kaggle.com/models/google/gemma/flax/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thp6hhqfTn1k",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da8771d2503046b99f92d9a5a7b7e77a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# # Log in\n",
        "# if \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n",
        "#   kagglehub.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "srH2s_jzTn1k",
      "metadata": {},
      "outputs": [
        {
          "ename": "KaggleApiHTTPError",
          "evalue": "403 Client Error.\n\nYou don't have permission to access resource at URL: https://www.kaggle.com/models/google/gemma-2/flax/gemma2-2b-it/1. The server reported the following issues: Permission denied on resource (or it may not exists).\nPlease make sure you are authenticated if you are trying to access a private resource or a resource requiring consent.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/exceptions.py:66\u001b[39m, in \u001b[36mkaggle_api_raise_for_status\u001b[39m\u001b[34m(response, resource_handle)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[31mHTTPError\u001b[39m: 403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/models/google/gemma-2/flax/gemma2-2b-it/1/files?page_size=25",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mKaggleApiHTTPError\u001b[39m                        Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m kaggle_ckpt_path = \u001b[43mkagglehub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_download\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoogle/gemma-2/flax/gemma2-2b-it\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/models.py:35\u001b[39m, in \u001b[36mmodel_download\u001b[39m\u001b[34m(handle, path, force_download)\u001b[39m\n\u001b[32m     33\u001b[39m h = parse_model_handle(handle)\n\u001b[32m     34\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloading Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh.to_url()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m, extra={**EXTRA_CONSOLE_BLOCK})\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m path, _ = \u001b[43mregistry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/registry.py:28\u001b[39m, in \u001b[36mMultiImplRegistry.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m._impls):\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m impl.is_supported(*args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     30\u001b[39m         fails.append(\u001b[38;5;28mtype\u001b[39m(impl).\u001b[34m__name__\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/resolver.py:29\u001b[39m, in \u001b[36mResolver.__call__\u001b[39m\u001b[34m(self, handle, path, force_download)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mself\u001b[39m, handle: T, path: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m, *, force_download: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     17\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[32m     18\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Resolves a handle into a path with the requested file(s) and the resource's version number.\u001b[39;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m \u001b[33;03m        Some cases where version number might be missing: Competition datasource, API-based models.\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     path, version = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_resolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# Note handles are immutable, so _resolve() could not have altered our reference\u001b[39;00m\n\u001b[32m     32\u001b[39m     register_datasource_access(handle, version)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/http_resolver.py:172\u001b[39m, in \u001b[36mModelHttpResolver._resolve\u001b[39m\u001b[34m(self, h, path, force_download)\u001b[39m\n\u001b[32m    167\u001b[39m     api_client.download_file(url_path, out_path, h, extract_auto_compressed_file=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# List the files and decide how to download them:\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;66;03m# - <= 25 files: Download files in parallel\u001b[39;00m\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# > 25 files: Download the archive and uncompress\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     (files, has_more) = \u001b[43m_list_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m has_more:\n\u001b[32m    174\u001b[39m         \u001b[38;5;66;03m# Downloading the full archived bundle.\u001b[39;00m\n\u001b[32m    175\u001b[39m         archive_path = get_cached_archive_path(h)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/http_resolver.py:311\u001b[39m, in \u001b[36m_list_files\u001b[39m\u001b[34m(api_client, h)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_list_files\u001b[39m(api_client: KaggleApiV1Client, h: ModelHandle) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     json_response = \u001b[43mapi_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_build_list_model_instance_version_files_url_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mfiles\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m json_response:\n\u001b[32m    313\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33mInvalid ListModelInstanceVersionFiles API response. Expected to include a \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfiles\u001b[39m\u001b[33m'\u001b[39m\u001b[33m field\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/clients.py:139\u001b[39m, in \u001b[36mKaggleApiV1Client.get\u001b[39m\u001b[34m(self, path, resource_handle)\u001b[39m\n\u001b[32m    132\u001b[39m url = \u001b[38;5;28mself\u001b[39m._build_url(path)\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m requests.get(\n\u001b[32m    134\u001b[39m     url,\n\u001b[32m    135\u001b[39m     headers={\u001b[33m\"\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m\"\u001b[39m: get_user_agent()},\n\u001b[32m    136\u001b[39m     auth=\u001b[38;5;28mself\u001b[39m._get_auth(),\n\u001b[32m    137\u001b[39m     timeout=(DEFAULT_CONNECT_TIMEOUT, DEFAULT_READ_TIMEOUT),\n\u001b[32m    138\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[43mkaggle_api_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_handle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_for_version_update(response)\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/exceptions.py:106\u001b[39m, in \u001b[36mkaggle_api_raise_for_status\u001b[39m\u001b[34m(response, resource_handle)\u001b[39m\n\u001b[32m     97\u001b[39m     message = (\n\u001b[32m     98\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     99\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    102\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease make sure you specified the correct resource identifiers.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    103\u001b[39m     )\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# Default handling\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m KaggleApiHTTPError(message, response=response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
            "\u001b[31mKaggleApiHTTPError\u001b[39m: 403 Client Error.\n\nYou don't have permission to access resource at URL: https://www.kaggle.com/models/google/gemma-2/flax/gemma2-2b-it/1. The server reported the following issues: Permission denied on resource (or it may not exists).\nPlease make sure you are authenticated if you are trying to access a private resource or a resource requiring consent."
          ]
        }
      ],
      "source": [
        "kaggle_ckpt_path = kagglehub.model_download(\"google/gemma-2/flax/gemma2-2b-it\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cIFAxgVOTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # This is a workaround. The checkpoints on Kaggle don't work with NNX. So, we\n",
        "# # load the model, save the checkpoint locally, and then reload the model\n",
        "# # (sharded).\n",
        "# params = params_lib.load_and_format_params(\n",
        "#     os.path.join(kaggle_ckpt_path, \"gemma2-2b-it\")\n",
        "# )\n",
        "# gemma = gemma_lib.Transformer.from_params(params, version=\"2-2b-it\")\n",
        "# checkpointer = ocp.StandardCheckpointer()\n",
        "# _, state = nnx.split(gemma)\n",
        "# checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JSz-XmQpTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Wait for the ckpt to save successfully.\n",
        "# time.sleep(60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_w8kav8sTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Delete the intermediate model to save memory.\n",
        "# del params\n",
        "# del gemma\n",
        "# del state\n",
        "# gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52b18135",
      "metadata": {},
      "source": [
        "### Load MaxText model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b6aa758c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-30 19:26:53.655874: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751311613.674996  373115 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751311613.680533  373115 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1751311613.695428  373115 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1751311613.695444  373115 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1751311613.695446  373115 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1751311613.695448  373115 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# add the parent directory (one level up) to sys.path\n",
        "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../../maxtext')))\n",
        "\n",
        "# ! pip install -r ../../maxtext/requirements.txt\n",
        "\n",
        "import MaxText as mt\n",
        "from MaxText import pyconfig\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eee062b3",
      "metadata": {},
      "source": [
        "#### Convert MaxText model to nnx (use a commit from MaxText repo prior to )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "m2KD-nmbTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_ref_maxtext_model():\n",
        "\n",
        "  #python3 -m MaxText.train MaxText/configs/base.yml base_output_directory=${BASE_OUTPUT_DIRECTORY} dataset_path=${DATASET_PATH} tokenizer_path=assets/tokenizer.gemma load_parameters_path=${CONVERTED_CHECKPOINT} per_device_batch_size=1 run_name=${FINETUNE_RUN_NAME} max_target_length=8192 steps=10 async_checkpointing=false model_name=gemma-2b checkpoint_period=5\n",
        "\n",
        "  #TODO: @mazumdera: change this to use Gemma2-2b-it\n",
        "  config = pyconfig.initialize(\n",
        "      [\"\", \"../../maxtext/MaxText/configs/base.yml\"], #TODO: @mazumdera: why decode.py?\n",
        "      base_output_directory=\"gs://dummy_output_dir\",  # This is not used in Tunix.\n",
        "      run_name=\"test-tunix-maxtext-gemma-2b\",\n",
        "      # dataset_path=we use Tunix's dataset\n",
        "      load_parameters_path=\"gs://maxtext-gemma/2b/\",\n",
        "      tokenizer_path=\"../../maxtext/assets/tokenizer.gemma\",\n",
        "      per_device_batch_size=1,\n",
        "      max_target_length=8192,\n",
        "      steps=10,\n",
        "      async_checkpointing=\"false\",\n",
        "      model_name=\"gemma-2b\",\n",
        "      checkpoint_period=5,\n",
        "      skip_jax_distributed_system=\"true\"\n",
        "\n",
        "  )\n",
        "  model = mt.from_pretrained(config)\n",
        "  mesh  = model.mesh\n",
        "  \n",
        "  # We can continue to use Tunix's model_config\n",
        "  model_config = gemma_lib.TransformerConfig.gemma2_2b()\n",
        "  \n",
        "  return model, mesh, model_config\n",
        "\n",
        "def get_ref_model(ckpt_path):\n",
        "  mesh = jax.make_mesh(*MESH)\n",
        "  model_config = gemma_lib.TransformerConfig.gemma2_2b()\n",
        "  abs_gemma: nnx.Module = nnx.eval_shape(\n",
        "      lambda: gemma_lib.Transformer(model_config, rngs=nnx.Rngs(params=0))\n",
        "  )\n",
        "  abs_state = nnx.state(abs_gemma)\n",
        "  abs_state = jax.tree.map(\n",
        "      lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.float32, sharding=s),\n",
        "      abs_state,\n",
        "      nnx.get_named_sharding(abs_state, mesh),\n",
        "  )\n",
        "  checkpointer = ocp.StandardCheckpointer()\n",
        "  restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n",
        "\n",
        "  graph_def, _ = nnx.split(abs_gemma)\n",
        "  gemma = nnx.merge(graph_def, restored_params)\n",
        "  return gemma, mesh, model_config\n",
        "\n",
        "\n",
        "def get_lora_model(base_model, mesh):\n",
        "  lora_provider = lora.LoraProvider(\n",
        "      module_path=(\n",
        "          \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
        "          \".*attn_vec_einsum\"\n",
        "      ),\n",
        "      rank=RANK,\n",
        "      alpha=ALPHA,\n",
        "  )\n",
        "\n",
        "  model_input = base_model.get_model_input()\n",
        "  lora_model = lora.apply_lora_to_model(\n",
        "      base_model, lora_provider, **model_input\n",
        "  )\n",
        "\n",
        "  with mesh:\n",
        "    state = nnx.state(lora_model)\n",
        "    pspecs = nnx.get_partition_spec(state)\n",
        "    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
        "    nnx.update(lora_model, sharded_state)\n",
        "\n",
        "  return lora_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "kSdZ7aGhTn1k",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updating keys from env and command line: ['run_name', 'model_name', 'load_parameters_path', 'async_checkpointing', 'checkpoint_period', 'base_output_directory', 'tokenizer_path', 'per_device_batch_size', 'steps', 'skip_jax_distributed_system', 'max_target_length']\n",
            "Running Model: gemma-2b\n",
            "Updating following parameters in config\n",
            "\n",
            "base_emb_dim: 2048\n",
            "base_num_query_heads: 8\n",
            "base_num_kv_heads: 1\n",
            "base_mlp_dim: 16384\n",
            "base_num_decoder_layers: 18\n",
            "head_dim: 256\n",
            "mlp_activations: ['gelu', 'linear']\n",
            "vocab_size: 256128\n",
            "decoder_block: gemma\n",
            "normalization_layer_epsilon: 1e-06\n",
            "logits_via_embedding: True\n",
            "Updating keys from model: ['base_emb_dim', 'base_num_query_heads', 'base_num_kv_heads', 'base_mlp_dim', 'base_num_decoder_layers', 'head_dim', 'mlp_activations', 'vocab_size', 'decoder_block', 'normalization_layer_epsilon', 'logits_via_embedding']\n",
            "Skipping jax distributed system due to skip_jax_distributed_system=True flag.\n",
            "Not using emergency checkpoint, ignoring local_checkpoint_directory, local_checkpoint_period, use_replicator_service and replicator_backup_interval_minutes\n",
            "dataset_type set to tfds, will use keys['dataset_path']='' and keys['dataset_name']='c4/en:3.0.1'\n",
            "Config param activations_in_float32: False\n",
            "Config param adam_b1: 0.9\n",
            "Config param adam_b2: 0.95\n",
            "Config param adam_eps: 1e-08\n",
            "Config param adam_eps_root: 0.0\n",
            "Config param adam_weight_decay: 0.1\n",
            "Config param add_bos: True\n",
            "Config param add_eos: True\n",
            "Config param allow_split_physical_axes: False\n",
            "Config param ar_cache_axis_order: 1,2,0,3\n",
            "Config param async_checkpointing: False\n",
            "Config param attention: autoselected\n",
            "Config param attention_type: global\n",
            "Config param attn_logits_soft_cap: None\n",
            "Config param autoregressive_decode_assert: \n",
            "Config param base_emb_dim: 2048\n",
            "Config param base_mlp_dim: 16384\n",
            "Config param base_moe_mlp_dim: 7168\n",
            "Config param base_num_decoder_layers: 18\n",
            "Config param base_num_kv_heads: 1\n",
            "Config param base_num_query_heads: 8\n",
            "Config param base_output_directory: gs://dummy_output_dir\n",
            "Config param beta_fast: 32\n",
            "Config param beta_slow: 1\n",
            "Config param capacity_factor: -1.0\n",
            "Config param cast_logits_to_fp32: True\n",
            "Config param checkpoint_dir: gs://dummy_output_dir/test-tunix-maxtext-gemma-2b/checkpoints/\n",
            "Config param checkpoint_is_quantized: False\n",
            "Config param checkpoint_period: 5\n",
            "Config param checkpoint_storage_concurrent_gb: 96\n",
            "Config param checkpoint_storage_target_data_file_size_bytes: 2147483648\n",
            "Config param checkpoint_storage_use_ocdbt: True\n",
            "Config param checkpoint_storage_use_zarr3: True\n",
            "Config param chunk_attn_window_size: 0\n",
            "Config param collect_stack_trace: False\n",
            "Config param colocated_python_data_input: False\n",
            "Config param compile_topology: \n",
            "Config param compile_topology_num_slices: -1\n",
            "Config param compiled_trainstep_file: \n",
            "Config param compute_axis_order: 0,1,2,3\n",
            "Config param context: remat\n",
            "Config param context_parallel_load_balance: True\n",
            "Config param cosine_learning_rate_final_fraction: 0.1\n",
            "Config param custom_mesh: \n",
            "Config param data_sharding: (('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'),)\n",
            "Config param data_shuffle_seed: 0\n",
            "Config param dataset_name: c4/en:3.0.1\n",
            "Config param dataset_path: \n",
            "Config param dataset_type: tfds\n",
            "Config param dcn_autoregressive_parallelism: 1\n",
            "Config param dcn_context_autoregressive_parallelism: 1\n",
            "Config param dcn_context_parallelism: 1\n",
            "Config param dcn_data_parallelism: -1\n",
            "Config param dcn_expert_parallelism: 1\n",
            "Config param dcn_fsdp_parallelism: 1\n",
            "Config param dcn_fsdp_transpose_parallelism: 1\n",
            "Config param dcn_parallelism: [-1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Config param dcn_pipeline_parallelism: 1\n",
            "Config param dcn_sequence_parallelism: 1\n",
            "Config param dcn_tensor_parallelism: 1\n",
            "Config param dcn_tensor_sequence_parallelism: 1\n",
            "Config param dcn_tensor_transpose_parallelism: 1\n",
            "Config param decode_sampling_nucleus_p: -1\n",
            "Config param decode_sampling_strategy: greedy\n",
            "Config param decode_sampling_temperature: 1.0\n",
            "Config param decode_sampling_top_k: 0\n",
            "Config param decoder_block: DecoderBlockType.GEMMA\n",
            "Config param decoder_layer_input: device\n",
            "Config param dpo_beta: 0.1\n",
            "Config param dpo_label_smoothing: 0.0\n",
            "Config param dropout_rate: 0.0\n",
            "Config param dtype: bfloat16\n",
            "Config param dtype_mm: float32\n",
            "Config param dump_hlo: False\n",
            "Config param dump_hlo_delete_local_after: True\n",
            "Config param dump_hlo_gcs_dir: \n",
            "Config param dump_hlo_local_dir: /tmp/xla_dump/\n",
            "Config param dump_hlo_module_name: jit_train_step\n",
            "Config param dump_hlo_upload_all: False\n",
            "Config param dump_hlo_xla_flags: \n",
            "Config param dump_step: -1\n",
            "Config param emb_dim: 2048\n",
            "Config param enable_checkpoint_cloud_logger: False\n",
            "Config param enable_checkpointing: True\n",
            "Config param enable_data_shuffling: True\n",
            "Config param enable_dropout: True\n",
            "Config param enable_emergency_checkpoint: False\n",
            "Config param enable_gcp_goodput_metrics: True\n",
            "Config param enable_gcp_step_deviation_metrics: True\n",
            "Config param enable_goodput_recording: False\n",
            "Config param enable_jax_profiler: False\n",
            "Config param enable_llm_inference_pool: False\n",
            "Config param enable_model_warmup: False\n",
            "Config param enable_padding_causal_mask: True\n",
            "Config param enable_pathways_goodput: False\n",
            "Config param enable_prefix_caching: False\n",
            "Config param enable_single_controller: False\n",
            "Config param enable_single_replica_ckpt_restoring: False\n",
            "Config param enable_tensorboard: True\n",
            "Config param eval_data_columns: ['text']\n",
            "Config param eval_dataset_name: c4/en:3.0.1\n",
            "Config param eval_interval: -1\n",
            "Config param eval_per_device_batch_size: 1.0\n",
            "Config param eval_split: validation\n",
            "Config param eval_steps: -1\n",
            "Config param expansion_factor_real_data: -1\n",
            "Config param final_logits_soft_cap: None\n",
            "Config param first_num_dense_layers: 0\n",
            "Config param float32_logits: False\n",
            "Config param float32_qk_product: False\n",
            "Config param force_unroll: False\n",
            "Config param freeze_vision_encoder_params: True\n",
            "Config param fused_mlp: False\n",
            "Config param fused_qkv: False\n",
            "Config param gcs_metrics: False\n",
            "Config param generate_slice: v5e-16\n",
            "Config param global_batch_size_to_eval_on: 1\n",
            "Config param global_batch_size_to_load: 1\n",
            "Config param global_batch_size_to_load_eval: 1\n",
            "Config param global_batch_size_to_train_on: 1\n",
            "Config param global_parameter_scale: 1\n",
            "Config param goodput_upload_interval_seconds: 30\n",
            "Config param gradient_accumulation_steps: 1\n",
            "Config param gradient_clipping_threshold: 1.0\n",
            "Config param grain_eval_files: \n",
            "Config param grain_file_type: arrayrecord\n",
            "Config param grain_train_files: \n",
            "Config param grain_worker_count: 1\n",
            "Config param grain_worker_count_eval: 1\n",
            "Config param hardware: tpu\n",
            "Config param head_dim: 256\n",
            "Config param heartbeat_reporting_interval_in_seconds: 5\n",
            "Config param hf_data_dir: \n",
            "Config param hf_eval_files: \n",
            "Config param hf_eval_split: \n",
            "Config param hf_path: \n",
            "Config param hf_train_files: \n",
            "Config param hidden_size_for_vit: 1408\n",
            "Config param ici_autoregressive_parallelism: 1\n",
            "Config param ici_context_autoregressive_parallelism: 1\n",
            "Config param ici_context_parallelism: 1\n",
            "Config param ici_data_parallelism: 1\n",
            "Config param ici_expert_parallelism: 1\n",
            "Config param ici_fsdp_parallelism: -1\n",
            "Config param ici_fsdp_transpose_parallelism: 1\n",
            "Config param ici_parallelism: [1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Config param ici_pipeline_parallelism: 1\n",
            "Config param ici_sequence_parallelism: 1\n",
            "Config param ici_tensor_parallelism: 1\n",
            "Config param ici_tensor_sequence_parallelism: 1\n",
            "Config param ici_tensor_transpose_parallelism: 1\n",
            "Config param image_path: \n",
            "Config param image_size_for_vit: 896\n",
            "Config param inference_benchmark_test: False\n",
            "Config param inference_metadata_file: \n",
            "Config param inference_microbenchmark_log_file_path: \n",
            "Config param inference_microbenchmark_loop_iters: 10\n",
            "Config param inference_microbenchmark_num_samples: [1, 2, 3, 4, 5]\n",
            "Config param inference_microbenchmark_prefill_lengths: 64,128,256,512,1024\n",
            "Config param inference_microbenchmark_stages: prefill,generate\n",
            "Config param inference_server: MaxtextInterleavedServer\n",
            "Config param inhomogeneous_layer_cycle_interval: 1\n",
            "Config param init_weights_seed: 0\n",
            "Config param input_data_sharding_logical_axes: ['activation_embed_and_logits_batch', 'activation_norm_length']\n",
            "Config param interleave_moe_layer_step: 1\n",
            "Config param intermediate_size_for_vit: 5632\n",
            "Config param jax_cache_dir: ~/jax_cache\n",
            "Config param jax_debug_log_modules: \n",
            "Config param jax_distributed_initialization_timeout: 300\n",
            "Config param jax_profiler_port: 9999\n",
            "Config param key_proj: remat\n",
            "Config param kv_lora_rank: 512\n",
            "Config param kv_quant_axis: heads_and_dkv\n",
            "Config param kv_quant_dtype: int8\n",
            "Config param learning_rate: 3e-05\n",
            "Config param learning_rate_schedule_steps: 10\n",
            "Config param load_balance_loss_weight: 0.01\n",
            "Config param load_from_prefill_dir: False\n",
            "Config param load_full_state_path: \n",
            "Config param load_parameters_path: gs://maxtext-gemma/2b/\n",
            "Config param local_checkpoint_directory: \n",
            "Config param local_checkpoint_period: 0\n",
            "Config param local_rope_max_timescale: -1\n",
            "Config param log_config: True\n",
            "Config param log_period: 100\n",
            "Config param logical_axis_rules: (('activation_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_batch_no_exp', ('data', 'fsdp', 'fsdp_transpose')), ('activation_embed_and_logits_batch', ('data', 'stage', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence', 'autoregressive')), ('activation_kv_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_length', ('sequence', 'context')), ('activation_length', ('context',)), ('activation_norm_length', ('tensor_sequence', 'context', 'sequence')), ('activation_q_length', ('context',)), ('activation_kv_length', ()), ('activation_embed', ('tensor', 'tensor_transpose')), ('activation_mlp', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_kv', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_prefill_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_head_dim', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose')), ('activation_vocab', 'tensor_sequence'), ('activation_vocab', ('sequence', 'context')), ('activation_stage', 'stage'), ('activation_exp', ('expert',)), ('decode_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('decode_length', ('sequence',)), ('mlp', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), ('vocab', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('q_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('kv_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'context', 'expert')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'context')), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'expert')), ('norm', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('layers', 'stage'), ('kv', ()), ('kv_head_dim', ()), ('cache_batch_prefill', ()), ('cache_batch', ()), ('cache_heads_none', ()), ('cache_heads', ('autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence')), ('cache_heads', ('autoregressive', 'tensor', 'tensor_sequence')), ('cache_kv', ()), ('cache_sequence', ()), ('exp', 'expert'), ('paged_kv_heads', ('tensor',)), ('num_pages', ()), ('tokens_per_page', ()), ('paged_kv_head_dim_size', ()))\n",
            "Config param logits_dot_in_fp32: False\n",
            "Config param logits_via_embedding: True\n",
            "Config param lora_input_adapters_path: \n",
            "Config param matmul_precision: default\n",
            "Config param max_checkify: False\n",
            "Config param max_corpus_chars: 10000000\n",
            "Config param max_position_embeddings: 163840\n",
            "Config param max_prefill_predict_length: 64\n",
            "Config param max_target_length: 8192\n",
            "Config param megablox: True\n",
            "Config param mesh_axes: ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive']\n",
            "Config param metrics_dir: gs://dummy_output_dir/test-tunix-maxtext-gemma-2b/metrics/\n",
            "Config param metrics_file: \n",
            "Config param micro_batch_size_to_eval_on: 1\n",
            "Config param micro_batch_size_to_train_on: 1\n",
            "Config param mla_naive_kvcache: True\n",
            "Config param mlp_activations: ['gelu', 'linear']\n",
            "Config param mlp_dim: 16384\n",
            "Config param mlpwi: remat\n",
            "Config param mlpwi_0: remat\n",
            "Config param mlpwi_1: remat\n",
            "Config param mlpwo: remat\n",
            "Config param model_call_mode: \n",
            "Config param model_name: gemma-2b\n",
            "Config param moe_mlp_dim: 7168\n",
            "Config param monitor_goodput: False\n",
            "Config param monitor_step_time_deviation: True\n",
            "Config param mscale: 1.0\n",
            "Config param mu_dtype: float32\n",
            "Config param multi_sampling: False\n",
            "Config param n_routing_groups: -1\n",
            "Config param nope_layer_interval: -1\n",
            "Config param normalization_layer_epsilon: 1e-06\n",
            "Config param normalize_embedding_logits: True\n",
            "Config param num_attention_heads_for_vit: 16\n",
            "Config param num_channels_for_vit: 3\n",
            "Config param num_decoder_layers: 18\n",
            "Config param num_epoch: 1\n",
            "Config param num_experts: 1\n",
            "Config param num_experts_per_tok: 1\n",
            "Config param num_hidden_layers_for_vit: 34\n",
            "Config param num_kv_heads: 1\n",
            "Config param num_layers_per_pipeline_stage: 1\n",
            "Config param num_pipeline_microbatches: -1\n",
            "Config param num_pipeline_repeats: -1\n",
            "Config param num_query_heads: 8\n",
            "Config param num_slices: 1\n",
            "Config param opt_type: adamw\n",
            "Config param optimize_mesh_for_tpu_v6e: False\n",
            "Config param optimizer_memory_host_offload: False\n",
            "Config param original_max_position_embeddings: 4096\n",
            "Config param out_proj: remat\n",
            "Config param override_model_config: False\n",
            "Config param packing: True\n",
            "Config param pagedattn_head_dim_alignment: 128\n",
            "Config param pagedattn_max_pages_per_group: 256\n",
            "Config param pagedattn_num_pages: 64\n",
            "Config param pagedattn_pages_per_compute_block: 4\n",
            "Config param pagedattn_tokens_per_page: 32\n",
            "Config param param_scan_axis: 1\n",
            "Config param parameter_memory_host_offload: False\n",
            "Config param patch_size_for_vit: 14\n",
            "Config param per_device_batch_size: 1.0\n",
            "Config param pipeline_delay_activation_forwarding: False\n",
            "Config param pipeline_fsdp_ag_once: False\n",
            "Config param pipeline_parallel_layers: -1\n",
            "Config param pixel_shuffle_ratio_for_vit: 0.5\n",
            "Config param prefill_cache_axis_order: 1,2,0,3\n",
            "Config param prefill_cache_dir: \n",
            "Config param prefill_chunk_size: 256\n",
            "Config param prefill_slice: v5e-16\n",
            "Config param prefix_caching_dram_byte: 100000000000\n",
            "Config param prefix_caching_hbm_byte: 10000000000\n",
            "Config param profile_cleanly: True\n",
            "Config param profile_periodically_period: -1\n",
            "Config param profiler: \n",
            "Config param profiler_steps: 5\n",
            "Config param projector_dropout_for_vit: 0.0\n",
            "Config param projector_input_dim_for_vit: 4096\n",
            "Config param projector_output_dim_for_vit: 4096\n",
            "Config param prometheus_port: 0\n",
            "Config param prompt: I love to\n",
            "Config param q_lora_rank: 0\n",
            "Config param qk_nope_head_dim: 128\n",
            "Config param qk_rope_head_dim: 64\n",
            "Config param qkv_proj: remat\n",
            "Config param quant_cfg_path: \n",
            "Config param quantization: \n",
            "Config param quantization_local_shard_count: 1\n",
            "Config param quantize_kvcache: False\n",
            "Config param query_proj: remat\n",
            "Config param ragged_block_size: 256\n",
            "Config param record_internal_nn_metrics: 0\n",
            "Config param remat_policy: full\n",
            "Config param remat_policy_for_vit: minimal\n",
            "Config param replicate_quant_scale: False\n",
            "Config param replicator_backup_interval_minutes: 0\n",
            "Config param report_heartbeat_metric_for_gcp_monitoring: False\n",
            "Config param report_performance_metric_for_gcp_monitoring: False\n",
            "Config param reshape_q: False\n",
            "Config param return_log_prob: False\n",
            "Config param reuse_example_batch: 0\n",
            "Config param rope_factor: 40\n",
            "Config param rope_max_timescale: 10000\n",
            "Config param rope_min_timescale: 1\n",
            "Config param rope_theta_for_vit: 10000\n",
            "Config param rope_type: default\n",
            "Config param rope_use_scale: True\n",
            "Config param routed_bias: False\n",
            "Config param routed_scaling_factor: 1.0\n",
            "Config param routed_score_func: \n",
            "Config param run_name: test-tunix-maxtext-gemma-2b\n",
            "Config param sa_block_kv: 512\n",
            "Config param sa_block_kv_compute: 512\n",
            "Config param sa_block_kv_dkv: 512\n",
            "Config param sa_block_kv_dkv_compute: 512\n",
            "Config param sa_block_kv_dq: 512\n",
            "Config param sa_block_q: 512\n",
            "Config param sa_block_q_dkv: 512\n",
            "Config param sa_block_q_dq: 512\n",
            "Config param sa_k_layout: HEAD_DIM_MINOR\n",
            "Config param sa_q_layout: HEAD_DIM_MINOR\n",
            "Config param sa_use_fused_bwd_kernel: False\n",
            "Config param sa_v_layout: HEAD_DIM_MINOR\n",
            "Config param save_config_to_gcs: False\n",
            "Config param save_quantized_params_path: \n",
            "Config param scan_layers: True\n",
            "Config param scan_layers_per_stage: False\n",
            "Config param scan_pipeline_iterations: True\n",
            "Config param set_remat_policy_on_layers_per_stage: False\n",
            "Config param set_remat_policy_on_pipeline_iterations: True\n",
            "Config param sft_train_on_completion_only: False\n",
            "Config param sharding_tolerance: 0.02\n",
            "Config param shared_experts: 1\n",
            "Config param skip_first_n_steps_for_profiler: 1\n",
            "Config param skip_jax_distributed_system: True\n",
            "Config param sliding_window_size: 0\n",
            "Config param sparse_matmul: True\n",
            "Config param stack_prefill_result_cache: False\n",
            "Config param stack_trace_interval_seconds: 600\n",
            "Config param stack_trace_to_cloud: False\n",
            "Config param step_deviation_interval_seconds: 30\n",
            "Config param steps: 10\n",
            "Config param subslice_shape: \n",
            "Config param target_eval_loss: 0.0\n",
            "Config param temperature_tuning: False\n",
            "Config param tensorboard_dir: gs://dummy_output_dir/test-tunix-maxtext-gemma-2b/tensorboard/\n",
            "Config param tile_activation_dim: 1024\n",
            "Config param tile_batch_seq: 512\n",
            "Config param tile_size_for_vit: 336\n",
            "Config param tile_weight_dim: 1024\n",
            "Config param tokenize_eval_data: True\n",
            "Config param tokenize_train_data: True\n",
            "Config param tokenizer_path: ../../maxtext/assets/tokenizer.gemma\n",
            "Config param tokenizer_type: sentencepiece\n",
            "Config param topk_routing_group: -1\n",
            "Config param train_data_columns: ['text']\n",
            "Config param train_split: train\n",
            "Config param trainable_position_size: -1\n",
            "Config param upload_all_profiler_results: False\n",
            "Config param use_chat_template: False\n",
            "Config param use_chunked_prefill: False\n",
            "Config param use_dpo: False\n",
            "Config param use_iota_embed: False\n",
            "Config param use_multimodal: False\n",
            "Config param use_post_attn_norm: False\n",
            "Config param use_post_ffw_norm: False\n",
            "Config param use_qk_norm: False\n",
            "Config param use_ragged_attention: False\n",
            "Config param use_random_routing: False\n",
            "Config param use_replicator_service: False\n",
            "Config param use_sft: False\n",
            "Config param use_untrainable_positional_embedding: False\n",
            "Config param use_vertex_tensorboard: False\n",
            "Config param using_pipeline_parallelism: False\n",
            "Config param v_head_dim: 128\n",
            "Config param value_proj: remat\n",
            "Config param vertex_tensorboard_project: \n",
            "Config param vertex_tensorboard_region: \n",
            "Config param vision_output_dim_for_vit: 4096\n",
            "Config param vocab_size: 256128\n",
            "Config param warmup_steps_fraction: 0.1\n",
            "Config param weight_dtype: float32\n",
            "Num_devices: 1, shape (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n"
          ]
        }
      ],
      "source": [
        "# Reference model\n",
        "# gemma, mesh, model_config = get_ref_model(\n",
        "#     ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\n",
        "# )\n",
        "gemma, mesh, model_config = get_ref_maxtext_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "05ad3dfe",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<script> (()=>{ if (customElements.get('treescope-container') === undefined) { class TreescopeContainer extends HTMLElement { constructor() { super(); this.attachShadow({mode: \"open\"}); this.defns = {}; this.state = {}; } } customElements.define(\"treescope-container\", TreescopeContainer); } if (customElements.get('treescope-run-here') === undefined) { class RunHere extends HTMLElement { constructor() { super() } connectedCallback() { const run = child => { const fn = new Function(child.textContent); child.textContent = \"\"; fn.call(this); this.remove(); }; const child = this.querySelector(\"script\"); if (child) { run(child); } else { new MutationObserver(()=>{ run(this.querySelector(\"script\")); }).observe(this, {childList: true}); } } } customElements.define(\"treescope-run-here\", RunHere); } })(); </script> <treescope-container class=\"treescope_out_a73314b1b7e24bb8a2e33b7d3737e687\" style=\"display:block\"></treescope-container> <treescope-run-here><script type=\"application/octet-stream\"> const root = ( Array.from(document.getElementsByClassName( \"treescope_out_a73314b1b7e24bb8a2e33b7d3737e687\")) .filter((elt) => !elt.dataset.setup) )[0]; root.dataset.setup = 1; const msg = document.createElement(\"span\"); msg.style = \"color: #cccccc; font-family: monospace;\"; msg.textContent = \"(Loading...)\"; root.state.loadingMsg = msg; root.shadowRoot.appendChild(msg); root.state.chain = new Promise((resolve, reject) => { const observer = new IntersectionObserver((entries) => { for (const entry of entries) { if (entry.isIntersecting) { resolve(); observer.disconnect(); return; } } }, {rootMargin: \"1000px\"}); window.setTimeout(() => { observer.observe(root); }, 0); }); root.state.deferring = false; const _insertNode = (node) => { for (let oldScript of node.querySelectorAll(\"script\")) { let newScript = document.createElement(\"script\"); newScript.type = oldScript.type; newScript.textContent = oldScript.textContent; oldScript.parentNode.replaceChild(newScript, oldScript); } if (root.state.loadingMsg) { root.state.loadingMsg.remove(); root.state.loadingMsg = null; } root.shadowRoot.appendChild(node); }; root.defns.insertContent = ((contentNode, compressed) => { if (compressed) { root.state.deferring = true; } if (root.state.deferring) { root.state.chain = (async () => { await root.state.chain; if (compressed) { const encoded = contentNode.textContent; const blob = new Blob([ Uint8Array.from(atob(encoded), (m) => m.codePointAt(0)) ]); const reader = blob.stream().pipeThrough( new DecompressionStream(\"deflate\") ).pipeThrough( new TextDecoderStream(\"utf-8\") ).getReader(); const parts = []; while (true) { const step = await reader.read(); if (step.done) { break; } parts.push(step.value); } const tpl = document.createElement('template'); tpl.innerHTML = parts.join(\"\"); _insertNode(tpl.content); } else { _insertNode(contentNode.content); } })(); } else { _insertNode(contentNode.content); } }); </script></treescope-run-here><div style=\"display:none\"> <script type=\"application/octet-stream\" >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</script> <treescope-run-here><script type=\"application/octet-stream\"> const root = ( Array.from(document.getElementsByClassName( \"treescope_out_a73314b1b7e24bb8a2e33b7d3737e687\")) .filter((elt) => !elt.dataset['step0']) )[0]; root.dataset['step0'] = 1; root.defns.insertContent( this.parentNode.querySelector('script[type=\"application/octet-stream\"]'), true ); this.parentNode.remove(); </script></treescope-run-here> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from flax import nnx\n",
        "from flax import linen as nn\n",
        "from flax.nnx import bridge\n",
        "\n",
        "\n",
        "model_maxtext_nnx = nnx.bridge.ToNNX(gemma)\n",
        "nnx.display(model_maxtext_nnx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "4i3CfJ1gTn1k",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updating keys from env and command line: ['run_name', 'model_name', 'load_parameters_path', 'async_checkpointing', 'checkpoint_period', 'base_output_directory', 'tokenizer_path', 'per_device_batch_size', 'steps', 'skip_jax_distributed_system', 'max_target_length']\n",
            "Running Model: gemma-2b\n",
            "Updating following parameters in config\n",
            "\n",
            "base_emb_dim: 2048\n",
            "base_num_query_heads: 8\n",
            "base_num_kv_heads: 1\n",
            "base_mlp_dim: 16384\n",
            "base_num_decoder_layers: 18\n",
            "head_dim: 256\n",
            "mlp_activations: ['gelu', 'linear']\n",
            "vocab_size: 256128\n",
            "decoder_block: gemma\n",
            "normalization_layer_epsilon: 1e-06\n",
            "logits_via_embedding: True\n",
            "Updating keys from model: ['base_emb_dim', 'base_num_query_heads', 'base_num_kv_heads', 'base_mlp_dim', 'base_num_decoder_layers', 'head_dim', 'mlp_activations', 'vocab_size', 'decoder_block', 'normalization_layer_epsilon', 'logits_via_embedding']\n",
            "Skipping jax distributed system due to skip_jax_distributed_system=True flag.\n",
            "Not using emergency checkpoint, ignoring local_checkpoint_directory, local_checkpoint_period, use_replicator_service and replicator_backup_interval_minutes\n",
            "dataset_type set to tfds, will use keys['dataset_path']='' and keys['dataset_name']='c4/en:3.0.1'\n",
            "Config param activations_in_float32: False\n",
            "Config param adam_b1: 0.9\n",
            "Config param adam_b2: 0.95\n",
            "Config param adam_eps: 1e-08\n",
            "Config param adam_eps_root: 0.0\n",
            "Config param adam_weight_decay: 0.1\n",
            "Config param add_bos: True\n",
            "Config param add_eos: True\n",
            "Config param allow_split_physical_axes: False\n",
            "Config param ar_cache_axis_order: 1,2,0,3\n",
            "Config param async_checkpointing: False\n",
            "Config param attention: autoselected\n",
            "Config param attention_type: global\n",
            "Config param attn_logits_soft_cap: None\n",
            "Config param autoregressive_decode_assert: \n",
            "Config param base_emb_dim: 2048\n",
            "Config param base_mlp_dim: 16384\n",
            "Config param base_moe_mlp_dim: 7168\n",
            "Config param base_num_decoder_layers: 18\n",
            "Config param base_num_kv_heads: 1\n",
            "Config param base_num_query_heads: 8\n",
            "Config param base_output_directory: gs://dummy_output_dir\n",
            "Config param beta_fast: 32\n",
            "Config param beta_slow: 1\n",
            "Config param capacity_factor: -1.0\n",
            "Config param cast_logits_to_fp32: True\n",
            "Config param checkpoint_dir: gs://dummy_output_dir/test-tunix-maxtext-gemma-2b/checkpoints/\n",
            "Config param checkpoint_is_quantized: False\n",
            "Config param checkpoint_period: 5\n",
            "Config param checkpoint_storage_concurrent_gb: 96\n",
            "Config param checkpoint_storage_target_data_file_size_bytes: 2147483648\n",
            "Config param checkpoint_storage_use_ocdbt: True\n",
            "Config param checkpoint_storage_use_zarr3: True\n",
            "Config param chunk_attn_window_size: 0\n",
            "Config param collect_stack_trace: False\n",
            "Config param colocated_python_data_input: False\n",
            "Config param compile_topology: \n",
            "Config param compile_topology_num_slices: -1\n",
            "Config param compiled_trainstep_file: \n",
            "Config param compute_axis_order: 0,1,2,3\n",
            "Config param context: remat\n",
            "Config param context_parallel_load_balance: True\n",
            "Config param cosine_learning_rate_final_fraction: 0.1\n",
            "Config param custom_mesh: \n",
            "Config param data_sharding: (('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'),)\n",
            "Config param data_shuffle_seed: 0\n",
            "Config param dataset_name: c4/en:3.0.1\n",
            "Config param dataset_path: \n",
            "Config param dataset_type: tfds\n",
            "Config param dcn_autoregressive_parallelism: 1\n",
            "Config param dcn_context_autoregressive_parallelism: 1\n",
            "Config param dcn_context_parallelism: 1\n",
            "Config param dcn_data_parallelism: -1\n",
            "Config param dcn_expert_parallelism: 1\n",
            "Config param dcn_fsdp_parallelism: 1\n",
            "Config param dcn_fsdp_transpose_parallelism: 1\n",
            "Config param dcn_parallelism: [-1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Config param dcn_pipeline_parallelism: 1\n",
            "Config param dcn_sequence_parallelism: 1\n",
            "Config param dcn_tensor_parallelism: 1\n",
            "Config param dcn_tensor_sequence_parallelism: 1\n",
            "Config param dcn_tensor_transpose_parallelism: 1\n",
            "Config param decode_sampling_nucleus_p: -1\n",
            "Config param decode_sampling_strategy: greedy\n",
            "Config param decode_sampling_temperature: 1.0\n",
            "Config param decode_sampling_top_k: 0\n",
            "Config param decoder_block: DecoderBlockType.GEMMA\n",
            "Config param decoder_layer_input: device\n",
            "Config param dpo_beta: 0.1\n",
            "Config param dpo_label_smoothing: 0.0\n",
            "Config param dropout_rate: 0.0\n",
            "Config param dtype: bfloat16\n",
            "Config param dtype_mm: float32\n",
            "Config param dump_hlo: False\n",
            "Config param dump_hlo_delete_local_after: True\n",
            "Config param dump_hlo_gcs_dir: \n",
            "Config param dump_hlo_local_dir: /tmp/xla_dump/\n",
            "Config param dump_hlo_module_name: jit_train_step\n",
            "Config param dump_hlo_upload_all: False\n",
            "Config param dump_hlo_xla_flags: \n",
            "Config param dump_step: -1\n",
            "Config param emb_dim: 2048\n",
            "Config param enable_checkpoint_cloud_logger: False\n",
            "Config param enable_checkpointing: True\n",
            "Config param enable_data_shuffling: True\n",
            "Config param enable_dropout: True\n",
            "Config param enable_emergency_checkpoint: False\n",
            "Config param enable_gcp_goodput_metrics: True\n",
            "Config param enable_gcp_step_deviation_metrics: True\n",
            "Config param enable_goodput_recording: False\n",
            "Config param enable_jax_profiler: False\n",
            "Config param enable_llm_inference_pool: False\n",
            "Config param enable_model_warmup: False\n",
            "Config param enable_padding_causal_mask: True\n",
            "Config param enable_pathways_goodput: False\n",
            "Config param enable_prefix_caching: False\n",
            "Config param enable_single_controller: False\n",
            "Config param enable_single_replica_ckpt_restoring: False\n",
            "Config param enable_tensorboard: True\n",
            "Config param eval_data_columns: ['text']\n",
            "Config param eval_dataset_name: c4/en:3.0.1\n",
            "Config param eval_interval: -1\n",
            "Config param eval_per_device_batch_size: 1.0\n",
            "Config param eval_split: validation\n",
            "Config param eval_steps: -1\n",
            "Config param expansion_factor_real_data: -1\n",
            "Config param final_logits_soft_cap: None\n",
            "Config param first_num_dense_layers: 0\n",
            "Config param float32_logits: False\n",
            "Config param float32_qk_product: False\n",
            "Config param force_unroll: False\n",
            "Config param freeze_vision_encoder_params: True\n",
            "Config param fused_mlp: False\n",
            "Config param fused_qkv: False\n",
            "Config param gcs_metrics: False\n",
            "Config param generate_slice: v5e-16\n",
            "Config param global_batch_size_to_eval_on: 1\n",
            "Config param global_batch_size_to_load: 1\n",
            "Config param global_batch_size_to_load_eval: 1\n",
            "Config param global_batch_size_to_train_on: 1\n",
            "Config param global_parameter_scale: 1\n",
            "Config param goodput_upload_interval_seconds: 30\n",
            "Config param gradient_accumulation_steps: 1\n",
            "Config param gradient_clipping_threshold: 1.0\n",
            "Config param grain_eval_files: \n",
            "Config param grain_file_type: arrayrecord\n",
            "Config param grain_train_files: \n",
            "Config param grain_worker_count: 1\n",
            "Config param grain_worker_count_eval: 1\n",
            "Config param hardware: tpu\n",
            "Config param head_dim: 256\n",
            "Config param heartbeat_reporting_interval_in_seconds: 5\n",
            "Config param hf_data_dir: \n",
            "Config param hf_eval_files: \n",
            "Config param hf_eval_split: \n",
            "Config param hf_path: \n",
            "Config param hf_train_files: \n",
            "Config param hidden_size_for_vit: 1408\n",
            "Config param ici_autoregressive_parallelism: 1\n",
            "Config param ici_context_autoregressive_parallelism: 1\n",
            "Config param ici_context_parallelism: 1\n",
            "Config param ici_data_parallelism: 1\n",
            "Config param ici_expert_parallelism: 1\n",
            "Config param ici_fsdp_parallelism: -1\n",
            "Config param ici_fsdp_transpose_parallelism: 1\n",
            "Config param ici_parallelism: [1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Config param ici_pipeline_parallelism: 1\n",
            "Config param ici_sequence_parallelism: 1\n",
            "Config param ici_tensor_parallelism: 1\n",
            "Config param ici_tensor_sequence_parallelism: 1\n",
            "Config param ici_tensor_transpose_parallelism: 1\n",
            "Config param image_path: \n",
            "Config param image_size_for_vit: 896\n",
            "Config param inference_benchmark_test: False\n",
            "Config param inference_metadata_file: \n",
            "Config param inference_microbenchmark_log_file_path: \n",
            "Config param inference_microbenchmark_loop_iters: 10\n",
            "Config param inference_microbenchmark_num_samples: [1, 2, 3, 4, 5]\n",
            "Config param inference_microbenchmark_prefill_lengths: 64,128,256,512,1024\n",
            "Config param inference_microbenchmark_stages: prefill,generate\n",
            "Config param inference_server: MaxtextInterleavedServer\n",
            "Config param inhomogeneous_layer_cycle_interval: 1\n",
            "Config param init_weights_seed: 0\n",
            "Config param input_data_sharding_logical_axes: ['activation_embed_and_logits_batch', 'activation_norm_length']\n",
            "Config param interleave_moe_layer_step: 1\n",
            "Config param intermediate_size_for_vit: 5632\n",
            "Config param jax_cache_dir: ~/jax_cache\n",
            "Config param jax_debug_log_modules: \n",
            "Config param jax_distributed_initialization_timeout: 300\n",
            "Config param jax_profiler_port: 9999\n",
            "Config param key_proj: remat\n",
            "Config param kv_lora_rank: 512\n",
            "Config param kv_quant_axis: heads_and_dkv\n",
            "Config param kv_quant_dtype: int8\n",
            "Config param learning_rate: 3e-05\n",
            "Config param learning_rate_schedule_steps: 10\n",
            "Config param load_balance_loss_weight: 0.01\n",
            "Config param load_from_prefill_dir: False\n",
            "Config param load_full_state_path: \n",
            "Config param load_parameters_path: gs://maxtext-gemma/2b/\n",
            "Config param local_checkpoint_directory: \n",
            "Config param local_checkpoint_period: 0\n",
            "Config param local_rope_max_timescale: -1\n",
            "Config param log_config: True\n",
            "Config param log_period: 100\n",
            "Config param logical_axis_rules: (('activation_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_batch_no_exp', ('data', 'fsdp', 'fsdp_transpose')), ('activation_embed_and_logits_batch', ('data', 'stage', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence', 'autoregressive')), ('activation_kv_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_length', ('sequence', 'context')), ('activation_length', ('context',)), ('activation_norm_length', ('tensor_sequence', 'context', 'sequence')), ('activation_q_length', ('context',)), ('activation_kv_length', ()), ('activation_embed', ('tensor', 'tensor_transpose')), ('activation_mlp', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_kv', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_prefill_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_head_dim', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose')), ('activation_vocab', 'tensor_sequence'), ('activation_vocab', ('sequence', 'context')), ('activation_stage', 'stage'), ('activation_exp', ('expert',)), ('decode_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('decode_length', ('sequence',)), ('mlp', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), ('vocab', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('q_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('kv_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'context', 'expert')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'context')), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'expert')), ('norm', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('layers', 'stage'), ('kv', ()), ('kv_head_dim', ()), ('cache_batch_prefill', ()), ('cache_batch', ()), ('cache_heads_none', ()), ('cache_heads', ('autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence')), ('cache_heads', ('autoregressive', 'tensor', 'tensor_sequence')), ('cache_kv', ()), ('cache_sequence', ()), ('exp', 'expert'), ('paged_kv_heads', ('tensor',)), ('num_pages', ()), ('tokens_per_page', ()), ('paged_kv_head_dim_size', ()))\n",
            "Config param logits_dot_in_fp32: False\n",
            "Config param logits_via_embedding: True\n",
            "Config param lora_input_adapters_path: \n",
            "Config param matmul_precision: default\n",
            "Config param max_checkify: False\n",
            "Config param max_corpus_chars: 10000000\n",
            "Config param max_position_embeddings: 163840\n",
            "Config param max_prefill_predict_length: 64\n",
            "Config param max_target_length: 8192\n",
            "Config param megablox: True\n",
            "Config param mesh_axes: ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive']\n",
            "Config param metrics_dir: gs://dummy_output_dir/test-tunix-maxtext-gemma-2b/metrics/\n",
            "Config param metrics_file: \n",
            "Config param micro_batch_size_to_eval_on: 1\n",
            "Config param micro_batch_size_to_train_on: 1\n",
            "Config param mla_naive_kvcache: True\n",
            "Config param mlp_activations: ['gelu', 'linear']\n",
            "Config param mlp_dim: 16384\n",
            "Config param mlpwi: remat\n",
            "Config param mlpwi_0: remat\n",
            "Config param mlpwi_1: remat\n",
            "Config param mlpwo: remat\n",
            "Config param model_call_mode: \n",
            "Config param model_name: gemma-2b\n",
            "Config param moe_mlp_dim: 7168\n",
            "Config param monitor_goodput: False\n",
            "Config param monitor_step_time_deviation: True\n",
            "Config param mscale: 1.0\n",
            "Config param mu_dtype: float32\n",
            "Config param multi_sampling: False\n",
            "Config param n_routing_groups: -1\n",
            "Config param nope_layer_interval: -1\n",
            "Config param normalization_layer_epsilon: 1e-06\n",
            "Config param normalize_embedding_logits: True\n",
            "Config param num_attention_heads_for_vit: 16\n",
            "Config param num_channels_for_vit: 3\n",
            "Config param num_decoder_layers: 18\n",
            "Config param num_epoch: 1\n",
            "Config param num_experts: 1\n",
            "Config param num_experts_per_tok: 1\n",
            "Config param num_hidden_layers_for_vit: 34\n",
            "Config param num_kv_heads: 1\n",
            "Config param num_layers_per_pipeline_stage: 1\n",
            "Config param num_pipeline_microbatches: -1\n",
            "Config param num_pipeline_repeats: -1\n",
            "Config param num_query_heads: 8\n",
            "Config param num_slices: 1\n",
            "Config param opt_type: adamw\n",
            "Config param optimize_mesh_for_tpu_v6e: False\n",
            "Config param optimizer_memory_host_offload: False\n",
            "Config param original_max_position_embeddings: 4096\n",
            "Config param out_proj: remat\n",
            "Config param override_model_config: False\n",
            "Config param packing: True\n",
            "Config param pagedattn_head_dim_alignment: 128\n",
            "Config param pagedattn_max_pages_per_group: 256\n",
            "Config param pagedattn_num_pages: 64\n",
            "Config param pagedattn_pages_per_compute_block: 4\n",
            "Config param pagedattn_tokens_per_page: 32\n",
            "Config param param_scan_axis: 1\n",
            "Config param parameter_memory_host_offload: False\n",
            "Config param patch_size_for_vit: 14\n",
            "Config param per_device_batch_size: 1.0\n",
            "Config param pipeline_delay_activation_forwarding: False\n",
            "Config param pipeline_fsdp_ag_once: False\n",
            "Config param pipeline_parallel_layers: -1\n",
            "Config param pixel_shuffle_ratio_for_vit: 0.5\n",
            "Config param prefill_cache_axis_order: 1,2,0,3\n",
            "Config param prefill_cache_dir: \n",
            "Config param prefill_chunk_size: 256\n",
            "Config param prefill_slice: v5e-16\n",
            "Config param prefix_caching_dram_byte: 100000000000\n",
            "Config param prefix_caching_hbm_byte: 10000000000\n",
            "Config param profile_cleanly: True\n",
            "Config param profile_periodically_period: -1\n",
            "Config param profiler: \n",
            "Config param profiler_steps: 5\n",
            "Config param projector_dropout_for_vit: 0.0\n",
            "Config param projector_input_dim_for_vit: 4096\n",
            "Config param projector_output_dim_for_vit: 4096\n",
            "Config param prometheus_port: 0\n",
            "Config param prompt: I love to\n",
            "Config param q_lora_rank: 0\n",
            "Config param qk_nope_head_dim: 128\n",
            "Config param qk_rope_head_dim: 64\n",
            "Config param qkv_proj: remat\n",
            "Config param quant_cfg_path: \n",
            "Config param quantization: \n",
            "Config param quantization_local_shard_count: 1\n",
            "Config param quantize_kvcache: False\n",
            "Config param query_proj: remat\n",
            "Config param ragged_block_size: 256\n",
            "Config param record_internal_nn_metrics: 0\n",
            "Config param remat_policy: full\n",
            "Config param remat_policy_for_vit: minimal\n",
            "Config param replicate_quant_scale: False\n",
            "Config param replicator_backup_interval_minutes: 0\n",
            "Config param report_heartbeat_metric_for_gcp_monitoring: False\n",
            "Config param report_performance_metric_for_gcp_monitoring: False\n",
            "Config param reshape_q: False\n",
            "Config param return_log_prob: False\n",
            "Config param reuse_example_batch: 0\n",
            "Config param rope_factor: 40\n",
            "Config param rope_max_timescale: 10000\n",
            "Config param rope_min_timescale: 1\n",
            "Config param rope_theta_for_vit: 10000\n",
            "Config param rope_type: default\n",
            "Config param rope_use_scale: True\n",
            "Config param routed_bias: False\n",
            "Config param routed_scaling_factor: 1.0\n",
            "Config param routed_score_func: \n",
            "Config param run_name: test-tunix-maxtext-gemma-2b\n",
            "Config param sa_block_kv: 512\n",
            "Config param sa_block_kv_compute: 512\n",
            "Config param sa_block_kv_dkv: 512\n",
            "Config param sa_block_kv_dkv_compute: 512\n",
            "Config param sa_block_kv_dq: 512\n",
            "Config param sa_block_q: 512\n",
            "Config param sa_block_q_dkv: 512\n",
            "Config param sa_block_q_dq: 512\n",
            "Config param sa_k_layout: HEAD_DIM_MINOR\n",
            "Config param sa_q_layout: HEAD_DIM_MINOR\n",
            "Config param sa_use_fused_bwd_kernel: False\n",
            "Config param sa_v_layout: HEAD_DIM_MINOR\n",
            "Config param save_config_to_gcs: False\n",
            "Config param save_quantized_params_path: \n",
            "Config param scan_layers: True\n",
            "Config param scan_layers_per_stage: False\n",
            "Config param scan_pipeline_iterations: True\n",
            "Config param set_remat_policy_on_layers_per_stage: False\n",
            "Config param set_remat_policy_on_pipeline_iterations: True\n",
            "Config param sft_train_on_completion_only: False\n",
            "Config param sharding_tolerance: 0.02\n",
            "Config param shared_experts: 1\n",
            "Config param skip_first_n_steps_for_profiler: 1\n",
            "Config param skip_jax_distributed_system: True\n",
            "Config param sliding_window_size: 0\n",
            "Config param sparse_matmul: True\n",
            "Config param stack_prefill_result_cache: False\n",
            "Config param stack_trace_interval_seconds: 600\n",
            "Config param stack_trace_to_cloud: False\n",
            "Config param step_deviation_interval_seconds: 30\n",
            "Config param steps: 10\n",
            "Config param subslice_shape: \n",
            "Config param target_eval_loss: 0.0\n",
            "Config param temperature_tuning: False\n",
            "Config param tensorboard_dir: gs://dummy_output_dir/test-tunix-maxtext-gemma-2b/tensorboard/\n",
            "Config param tile_activation_dim: 1024\n",
            "Config param tile_batch_seq: 512\n",
            "Config param tile_size_for_vit: 336\n",
            "Config param tile_weight_dim: 1024\n",
            "Config param tokenize_eval_data: True\n",
            "Config param tokenize_train_data: True\n",
            "Config param tokenizer_path: ../../maxtext/assets/tokenizer.gemma\n",
            "Config param tokenizer_type: sentencepiece\n",
            "Config param topk_routing_group: -1\n",
            "Config param train_data_columns: ['text']\n",
            "Config param train_split: train\n",
            "Config param trainable_position_size: -1\n",
            "Config param upload_all_profiler_results: False\n",
            "Config param use_chat_template: False\n",
            "Config param use_chunked_prefill: False\n",
            "Config param use_dpo: False\n",
            "Config param use_iota_embed: False\n",
            "Config param use_multimodal: False\n",
            "Config param use_post_attn_norm: False\n",
            "Config param use_post_ffw_norm: False\n",
            "Config param use_qk_norm: False\n",
            "Config param use_ragged_attention: False\n",
            "Config param use_random_routing: False\n",
            "Config param use_replicator_service: False\n",
            "Config param use_sft: False\n",
            "Config param use_untrainable_positional_embedding: False\n",
            "Config param use_vertex_tensorboard: False\n",
            "Config param using_pipeline_parallelism: False\n",
            "Config param v_head_dim: 128\n",
            "Config param value_proj: remat\n",
            "Config param vertex_tensorboard_project: \n",
            "Config param vertex_tensorboard_region: \n",
            "Config param vision_output_dim_for_vit: 4096\n",
            "Config param vocab_size: 256128\n",
            "Config param warmup_steps_fraction: 0.1\n",
            "Config param weight_dtype: float32\n",
            "Num_devices: 1, shape (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n"
          ]
        }
      ],
      "source": [
        "# Policy model\n",
        "# This can remain unchanged from default Tunix's colab\n",
        "# lora_gemma = get_lora_model(gemma, mesh=mesh)\n",
        "\n",
        "# TODO: @mazumdera: change this to use lora\n",
        "# lora_gemma = get_lora_model(gemma, mesh=mesh)\n",
        "# nnx.display(lora_gemma)\n",
        "\n",
        "gemma_policy, mesh, model_config = get_ref_maxtext_model()\n",
        "gemma_policy_maxtext_nnx = nnx.bridge.ToNNX(gemma_policy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zLzR1tJfTn1k",
      "metadata": {},
      "source": [
        "## Define reward functions\n",
        "\n",
        "We define four reward functions:\n",
        "\n",
        "- reward if the format of the output exactly matches the instruction given in\n",
        "`TEMPLATE`;\n",
        "- reward if the format of the output approximately matches the instruction given\n",
        "in `TEMPLATE`;\n",
        "- reward if the answer is correct/partially correct;\n",
        "- Sometimes, the text between `<answer>`, `</answer>` might not be one\n",
        "  number. So, extract the number, and reward the model if the answer is correct.\n",
        "\n",
        "The reward functions are inspired from\n",
        "[here](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb).\n",
        "\n",
        "First off, let's define a RegEx for checking whether the format matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "C7Beft8wTn1k",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'reasoning_start' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m match_format = re.compile(\n\u001b[32m      2\u001b[39m     \u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m^[\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms]\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33m0,\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mreasoning_start\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.+?\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreasoning_end\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.*?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m     \u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msolution_start\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m(.+?)\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msolution_end\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms]\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33m0,\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m$\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     flags=re.MULTILINE | re.DOTALL,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m match_format.search(\n\u001b[32m     10\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreasoning_start\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mLet me\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m think!\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreasoning_end\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msolution_start\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m2\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msolution_end\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m )\n",
            "\u001b[31mNameError\u001b[39m: name 'reasoning_start' is not defined"
          ]
        }
      ],
      "source": [
        "match_format = re.compile(\n",
        "    rf\"^[\\s]{{0,}}\"\n",
        "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\n",
        "    rf\"{solution_start}(.+?){solution_end}\"\n",
        "    rf\"[\\s]{{0,}}$\",\n",
        "    flags=re.MULTILINE | re.DOTALL,\n",
        ")\n",
        "\n",
        "match_format.search(\n",
        "    f\"{reasoning_start}Let me\"\n",
        "    f\" think!{reasoning_end}{solution_start}2{solution_end}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fe1rF15zTn1k",
      "metadata": {},
      "source": [
        "Give the model a reward of 3 points if the format matches exactly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_fhQ6pY2Tn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def match_format_exactly(prompts, completions, **kargs):\n",
        "  scores = []\n",
        "  for completion in completions:\n",
        "    score = 0\n",
        "    response = completion\n",
        "    # Match if format is seen exactly!\n",
        "    if match_format.search(response) is not None:\n",
        "      score += 3.0\n",
        "    scores.append(score)\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sWdAdUHuTn1k",
      "metadata": {},
      "source": [
        "We also reward the model if the format of the output matches partially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uOhO4f3-Tn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def match_format_approximately(prompts, completions, **kargs):\n",
        "  scores = []\n",
        "\n",
        "  for completion in completions:\n",
        "    score = 0\n",
        "    response = completion\n",
        "    # Count how many keywords are seen - we penalize if too many!\n",
        "    # If we see 1, then plus some points!\n",
        "    score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n",
        "    score += 0.5 if response.count(reasoning_end) == 1 else -0.5\n",
        "    score += 0.5 if response.count(solution_start) == 1 else -0.5\n",
        "    score += 0.5 if response.count(solution_end) == 1 else -0.5\n",
        "    scores.append(score)\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A2fNZDgTTn1k",
      "metadata": {},
      "source": [
        "Reward the model if the answer is correct. A reward is also given if the answer\n",
        "does not match exactly, i.e., based on how close the answer is to the correct\n",
        "value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S8zcWsmhTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_answer(prompts, completions, answer, **kargs):\n",
        "  responses = completions\n",
        "\n",
        "  extracted_responses = [\n",
        "      guess.group(1) if (guess := match_format.search(r)) is not None else None\n",
        "      for r in responses\n",
        "  ]\n",
        "\n",
        "  scores = []\n",
        "  for guess, true_answer in zip(extracted_responses, answer):\n",
        "    score = 0\n",
        "    if guess is None:\n",
        "      scores.append(0)\n",
        "      continue\n",
        "    # Correct answer gets 3 points!\n",
        "    if guess == true_answer:\n",
        "      score += 3.0\n",
        "    # Match if spaces are seen\n",
        "    elif guess.strip() == true_answer.strip():\n",
        "      score += 1.5\n",
        "    else:\n",
        "      # We also reward it if the answer is close via ratios!\n",
        "      # Ie if the answer is within some range, reward it!\n",
        "      try:\n",
        "        ratio = float(guess) / float(true_answer)\n",
        "        if ratio >= 0.9 and ratio <= 1.1:\n",
        "          score += 0.5\n",
        "        elif ratio >= 0.8 and ratio <= 1.2:\n",
        "          score += 0.25\n",
        "        else:\n",
        "          score -= 1.0  # Penalize wrong answers\n",
        "      except:\n",
        "        score -= 0.5  # Penalize\n",
        "    scores.append(score)\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nIpOVv78Tn1k",
      "metadata": {},
      "source": [
        "Sometimes, the text between `<answer>` and `</answer>` might not be one\n",
        "number; it can be a sentence. So, we extract the number and compare the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NXvRtbk8Tn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "match_numbers = re.compile(\n",
        "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\", flags=re.MULTILINE | re.DOTALL\n",
        ")\n",
        "match_numbers.findall(f\"{solution_start}  0.34  {solution_end}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oxZQAFKOTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_numbers(prompts, completions, answer, **kargs):\n",
        "  question = kargs[\"question\"]\n",
        "  responses = completions\n",
        "\n",
        "  extracted_responses = [\n",
        "      guess.group(1) if (guess := match_numbers.search(r)) is not None else None\n",
        "      for r in responses\n",
        "  ]\n",
        "\n",
        "  scores = []\n",
        "  print(\"START ============================\")\n",
        "  print(f\"Question: {question[0]}\")\n",
        "  print(f\"Answer: {answer[0]}\")\n",
        "  print(f\"Response: {responses[0]}\")\n",
        "  print(f\"Extracted: {extracted_responses[0]}\")\n",
        "  print(\"END ==============================\")\n",
        "  for guess, true_answer in zip(extracted_responses, answer):\n",
        "    if guess is None:\n",
        "      scores.append(0)\n",
        "      continue\n",
        "    # Convert to numbers\n",
        "    try:\n",
        "      true_answer = float(true_answer.strip())\n",
        "      guess = float(guess.strip())\n",
        "      scores.append(1.5 if guess == true_answer else 0.0)\n",
        "    except:\n",
        "      scores.append(0)\n",
        "      continue\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AaiYMJxFTn1k",
      "metadata": {},
      "source": [
        "## Evaluate\n",
        "\n",
        "\n",
        "Before we train the model, let's evaluate the model on the test set so we can\n",
        "see the improvement post training.\n",
        "\n",
        "We evaluate it in two ways:\n",
        "\n",
        "**Quantitative**\n",
        "\n",
        "* **Answer Accuracy**: percentage of samples for which the model predicts the\n",
        "correct final numerical answer  \n",
        "* **Answer (Partial) Accuracy**: percentage of samples for which the model\n",
        "predicts a final numerical answer such that the \\`model answer / answer\\`\n",
        "ratio lies between 0.9 and 1.1.  \n",
        "* **Format Accuracy**: percentage of samples for which the model outputs the\n",
        "correct format, i.e., reasoning between the reasoning special tokens, and the\n",
        "final answer between the \\`\\<start\\_answer\\>\\`, \\`\\<end\\_answer\\>\\` tokens.\n",
        "\n",
        "**Qualitative**\n",
        "\n",
        "We'll also print outputs for a few given questions so that we can compare the generated output later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_k58bOicUHJy",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate(\n",
        "    question, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None\n",
        "):\n",
        "  \"\"\"Given prompt, generates text.\"\"\"\n",
        "\n",
        "  if isinstance(question, str):\n",
        "    input_batch = [\n",
        "        TEMPLATE.format(\n",
        "            system_prompt=SYSTEM_PROMPT,\n",
        "            question=question,\n",
        "        ),\n",
        "    ]\n",
        "  else:\n",
        "    input_batch = [\n",
        "        TEMPLATE.format(\n",
        "            system_prompt=SYSTEM_PROMPT,\n",
        "            question=q,\n",
        "        )\n",
        "        for q in question\n",
        "    ]\n",
        "\n",
        "  out_data = sampler(\n",
        "      input_strings=input_batch,\n",
        "      total_generation_steps=768,\n",
        "      temperature=temperature,\n",
        "      top_k=top_k,\n",
        "      top_p=top_p,\n",
        "      echo=False,\n",
        "      seed=seed if seed is not None else None,\n",
        "  )\n",
        "\n",
        "  output = out_data.text\n",
        "  if isinstance(question, str):\n",
        "    return output[0]\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yJo2nuKB-wlw",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    dataset,\n",
        "    sampler,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    num_passes=1,\n",
        "    corr_lst=False,\n",
        "    make_lst=False,\n",
        "):\n",
        "  \"\"\"Computes accuracy and percentage of outputs matching the format.\"\"\"\n",
        "\n",
        "  response_lst = []\n",
        "  corr = 0\n",
        "  partially_corr = 0\n",
        "  corr_format = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch in tqdm(dataset):\n",
        "    answers = batch[\"answer\"]\n",
        "    questions = batch[\"question\"]\n",
        "\n",
        "    multiple_call_responses = [[] for _ in range(len(questions))]\n",
        "    for p in range(num_passes):\n",
        "      responses = generate(\n",
        "          questions, sampler, temperature, top_k, top_p, seed=p\n",
        "      )\n",
        "      for idx, response in enumerate(responses):\n",
        "        multiple_call_responses[idx].append(response)\n",
        "\n",
        "    for question, multiple_call_response, answer in zip(\n",
        "        questions, multiple_call_responses, answers\n",
        "    ):\n",
        "      # check answer\n",
        "      corr_ctr_per_question = 0\n",
        "      partially_corr_per_question = 0\n",
        "      corr_format_per_question = 0\n",
        "      for response in multiple_call_response:\n",
        "        extracted_response = (\n",
        "            guess.group(1)\n",
        "            if (guess := match_numbers.search(response)) is not None\n",
        "            else \"-1000000\"\n",
        "        )\n",
        "        try:\n",
        "          if float(extracted_response.strip()) == float(answer.strip()):\n",
        "            corr_ctr_per_question += 1\n",
        "\n",
        "          ratio = float(extracted_response.strip()) / float(answer.strip())\n",
        "          if ratio >= 0.9 and ratio <= 1.1:\n",
        "            partially_corr_per_question += 1\n",
        "        except:\n",
        "          print(\"SKIPPED\")\n",
        "\n",
        "        # check format\n",
        "        if match_format.search(response) is not None:\n",
        "          corr_format_per_question += 1\n",
        "\n",
        "        if (\n",
        "            corr_ctr_per_question > 0\n",
        "            and partially_corr_per_question > 0\n",
        "            and corr_format_per_question > 0\n",
        "        ):\n",
        "          break\n",
        "\n",
        "      if corr_ctr_per_question > 0:\n",
        "        corr += 1\n",
        "        if corr_lst and make_lst:\n",
        "          response_lst.append((question, answer, multiple_call_response))\n",
        "      else:\n",
        "        if not corr_lst and make_lst:\n",
        "          response_lst.append((question, answer, multiple_call_response))\n",
        "      if partially_corr_per_question > 0:\n",
        "        partially_corr += 1\n",
        "      if corr_format_per_question > 0:\n",
        "        corr_format += 1\n",
        "\n",
        "      total += 1\n",
        "      if total % 10 == 0:\n",
        "        print(\n",
        "            f\"===> {corr=}, {total=}, {corr / total * 100=}, \"\n",
        "            f\"{partially_corr / total * 100=}, {corr_format / total * 100=}\"\n",
        "        )\n",
        "\n",
        "  to_return = (\n",
        "      corr,\n",
        "      total,\n",
        "      corr / total * 100,\n",
        "      partially_corr / total * 100,\n",
        "      corr_format / total * 100,\n",
        "  )\n",
        "  if make_lst:\n",
        "    return to_return, response_lst\n",
        "  return to_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HZMO-KflTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "gemma_tokenizer = data_lib.GemmaTokenizer()\n",
        "sampler = sampler_lib.Sampler(\n",
        "    # transformer=lora_gemma,\n",
        "    transformer=gemma_policy_maxtext_nnx,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YQM-tzXWUmoE",
      "metadata": {},
      "outputs": [],
      "source": [
        "(corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n",
        "    test_dataset,\n",
        "    sampler,\n",
        "    **GENERATION_CONFIGS[\"greedy\"],\n",
        ")\n",
        "print(\n",
        "    f\"{corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n",
        "    f\" {format_accuracy=}%\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PJV6wNGY-3PG",
      "metadata": {},
      "outputs": [],
      "source": [
        "# for eval_example in QUALITATIVE_EVAL_EXAMPLES:\n",
        "#   question = eval_example[\"question\"]\n",
        "#   answer = eval_example[\"answer\"]\n",
        "#   response = generate(\n",
        "#       question,\n",
        "#       sampler,\n",
        "#       temperature=INFERENCE_TEMPERATURE,\n",
        "#       top_k=INFERENCE_TOP_K,\n",
        "#       top_p=INFERENCE_TOP_P,\n",
        "#   )\n",
        "\n",
        "#   print(f\"Question:\\n{question}\")\n",
        "#   print(f\"Answer:\\n{answer}\")\n",
        "#   print(f\"Response:\\n{response}\")\n",
        "#   print(\"===============\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-CmB2ZT9Tn1l",
      "metadata": {},
      "source": [
        "## Train\n",
        "\n",
        "Let's set up all the configs first - checkpointing, metric logging and training.\n",
        "We then train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mHzdsYsGTn1l",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ckpt saving\n",
        "checkpointing_options = ocp.CheckpointManagerOptions(\n",
        "    save_interval_steps=SAVE_INTERVAL_STEPS, max_to_keep=MAX_TO_KEEP\n",
        ")\n",
        "\n",
        "# Metrics logger\n",
        "metrics_logging_options = metrics_logger.MetricsLoggerOptions(\n",
        "    log_dir=\"/content/tmp/tensorboard/grpo\", flush_every_n_steps=20\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u1Sc1fNC_CJ7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logs\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/tmp/tensorboard/grpo --port=0"
      ]
    },
    {
      "metadata": {},
      "id": "_6VxFW1ZTn1l",
      "cell_type": "code",
      "execution_count": null,
      "id": "_6VxFW1ZTn1l",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training config\n",
        "training_config = GrpoTrainingConfig(\n",
        "    max_prompt_length=MAX_PROMPT_LENGTH,\n",
        "    total_generation_steps=TOTAL_GENERATION_STEPS,\n",
        "    num_generations=NUM_GENERATIONS,\n",
        "    num_iterations=NUM_ITERATIONS,\n",
        "    beta=BETA,\n",
        "    epsilon=EPSILON,\n",
        "    temperature=TEMPERATURE,\n",
        "    top_p=TOP_P,\n",
        "    top_k=TOP_K,\n",
        "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "    max_steps=MAX_STEPS,\n",
        "    # metrics logging\n",
        "    metrics_logging_options=metrics_logging_options,\n",
        "    # checkpoint saving\n",
        "    checkpoint_root_directory=CKPT_DIR,\n",
        "    checkpointing_options=checkpointing_options,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "id": "OIe1lO08Tn1l",
      "cell_type": "code",
      "source": [
        "# Sampler\n",
        "gemma_tokenizer = data_lib.GemmaTokenizer()\n",
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_gemma,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Optimizer, learning rate scheduler, gradient clipping\n",
        "optimizer = optax.adamw(\n",
        "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
        "        init_value=0.0,\n",
        "        peak_value=LEARNING_RATE,\n",
        "        warmup_steps=WARMUP_STEPS,\n",
        "        decay_steps=MAX_STEPS,\n",
        "        end_value=0.0,\n",
        "    ),\n",
        "    b1=B1,\n",
        "    b2=B2,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        ")\n",
        "if MAX_GRAD_NORM is not None:\n",
        "  optimizer = optax.chain(\n",
        "      optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n",
        "      optimizer,\n",
        "  )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "id": "_6VxFW1ZTn1l",
      "cell_type": "code",
      "source": [
        "# Training config\n",
        "cluster_config = rl_cluster_lib.ClusterConfig(\n",
        "    role_to_mesh={\n",
        "        rl_cluster_lib.Role.ACTOR: mesh,\n",
        "        rl_cluster_lib.Role.REFERENCE: mesh,\n",
        "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
        "    },\n",
        "    rollout_engine='vanilla',\n",
        "    offload_to_cpu=False,\n",
        "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
        "        actor_optimizer=optimizer,\n",
        "        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "        max_steps=MAX_STEPS,\n",
        "        gradient_accumulation_steps=1,\n",
        "        # metrics logging\n",
        "        metrics_logging_options=metrics_logging_options,\n",
        "        # checkpoint saving\n",
        "        checkpoint_root_directory=CKPT_DIR,\n",
        "        checkpointing_options=checkpointing_options,\n",
        "    ),\n",
        "    rollout_config=base_rollout.RolloutConfig(\n",
        "        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n",
        "        max_prompt_length=MAX_PROMPT_LENGTH,\n",
        "        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_p=TOP_P,\n",
        "        top_k=TOP_K,\n",
        "    ),\n",
        ")\n",
        "\n",
        "grpo_config = GrpoConfig(\n",
        "    num_generations=NUM_GENERATIONS,\n",
        "    num_iterations=NUM_ITERATIONS,\n",
        "    beta=BETA,\n",
        "    epsilon=EPSILON,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "id": "OIe1lO08Tn1l",
      "cell_type": "code",
      "source": [
        "# RL cluster\n",
        "rl_cluster = rl_cluster_lib.RLCluster(\n",
        "    actor=lora_gemma,\n",
        "    reference=gemma,\n",
        "    tokenizer=data_lib.GemmaTokenizer(),\n",
        "    cluster_config=cluster_config,\n",
        ")\n",
        "\n",
        "# GRPO Trainer\n",
        "grpo_trainer = GrpoLearner(\n",
        "    rl_cluster=rl_cluster,\n",
        "    reward_fns=[\n",
        "        match_format_exactly,\n",
        "        match_format_approximately,\n",
        "        check_answer,\n",
        "        check_numbers,\n",
        "    ],\n",
        "    grpo_config=grpo_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S27XDebYTn1l",
      "metadata": {},
      "outputs": [],
      "source": [
        "with mesh:\n",
        "  grpo_trainer.train(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FzIP8glkTn1l",
      "metadata": {},
      "source": [
        "## Evaluate\n",
        "\n",
        "Let's evaluate our model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V-73HfP1Tn1l",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load checkpoint first.\n",
        "\n",
        "trained_ckpt_path = os.path.join(CKPT_DIR, str(MAX_STEPS), \"model_params\")\n",
        "\n",
        "abs_params = jax.tree.map(\n",
        "    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
        "    nnx.state(lora_gemma, nnx.LoRAParam),\n",
        ")\n",
        "checkpointer = ocp.StandardCheckpointer()\n",
        "trained_lora_params = checkpointer.restore(trained_ckpt_path, target=abs_params)\n",
        "\n",
        "nnx.update(\n",
        "    lora_gemma,\n",
        "    jax.tree.map(\n",
        "        lambda a, b: b,\n",
        "        nnx.state(lora_gemma, nnx.LoRAParam),\n",
        "        trained_lora_params,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1vY9kl-ITn1l",
      "metadata": {},
      "outputs": [],
      "source": [
        "gemma_tokenizer = data_lib.GemmaTokenizer()\n",
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_gemma,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nz0q_gGHqYz6",
      "metadata": {},
      "outputs": [],
      "source": [
        "(corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n",
        "    test_dataset,\n",
        "    sampler,\n",
        "    **GENERATION_CONFIGS[\"greedy\"],\n",
        ")\n",
        "print(\n",
        "    f\"{corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n",
        "    f\" {format_accuracy=}%\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jd9gpYVpUd3_",
      "metadata": {},
      "outputs": [],
      "source": [
        "# for eval_example in QUALITATIVE_EVAL_EXAMPLES:\n",
        "#   question = eval_example[\"question\"]\n",
        "#   answer = eval_example[\"answer\"]\n",
        "#   response = generate(\n",
        "#       question,\n",
        "#       sampler,\n",
        "#       temperature=INFERENCE_TEMPERATURE,\n",
        "#       top_k=INFERENCE_TOP_K,\n",
        "#       top_p=INFERENCE_TOP_P,\n",
        "#   )\n",
        "\n",
        "#   print(f\"Question:\\n{question}\")\n",
        "#   print(f\"Answer:\\n{answer}\")\n",
        "#   print(f\"Response:\\n{response}\")\n",
        "#   print(\"===============\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RrE_Rvaz_8CV",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
