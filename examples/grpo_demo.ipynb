{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "abdhOBYHqYz6",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/tunix/blob/main/examples/grpo_demo.ipynb\" ><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This tutorial demonstrates training the Gemma 2 2B-IT model on the GSM8K math\n",
        "reasoning benchmark using Group Relative Policy Optimization (GRPO). GRPO can\n",
        "enhance your model's problem-solving skills on mathematical word problems,\n",
        "coding problems, etc.\n",
        "\n",
        "GRPO is an RL algorithm designed to enhance the reasoning abilities of LLMs. It\n",
        "is a variant of Proximal Policy Optimization (PPO) that reduces memory usage by\n",
        "eliminating the need for a separate value function model. GRPO works by\n",
        "generating multiple responses for a given prompt, evaluating these responses\n",
        "using a reward model, and then calculating a relative advantage based on the\n",
        "group's performance to update the policy.\n",
        "\n",
        "In this tutorial we use Colab's `v2-8` TPU. Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afofSj37qYz6",
      "metadata": {},
      "source": [
        "## Install necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "Z03GnyApTn1j",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Obtaining file:///home/mazumdera_google_com/tunix\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: jax in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tunix==0.0.0) (0.6.2)\n",
            "Requirement already satisfied: jaxtyping in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tunix==0.0.0) (0.3.2)\n",
            "Requirement already satisfied: flax in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tunix==0.0.0) (0.11.0)\n",
            "Requirement already satisfied: sentencepiece in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tunix==0.0.0) (0.2.0)\n",
            "Requirement already satisfied: tensorboardX in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tunix==0.0.0) (2.6.4)\n",
            "Requirement already satisfied: tqdm in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tunix==0.0.0) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->tunix==0.0.0) (2.1.3)\n",
            "Requirement already satisfied: msgpack in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->tunix==0.0.0) (1.1.1)\n",
            "Requirement already satisfied: optax in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->tunix==0.0.0) (0.2.5)\n",
            "Requirement already satisfied: orbax-checkpoint in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->tunix==0.0.0) (0.11.20)\n",
            "Requirement already satisfied: tensorstore in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->tunix==0.0.0) (0.1.76)\n",
            "Requirement already satisfied: rich>=11.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->tunix==0.0.0) (14.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->tunix==0.0.0) (4.14.1)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->tunix==0.0.0) (6.0.2)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from flax->tunix==0.0.0) (0.1.9)\n",
            "Requirement already satisfied: jaxlib<=0.6.2,>=0.6.2 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from jax->tunix==0.0.0) (0.6.2)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from jax->tunix==0.0.0) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from jax->tunix==0.0.0) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.12 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from jax->tunix==0.0.0) (1.16.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from rich>=11.1->flax->tunix==0.0.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from rich>=11.1->flax->tunix==0.0.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax->tunix==0.0.0) (0.1.2)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.3 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from jaxtyping->tunix==0.0.0) (0.1.7)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from optax->flax->tunix==0.0.0) (2.3.1)\n",
            "Requirement already satisfied: chex>=0.1.87 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from optax->flax->tunix==0.0.0) (0.1.90)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from chex>=0.1.87->optax->flax->tunix==0.0.0) (1.0.0)\n",
            "Requirement already satisfied: etils[epath,epy] in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint->flax->tunix==0.0.0) (1.13.0)\n",
            "Requirement already satisfied: nest_asyncio in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint->flax->tunix==0.0.0) (1.6.0)\n",
            "Requirement already satisfied: aiofiles in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint->flax->tunix==0.0.0) (24.1.0)\n",
            "Requirement already satisfied: protobuf in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint->flax->tunix==0.0.0) (5.29.5)\n",
            "Requirement already satisfied: humanize in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint->flax->tunix==0.0.0) (4.12.3)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from orbax-checkpoint->flax->tunix==0.0.0) (3.20.1)\n",
            "Requirement already satisfied: fsspec in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from etils[epath,epy]->orbax-checkpoint->flax->tunix==0.0.0) (2025.7.0)\n",
            "Requirement already satisfied: importlib_resources in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from etils[epath,epy]->orbax-checkpoint->flax->tunix==0.0.0) (6.5.2)\n",
            "Requirement already satisfied: zipp in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from etils[epath,epy]->orbax-checkpoint->flax->tunix==0.0.0) (3.23.0)\n",
            "Requirement already satisfied: packaging in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from tensorboardX->tunix==0.0.0) (25.0)\n",
            "Building wheels for collected packages: tunix\n",
            "  Building editable for tunix (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for tunix: filename=tunix-0.0.0-0.editable-py3-none-any.whl size=8832 sha256=8f713fbc8815f6ad8b1c990a70562146cf9b438de0ff6f7450f45884e4e97e4b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-a93iloxf/wheels/11/21/09/69b89c4beb2859f209f80d2843c639c5349c106c5659c045a0\n",
            "Successfully built tunix\n",
            "Installing collected packages: tunix\n",
            "  Attempting uninstall: tunix\n",
            "    Found existing installation: tunix 0.0.0\n",
            "    Uninstalling tunix-0.0.0:\n",
            "      Successfully uninstalled tunix-0.0.0\n",
            "Successfully installed tunix-0.0.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kagglehub\n",
        "\n",
        "!pip install -q tensorflow\n",
        "!pip install -q tensorboardX\n",
        "!pip install -q grain\n",
        "!pip install -q jax==0.6.2 jaxlib==0.6.2\n",
        "# !pip install -q git+https://github.com/google/tunix\n",
        "! pip install -e ~/tunix/\n",
        "!pip install -q git+https://github.com/google/qwix\n",
        "\n",
        "!pip uninstall -q -y flax\n",
        "!pip install -q git+https://github.com/google/flax.git\n",
        "\n",
        "!pip install -q tensorflow-datasets\n",
        "\n",
        "!pip install -q git+https://github.com/AI-Hypercomputer/pathways-utils.git\n",
        "\n",
        "! pip install -q ~/tunix/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f95eb96c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipywidgets in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (8.1.7)\n",
            "Requirement already satisfied: comm>=0.1.3 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from ipywidgets) (9.4.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from ipywidgets) (4.0.14)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: decorator in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
            "Requirement already satisfied: ipython-pygments-lexers in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: stack_data in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
            "Requirement already satisfied: typing_extensions>=4.6 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.14.1)\n",
            "Requirement already satisfied: wcwidth in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: executing>=1.2.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in /home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LnF9ZACiTn1k",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "McTNo_r8Tn1k",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages/jax/__init__.py:31: UserWarning: cloud_tpu_init failed: AttributeError(\"module 'libtpu' has no attribute 'get_library_path'\")\n",
            " This a JAX bug; please report an issue at https://github.com/jax-ml/jax/issues\n",
            "  _warn(f\"cloud_tpu_init failed: {exc!r}\\n This a JAX bug; please report \"\n"
          ]
        }
      ],
      "source": [
        "import functools\n",
        "import gc\n",
        "import os\n",
        "from pprint import pprint\n",
        "import re\n",
        "import time\n",
        "\n",
        "from flax import nnx\n",
        "import grain\n",
        "import humanize\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import kagglehub\n",
        "import optax\n",
        "from orbax import checkpoint as ocp\n",
        "from qwix import lora\n",
        "import tensorflow_datasets as tfds\n",
        "from tqdm.auto import tqdm\n",
        "from tunix.generate import sampler as sampler_lib\n",
        "from tunix.models.gemma import data as data_lib\n",
        "from tunix.models.gemma import gemma as gemma_lib\n",
        "from tunix.models.gemma import params as params_lib\n",
        "from tunix.rl import rl_cluster as rl_cluster_lib\n",
        "from tunix.rl.rollout import base_rollout\n",
        "from tunix.rl.grpo.grpo_learner import GrpoConfig, GrpoLearner\n",
        "from tunix.sft import metrics_logger\n",
        "\n",
        "os.environ['TPU_LIBRARY_PATH'] = '/home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages/libtpu/libtpu.so'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Eu_NI9nHTn1k",
      "metadata": {},
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Let's define the configuration we are going to use. Note that this is by no\n",
        "means a \"perfect\" set of hyperparameters. To get good results, you might have\n",
        "to train the model for longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ZPPKme47Tn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====== Data ======\n",
        "TRAIN_DATA_DIR = \"./data/train\"\n",
        "TEST_DATA_DIR = \"./data/test\"\n",
        "TRAIN_FRACTION = 1.0\n",
        "\n",
        "# ====== LoRA ======\n",
        "RANK = 64\n",
        "ALPHA = 64.0\n",
        "\n",
        "# ====== Sharding ======\n",
        "MESH = [(1, 4), (\"fsdp\", \"tp\")]\n",
        "\n",
        "# ====== GRPO ======\n",
        "# === Generation during GRPO training ===\n",
        "MAX_PROMPT_LENGTH = 256\n",
        "TOTAL_GENERATION_STEPS = 768\n",
        "# Important to keep a high-ish temperature for varied, diverse responses during\n",
        "# training.\n",
        "TEMPERATURE = 0.9\n",
        "TOP_P = 1.0\n",
        "TOP_K = 50\n",
        "# The number of times the policy generates multiple responses for a given prompt\n",
        "# within a single training step. This corresponds to `G` in Algorithm 1 in the\n",
        "# paper. The \"group\" in GRPO comes from here.\n",
        "NUM_GENERATIONS = 2\n",
        "\n",
        "# === other GRPO configs ===\n",
        "# The number of iterations per batch (ùúá in GRPO algo 1).\n",
        "NUM_ITERATIONS = 1\n",
        "# The coefficient for the KL divergence penalty (ùõΩ) in the GRPO loss function.\n",
        "# Important to keep a high enough value for this, otherwise, the KL divergence\n",
        "# can increase unchecked.\n",
        "BETA = 0.08\n",
        "# Epsilon value for clipping (ùúÄ in GRPO loss in paper). Similar to PPO, for\n",
        "# stable updates.\n",
        "EPSILON = 0.2\n",
        "\n",
        "# ====== Training ======\n",
        "BATCH_SIZE = 1\n",
        "# Increase `NUM_BATCHES` and `MAX_STEPS` for better results.\n",
        "NUM_BATCHES = 3738\n",
        "# Keep `NUM_TEST_BATCHES` low so that evaluation runs quickly. It can be\n",
        "# increased to a max. of 330 (if batch size is 4).\n",
        "NUM_TEST_BATCHES = 5 #100 #Anisha: making it small for quick eval\n",
        "\n",
        "EVAL_EVERY_N_STEPS = 10  # this doesn't matter if `TRAIN_FRACTION = 1.0`.\n",
        "NUM_EPOCHS = 1  # can potentially train for more epochs\n",
        "\n",
        "# Number of training steps.\n",
        "MAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\n",
        "\n",
        "# === AdamW, warmup, cosine scheduler ===\n",
        "LEARNING_RATE = 3e-6\n",
        "B1 = 0.9\n",
        "B2 = 0.99\n",
        "WEIGHT_DECAY = 0.1\n",
        "# == Cosine decay with warmup scheduler ==\n",
        "# Linearly increase learning rate from 0. to 5e-6 in the first 10% training\n",
        "# steps, and then gradually decrease the learning rate to 0 using cosine\n",
        "# scheduler.\n",
        "WARMUP_STEPS = 0.1 * MAX_STEPS\n",
        "# == Grad clipping ==\n",
        "# Grad clipping to prevent large gradients. Found this\n",
        "# important to keep KL divergence in check.\n",
        "MAX_GRAD_NORM = 0.1\n",
        "\n",
        "# Checkpoint saving\n",
        "INTERMEDIATE_CKPT_DIR = \"/home/mazumdera_google_com/content/intermediate_ckpt/\"\n",
        "CKPT_DIR = \"/home/mazumdera_google_com/content/ckpts/\"\n",
        "SAVE_INTERVAL_STEPS = 500\n",
        "MAX_TO_KEEP = 4\n",
        "\n",
        "# ====== Inference ======\n",
        "GENERATION_CONFIGS = {\n",
        "    # greedy search\n",
        "    \"greedy\": {\"temperature\": 1e-4, \"top_k\": 1, \"top_p\": 1.0},\n",
        "    # some randomness\n",
        "    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
        "    # liberal\n",
        "    \"liberal\": {\"temperature\": 0.85, \"top_k\": 2000, \"top_p\": 1.0},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ngjtE-63Tn1k",
      "metadata": {},
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "wjMFOr7aTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_hbm_usage():\n",
        "  \"\"\"Displays memory usage per device.\"\"\"\n",
        "  fmt_size = functools.partial(humanize.naturalsize, binary=True)\n",
        "\n",
        "  for d in jax.local_devices():\n",
        "    stats = d.memory_stats()\n",
        "    used = stats[\"bytes_in_use\"]\n",
        "    limit = stats[\"bytes_limit\"]\n",
        "    print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:%}) on {d}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6BtpYMlaTn1k",
      "metadata": {},
      "source": [
        "## Data preprocessing\n",
        "\n",
        "First, let's define some special tokens. We instruct the model to first reason\n",
        "between the `<reasoning>` and `</reasoning>` tokens. After\n",
        "reasoning, we expect it to provide the answer between the `<answer>` and\n",
        "`</answer>` tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "h6RGv1kSTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "reasoning_start = \"<reasoning>\"\n",
        "reasoning_end = \"</reasoning>\"\n",
        "solution_start = \"<answer>\"\n",
        "solution_end = \"</answer>\"\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT = f\"\"\"You are given a problem. Think about the problem and \\\n",
        "provide your reasoning. Place it between {reasoning_start} and \\\n",
        "{reasoning_end}. Then, provide the final answer (i.e., just one numerical \\\n",
        "value) between {solution_start} and {solution_end}.\"\"\"\n",
        "\n",
        "TEMPLATE = \"\"\"<start_of_turn>user\n",
        "{system_prompt}\n",
        "\n",
        "{question}<end_of_turn>\n",
        "<start_of_turn>model\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WASP9N5JTn1k",
      "metadata": {},
      "source": [
        "We use OpenAI's GSM8K dataset. GSM8K comprises grade school math word problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "gTGjcSMNTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_hash_answer(text: str) -> str | None:\n",
        "  if \"####\" not in text:\n",
        "    return None\n",
        "  return text.split(\"####\")[1].strip()\n",
        "\n",
        "\n",
        "def get_dataset(data_dir, split=\"train\") -> grain.MapDataset:\n",
        "  # Download data\n",
        "  if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "  data = tfds.data_source(\n",
        "      \"gsm8k\",\n",
        "      split=split,\n",
        "      data_dir=data_dir,\n",
        "      builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n",
        "      download=True,\n",
        "  )\n",
        "\n",
        "  dataset = (\n",
        "      grain.MapDataset.source(data)\n",
        "      .shuffle(seed=42)\n",
        "      .map(\n",
        "          lambda x: {\n",
        "              # passed to model forward pass\n",
        "              \"prompts\": TEMPLATE.format(\n",
        "                  system_prompt=SYSTEM_PROMPT,\n",
        "                  question=x[\"question\"].decode(\"utf-8\"),\n",
        "              ),\n",
        "              # passed to reward functions\n",
        "              \"question\": x[\"question\"].decode(\"utf-8\"),\n",
        "              # passed to reward functions\n",
        "              \"answer\": extract_hash_answer(x[\"answer\"].decode(\"utf-8\")),\n",
        "          }\n",
        "      )\n",
        "  )\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "KXhOL6GyTn1k",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3738, 0, 5)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = get_dataset(TRAIN_DATA_DIR, \"train\").batch(BATCH_SIZE)[:NUM_BATCHES]\n",
        "\n",
        "if TRAIN_FRACTION == 1.0:\n",
        "  train_dataset = dataset.repeat(NUM_EPOCHS)\n",
        "  val_dataset = None\n",
        "else:\n",
        "  train_dataset = dataset[: int(len(dataset) * TRAIN_FRACTION)]\n",
        "  train_dataset = train_dataset.repeat(NUM_EPOCHS)\n",
        "\n",
        "  val_dataset = dataset[int(len(dataset) * TRAIN_FRACTION) :].repeat(NUM_EPOCHS)\n",
        "\n",
        "test_dataset = get_dataset(TEST_DATA_DIR, \"test\").batch(BATCH_SIZE)[\n",
        "    :NUM_TEST_BATCHES\n",
        "]\n",
        "\n",
        "len(train_dataset), len(val_dataset) if val_dataset is not None else 0, len(\n",
        "    test_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k7n8L0VzTn1k",
      "metadata": {},
      "source": [
        "Let's see how one batch of the dataset looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5TF-wNQ2Tn1k",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'answer': array(['13'], dtype='<U2'),\n",
            " 'prompts': array(['<start_of_turn>user\\nYou are given a problem. Think about the problem and provide your reasoning. Place it between <reasoning> and </reasoning>. Then, provide the final answer (i.e., just one numerical value) between <answer> and </answer>.\\n\\nJane is painting her fingernails. She applies a base coat that takes 2 minutes to dry, two color coats that take 3 minutes each to dry, and a clear top coat that takes 5 minutes to dry. How many minutes total does Jane spend waiting for her nail polish to dry?<end_of_turn>\\n<start_of_turn>model'],\n",
            "      dtype='<U535'),\n",
            " 'question': array(['Jane is painting her fingernails. She applies a base coat that takes 2 minutes to dry, two color coats that take 3 minutes each to dry, and a clear top coat that takes 5 minutes to dry. How many minutes total does Jane spend waiting for her nail polish to dry?'],\n",
            "      dtype='<U260')}\n"
          ]
        }
      ],
      "source": [
        "for ele in train_dataset[:1]:\n",
        "  pprint(ele)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BZxBR7Y_Tn1k",
      "metadata": {},
      "source": [
        "## Load the policy model and the reference model\n",
        "\n",
        "The policy model is the model which is actually trained and whose weights are\n",
        "updated. The reference model is the model with which we compute KL divergence.\n",
        "This is to ensure that the policy updates are not huge and that it does not\n",
        "deviate too much from the reference model.\n",
        "\n",
        "Typically, the reference model is the base model, and the policy model is the\n",
        "same base model, but with LoRA parameters. Only the LoRA parameters are updated.\n",
        "\n",
        "Note: We perform full precision (fp32) training. You can, however, leverage\n",
        "Qwix for QAT.\n",
        "\n",
        "To load the model, you need to be on [Kaggle](https://www.kaggle.com/) and need\n",
        "to have agreed to the Gemma license\n",
        "[here](https://www.kaggle.com/models/google/gemma/flax/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "thp6hhqfTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Log in\n",
        "# if \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n",
        "#   kagglehub.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "srH2s_jzTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# kaggle_ckpt_path = kagglehub.model_download(\"google/gemma-2/flax/gemma2-2b-it\")\n",
        "kaggle_ckpt_path = kagglehub.model_download(\"google/gemma/flax/2b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cIFAxgVOTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # This is a workaround. The checkpoints on Kaggle don't work with NNX. So, we\n",
        "# # load the model, save the checkpoint locally, and then reload the model\n",
        "# # (sharded).\n",
        "# params = params_lib.load_and_format_params(\n",
        "#     os.path.join(kaggle_ckpt_path, \"gemma2-2b-it\")\n",
        "# )\n",
        "# gemma = gemma_lib.Transformer.from_params(params, version=\"2-2b-it\")\n",
        "# checkpointer = ocp.StandardCheckpointer()\n",
        "# _, state = nnx.split(gemma)\n",
        "# checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "JSz-XmQpTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Wait for the ckpt to save successfully.\n",
        "# time.sleep(60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "_w8kav8sTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Delete the intermediate model to save memory.\n",
        "# del params\n",
        "# del gemma\n",
        "# del state\n",
        "# gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52b18135",
      "metadata": {},
      "source": [
        "### Load MaxText model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b6aa758c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# add the parent directory (one level up) to sys.path\n",
        "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../../maxtext')))\n",
        "\n",
        "# ! pip install -r ../../maxtext/requirements.txt\n",
        "\n",
        "import MaxText as mt\n",
        "from MaxText import pyconfig\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eee062b3",
      "metadata": {},
      "source": [
        "#### Convert MaxText model to nnx (use a commit from MaxText repo prior to )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "m2KD-nmbTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from MaxText.integrations.tunix.tunix_utils import build_tunix_wrapper\n",
        "from flax import linen as nn\n",
        "\n",
        "def get_ref_maxtext_model():\n",
        "\n",
        "  #python3 -m MaxText.train MaxText/configs/base.yml base_output_directory=${BASE_OUTPUT_DIRECTORY} dataset_path=${DATASET_PATH} tokenizer_path=assets/tokenizer.gemma load_parameters_path=${CONVERTED_CHECKPOINT} per_device_batch_size=1 run_name=${FINETUNE_RUN_NAME} max_target_length=8192 steps=10 async_checkpointing=false model_name=gemma-2b checkpoint_period=5\n",
        "\n",
        "  #TODO: @mazumdera: change this to use Gemma2-2b-it\n",
        "  config = pyconfig.initialize(\n",
        "      [\"\", \"../../maxtext/MaxText/configs/base.yml\"], #TODO: @mazumdera: why decode.py?\n",
        "      base_output_directory=\"gs://dummy_output_dir\",  # This is not used in Tunix.\n",
        "      # run_name=\"test-tunix-maxtext-llama3-8b\",\n",
        "      run_name=\"test-tunix-maxtext-gemma-2b\",\n",
        "      # dataset_path=we use Tunix's dataset\n",
        "      # load_parameters_path=\"gs://maxtext-gemma/2b/\", #TODO: @mazumdera: change this to use checkpoint\n",
        "      # tokenizer_type=\"tiktoken\",\n",
        "      # tokenizer_path=\"assets/tokenizer_llama3.tiktoken\",\n",
        "      tokenizer_path=\"../../maxtext/assets/tokenizer.gemma\",\n",
        "      per_device_batch_size=8,\n",
        "      max_target_length=8192,\n",
        "      steps=10,\n",
        "      async_checkpointing=\"false\",\n",
        "      # model_name=\"llama3.1-8b\",\n",
        "      model_name=\"gemma-2b\",\n",
        "      checkpoint_period=5,\n",
        "      skip_jax_distributed_system=\"true\",\n",
        "      weight_dtype=\"bfloat16\",\n",
        "      attention=\"dot_product\"\n",
        "\n",
        "  )\n",
        "  \n",
        "  def create_model(config):\n",
        "    return mt.from_pretrained(config, rngs=nnx.Rngs(params=0, dropout=1))\n",
        "\n",
        "  model = nnx.eval_shape(create_model, config=config)\n",
        "\n",
        "  abstract_model = nnx.eval_shape(create_model, config=config)\n",
        "  graphdef, abstract_state = nnx.split(abstract_model)\n",
        "  print('The abstract NNX state (all leaves are abstract arrays):')\n",
        "  nnx.display(abstract_state)\n",
        "  checkpoint = mt.checkpointing.load_params_from_path(\n",
        "      load_parameters_from_path=\"gs://maxtext-gemma/2b/2025-08-05-04-37/0/items\",\n",
        "      abstract_unboxed_params=None,\n",
        "      checkpoint_storage_concurrent_gb=None,\n",
        "  )\n",
        "  print(\"{checkpoint=}\")\n",
        "  checkpoint = {}\n",
        "\n",
        "  @nnx.jit\n",
        "  def partial_init(checkpoint, config):\n",
        "    model = create_model(config)\n",
        "    nnx.update(model, checkpoint)\n",
        "    # shard model\n",
        "    state = nnx.state(model)\n",
        "    specs = nnx.get_partition_spec(state)\n",
        "    state = jax.lax.with_sharding_constraint(state, specs)\n",
        "    nnx.update(model, state)\n",
        "    return model\n",
        "\n",
        "  with jax.sharding.use_mesh(model.mesh), nn.logical_axis_rules(config.logical_axis_rules):\n",
        "    model = partial_init(checkpoint, config)\n",
        "  print(model)\n",
        "\n",
        "  \n",
        "  tunix_model = TunixMaxTextLlama(\n",
        "        base_model=model,\n",
        "        use_attention_mask=False,  # trust Tunix loss masking\n",
        "    )\n",
        "  mesh  = tunix_model.base.mesh\n",
        "  \n",
        "  #TODO: @mazumdera: change this to use llama3.1-8b\n",
        "  # model_config = None\n",
        "  # We can continue to use Tunix's model_config\n",
        "  model_config = gemma_lib.TransformerConfig.gemma2_2b()\n",
        "\n",
        "  # Add these lines to properly get the graph definition and state\n",
        "  graphdef, state = nnx.split(tunix_model)\n",
        "  tunix_model = nnx.merge(graphdef, state)  # Recreate model in proper NNX format\n",
        "    \n",
        "  \n",
        "  return tunix_model, mesh, model_config\n",
        "\n",
        "# def get_ref_maxtext_model(config, mesh=None):\n",
        "\n",
        "#   #python3 -m MaxText.train MaxText/configs/base.yml base_output_directory=${BASE_OUTPUT_DIRECTORY} dataset_path=${DATASET_PATH} tokenizer_path=assets/tokenizer.gemma load_parameters_path=${CONVERTED_CHECKPOINT} per_device_batch_size=1 run_name=${FINETUNE_RUN_NAME} max_target_length=8192 steps=10 async_checkpointing=false model_name=gemma-2b checkpoint_period=5\n",
        "\n",
        "  \n",
        "#   #TODO: Anisha: \n",
        "#   # model = mt.from_pretrained(config)\n",
        "  \n",
        "#   rngs = nnx.Rngs(1234)\n",
        "#   model = build_tunix_wrapper(\n",
        "#         config,\n",
        "#         rngs,\n",
        "#         enable_dropout=False,   # deterministic SFT (you can override at runtime)\n",
        "#         init_batch_size=1,\n",
        "#         init_seq_len=1,\n",
        "#         use_attention_mask=False,  # trust Tunix loss masking\n",
        "#     )\n",
        "#   mesh  = model.base.mesh\n",
        "  \n",
        "\n",
        "#   # We can continue to use Tunix's model_config\n",
        "#   model_config = gemma_lib.TransformerConfig.gemma2_2b()\n",
        "\n",
        "#   # Add these lines to properly get the graph definition and state\n",
        "#   graphdef, state = nnx.split(model)\n",
        "#   model = nnx.merge(graphdef, state)  # Recreate model in proper NNX format\n",
        "\n",
        "    \n",
        "  \n",
        "#   return model, mesh, model_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "kSdZ7aGhTn1k",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updating keys from env and command line: ['run_name', 'model_name', 'async_checkpointing', 'checkpoint_period', 'weight_dtype', 'attention', 'base_output_directory', 'tokenizer_path', 'per_device_batch_size', 'steps', 'skip_jax_distributed_system', 'max_target_length']\n",
            "Running Model: gemma-2b\n",
            "Updating following parameters in config\n",
            "\n",
            "base_emb_dim: 2048\n",
            "base_num_query_heads: 8\n",
            "base_num_kv_heads: 1\n",
            "base_mlp_dim: 16384\n",
            "base_num_decoder_layers: 18\n",
            "head_dim: 256\n",
            "mlp_activations: ['gelu', 'linear']\n",
            "vocab_size: 256128\n",
            "decoder_block: gemma\n",
            "normalization_layer_epsilon: 1e-06\n",
            "logits_via_embedding: True\n",
            "Updating keys from model: ['base_emb_dim', 'base_num_query_heads', 'base_num_kv_heads', 'base_mlp_dim', 'base_num_decoder_layers', 'head_dim', 'mlp_activations', 'vocab_size', 'decoder_block', 'normalization_layer_epsilon', 'logits_via_embedding']\n",
            "Skipping jax distributed system due to skip_jax_distributed_system=True flag.\n",
            "Not using emergency checkpoint, ignoring local_checkpoint_directory, local_checkpoint_period, use_replicator_service and replicator_backup_interval_minutes\n",
            "dataset_type set to tfds, will use keys['dataset_path']='' and keys['dataset_name']='c4/en:3.0.1'\n",
            "Config param activations_in_float32: False\n",
            "Config param adam_b1: 0.9\n",
            "Config param adam_b2: 0.95\n",
            "Config param adam_eps: 1e-08\n",
            "Config param adam_eps_root: 0.0\n",
            "Config param adam_weight_decay: 0.1\n",
            "Config param add_bos: True\n",
            "Config param add_eos: True\n",
            "Config param allow_split_physical_axes: False\n",
            "Config param ar_cache_axis_order: 1,2,0,3\n",
            "Config param async_checkpointing: False\n",
            "Config param attention: dot_product\n",
            "Config param attention_type: global\n",
            "Config param attn_logits_soft_cap: None\n",
            "Config param autoregressive_decode_assert: \n",
            "Config param base_emb_dim: 2048\n",
            "Config param base_mlp_dim: 16384\n",
            "Config param base_moe_mlp_dim: 7168\n",
            "Config param base_num_decoder_layers: 18\n",
            "Config param base_num_kv_heads: 1\n",
            "Config param base_num_query_heads: 8\n",
            "Config param base_output_directory: gs://dummy_output_dir\n",
            "Config param beta_fast: 32\n",
            "Config param beta_slow: 1\n",
            "Config param capacity_factor: -1.0\n",
            "Config param cast_logits_to_fp32: True\n",
            "Config param checkpoint_conversion_fn: None\n",
            "Config param checkpoint_dir: gs://dummy_output_dir/test-tunix-maxtext-gemma-2b/checkpoints/\n",
            "Config param checkpoint_is_quantized: False\n",
            "Config param checkpoint_period: 5\n",
            "Config param checkpoint_storage_concurrent_gb: 96\n",
            "Config param checkpoint_storage_target_data_file_size_bytes: 2147483648\n",
            "Config param checkpoint_storage_use_ocdbt: True\n",
            "Config param checkpoint_storage_use_zarr3: True\n",
            "Config param chunk_attn_window_size: 0\n",
            "Config param collect_stack_trace: False\n",
            "Config param colocated_python_data_input: False\n",
            "Config param compile_topology: \n",
            "Config param compile_topology_num_slices: -1\n",
            "Config param compiled_trainstep_file: \n",
            "Config param compute_axis_order: 0,1,2,3\n",
            "Config param constant_bound_config: []\n",
            "Config param context: remat\n",
            "Config param context_parallel_load_balance: True\n",
            "Config param cosine_learning_rate_final_fraction: 0.1\n",
            "Config param custom_mesh: \n",
            "Config param data_sharding: (('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'),)\n",
            "Config param data_shuffle_seed: 0\n",
            "Config param dataset_name: c4/en:3.0.1\n",
            "Config param dataset_path: \n",
            "Config param dataset_type: tfds\n",
            "Config param dcn_autoregressive_parallelism: 1\n",
            "Config param dcn_context_autoregressive_parallelism: 1\n",
            "Config param dcn_context_parallelism: 1\n",
            "Config param dcn_data_parallelism: -1\n",
            "Config param dcn_expert_parallelism: 1\n",
            "Config param dcn_fsdp_parallelism: 1\n",
            "Config param dcn_fsdp_transpose_parallelism: 1\n",
            "Config param dcn_parallelism: [-1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Config param dcn_pipeline_parallelism: 1\n",
            "Config param dcn_sequence_parallelism: 1\n",
            "Config param dcn_tensor_parallelism: 1\n",
            "Config param dcn_tensor_sequence_parallelism: 1\n",
            "Config param dcn_tensor_transpose_parallelism: 1\n",
            "Config param decode_sampling_nucleus_p: -1\n",
            "Config param decode_sampling_strategy: greedy\n",
            "Config param decode_sampling_temperature: 1.0\n",
            "Config param decode_sampling_top_k: 0\n",
            "Config param decoder_block: DecoderBlockType.GEMMA\n",
            "Config param decoder_layer_input: device\n",
            "Config param dpo_beta: 0.1\n",
            "Config param dpo_label_smoothing: 0.0\n",
            "Config param dropout_rate: 0.0\n",
            "Config param dtype: bfloat16\n",
            "Config param dtype_mm: float32\n",
            "Config param dump_hlo: False\n",
            "Config param dump_hlo_delete_local_after: True\n",
            "Config param dump_hlo_gcs_dir: \n",
            "Config param dump_hlo_local_dir: /tmp/xla_dump/\n",
            "Config param dump_hlo_module_name: jit_train_step\n",
            "Config param dump_hlo_upload_all: False\n",
            "Config param dump_hlo_xla_flags: \n",
            "Config param dump_step: -1\n",
            "Config param emb_dim: 2048\n",
            "Config param enable_checkpoint_cloud_logger: False\n",
            "Config param enable_checkpointing: True\n",
            "Config param enable_data_shuffling: True\n",
            "Config param enable_dropout: True\n",
            "Config param enable_emergency_checkpoint: False\n",
            "Config param enable_gcp_goodput_metrics: True\n",
            "Config param enable_gcp_step_deviation_metrics: True\n",
            "Config param enable_goodput_recording: False\n",
            "Config param enable_jax_profiler: False\n",
            "Config param enable_llm_inference_pool: False\n",
            "Config param enable_model_warmup: False\n",
            "Config param enable_orbax_v1: False\n",
            "Config param enable_padding_causal_mask: True\n",
            "Config param enable_pathways_goodput: False\n",
            "Config param enable_prefix_caching: False\n",
            "Config param enable_single_controller: False\n",
            "Config param enable_single_replica_ckpt_restoring: False\n",
            "Config param enable_tensorboard: True\n",
            "Config param eval_data_columns: ['text']\n",
            "Config param eval_dataset_name: c4/en:3.0.1\n",
            "Config param eval_image_column: image\n",
            "Config param eval_interval: -1\n",
            "Config param eval_per_device_batch_size: 8.0\n",
            "Config param eval_split: validation\n",
            "Config param eval_steps: -1\n",
            "Config param expansion_factor_real_data: -1\n",
            "Config param final_logits_soft_cap: None\n",
            "Config param first_num_dense_layers: 0\n",
            "Config param float32_logits: False\n",
            "Config param float32_qk_product: False\n",
            "Config param force_unroll: False\n",
            "Config param freeze_vision_encoder_params: True\n",
            "Config param fused_mlp: False\n",
            "Config param fused_qkv: False\n",
            "Config param gcs_metrics: False\n",
            "Config param generate_slice: v5e-16\n",
            "Config param global_batch_size_to_eval_on: 64\n",
            "Config param global_batch_size_to_load: 64\n",
            "Config param global_batch_size_to_load_eval: 64\n",
            "Config param global_batch_size_to_train_on: 64\n",
            "Config param global_parameter_scale: 1\n",
            "Config param goodput_upload_interval_seconds: 30\n",
            "Config param gradient_accumulation_steps: 1\n",
            "Config param gradient_clipping_threshold: 1.0\n",
            "Config param grain_eval_files: \n",
            "Config param grain_file_type: arrayrecord\n",
            "Config param grain_train_files: \n",
            "Config param grain_worker_count: 1\n",
            "Config param grain_worker_count_eval: 1\n",
            "Config param hardware: tpu\n",
            "Config param head_dim: 256\n",
            "Config param heartbeat_reporting_interval_in_seconds: 5\n",
            "Config param hf_data_dir: \n",
            "Config param hf_eval_files: \n",
            "Config param hf_eval_split: \n",
            "Config param hf_path: \n",
            "Config param hf_train_files: \n",
            "Config param hidden_size_for_vit: 1408\n",
            "Config param ici_autoregressive_parallelism: 1\n",
            "Config param ici_context_autoregressive_parallelism: 1\n",
            "Config param ici_context_parallelism: 1\n",
            "Config param ici_data_parallelism: 1\n",
            "Config param ici_expert_parallelism: 1\n",
            "Config param ici_fsdp_parallelism: -1\n",
            "Config param ici_fsdp_transpose_parallelism: 1\n",
            "Config param ici_parallelism: [1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Config param ici_pipeline_parallelism: 1\n",
            "Config param ici_sequence_parallelism: 1\n",
            "Config param ici_tensor_parallelism: 1\n",
            "Config param ici_tensor_sequence_parallelism: 1\n",
            "Config param ici_tensor_transpose_parallelism: 1\n",
            "Config param image_path: \n",
            "Config param image_placeholder: <|image|>\n",
            "Config param image_size_for_vit: 896\n",
            "Config param inference_benchmark_test: False\n",
            "Config param inference_metadata_file: \n",
            "Config param inference_microbenchmark_log_file_path: \n",
            "Config param inference_microbenchmark_loop_iters: 10\n",
            "Config param inference_microbenchmark_num_samples: [1, 2, 3, 4, 5]\n",
            "Config param inference_microbenchmark_prefill_lengths: 64,128,256,512,1024\n",
            "Config param inference_microbenchmark_stages: prefill,generate\n",
            "Config param inference_server: MaxtextInterleavedServer\n",
            "Config param inhomogeneous_layer_cycle_interval: 1\n",
            "Config param init_weights_seed: 0\n",
            "Config param input_data_sharding_logical_axes: ['activation_embed_and_logits_batch', 'activation_norm_length']\n",
            "Config param interleave_moe_layer_step: 1\n",
            "Config param intermediate_size_for_vit: 5632\n",
            "Config param jax_cache_dir: ~/jax_cache\n",
            "Config param jax_debug_log_modules: \n",
            "Config param jax_distributed_initialization_timeout: 300\n",
            "Config param jax_profiler_port: 9999\n",
            "Config param key_proj: remat\n",
            "Config param kv_lora_rank: 512\n",
            "Config param kv_quant_axis: heads_and_dkv\n",
            "Config param kv_quant_dtype: int8\n",
            "Config param learning_rate: 3e-05\n",
            "Config param learning_rate_schedule_steps: 10\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config param load_balance_loss_weight: 0.01\n",
            "Config param load_from_prefill_dir: False\n",
            "Config param load_full_state_path: \n",
            "Config param load_parameters_path: \n",
            "Config param local_checkpoint_directory: \n",
            "Config param local_checkpoint_period: 0\n",
            "Config param local_rope_max_timescale: -1\n",
            "Config param log_config: True\n",
            "Config param log_period: 100\n",
            "Config param logical_axis_rules: (('activation_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_batch_no_exp', ('data', 'fsdp', 'fsdp_transpose')), ('activation_embed_and_logits_batch', ('data', 'stage', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence', 'autoregressive')), ('activation_kv_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_length', ('sequence', 'context')), ('prefill_activation_length', ('sequence', 'context')), ('activation_length', ('context',)), ('activation_norm_length', ('tensor_sequence', 'context', 'sequence')), ('prefill_activation_norm_length', ('tensor_sequence', 'context', 'sequence')), ('activation_q_length', ('context',)), ('activation_kv_length', ()), ('activation_embed', ('tensor', 'tensor_transpose')), ('activation_mlp', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_kv', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_prefill_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_head_dim', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose')), ('activation_vocab', 'tensor_sequence'), ('activation_vocab', ('sequence', 'context')), ('activation_stage', 'stage'), ('activation_exp', ('expert',)), ('decode_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('decode_length', ('sequence',)), ('mlp', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), ('vocab', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('q_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('kv_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'context', 'expert')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'context')), ('embed_tensor_transpose', ('tensor_transpose',)), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'expert')), ('norm', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('layers', 'stage'), ('kv', ()), ('kv_head_dim', ()), ('cache_batch_prefill', ()), ('cache_batch', ()), ('cache_heads_none', ()), ('cache_heads', ('autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence')), ('cache_heads', ('autoregressive', 'tensor', 'tensor_sequence')), ('cache_kv', ()), ('cache_sequence', ()), ('exp', 'expert'), ('paged_kv_heads', ('tensor',)), ('num_pages', ()), ('tokens_per_page', ()), ('paged_kv_head_dim_size', ()))\n",
            "Config param logits_dot_in_fp32: False\n",
            "Config param logits_via_embedding: True\n",
            "Config param lora_input_adapters_path: \n",
            "Config param matmul_precision: default\n",
            "Config param max_checkify: False\n",
            "Config param max_corpus_chars: 10000000\n",
            "Config param max_position_embeddings: 163840\n",
            "Config param max_prefill_predict_length: 64\n",
            "Config param max_target_length: 8192\n",
            "Config param megablox: True\n",
            "Config param mesh_axes: ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive']\n",
            "Config param metrics_dir: gs://dummy_output_dir/test-tunix-maxtext-gemma-2b/metrics/\n",
            "Config param metrics_file: \n",
            "Config param micro_batch_size_to_eval_on: 64\n",
            "Config param micro_batch_size_to_train_on: 64\n",
            "Config param mla_naive_kvcache: True\n",
            "Config param mlp_activations: ['gelu', 'linear']\n",
            "Config param mlp_dim: 16384\n",
            "Config param mlpwi: remat\n",
            "Config param mlpwi_0: remat\n",
            "Config param mlpwi_1: remat\n",
            "Config param mlpwo: remat\n",
            "Config param model_call_mode: \n",
            "Config param model_fsdp_ag_once: False\n",
            "Config param model_name: gemma-2b\n",
            "Config param moe_mlp_dim: 7168\n",
            "Config param monitor_goodput: False\n",
            "Config param monitor_step_time_deviation: True\n",
            "Config param mscale: 1.0\n",
            "Config param mtp_eval_target_module: 0\n",
            "Config param mtp_loss_scaling_factor: 0.1\n",
            "Config param mtp_num_layers: 0\n",
            "Config param mu_dtype: bfloat16\n",
            "Config param multi_sampling: False\n",
            "Config param n_routing_groups: -1\n",
            "Config param nope_layer_interval: -1\n",
            "Config param normalization_layer_epsilon: 1e-06\n",
            "Config param normalize_embedding_logits: True\n",
            "Config param num_attention_heads_for_vit: 16\n",
            "Config param num_channels_for_vit: 3\n",
            "Config param num_decoder_layers: 18\n",
            "Config param num_epoch: 1\n",
            "Config param num_experts: 1\n",
            "Config param num_experts_per_tok: 1\n",
            "Config param num_hidden_layers_for_vit: 34\n",
            "Config param num_kv_heads: 1\n",
            "Config param num_layers_per_pipeline_stage: 1\n",
            "Config param num_pipeline_microbatches: -1\n",
            "Config param num_pipeline_repeats: -1\n",
            "Config param num_query_heads: 8\n",
            "Config param num_slices: 1\n",
            "Config param opt_type: adamw\n",
            "Config param optimize_mesh_for_tpu_v6e: False\n",
            "Config param optimizer_memory_host_offload: False\n",
            "Config param original_max_position_embeddings: 4096\n",
            "Config param out_proj: remat\n",
            "Config param override_model_config: False\n",
            "Config param packing: True\n",
            "Config param pagedattn_head_dim_alignment: 128\n",
            "Config param pagedattn_max_pages_per_group: 256\n",
            "Config param pagedattn_num_pages: 64\n",
            "Config param pagedattn_pages_per_compute_block: 4\n",
            "Config param pagedattn_tokens_per_page: 32\n",
            "Config param param_scan_axis: 1\n",
            "Config param parameter_memory_host_offload: False\n",
            "Config param patch_size_for_vit: 14\n",
            "Config param per_device_batch_size: 8.0\n",
            "Config param pipeline_delay_activation_forwarding: False\n",
            "Config param pipeline_fsdp_ag_once: False\n",
            "Config param pipeline_parallel_layers: -1\n",
            "Config param pixel_shuffle_ratio_for_vit: 0.5\n",
            "Config param prefill_cache_axis_order: 1,2,0,3\n",
            "Config param prefill_cache_dir: \n",
            "Config param prefill_chunk_size: 256\n",
            "Config param prefill_slice: v5e-16\n",
            "Config param prefix_caching_dram_byte: 100000000000\n",
            "Config param prefix_caching_hbm_byte: 10000000000\n",
            "Config param profile_cleanly: True\n",
            "Config param profile_periodically_period: -1\n",
            "Config param profiler: \n",
            "Config param profiler_steps: 5\n",
            "Config param projector_dropout_for_vit: 0.0\n",
            "Config param projector_input_dim_for_vit: 4096\n",
            "Config param projector_output_dim_for_vit: 4096\n",
            "Config param prometheus_port: 0\n",
            "Config param prompt: I love to\n",
            "Config param q_lora_rank: 0\n",
            "Config param qk_nope_head_dim: 128\n",
            "Config param qk_rope_head_dim: 64\n",
            "Config param qkv_proj: remat\n",
            "Config param quant_cfg_path: \n",
            "Config param quantization: \n",
            "Config param quantization_calibration_method: absmax\n",
            "Config param quantization_local_shard_count: 1\n",
            "Config param quantize_kvcache: False\n",
            "Config param query_proj: remat\n",
            "Config param ragged_block_size: 256\n",
            "Config param record_internal_nn_metrics: 0\n",
            "Config param remat_policy: full\n",
            "Config param remat_policy_for_vit: minimal\n",
            "Config param replicate_quant_scale: False\n",
            "Config param replicator_backup_interval_minutes: 0\n",
            "Config param report_heartbeat_metric_for_gcp_monitoring: False\n",
            "Config param report_performance_metric_for_gcp_monitoring: False\n",
            "Config param reshape_q: False\n",
            "Config param return_log_prob: False\n",
            "Config param reuse_example_batch: 0\n",
            "Config param rope_factor: 40\n",
            "Config param rope_max_timescale: 10000\n",
            "Config param rope_min_timescale: 1\n",
            "Config param rope_theta_for_vit: 10000\n",
            "Config param rope_type: default\n",
            "Config param rope_use_scale: True\n",
            "Config param routed_bias: False\n",
            "Config param routed_scaling_factor: 1.0\n",
            "Config param routed_score_func: \n",
            "Config param run_name: test-tunix-maxtext-gemma-2b\n",
            "Config param sa_block_kv: 512\n",
            "Config param sa_block_kv_compute: 512\n",
            "Config param sa_block_kv_dkv: 512\n",
            "Config param sa_block_kv_dkv_compute: 512\n",
            "Config param sa_block_kv_dq: 512\n",
            "Config param sa_block_q: 512\n",
            "Config param sa_block_q_dkv: 512\n",
            "Config param sa_block_q_dq: 512\n",
            "Config param sa_k_layout: HEAD_DIM_MINOR\n",
            "Config param sa_q_layout: HEAD_DIM_MINOR\n",
            "Config param sa_use_fused_bwd_kernel: False\n",
            "Config param sa_v_layout: HEAD_DIM_MINOR\n",
            "Config param save_config_to_gcs: False\n",
            "Config param save_quantized_params_path: \n",
            "Config param scan_layers: True\n",
            "Config param scan_layers_per_stage: False\n",
            "Config param scan_pipeline_iterations: True\n",
            "Config param set_remat_policy_on_layers_per_stage: False\n",
            "Config param set_remat_policy_on_pipeline_iterations: True\n",
            "Config param sft_train_on_completion_only: False\n",
            "Config param sharding_tolerance: 0.02\n",
            "Config param shardy: True\n",
            "Config param shared_experts: 1\n",
            "Config param skip_first_n_steps_for_profiler: 1\n",
            "Config param skip_jax_distributed_system: True\n",
            "Config param sliding_window_size: 0\n",
            "Config param source_checkpoint_layout: orbax\n",
            "Config param sparse_matmul: True\n",
            "Config param stack_prefill_result_cache: False\n",
            "Config param stack_trace_interval_seconds: 600\n",
            "Config param stack_trace_to_cloud: False\n",
            "Config param step_deviation_interval_seconds: 30\n",
            "Config param steps: 10\n",
            "Config param subslice_shape: \n",
            "Config param target_eval_loss: 0.0\n",
            "Config param temperature_tuning: False\n",
            "Config param tensorboard_dir: gs://dummy_output_dir/test-tunix-maxtext-gemma-2b/tensorboard/\n",
            "Config param tile_activation_dim: 1024\n",
            "Config param tile_batch_seq: 512\n",
            "Config param tile_size_for_vit: 336\n",
            "Config param tile_weight_dim: 1024\n",
            "Config param tokenize_eval_data: True\n",
            "Config param tokenize_train_data: True\n",
            "Config param tokenizer_path: ../../maxtext/assets/tokenizer.gemma\n",
            "Config param tokenizer_type: sentencepiece\n",
            "Config param topk_routing_group: -1\n",
            "Config param train_data_columns: ['text']\n",
            "Config param train_image_column: image\n",
            "Config param train_split: train\n",
            "Config param trainable_position_size: -1\n",
            "Config param upload_all_profiler_results: False\n",
            "Config param use_chat_template: False\n",
            "Config param use_chunked_prefill: False\n",
            "Config param use_dpo: False\n",
            "Config param use_iota_embed: False\n",
            "Config param use_multimodal: False\n",
            "Config param use_post_attn_norm: False\n",
            "Config param use_post_ffw_norm: False\n",
            "Config param use_qk_norm: False\n",
            "Config param use_qwix_quantization: False\n",
            "Config param use_ragged_attention: False\n",
            "Config param use_random_routing: False\n",
            "Config param use_replicator_service: False\n",
            "Config param use_sft: False\n",
            "Config param use_untrainable_positional_embedding: False\n",
            "Config param use_vertex_tensorboard: False\n",
            "Config param using_pipeline_parallelism: False\n",
            "Config param v_head_dim: 128\n",
            "Config param value_proj: remat\n",
            "Config param vertex_tensorboard_project: \n",
            "Config param vertex_tensorboard_region: \n",
            "Config param vision_output_dim_for_vit: 4096\n",
            "Config param vocab_size: 256128\n",
            "Config param warmup_steps_fraction: 0.1\n",
            "Config param weight_dtype: bfloat16\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'__jax_array__'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Base model\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# gemma, mesh, model_config = get_base_model(\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#     ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMaxText\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegration\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtunix\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtunix_adaptor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TunixMaxTextLlama\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m gemma, mesh, model_config = \u001b[43mget_ref_maxtext_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# gemma_maxtext_nnx = nnx.bridge.ToNNX(gemma)\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Instead of:\u001b[39;00m\n\u001b[32m     10\u001b[39m nnx.display(gemma)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mget_ref_maxtext_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_model\u001b[39m(config):\n\u001b[32m     33\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m mt.from_pretrained(config, rngs=nnx.Rngs(params=\u001b[32m0\u001b[39m, dropout=\u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m model = \u001b[43mnnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m abstract_model = nnx.eval_shape(create_model, config=config)\n\u001b[32m     38\u001b[39m graphdef, abstract_state = nnx.split(abstract_model)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/flax/nnx/transforms/transforms.py:151\u001b[39m, in \u001b[36meval_shape\u001b[39m\u001b[34m(f, *args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m   out = f(*args, **kwargs)\n\u001b[32m    149\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m graph.to_arrays(extract.to_tree(out), allow_duplicates=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m out = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_eval_shape_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m extract.from_tree(out)\n",
            "    \u001b[31m[... skipping hidden 8 frame]\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/maxtext/MaxText/pyconfig.py:1142\u001b[39m, in \u001b[36mHyperParameters.__getattr__\u001b[39m\u001b[34m(self, attr)\u001b[39m\n\u001b[32m   1139\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr):\n\u001b[32m   1140\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1141\u001b[39m     \u001b[38;5;66;03m# Attempt to perform the normal lookup\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1142\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_config\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m[\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1143\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m   1144\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
            "\u001b[31mKeyError\u001b[39m: '__jax_array__'"
          ]
        }
      ],
      "source": [
        "# Base model\n",
        "# gemma, mesh, model_config = get_base_model(\n",
        "#     ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\n",
        "# )\n",
        "from MaxText.integration.tunix.tunix_adaptor import TunixMaxTextLlama\n",
        "\n",
        "gemma, mesh, model_config = get_ref_maxtext_model()\n",
        "# gemma_maxtext_nnx = nnx.bridge.ToNNX(gemma)\n",
        "# Instead of:\n",
        "nnx.display(gemma)\n",
        "\n",
        "# Use:\n",
        "print(\"Model initialized successfully\")\n",
        "print(f\"Model mesh shape: {mesh.shape}\")\n",
        "print(f\"Model config: {model_config}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f04a92e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updating keys from env and command line: ['run_name', 'model_name', 'async_checkpointing', 'checkpoint_period', 'weight_dtype', 'attention', 'base_output_directory', 'tokenizer_path', 'per_device_batch_size', 'steps', 'skip_jax_distributed_system', 'max_target_length']\n",
            "Running Model: gemma-2b\n",
            "Updating following parameters in config\n",
            "\n",
            "base_emb_dim: 2048\n",
            "base_num_query_heads: 8\n",
            "base_num_kv_heads: 1\n",
            "base_mlp_dim: 16384\n",
            "base_num_decoder_layers: 18\n",
            "head_dim: 256\n",
            "mlp_activations: ['gelu', 'linear']\n",
            "vocab_size: 256128\n",
            "decoder_block: gemma\n",
            "normalization_layer_epsilon: 1e-06\n",
            "logits_via_embedding: True\n",
            "Updating keys from model: ['base_emb_dim', 'base_num_query_heads', 'base_num_kv_heads', 'base_mlp_dim', 'base_num_decoder_layers', 'head_dim', 'mlp_activations', 'vocab_size', 'decoder_block', 'normalization_layer_epsilon', 'logits_via_embedding']\n",
            "Skipping jax distributed system due to skip_jax_distributed_system=True flag.\n",
            "Not using emergency checkpoint, ignoring local_checkpoint_directory, local_checkpoint_period, use_replicator_service and replicator_backup_interval_minutes\n",
            "dataset_type set to tfds, will use keys['dataset_path']='' and keys['dataset_name']='c4/en:3.0.1'\n",
            "Config param activations_in_float32: False\n",
            "Config param adam_b1: 0.9\n",
            "Config param adam_b2: 0.95\n",
            "Config param adam_eps: 1e-08\n",
            "Config param adam_eps_root: 0.0\n",
            "Config param adam_weight_decay: 0.1\n",
            "Config param add_bos: True\n",
            "Config param add_eos: True\n",
            "Config param allow_split_physical_axes: False\n",
            "Config param ar_cache_axis_order: 1,2,0,3\n",
            "Config param async_checkpointing: False\n",
            "Config param attention: dot_product\n",
            "Config param attention_type: global\n",
            "Config param attn_logits_soft_cap: None\n",
            "Config param autoregressive_decode_assert: \n",
            "Config param base_emb_dim: 2048\n",
            "Config param base_mlp_dim: 16384\n",
            "Config param base_moe_mlp_dim: 7168\n",
            "Config param base_num_decoder_layers: 18\n",
            "Config param base_num_kv_heads: 1\n",
            "Config param base_num_query_heads: 8\n",
            "Config param base_output_directory: gs://dummy_output_dir\n",
            "Config param beta_fast: 32\n",
            "Config param beta_slow: 1\n",
            "Config param capacity_factor: -1.0\n",
            "Config param cast_logits_to_fp32: True\n",
            "Config param checkpoint_conversion_fn: None\n",
            "Config param checkpoint_dir: gs://dummy_output_dir/test-tunix-maxtext-gemma-2b/checkpoints/\n",
            "Config param checkpoint_is_quantized: False\n",
            "Config param checkpoint_period: 5\n",
            "Config param checkpoint_storage_concurrent_gb: 96\n",
            "Config param checkpoint_storage_target_data_file_size_bytes: 2147483648\n",
            "Config param checkpoint_storage_use_ocdbt: True\n",
            "Config param checkpoint_storage_use_zarr3: True\n",
            "Config param chunk_attn_window_size: 0\n",
            "Config param collect_stack_trace: False\n",
            "Config param colocated_python_data_input: False\n",
            "Config param compile_topology: \n",
            "Config param compile_topology_num_slices: -1\n",
            "Config param compiled_trainstep_file: \n",
            "Config param compute_axis_order: 0,1,2,3\n",
            "Config param constant_bound_config: []\n",
            "Config param context: remat\n",
            "Config param context_parallel_load_balance: True\n",
            "Config param cosine_learning_rate_final_fraction: 0.1\n",
            "Config param custom_mesh: \n",
            "Config param data_sharding: (('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'),)\n",
            "Config param data_shuffle_seed: 0\n",
            "Config param dataset_name: c4/en:3.0.1\n",
            "Config param dataset_path: \n",
            "Config param dataset_type: tfds\n",
            "Config param dcn_autoregressive_parallelism: 1\n",
            "Config param dcn_context_autoregressive_parallelism: 1\n",
            "Config param dcn_context_parallelism: 1\n",
            "Config param dcn_data_parallelism: -1\n",
            "Config param dcn_expert_parallelism: 1\n",
            "Config param dcn_fsdp_parallelism: 1\n",
            "Config param dcn_fsdp_transpose_parallelism: 1\n",
            "Config param dcn_parallelism: [-1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Config param dcn_pipeline_parallelism: 1\n",
            "Config param dcn_sequence_parallelism: 1\n",
            "Config param dcn_tensor_parallelism: 1\n",
            "Config param dcn_tensor_sequence_parallelism: 1\n",
            "Config param dcn_tensor_transpose_parallelism: 1\n",
            "Config param decode_sampling_nucleus_p: -1\n",
            "Config param decode_sampling_strategy: greedy\n",
            "Config param decode_sampling_temperature: 1.0\n",
            "Config param decode_sampling_top_k: 0\n",
            "Config param decoder_block: DecoderBlockType.GEMMA\n",
            "Config param decoder_layer_input: device\n",
            "Config param dpo_beta: 0.1\n",
            "Config param dpo_label_smoothing: 0.0\n",
            "Config param dropout_rate: 0.0\n",
            "Config param dtype: bfloat16\n",
            "Config param dtype_mm: float32\n",
            "Config param dump_hlo: False\n",
            "Config param dump_hlo_delete_local_after: True\n",
            "Config param dump_hlo_gcs_dir: \n",
            "Config param dump_hlo_local_dir: /tmp/xla_dump/\n",
            "Config param dump_hlo_module_name: jit_train_step\n",
            "Config param dump_hlo_upload_all: False\n",
            "Config param dump_hlo_xla_flags: \n",
            "Config param dump_step: -1\n",
            "Config param emb_dim: 2048\n",
            "Config param enable_checkpoint_cloud_logger: False\n",
            "Config param enable_checkpointing: True\n",
            "Config param enable_data_shuffling: True\n",
            "Config param enable_dropout: True\n",
            "Config param enable_emergency_checkpoint: False\n",
            "Config param enable_gcp_goodput_metrics: True\n",
            "Config param enable_gcp_step_deviation_metrics: True\n",
            "Config param enable_goodput_recording: False\n",
            "Config param enable_jax_profiler: False\n",
            "Config param enable_llm_inference_pool: False\n",
            "Config param enable_model_warmup: False\n",
            "Config param enable_orbax_v1: False\n",
            "Config param enable_padding_causal_mask: True\n",
            "Config param enable_pathways_goodput: False\n",
            "Config param enable_prefix_caching: False\n",
            "Config param enable_single_controller: False\n",
            "Config param enable_single_replica_ckpt_restoring: False\n",
            "Config param enable_tensorboard: True\n",
            "Config param eval_data_columns: ['text']\n",
            "Config param eval_dataset_name: c4/en:3.0.1\n",
            "Config param eval_image_column: image\n",
            "Config param eval_interval: -1\n",
            "Config param eval_per_device_batch_size: 8.0\n",
            "Config param eval_split: validation\n",
            "Config param eval_steps: -1\n",
            "Config param expansion_factor_real_data: -1\n",
            "Config param final_logits_soft_cap: None\n",
            "Config param first_num_dense_layers: 0\n",
            "Config param float32_logits: False\n",
            "Config param float32_qk_product: False\n",
            "Config param force_unroll: False\n",
            "Config param freeze_vision_encoder_params: True\n",
            "Config param fused_mlp: False\n",
            "Config param fused_qkv: False\n",
            "Config param gcs_metrics: False\n",
            "Config param generate_slice: v5e-16\n",
            "Config param global_batch_size_to_eval_on: 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config param global_batch_size_to_load: 64\n",
            "Config param global_batch_size_to_load_eval: 64\n",
            "Config param global_batch_size_to_train_on: 64\n",
            "Config param global_parameter_scale: 1\n",
            "Config param goodput_upload_interval_seconds: 30\n",
            "Config param gradient_accumulation_steps: 1\n",
            "Config param gradient_clipping_threshold: 1.0\n",
            "Config param grain_eval_files: \n",
            "Config param grain_file_type: arrayrecord\n",
            "Config param grain_train_files: \n",
            "Config param grain_worker_count: 1\n",
            "Config param grain_worker_count_eval: 1\n",
            "Config param hardware: tpu\n",
            "Config param head_dim: 256\n",
            "Config param heartbeat_reporting_interval_in_seconds: 5\n",
            "Config param hf_data_dir: \n",
            "Config param hf_eval_files: \n",
            "Config param hf_eval_split: \n",
            "Config param hf_path: \n",
            "Config param hf_train_files: \n",
            "Config param hidden_size_for_vit: 1408\n",
            "Config param ici_autoregressive_parallelism: 1\n",
            "Config param ici_context_autoregressive_parallelism: 1\n",
            "Config param ici_context_parallelism: 1\n",
            "Config param ici_data_parallelism: 1\n",
            "Config param ici_expert_parallelism: 1\n",
            "Config param ici_fsdp_parallelism: -1\n",
            "Config param ici_fsdp_transpose_parallelism: 1\n",
            "Config param ici_parallelism: [1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Config param ici_pipeline_parallelism: 1\n",
            "Config param ici_sequence_parallelism: 1\n",
            "Config param ici_tensor_parallelism: 1\n",
            "Config param ici_tensor_sequence_parallelism: 1\n",
            "Config param ici_tensor_transpose_parallelism: 1\n",
            "Config param image_path: \n",
            "Config param image_placeholder: <|image|>\n",
            "Config param image_size_for_vit: 896\n",
            "Config param inference_benchmark_test: False\n",
            "Config param inference_metadata_file: \n",
            "Config param inference_microbenchmark_log_file_path: \n",
            "Config param inference_microbenchmark_loop_iters: 10\n",
            "Config param inference_microbenchmark_num_samples: [1, 2, 3, 4, 5]\n",
            "Config param inference_microbenchmark_prefill_lengths: 64,128,256,512,1024\n",
            "Config param inference_microbenchmark_stages: prefill,generate\n",
            "Config param inference_server: MaxtextInterleavedServer\n",
            "Config param inhomogeneous_layer_cycle_interval: 1\n",
            "Config param init_weights_seed: 0\n",
            "Config param input_data_sharding_logical_axes: ['activation_embed_and_logits_batch', 'activation_norm_length']\n",
            "Config param interleave_moe_layer_step: 1\n",
            "Config param intermediate_size_for_vit: 5632\n",
            "Config param jax_cache_dir: ~/jax_cache\n",
            "Config param jax_debug_log_modules: \n",
            "Config param jax_distributed_initialization_timeout: 300\n",
            "Config param jax_profiler_port: 9999\n",
            "Config param key_proj: remat\n",
            "Config param kv_lora_rank: 512\n",
            "Config param kv_quant_axis: heads_and_dkv\n",
            "Config param kv_quant_dtype: int8\n",
            "Config param learning_rate: 3e-05\n",
            "Config param learning_rate_schedule_steps: 10\n",
            "Config param load_balance_loss_weight: 0.01\n",
            "Config param load_from_prefill_dir: False\n",
            "Config param load_full_state_path: \n",
            "Config param load_parameters_path: \n",
            "Config param local_checkpoint_directory: \n",
            "Config param local_checkpoint_period: 0\n",
            "Config param local_rope_max_timescale: -1\n",
            "Config param log_config: True\n",
            "Config param log_period: 100\n",
            "Config param logical_axis_rules: (('activation_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_batch_no_exp', ('data', 'fsdp', 'fsdp_transpose')), ('activation_embed_and_logits_batch', ('data', 'stage', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence', 'autoregressive')), ('activation_kv_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_length', ('sequence', 'context')), ('prefill_activation_length', ('sequence', 'context')), ('activation_length', ('context',)), ('activation_norm_length', ('tensor_sequence', 'context', 'sequence')), ('prefill_activation_norm_length', ('tensor_sequence', 'context', 'sequence')), ('activation_q_length', ('context',)), ('activation_kv_length', ()), ('activation_embed', ('tensor', 'tensor_transpose')), ('activation_mlp', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_kv', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_prefill_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_head_dim', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose')), ('activation_vocab', 'tensor_sequence'), ('activation_vocab', ('sequence', 'context')), ('activation_stage', 'stage'), ('activation_exp', ('expert',)), ('decode_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('decode_length', ('sequence',)), ('mlp', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), ('vocab', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('q_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('kv_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'context', 'expert')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'context')), ('embed_tensor_transpose', ('tensor_transpose',)), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'expert')), ('norm', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('layers', 'stage'), ('kv', ()), ('kv_head_dim', ()), ('cache_batch_prefill', ()), ('cache_batch', ()), ('cache_heads_none', ()), ('cache_heads', ('autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence')), ('cache_heads', ('autoregressive', 'tensor', 'tensor_sequence')), ('cache_kv', ()), ('cache_sequence', ()), ('exp', 'expert'), ('paged_kv_heads', ('tensor',)), ('num_pages', ()), ('tokens_per_page', ()), ('paged_kv_head_dim_size', ()))\n",
            "Config param logits_dot_in_fp32: False\n",
            "Config param logits_via_embedding: True\n",
            "Config param lora_input_adapters_path: \n",
            "Config param matmul_precision: default\n",
            "Config param max_checkify: False\n",
            "Config param max_corpus_chars: 10000000\n",
            "Config param max_position_embeddings: 163840\n",
            "Config param max_prefill_predict_length: 64\n",
            "Config param max_target_length: 8192\n",
            "Config param megablox: True\n",
            "Config param mesh_axes: ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive']\n",
            "Config param metrics_dir: gs://dummy_output_dir/test-tunix-maxtext-gemma-2b/metrics/\n",
            "Config param metrics_file: \n",
            "Config param micro_batch_size_to_eval_on: 64\n",
            "Config param micro_batch_size_to_train_on: 64\n",
            "Config param mla_naive_kvcache: True\n",
            "Config param mlp_activations: ['gelu', 'linear']\n",
            "Config param mlp_dim: 16384\n",
            "Config param mlpwi: remat\n",
            "Config param mlpwi_0: remat\n",
            "Config param mlpwi_1: remat\n",
            "Config param mlpwo: remat\n",
            "Config param model_call_mode: \n",
            "Config param model_fsdp_ag_once: False\n",
            "Config param model_name: gemma-2b\n",
            "Config param moe_mlp_dim: 7168\n",
            "Config param monitor_goodput: False\n",
            "Config param monitor_step_time_deviation: True\n",
            "Config param mscale: 1.0\n",
            "Config param mtp_eval_target_module: 0\n",
            "Config param mtp_loss_scaling_factor: 0.1\n",
            "Config param mtp_num_layers: 0\n",
            "Config param mu_dtype: bfloat16\n",
            "Config param multi_sampling: False\n",
            "Config param n_routing_groups: -1\n",
            "Config param nope_layer_interval: -1\n",
            "Config param normalization_layer_epsilon: 1e-06\n",
            "Config param normalize_embedding_logits: True\n",
            "Config param num_attention_heads_for_vit: 16\n",
            "Config param num_channels_for_vit: 3\n",
            "Config param num_decoder_layers: 18\n",
            "Config param num_epoch: 1\n",
            "Config param num_experts: 1\n",
            "Config param num_experts_per_tok: 1\n",
            "Config param num_hidden_layers_for_vit: 34\n",
            "Config param num_kv_heads: 1\n",
            "Config param num_layers_per_pipeline_stage: 1\n",
            "Config param num_pipeline_microbatches: -1\n",
            "Config param num_pipeline_repeats: -1\n",
            "Config param num_query_heads: 8\n",
            "Config param num_slices: 1\n",
            "Config param opt_type: adamw\n",
            "Config param optimize_mesh_for_tpu_v6e: False\n",
            "Config param optimizer_memory_host_offload: False\n",
            "Config param original_max_position_embeddings: 4096\n",
            "Config param out_proj: remat\n",
            "Config param override_model_config: False\n",
            "Config param packing: True\n",
            "Config param pagedattn_head_dim_alignment: 128\n",
            "Config param pagedattn_max_pages_per_group: 256\n",
            "Config param pagedattn_num_pages: 64\n",
            "Config param pagedattn_pages_per_compute_block: 4\n",
            "Config param pagedattn_tokens_per_page: 32\n",
            "Config param param_scan_axis: 1\n",
            "Config param parameter_memory_host_offload: False\n",
            "Config param patch_size_for_vit: 14\n",
            "Config param per_device_batch_size: 8.0\n",
            "Config param pipeline_delay_activation_forwarding: False\n",
            "Config param pipeline_fsdp_ag_once: False\n",
            "Config param pipeline_parallel_layers: -1\n",
            "Config param pixel_shuffle_ratio_for_vit: 0.5\n",
            "Config param prefill_cache_axis_order: 1,2,0,3\n",
            "Config param prefill_cache_dir: \n",
            "Config param prefill_chunk_size: 256\n",
            "Config param prefill_slice: v5e-16\n",
            "Config param prefix_caching_dram_byte: 100000000000\n",
            "Config param prefix_caching_hbm_byte: 10000000000\n",
            "Config param profile_cleanly: True\n",
            "Config param profile_periodically_period: -1\n",
            "Config param profiler: \n",
            "Config param profiler_steps: 5\n",
            "Config param projector_dropout_for_vit: 0.0\n",
            "Config param projector_input_dim_for_vit: 4096\n",
            "Config param projector_output_dim_for_vit: 4096\n",
            "Config param prometheus_port: 0\n",
            "Config param prompt: I love to\n",
            "Config param q_lora_rank: 0\n",
            "Config param qk_nope_head_dim: 128\n",
            "Config param qk_rope_head_dim: 64\n",
            "Config param qkv_proj: remat\n",
            "Config param quant_cfg_path: \n",
            "Config param quantization: \n",
            "Config param quantization_calibration_method: absmax\n",
            "Config param quantization_local_shard_count: 1\n",
            "Config param quantize_kvcache: False\n",
            "Config param query_proj: remat\n",
            "Config param ragged_block_size: 256\n",
            "Config param record_internal_nn_metrics: 0\n",
            "Config param remat_policy: full\n",
            "Config param remat_policy_for_vit: minimal\n",
            "Config param replicate_quant_scale: False\n",
            "Config param replicator_backup_interval_minutes: 0\n",
            "Config param report_heartbeat_metric_for_gcp_monitoring: False\n",
            "Config param report_performance_metric_for_gcp_monitoring: False\n",
            "Config param reshape_q: False\n",
            "Config param return_log_prob: False\n",
            "Config param reuse_example_batch: 0\n",
            "Config param rope_factor: 40\n",
            "Config param rope_max_timescale: 10000\n",
            "Config param rope_min_timescale: 1\n",
            "Config param rope_theta_for_vit: 10000\n",
            "Config param rope_type: default\n",
            "Config param rope_use_scale: True\n",
            "Config param routed_bias: False\n",
            "Config param routed_scaling_factor: 1.0\n",
            "Config param routed_score_func: \n",
            "Config param run_name: test-tunix-maxtext-gemma-2b\n",
            "Config param sa_block_kv: 512\n",
            "Config param sa_block_kv_compute: 512\n",
            "Config param sa_block_kv_dkv: 512\n",
            "Config param sa_block_kv_dkv_compute: 512\n",
            "Config param sa_block_kv_dq: 512\n",
            "Config param sa_block_q: 512\n",
            "Config param sa_block_q_dkv: 512\n",
            "Config param sa_block_q_dq: 512\n",
            "Config param sa_k_layout: HEAD_DIM_MINOR\n",
            "Config param sa_q_layout: HEAD_DIM_MINOR\n",
            "Config param sa_use_fused_bwd_kernel: False\n",
            "Config param sa_v_layout: HEAD_DIM_MINOR\n",
            "Config param save_config_to_gcs: False\n",
            "Config param save_quantized_params_path: \n",
            "Config param scan_layers: True\n",
            "Config param scan_layers_per_stage: False\n",
            "Config param scan_pipeline_iterations: True\n",
            "Config param set_remat_policy_on_layers_per_stage: False\n",
            "Config param set_remat_policy_on_pipeline_iterations: True\n",
            "Config param sft_train_on_completion_only: False\n",
            "Config param sharding_tolerance: 0.02\n",
            "Config param shardy: True\n",
            "Config param shared_experts: 1\n",
            "Config param skip_first_n_steps_for_profiler: 1\n",
            "Config param skip_jax_distributed_system: True\n",
            "Config param sliding_window_size: 0\n",
            "Config param source_checkpoint_layout: orbax\n",
            "Config param sparse_matmul: True\n",
            "Config param stack_prefill_result_cache: False\n",
            "Config param stack_trace_interval_seconds: 600\n",
            "Config param stack_trace_to_cloud: False\n",
            "Config param step_deviation_interval_seconds: 30\n",
            "Config param steps: 10\n",
            "Config param subslice_shape: \n",
            "Config param target_eval_loss: 0.0\n",
            "Config param temperature_tuning: False\n",
            "Config param tensorboard_dir: gs://dummy_output_dir/test-tunix-maxtext-gemma-2b/tensorboard/\n",
            "Config param tile_activation_dim: 1024\n",
            "Config param tile_batch_seq: 512\n",
            "Config param tile_size_for_vit: 336\n",
            "Config param tile_weight_dim: 1024\n",
            "Config param tokenize_eval_data: True\n",
            "Config param tokenize_train_data: True\n",
            "Config param tokenizer_path: ../../maxtext/assets/tokenizer.gemma\n",
            "Config param tokenizer_type: sentencepiece\n",
            "Config param topk_routing_group: -1\n",
            "Config param train_data_columns: ['text']\n",
            "Config param train_image_column: image\n",
            "Config param train_split: train\n",
            "Config param trainable_position_size: -1\n",
            "Config param upload_all_profiler_results: False\n",
            "Config param use_chat_template: False\n",
            "Config param use_chunked_prefill: False\n",
            "Config param use_dpo: False\n",
            "Config param use_iota_embed: False\n",
            "Config param use_multimodal: False\n",
            "Config param use_post_attn_norm: False\n",
            "Config param use_post_ffw_norm: False\n",
            "Config param use_qk_norm: False\n",
            "Config param use_qwix_quantization: False\n",
            "Config param use_ragged_attention: False\n",
            "Config param use_random_routing: False\n",
            "Config param use_replicator_service: False\n",
            "Config param use_sft: False\n",
            "Config param use_untrainable_positional_embedding: False\n",
            "Config param use_vertex_tensorboard: False\n",
            "Config param using_pipeline_parallelism: False\n",
            "Config param v_head_dim: 128\n",
            "Config param value_proj: remat\n",
            "Config param vertex_tensorboard_project: \n",
            "Config param vertex_tensorboard_region: \n",
            "Config param vision_output_dim_for_vit: 4096\n",
            "Config param vocab_size: 256128\n",
            "Config param warmup_steps_fraction: 0.1\n",
            "Config param weight_dtype: bfloat16\n",
            "Num_devices: 8, shape (1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
            "Num_devices: 8, shape (1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
            "The abstract NNX state (all leaves are abstract arrays):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<script> (()=>{ if (customElements.get('treescope-container') === undefined) { class TreescopeContainer extends HTMLElement { constructor() { super(); this.attachShadow({mode: \"open\"}); this.defns = {}; this.state = {}; } } customElements.define(\"treescope-container\", TreescopeContainer); } if (customElements.get('treescope-run-here') === undefined) { class RunHere extends HTMLElement { constructor() { super() } connectedCallback() { const run = child => { const fn = new Function(child.textContent); child.textContent = \"\"; fn.call(this); this.remove(); }; const child = this.querySelector(\"script\"); if (child) { run(child); } else { new MutationObserver(()=>{ run(this.querySelector(\"script\")); }).observe(this, {childList: true}); } } } customElements.define(\"treescope-run-here\", RunHere); } })(); </script> <treescope-container class=\"treescope_out_69289fde648b40368394c66e89f9abf7\" style=\"display:block\"></treescope-container> <treescope-run-here><script type=\"application/octet-stream\"> const root = ( Array.from(document.getElementsByClassName( \"treescope_out_69289fde648b40368394c66e89f9abf7\")) .filter((elt) => !elt.dataset.setup) )[0]; root.dataset.setup = 1; const msg = document.createElement(\"span\"); msg.style = \"color: #cccccc; font-family: monospace;\"; msg.textContent = \"(Loading...)\"; root.state.loadingMsg = msg; root.shadowRoot.appendChild(msg); root.state.chain = new Promise((resolve, reject) => { const observer = new IntersectionObserver((entries) => { for (const entry of entries) { if (entry.isIntersecting) { resolve(); observer.disconnect(); return; } } }, {rootMargin: \"1000px\"}); window.setTimeout(() => { observer.observe(root); }, 0); }); root.state.deferring = false; const _insertNode = (node) => { for (let oldScript of node.querySelectorAll(\"script\")) { let newScript = document.createElement(\"script\"); newScript.type = oldScript.type; newScript.textContent = oldScript.textContent; oldScript.parentNode.replaceChild(newScript, oldScript); } if (root.state.loadingMsg) { root.state.loadingMsg.remove(); root.state.loadingMsg = null; } root.shadowRoot.appendChild(node); }; root.defns.insertContent = ((contentNode, compressed) => { if (compressed) { root.state.deferring = true; } if (root.state.deferring) { root.state.chain = (async () => { await root.state.chain; if (compressed) { const encoded = contentNode.textContent; const blob = new Blob([ Uint8Array.from(atob(encoded), (m) => m.codePointAt(0)) ]); const reader = blob.stream().pipeThrough( new DecompressionStream(\"deflate\") ).pipeThrough( new TextDecoderStream(\"utf-8\") ).getReader(); const parts = []; while (true) { const step = await reader.read(); if (step.done) { break; } parts.push(step.value); } const tpl = document.createElement('template'); tpl.innerHTML = parts.join(\"\"); _insertNode(tpl.content); } else { _insertNode(contentNode.content); } })(); } else { _insertNode(contentNode.content); } }); </script></treescope-run-here><div style=\"display:none\"> <script type=\"application/octet-stream\" >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</script> <treescope-run-here><script type=\"application/octet-stream\"> const root = ( Array.from(document.getElementsByClassName( \"treescope_out_69289fde648b40368394c66e89f9abf7\")) .filter((elt) => !elt.dataset['step0']) )[0]; root.dataset['step0'] = 1; root.defns.insertContent( this.parentNode.querySelector('script[type=\"application/octet-stream\"]'), true ); this.parentNode.remove(); </script></treescope-run-here> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "restoring params from gs://maxtext-gemma/2b/2025-08-05-04-37/0/items\n",
            "Creating checkpoint manager with ocdbt=True and zarr3=True\n",
            "Checkpoint manager created!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:The transformations API will eventually be replaced by an upgraded design. The current API will not be removed until this point, but it will no longer be actively worked on.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{checkpoint=}\n",
            "Num_devices: 8, shape (1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
            "inputs=[[[1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  ...\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]]\n",
            "\n",
            " [[1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  ...\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]]\n",
            "\n",
            " [[1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  ...\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  ...\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]]\n",
            "\n",
            " [[1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  ...\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]]\n",
            "\n",
            " [[1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  ...\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]\n",
            "  [1.78125 0.707031 1.24219 ... 1.05469 0.0441895 1.375]]]\n",
            "lnx=[[[1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  ...\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]]\n",
            "\n",
            " [[1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  ...\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]]\n",
            "\n",
            " [[1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  ...\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  ...\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]]\n",
            "\n",
            " [[1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  ...\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]]\n",
            "\n",
            " [[1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  ...\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]\n",
            "  [1.79688 0.714844 1.25 ... 1.0625 0.0446777 1.39062]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    ...\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00]]]]]\n",
            "attention_lnx=[[[-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.972656 -0.0361328 ... -0.261719 0.470703 -2.42188]\n",
            "  ...\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]]\n",
            "\n",
            " [[-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.972656 -0.0361328 ... -0.261719 0.470703 -2.42188]\n",
            "  ...\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]]\n",
            "\n",
            " [[-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.972656 -0.0361328 ... -0.261719 0.470703 -2.42188]\n",
            "  ...\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.972656 -0.0361328 ... -0.261719 0.470703 -2.42188]\n",
            "  ...\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]]\n",
            "\n",
            " [[-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.972656 -0.0361328 ... -0.261719 0.470703 -2.42188]\n",
            "  ...\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]]\n",
            "\n",
            " [[-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.972656 -0.0361328 ... -0.261719 0.470703 -2.42188]\n",
            "  ...\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]\n",
            "  [-0.410156 0.96875 -0.041748 ... -0.259766 0.466797 -2.42188]]]\n",
            "attn_output=[[[0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.964844 1.1875 0.847656 ... 0.558594 0.363281 -0.738281]\n",
            "  ...\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]]\n",
            "\n",
            " [[0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.964844 1.1875 0.847656 ... 0.558594 0.363281 -0.738281]\n",
            "  ...\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]]\n",
            "\n",
            " [[0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.964844 1.1875 0.847656 ... 0.558594 0.363281 -0.738281]\n",
            "  ...\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.964844 1.1875 0.847656 ... 0.558594 0.363281 -0.738281]\n",
            "  ...\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]]\n",
            "\n",
            " [[0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.964844 1.1875 0.847656 ... 0.558594 0.363281 -0.738281]\n",
            "  ...\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]]\n",
            "\n",
            " [[0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.964844 1.1875 0.847656 ... 0.558594 0.363281 -0.738281]\n",
            "  ...\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]\n",
            "  [0.96875 1.17969 0.847656 ... 0.5625 0.361328 -0.742188]]]\n",
            "next_layer_addition_dropped_out=[[[1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.61719 0.34375 -0.386719 ... 0.894531 0.507812 -0.396484]\n",
            "  ...\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]]\n",
            "\n",
            " [[1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.61719 0.34375 -0.386719 ... 0.894531 0.507812 -0.396484]\n",
            "  ...\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]]\n",
            "\n",
            " [[1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.61719 0.34375 -0.386719 ... 0.894531 0.507812 -0.396484]\n",
            "  ...\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.61719 0.34375 -0.386719 ... 0.894531 0.507812 -0.396484]\n",
            "  ...\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]]\n",
            "\n",
            " [[1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.61719 0.34375 -0.386719 ... 0.894531 0.507812 -0.396484]\n",
            "  ...\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]]\n",
            "\n",
            " [[1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.61719 0.34375 -0.386719 ... 0.894531 0.507812 -0.396484]\n",
            "  ...\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]]]\n",
            "inputs=[[[1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.61719 0.34375 -0.386719 ... 0.894531 0.507812 -0.396484]\n",
            "  ...\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]]\n",
            "\n",
            " [[1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.61719 0.34375 -0.386719 ... 0.894531 0.507812 -0.396484]\n",
            "  ...\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]]\n",
            "\n",
            " [[1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.61719 0.34375 -0.386719 ... 0.894531 0.507812 -0.396484]\n",
            "  ...\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.61719 0.34375 -0.386719 ... 0.894531 0.507812 -0.396484]\n",
            "  ...\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]]\n",
            "\n",
            " [[1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.61719 0.34375 -0.386719 ... 0.894531 0.507812 -0.396484]\n",
            "  ...\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]]\n",
            "\n",
            " [[1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.61719 0.34375 -0.386719 ... 0.894531 0.507812 -0.396484]\n",
            "  ...\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]\n",
            "  [1.625 0.339844 -0.388672 ... 0.902344 0.503906 -0.404297]]]\n",
            "lnx=[[[1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03125 0.219727 -0.24707 ... 0.570312 0.324219 -0.253906]\n",
            "  ...\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]]\n",
            "\n",
            " [[1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03125 0.219727 -0.24707 ... 0.570312 0.324219 -0.253906]\n",
            "  ...\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]]\n",
            "\n",
            " [[1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03125 0.219727 -0.24707 ... 0.570312 0.324219 -0.253906]\n",
            "  ...\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03125 0.219727 -0.24707 ... 0.570312 0.324219 -0.253906]\n",
            "  ...\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]]\n",
            "\n",
            " [[1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03125 0.219727 -0.24707 ... 0.570312 0.324219 -0.253906]\n",
            "  ...\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]]\n",
            "\n",
            " [[1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03125 0.219727 -0.24707 ... 0.570312 0.324219 -0.253906]\n",
            "  ...\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]\n",
            "  [1.03906 0.216797 -0.248047 ... 0.578125 0.322266 -0.257812]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    ...\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00]]]]]\n",
            "attention_lnx=[[[-0.988281 0.208008 -1.79688 ... 1.41406 0.129883 0.734375]\n",
            "  [-0.988281 0.208008 -1.79688 ... 1.41406 0.129883 0.734375]\n",
            "  [-0.988281 0.208984 -1.78906 ... 1.41406 0.129883 0.730469]\n",
            "  ...\n",
            "  [-0.988281 0.209961 -1.79688 ... 1.41406 0.124023 0.730469]\n",
            "  [-0.988281 0.209961 -1.78906 ... 1.41406 0.124023 0.730469]\n",
            "  [-0.988281 0.208984 -1.79688 ... 1.41406 0.123535 0.730469]]\n",
            "\n",
            " [[-0.988281 0.208008 -1.79688 ... 1.41406 0.129883 0.734375]\n",
            "  [-0.988281 0.208008 -1.79688 ... 1.41406 0.129883 0.734375]\n",
            "  [-0.988281 0.208984 -1.78906 ... 1.41406 0.129883 0.730469]\n",
            "  ...\n",
            "  [-0.988281 0.209961 -1.79688 ... 1.41406 0.124023 0.730469]\n",
            "  [-0.988281 0.209961 -1.78906 ... 1.41406 0.124023 0.730469]\n",
            "  [-0.988281 0.208984 -1.79688 ... 1.41406 0.123535 0.730469]]\n",
            "\n",
            " [[-0.988281 0.208008 -1.79688 ... 1.41406 0.129883 0.734375]\n",
            "  [-0.988281 0.208008 -1.79688 ... 1.41406 0.129883 0.734375]\n",
            "  [-0.988281 0.208984 -1.78906 ... 1.41406 0.129883 0.730469]\n",
            "  ...\n",
            "  [-0.988281 0.209961 -1.79688 ... 1.41406 0.124023 0.730469]\n",
            "  [-0.988281 0.209961 -1.78906 ... 1.41406 0.124023 0.730469]\n",
            "  [-0.988281 0.208984 -1.79688 ... 1.41406 0.123535 0.730469]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.988281 0.208008 -1.79688 ... 1.41406 0.129883 0.734375]\n",
            "  [-0.988281 0.208008 -1.79688 ... 1.41406 0.129883 0.734375]\n",
            "  [-0.988281 0.208984 -1.78906 ... 1.41406 0.129883 0.730469]\n",
            "  ...\n",
            "  [-0.988281 0.209961 -1.79688 ... 1.41406 0.124023 0.730469]\n",
            "  [-0.988281 0.209961 -1.78906 ... 1.41406 0.124023 0.730469]\n",
            "  [-0.988281 0.208984 -1.79688 ... 1.41406 0.123535 0.730469]]\n",
            "\n",
            " [[-0.988281 0.208008 -1.79688 ... 1.41406 0.129883 0.734375]\n",
            "  [-0.988281 0.208008 -1.79688 ... 1.41406 0.129883 0.734375]\n",
            "  [-0.988281 0.208984 -1.78906 ... 1.41406 0.129883 0.730469]\n",
            "  ...\n",
            "  [-0.988281 0.209961 -1.79688 ... 1.41406 0.124023 0.730469]\n",
            "  [-0.988281 0.209961 -1.78906 ... 1.41406 0.124023 0.730469]\n",
            "  [-0.988281 0.208984 -1.79688 ... 1.41406 0.123535 0.730469]]\n",
            "\n",
            " [[-0.988281 0.208008 -1.79688 ... 1.41406 0.129883 0.734375]\n",
            "  [-0.988281 0.208008 -1.79688 ... 1.41406 0.129883 0.734375]\n",
            "  [-0.988281 0.208984 -1.78906 ... 1.41406 0.129883 0.730469]\n",
            "  ...\n",
            "  [-0.988281 0.209961 -1.79688 ... 1.41406 0.124023 0.730469]\n",
            "  [-0.988281 0.209961 -1.78906 ... 1.41406 0.124023 0.730469]\n",
            "  [-0.988281 0.208984 -1.79688 ... 1.41406 0.123535 0.730469]]]\n",
            "attn_output=[[[0.345703 0.296875 -1.1875 ... 1.25781 0.34375 0.179688]\n",
            "  [0.345703 0.296875 -1.1875 ... 1.25781 0.34375 0.179688]\n",
            "  [0.341797 0.298828 -1.17969 ... 1.25781 0.345703 0.182617]\n",
            "  ...\n",
            "  [0.345703 0.298828 -1.1875 ... 1.25781 0.341797 0.177734]\n",
            "  [0.345703 0.298828 -1.1875 ... 1.25781 0.341797 0.178711]\n",
            "  [0.345703 0.298828 -1.1875 ... 1.25781 0.341797 0.178711]]\n",
            "\n",
            " [[0.345703 0.296875 -1.1875 ... 1.25781 0.34375 0.179688]\n",
            "  [0.345703 0.296875 -1.1875 ... 1.25781 0.34375 0.179688]\n",
            "  [0.341797 0.298828 -1.17969 ... 1.25781 0.345703 0.182617]\n",
            "  ...\n",
            "  [0.345703 0.298828 -1.1875 ... 1.25781 0.341797 0.177734]\n",
            "  [0.345703 0.298828 -1.1875 ... 1.25781 0.341797 0.178711]\n",
            "  [0.345703 0.298828 -1.1875 ... 1.25781 0.341797 0.178711]]\n",
            "\n",
            " [[0.345703 0.296875 -1.1875 ... 1.25781 0.34375 0.179688]\n",
            "  [0.345703 0.296875 -1.1875 ... 1.25781 0.34375 0.179688]\n",
            "  [0.341797 0.298828 -1.17969 ... 1.25781 0.345703 0.182617]\n",
            "  ...\n",
            "  [0.345703 0.298828 -1.1875 ... 1.25781 0.341797 0.177734]\n",
            "  [0.345703 0.298828 -1.1875 ... 1.25781 0.341797 0.178711]\n",
            "  [0.345703 0.298828 -1.1875 ... 1.25781 0.341797 0.178711]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.345703 0.296875 -1.1875 ... 1.25781 0.34375 0.179688]\n",
            "  [0.345703 0.296875 -1.1875 ... 1.25781 0.34375 0.179688]\n",
            "  [0.341797 0.298828 -1.17969 ... 1.25781 0.345703 0.182617]\n",
            "  ...\n",
            "  [0.345703 0.298828 -1.1875 ... 1.25781 0.341797 0.177734]\n",
            "  [0.345703 0.298828 -1.1875 ... 1.25781 0.341797 0.178711]\n",
            "  [0.345703 0.298828 -1.1875 ... 1.25781 0.341797 0.178711]]\n",
            "\n",
            " [[0.345703 0.296875 -1.1875 ... 1.25781 0.34375 0.179688]\n",
            "  [0.345703 0.296875 -1.1875 ... 1.25781 0.34375 0.179688]\n",
            "  [0.341797 0.298828 -1.17969 ... 1.25781 0.345703 0.182617]\n",
            "  ...\n",
            "  [0.345703 0.298828 -1.1875 ... 1.25781 0.341797 0.177734]\n",
            "  [0.345703 0.298828 -1.1875 ... 1.25781 0.341797 0.178711]\n",
            "  [0.345703 0.298828 -1.1875 ... 1.25781 0.341797 0.178711]]\n",
            "\n",
            " [[0.345703 0.296875 -1.1875 ... 1.25781 0.34375 0.179688]\n",
            "  [0.345703 0.296875 -1.1875 ... 1.25781 0.34375 0.179688]\n",
            "  [0.341797 0.298828 -1.17969 ... 1.25781 0.345703 0.182617]\n",
            "  ...\n",
            "  [0.345703 0.298828 -1.1875 ... 1.25781 0.341797 0.177734]\n",
            "  [0.345703 0.298828 -1.1875 ... 1.25781 0.341797 0.178711]\n",
            "  [0.345703 0.298828 -1.1875 ... 1.25781 0.341797 0.178711]]]\n",
            "next_layer_addition_dropped_out=[[[0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.550781 0.0703125 -3.45312 ... 2.01562 0.429688 0.0844727]\n",
            "  ...\n",
            "  [0.558594 0.0693359 -3.46875 ... 2.01562 0.419922 0.0766602]\n",
            "  [0.5625 0.0693359 -3.46875 ... 2.01562 0.419922 0.081543]\n",
            "  [0.5625 0.0722656 -3.46875 ... 2.01562 0.416016 0.0771484]]\n",
            "\n",
            " [[0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.550781 0.0703125 -3.45312 ... 2.01562 0.429688 0.0844727]\n",
            "  ...\n",
            "  [0.558594 0.0693359 -3.46875 ... 2.01562 0.419922 0.0766602]\n",
            "  [0.5625 0.0693359 -3.46875 ... 2.01562 0.419922 0.081543]\n",
            "  [0.5625 0.0722656 -3.46875 ... 2.01562 0.416016 0.0771484]]\n",
            "\n",
            " [[0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.550781 0.0703125 -3.45312 ... 2.01562 0.429688 0.0844727]\n",
            "  ...\n",
            "  [0.558594 0.0693359 -3.46875 ... 2.01562 0.419922 0.0766602]\n",
            "  [0.5625 0.0693359 -3.46875 ... 2.01562 0.419922 0.081543]\n",
            "  [0.5625 0.0722656 -3.46875 ... 2.01562 0.416016 0.0771484]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.550781 0.0703125 -3.45312 ... 2.01562 0.429688 0.0844727]\n",
            "  ...\n",
            "  [0.558594 0.0693359 -3.46875 ... 2.01562 0.419922 0.0766602]\n",
            "  [0.5625 0.0693359 -3.46875 ... 2.01562 0.419922 0.081543]\n",
            "  [0.5625 0.0722656 -3.46875 ... 2.01562 0.416016 0.0771484]]\n",
            "\n",
            " [[0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.550781 0.0703125 -3.45312 ... 2.01562 0.429688 0.0844727]\n",
            "  ...\n",
            "  [0.558594 0.0693359 -3.46875 ... 2.01562 0.419922 0.0766602]\n",
            "  [0.5625 0.0693359 -3.46875 ... 2.01562 0.419922 0.081543]\n",
            "  [0.5625 0.0722656 -3.46875 ... 2.01562 0.416016 0.0771484]]\n",
            "\n",
            " [[0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.550781 0.0703125 -3.45312 ... 2.01562 0.429688 0.0844727]\n",
            "  ...\n",
            "  [0.558594 0.0693359 -3.46875 ... 2.01562 0.419922 0.0766602]\n",
            "  [0.5625 0.0693359 -3.46875 ... 2.01562 0.419922 0.081543]\n",
            "  [0.5625 0.0722656 -3.46875 ... 2.01562 0.416016 0.0771484]]]\n",
            "inputs=[[[0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.550781 0.0703125 -3.45312 ... 2.01562 0.429688 0.0844727]\n",
            "  ...\n",
            "  [0.558594 0.0693359 -3.46875 ... 2.01562 0.419922 0.0766602]\n",
            "  [0.5625 0.0693359 -3.46875 ... 2.01562 0.419922 0.081543]\n",
            "  [0.5625 0.0722656 -3.46875 ... 2.01562 0.416016 0.0771484]]\n",
            "\n",
            " [[0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.550781 0.0703125 -3.45312 ... 2.01562 0.429688 0.0844727]\n",
            "  ...\n",
            "  [0.558594 0.0693359 -3.46875 ... 2.01562 0.419922 0.0766602]\n",
            "  [0.5625 0.0693359 -3.46875 ... 2.01562 0.419922 0.081543]\n",
            "  [0.5625 0.0722656 -3.46875 ... 2.01562 0.416016 0.0771484]]\n",
            "\n",
            " [[0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.550781 0.0703125 -3.45312 ... 2.01562 0.429688 0.0844727]\n",
            "  ...\n",
            "  [0.558594 0.0693359 -3.46875 ... 2.01562 0.419922 0.0766602]\n",
            "  [0.5625 0.0693359 -3.46875 ... 2.01562 0.419922 0.081543]\n",
            "  [0.5625 0.0722656 -3.46875 ... 2.01562 0.416016 0.0771484]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.550781 0.0703125 -3.45312 ... 2.01562 0.429688 0.0844727]\n",
            "  ...\n",
            "  [0.558594 0.0693359 -3.46875 ... 2.01562 0.419922 0.0766602]\n",
            "  [0.5625 0.0693359 -3.46875 ... 2.01562 0.419922 0.081543]\n",
            "  [0.5625 0.0722656 -3.46875 ... 2.01562 0.416016 0.0771484]]\n",
            "\n",
            " [[0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.550781 0.0703125 -3.45312 ... 2.01562 0.429688 0.0844727]\n",
            "  ...\n",
            "  [0.558594 0.0693359 -3.46875 ... 2.01562 0.419922 0.0766602]\n",
            "  [0.5625 0.0693359 -3.46875 ... 2.01562 0.419922 0.081543]\n",
            "  [0.5625 0.0722656 -3.46875 ... 2.01562 0.416016 0.0771484]]\n",
            "\n",
            " [[0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.5625 0.0634766 -3.46875 ... 2.01562 0.423828 0.0800781]\n",
            "  [0.550781 0.0703125 -3.45312 ... 2.01562 0.429688 0.0844727]\n",
            "  ...\n",
            "  [0.558594 0.0693359 -3.46875 ... 2.01562 0.419922 0.0766602]\n",
            "  [0.5625 0.0693359 -3.46875 ... 2.01562 0.419922 0.081543]\n",
            "  [0.5625 0.0722656 -3.46875 ... 2.01562 0.416016 0.0771484]]]\n",
            "lnx=[[[0.287109 0.0324707 -1.77344 ... 1.03125 0.216797 0.0410156]\n",
            "  [0.287109 0.0324707 -1.77344 ... 1.03125 0.216797 0.0410156]\n",
            "  [0.28125 0.0358887 -1.76562 ... 1.03125 0.219727 0.0432129]\n",
            "  ...\n",
            "  [0.285156 0.0354004 -1.77344 ... 1.03125 0.214844 0.0393066]\n",
            "  [0.287109 0.0354004 -1.77344 ... 1.03125 0.214844 0.041748]\n",
            "  [0.287109 0.0368652 -1.77344 ... 1.03125 0.212891 0.0395508]]\n",
            "\n",
            " [[0.287109 0.0324707 -1.77344 ... 1.03125 0.216797 0.0410156]\n",
            "  [0.287109 0.0324707 -1.77344 ... 1.03125 0.216797 0.0410156]\n",
            "  [0.28125 0.0358887 -1.76562 ... 1.03125 0.219727 0.0432129]\n",
            "  ...\n",
            "  [0.285156 0.0354004 -1.77344 ... 1.03125 0.214844 0.0393066]\n",
            "  [0.287109 0.0354004 -1.77344 ... 1.03125 0.214844 0.041748]\n",
            "  [0.287109 0.0368652 -1.77344 ... 1.03125 0.212891 0.0395508]]\n",
            "\n",
            " [[0.287109 0.0324707 -1.77344 ... 1.03125 0.216797 0.0410156]\n",
            "  [0.287109 0.0324707 -1.77344 ... 1.03125 0.216797 0.0410156]\n",
            "  [0.28125 0.0358887 -1.76562 ... 1.03125 0.219727 0.0432129]\n",
            "  ...\n",
            "  [0.285156 0.0354004 -1.77344 ... 1.03125 0.214844 0.0393066]\n",
            "  [0.287109 0.0354004 -1.77344 ... 1.03125 0.214844 0.041748]\n",
            "  [0.287109 0.0368652 -1.77344 ... 1.03125 0.212891 0.0395508]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.287109 0.0324707 -1.77344 ... 1.03125 0.216797 0.0410156]\n",
            "  [0.287109 0.0324707 -1.77344 ... 1.03125 0.216797 0.0410156]\n",
            "  [0.28125 0.0358887 -1.76562 ... 1.03125 0.219727 0.0432129]\n",
            "  ...\n",
            "  [0.285156 0.0354004 -1.77344 ... 1.03125 0.214844 0.0393066]\n",
            "  [0.287109 0.0354004 -1.77344 ... 1.03125 0.214844 0.041748]\n",
            "  [0.287109 0.0368652 -1.77344 ... 1.03125 0.212891 0.0395508]]\n",
            "\n",
            " [[0.287109 0.0324707 -1.77344 ... 1.03125 0.216797 0.0410156]\n",
            "  [0.287109 0.0324707 -1.77344 ... 1.03125 0.216797 0.0410156]\n",
            "  [0.28125 0.0358887 -1.76562 ... 1.03125 0.219727 0.0432129]\n",
            "  ...\n",
            "  [0.285156 0.0354004 -1.77344 ... 1.03125 0.214844 0.0393066]\n",
            "  [0.287109 0.0354004 -1.77344 ... 1.03125 0.214844 0.041748]\n",
            "  [0.287109 0.0368652 -1.77344 ... 1.03125 0.212891 0.0395508]]\n",
            "\n",
            " [[0.287109 0.0324707 -1.77344 ... 1.03125 0.216797 0.0410156]\n",
            "  [0.287109 0.0324707 -1.77344 ... 1.03125 0.216797 0.0410156]\n",
            "  [0.28125 0.0358887 -1.76562 ... 1.03125 0.219727 0.0432129]\n",
            "  ...\n",
            "  [0.285156 0.0354004 -1.77344 ... 1.03125 0.214844 0.0393066]\n",
            "  [0.287109 0.0354004 -1.77344 ... 1.03125 0.214844 0.041748]\n",
            "  [0.287109 0.0368652 -1.77344 ... 1.03125 0.212891 0.0395508]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    ...\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00]]]]]\n",
            "attention_lnx=[[[-0.394531 1.6875 0.498047 ... 1.07812 0.921875 -1.30469]\n",
            "  [-0.394531 1.6875 0.498047 ... 1.07812 0.921875 -1.30469]\n",
            "  [-0.392578 1.67969 0.5 ... 1.08594 0.914062 -1.30469]\n",
            "  ...\n",
            "  [-0.396484 1.67969 0.5 ... 1.08594 0.917969 -1.3125]\n",
            "  [-0.392578 1.67969 0.5 ... 1.08594 0.917969 -1.30469]\n",
            "  [-0.394531 1.67969 0.503906 ... 1.07812 0.925781 -1.30469]]\n",
            "\n",
            " [[-0.394531 1.6875 0.498047 ... 1.07812 0.921875 -1.30469]\n",
            "  [-0.394531 1.6875 0.498047 ... 1.07812 0.921875 -1.30469]\n",
            "  [-0.392578 1.67969 0.5 ... 1.08594 0.914062 -1.30469]\n",
            "  ...\n",
            "  [-0.396484 1.67969 0.5 ... 1.08594 0.917969 -1.3125]\n",
            "  [-0.392578 1.67969 0.5 ... 1.08594 0.917969 -1.30469]\n",
            "  [-0.394531 1.67969 0.503906 ... 1.07812 0.925781 -1.30469]]\n",
            "\n",
            " [[-0.394531 1.6875 0.498047 ... 1.07812 0.921875 -1.30469]\n",
            "  [-0.394531 1.6875 0.498047 ... 1.07812 0.921875 -1.30469]\n",
            "  [-0.392578 1.67969 0.5 ... 1.08594 0.914062 -1.30469]\n",
            "  ...\n",
            "  [-0.396484 1.67969 0.5 ... 1.08594 0.917969 -1.3125]\n",
            "  [-0.392578 1.67969 0.5 ... 1.08594 0.917969 -1.30469]\n",
            "  [-0.394531 1.67969 0.503906 ... 1.07812 0.925781 -1.30469]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.394531 1.6875 0.498047 ... 1.07812 0.921875 -1.30469]\n",
            "  [-0.394531 1.6875 0.498047 ... 1.07812 0.921875 -1.30469]\n",
            "  [-0.392578 1.67969 0.5 ... 1.08594 0.914062 -1.30469]\n",
            "  ...\n",
            "  [-0.396484 1.67969 0.5 ... 1.08594 0.917969 -1.3125]\n",
            "  [-0.392578 1.67969 0.5 ... 1.08594 0.917969 -1.30469]\n",
            "  [-0.394531 1.67969 0.503906 ... 1.07812 0.925781 -1.30469]]\n",
            "\n",
            " [[-0.394531 1.6875 0.498047 ... 1.07812 0.921875 -1.30469]\n",
            "  [-0.394531 1.6875 0.498047 ... 1.07812 0.921875 -1.30469]\n",
            "  [-0.392578 1.67969 0.5 ... 1.08594 0.914062 -1.30469]\n",
            "  ...\n",
            "  [-0.396484 1.67969 0.5 ... 1.08594 0.917969 -1.3125]\n",
            "  [-0.392578 1.67969 0.5 ... 1.08594 0.917969 -1.30469]\n",
            "  [-0.394531 1.67969 0.503906 ... 1.07812 0.925781 -1.30469]]\n",
            "\n",
            " [[-0.394531 1.6875 0.498047 ... 1.07812 0.921875 -1.30469]\n",
            "  [-0.394531 1.6875 0.498047 ... 1.07812 0.921875 -1.30469]\n",
            "  [-0.392578 1.67969 0.5 ... 1.08594 0.914062 -1.30469]\n",
            "  ...\n",
            "  [-0.396484 1.67969 0.5 ... 1.08594 0.917969 -1.3125]\n",
            "  [-0.392578 1.67969 0.5 ... 1.08594 0.917969 -1.30469]\n",
            "  [-0.394531 1.67969 0.503906 ... 1.07812 0.925781 -1.30469]]]\n",
            "attn_output=[[[0.0761719 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]\n",
            "  [0.0761719 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]\n",
            "  [0.0712891 0.796875 -1.34375 ... 1.41406 0.609375 -0.554688]\n",
            "  ...\n",
            "  [0.0737305 0.796875 -1.35156 ... 1.41406 0.609375 -0.5625]\n",
            "  [0.0771484 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]\n",
            "  [0.0761719 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]]\n",
            "\n",
            " [[0.0761719 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]\n",
            "  [0.0761719 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]\n",
            "  [0.0712891 0.796875 -1.34375 ... 1.41406 0.609375 -0.554688]\n",
            "  ...\n",
            "  [0.0737305 0.796875 -1.35156 ... 1.41406 0.609375 -0.5625]\n",
            "  [0.0771484 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]\n",
            "  [0.0761719 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]]\n",
            "\n",
            " [[0.0761719 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]\n",
            "  [0.0761719 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]\n",
            "  [0.0712891 0.796875 -1.34375 ... 1.41406 0.609375 -0.554688]\n",
            "  ...\n",
            "  [0.0737305 0.796875 -1.35156 ... 1.41406 0.609375 -0.5625]\n",
            "  [0.0771484 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]\n",
            "  [0.0761719 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.0761719 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]\n",
            "  [0.0761719 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]\n",
            "  [0.0712891 0.796875 -1.34375 ... 1.41406 0.609375 -0.554688]\n",
            "  ...\n",
            "  [0.0737305 0.796875 -1.35156 ... 1.41406 0.609375 -0.5625]\n",
            "  [0.0771484 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]\n",
            "  [0.0761719 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]]\n",
            "\n",
            " [[0.0761719 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]\n",
            "  [0.0761719 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]\n",
            "  [0.0712891 0.796875 -1.34375 ... 1.41406 0.609375 -0.554688]\n",
            "  ...\n",
            "  [0.0737305 0.796875 -1.35156 ... 1.41406 0.609375 -0.5625]\n",
            "  [0.0771484 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]\n",
            "  [0.0761719 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]]\n",
            "\n",
            " [[0.0761719 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]\n",
            "  [0.0761719 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]\n",
            "  [0.0712891 0.796875 -1.34375 ... 1.41406 0.609375 -0.554688]\n",
            "  ...\n",
            "  [0.0737305 0.796875 -1.35156 ... 1.41406 0.609375 -0.5625]\n",
            "  [0.0771484 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]\n",
            "  [0.0761719 0.796875 -1.35156 ... 1.40625 0.609375 -0.558594]]]\n",
            "next_layer_addition_dropped_out=[[[-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.617188 2.8125 -2.73438 ... 3.65625 1.125 -1.76562]\n",
            "  ...\n",
            "  [-0.628906 2.8125 -2.75 ... 3.65625 1.10938 -1.78906]\n",
            "  [-0.617188 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]]\n",
            "\n",
            " [[-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.617188 2.8125 -2.73438 ... 3.65625 1.125 -1.76562]\n",
            "  ...\n",
            "  [-0.628906 2.8125 -2.75 ... 3.65625 1.10938 -1.78906]\n",
            "  [-0.617188 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]]\n",
            "\n",
            " [[-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.617188 2.8125 -2.73438 ... 3.65625 1.125 -1.76562]\n",
            "  ...\n",
            "  [-0.628906 2.8125 -2.75 ... 3.65625 1.10938 -1.78906]\n",
            "  [-0.617188 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.617188 2.8125 -2.73438 ... 3.65625 1.125 -1.76562]\n",
            "  ...\n",
            "  [-0.628906 2.8125 -2.75 ... 3.65625 1.10938 -1.78906]\n",
            "  [-0.617188 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]]\n",
            "\n",
            " [[-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.617188 2.8125 -2.73438 ... 3.65625 1.125 -1.76562]\n",
            "  ...\n",
            "  [-0.628906 2.8125 -2.75 ... 3.65625 1.10938 -1.78906]\n",
            "  [-0.617188 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]]\n",
            "\n",
            " [[-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.617188 2.8125 -2.73438 ... 3.65625 1.125 -1.76562]\n",
            "  ...\n",
            "  [-0.628906 2.8125 -2.75 ... 3.65625 1.10938 -1.78906]\n",
            "  [-0.617188 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]]]\n",
            "inputs=[[[-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.617188 2.8125 -2.73438 ... 3.65625 1.125 -1.76562]\n",
            "  ...\n",
            "  [-0.628906 2.8125 -2.75 ... 3.65625 1.10938 -1.78906]\n",
            "  [-0.617188 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]]\n",
            "\n",
            " [[-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.617188 2.8125 -2.73438 ... 3.65625 1.125 -1.76562]\n",
            "  ...\n",
            "  [-0.628906 2.8125 -2.75 ... 3.65625 1.10938 -1.78906]\n",
            "  [-0.617188 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]]\n",
            "\n",
            " [[-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.617188 2.8125 -2.73438 ... 3.65625 1.125 -1.76562]\n",
            "  ...\n",
            "  [-0.628906 2.8125 -2.75 ... 3.65625 1.10938 -1.78906]\n",
            "  [-0.617188 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.617188 2.8125 -2.73438 ... 3.65625 1.125 -1.76562]\n",
            "  ...\n",
            "  [-0.628906 2.8125 -2.75 ... 3.65625 1.10938 -1.78906]\n",
            "  [-0.617188 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]]\n",
            "\n",
            " [[-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.617188 2.8125 -2.73438 ... 3.65625 1.125 -1.76562]\n",
            "  ...\n",
            "  [-0.628906 2.8125 -2.75 ... 3.65625 1.10938 -1.78906]\n",
            "  [-0.617188 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]]\n",
            "\n",
            " [[-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.625 1.13281 -1.77344]\n",
            "  [-0.617188 2.8125 -2.73438 ... 3.65625 1.125 -1.76562]\n",
            "  ...\n",
            "  [-0.628906 2.8125 -2.75 ... 3.65625 1.10938 -1.78906]\n",
            "  [-0.617188 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]\n",
            "  [-0.613281 2.8125 -2.75 ... 3.64062 1.11719 -1.78125]]]\n",
            "lnx=[[[-0.267578 1.23438 -1.20312 ... 1.58594 0.496094 -0.777344]\n",
            "  [-0.267578 1.23438 -1.20312 ... 1.58594 0.496094 -0.777344]\n",
            "  [-0.269531 1.23438 -1.19531 ... 1.60156 0.492188 -0.773438]\n",
            "  ...\n",
            "  [-0.275391 1.22656 -1.20312 ... 1.60156 0.484375 -0.78125]\n",
            "  [-0.269531 1.23438 -1.20312 ... 1.59375 0.488281 -0.78125]\n",
            "  [-0.267578 1.23438 -1.20312 ... 1.59375 0.488281 -0.78125]]\n",
            "\n",
            " [[-0.267578 1.23438 -1.20312 ... 1.58594 0.496094 -0.777344]\n",
            "  [-0.267578 1.23438 -1.20312 ... 1.58594 0.496094 -0.777344]\n",
            "  [-0.269531 1.23438 -1.19531 ... 1.60156 0.492188 -0.773438]\n",
            "  ...\n",
            "  [-0.275391 1.22656 -1.20312 ... 1.60156 0.484375 -0.78125]\n",
            "  [-0.269531 1.23438 -1.20312 ... 1.59375 0.488281 -0.78125]\n",
            "  [-0.267578 1.23438 -1.20312 ... 1.59375 0.488281 -0.78125]]\n",
            "\n",
            " [[-0.267578 1.23438 -1.20312 ... 1.58594 0.496094 -0.777344]\n",
            "  [-0.267578 1.23438 -1.20312 ... 1.58594 0.496094 -0.777344]\n",
            "  [-0.269531 1.23438 -1.19531 ... 1.60156 0.492188 -0.773438]\n",
            "  ...\n",
            "  [-0.275391 1.22656 -1.20312 ... 1.60156 0.484375 -0.78125]\n",
            "  [-0.269531 1.23438 -1.20312 ... 1.59375 0.488281 -0.78125]\n",
            "  [-0.267578 1.23438 -1.20312 ... 1.59375 0.488281 -0.78125]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.267578 1.23438 -1.20312 ... 1.58594 0.496094 -0.777344]\n",
            "  [-0.267578 1.23438 -1.20312 ... 1.58594 0.496094 -0.777344]\n",
            "  [-0.269531 1.23438 -1.19531 ... 1.60156 0.492188 -0.773438]\n",
            "  ...\n",
            "  [-0.275391 1.22656 -1.20312 ... 1.60156 0.484375 -0.78125]\n",
            "  [-0.269531 1.23438 -1.20312 ... 1.59375 0.488281 -0.78125]\n",
            "  [-0.267578 1.23438 -1.20312 ... 1.59375 0.488281 -0.78125]]\n",
            "\n",
            " [[-0.267578 1.23438 -1.20312 ... 1.58594 0.496094 -0.777344]\n",
            "  [-0.267578 1.23438 -1.20312 ... 1.58594 0.496094 -0.777344]\n",
            "  [-0.269531 1.23438 -1.19531 ... 1.60156 0.492188 -0.773438]\n",
            "  ...\n",
            "  [-0.275391 1.22656 -1.20312 ... 1.60156 0.484375 -0.78125]\n",
            "  [-0.269531 1.23438 -1.20312 ... 1.59375 0.488281 -0.78125]\n",
            "  [-0.267578 1.23438 -1.20312 ... 1.59375 0.488281 -0.78125]]\n",
            "\n",
            " [[-0.267578 1.23438 -1.20312 ... 1.58594 0.496094 -0.777344]\n",
            "  [-0.267578 1.23438 -1.20312 ... 1.58594 0.496094 -0.777344]\n",
            "  [-0.269531 1.23438 -1.19531 ... 1.60156 0.492188 -0.773438]\n",
            "  ...\n",
            "  [-0.275391 1.22656 -1.20312 ... 1.60156 0.484375 -0.78125]\n",
            "  [-0.269531 1.23438 -1.20312 ... 1.59375 0.488281 -0.78125]\n",
            "  [-0.267578 1.23438 -1.20312 ... 1.59375 0.488281 -0.78125]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    ...\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00]]]]]\n",
            "attention_lnx=[[[-0.273438 -0.482422 -0.0246582 ... -0.275391 -1.27344 -0.890625]\n",
            "  [-0.273438 -0.482422 -0.0246582 ... -0.275391 -1.27344 -0.890625]\n",
            "  [-0.269531 -0.478516 -0.0228271 ... -0.275391 -1.27344 -0.890625]\n",
            "  ...\n",
            "  [-0.271484 -0.482422 -0.0229492 ... -0.271484 -1.27344 -0.894531]\n",
            "  [-0.273438 -0.484375 -0.0218506 ... -0.275391 -1.27344 -0.890625]\n",
            "  [-0.273438 -0.486328 -0.0201416 ... -0.271484 -1.27344 -0.894531]]\n",
            "\n",
            " [[-0.273438 -0.482422 -0.0246582 ... -0.275391 -1.27344 -0.890625]\n",
            "  [-0.273438 -0.482422 -0.0246582 ... -0.275391 -1.27344 -0.890625]\n",
            "  [-0.269531 -0.478516 -0.0228271 ... -0.275391 -1.27344 -0.890625]\n",
            "  ...\n",
            "  [-0.271484 -0.482422 -0.0229492 ... -0.271484 -1.27344 -0.894531]\n",
            "  [-0.273438 -0.484375 -0.0218506 ... -0.275391 -1.27344 -0.890625]\n",
            "  [-0.273438 -0.486328 -0.0201416 ... -0.271484 -1.27344 -0.894531]]\n",
            "\n",
            " [[-0.273438 -0.482422 -0.0246582 ... -0.275391 -1.27344 -0.890625]\n",
            "  [-0.273438 -0.482422 -0.0246582 ... -0.275391 -1.27344 -0.890625]\n",
            "  [-0.269531 -0.478516 -0.0228271 ... -0.275391 -1.27344 -0.890625]\n",
            "  ...\n",
            "  [-0.271484 -0.482422 -0.0229492 ... -0.271484 -1.27344 -0.894531]\n",
            "  [-0.273438 -0.484375 -0.0218506 ... -0.275391 -1.27344 -0.890625]\n",
            "  [-0.273438 -0.486328 -0.0201416 ... -0.271484 -1.27344 -0.894531]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.273438 -0.482422 -0.0246582 ... -0.275391 -1.27344 -0.890625]\n",
            "  [-0.273438 -0.482422 -0.0246582 ... -0.275391 -1.27344 -0.890625]\n",
            "  [-0.269531 -0.478516 -0.0228271 ... -0.275391 -1.27344 -0.890625]\n",
            "  ...\n",
            "  [-0.271484 -0.482422 -0.0229492 ... -0.271484 -1.27344 -0.894531]\n",
            "  [-0.273438 -0.484375 -0.0218506 ... -0.275391 -1.27344 -0.890625]\n",
            "  [-0.273438 -0.486328 -0.0201416 ... -0.271484 -1.27344 -0.894531]]\n",
            "\n",
            " [[-0.273438 -0.482422 -0.0246582 ... -0.275391 -1.27344 -0.890625]\n",
            "  [-0.273438 -0.482422 -0.0246582 ... -0.275391 -1.27344 -0.890625]\n",
            "  [-0.269531 -0.478516 -0.0228271 ... -0.275391 -1.27344 -0.890625]\n",
            "  ...\n",
            "  [-0.271484 -0.482422 -0.0229492 ... -0.271484 -1.27344 -0.894531]\n",
            "  [-0.273438 -0.484375 -0.0218506 ... -0.275391 -1.27344 -0.890625]\n",
            "  [-0.273438 -0.486328 -0.0201416 ... -0.271484 -1.27344 -0.894531]]\n",
            "\n",
            " [[-0.273438 -0.482422 -0.0246582 ... -0.275391 -1.27344 -0.890625]\n",
            "  [-0.273438 -0.482422 -0.0246582 ... -0.275391 -1.27344 -0.890625]\n",
            "  [-0.269531 -0.478516 -0.0228271 ... -0.275391 -1.27344 -0.890625]\n",
            "  ...\n",
            "  [-0.271484 -0.482422 -0.0229492 ... -0.271484 -1.27344 -0.894531]\n",
            "  [-0.273438 -0.484375 -0.0218506 ... -0.275391 -1.27344 -0.890625]\n",
            "  [-0.273438 -0.486328 -0.0201416 ... -0.271484 -1.27344 -0.894531]]]\n",
            "attn_output=[[[-0.363281 0.953125 -1.14062 ... 1.36719 -0.0563965 -1.09375]\n",
            "  [-0.363281 0.953125 -1.14062 ... 1.36719 -0.0563965 -1.09375]\n",
            "  [-0.363281 0.953125 -1.125 ... 1.38281 -0.0598145 -1.08594]\n",
            "  ...\n",
            "  [-0.369141 0.953125 -1.13281 ... 1.38281 -0.0673828 -1.10156]\n",
            "  [-0.363281 0.953125 -1.13281 ... 1.375 -0.0629883 -1.09375]\n",
            "  [-0.363281 0.953125 -1.13281 ... 1.38281 -0.0644531 -1.09375]]\n",
            "\n",
            " [[-0.363281 0.953125 -1.14062 ... 1.36719 -0.0563965 -1.09375]\n",
            "  [-0.363281 0.953125 -1.14062 ... 1.36719 -0.0563965 -1.09375]\n",
            "  [-0.363281 0.953125 -1.125 ... 1.38281 -0.0598145 -1.08594]\n",
            "  ...\n",
            "  [-0.369141 0.953125 -1.13281 ... 1.38281 -0.0673828 -1.10156]\n",
            "  [-0.363281 0.953125 -1.13281 ... 1.375 -0.0629883 -1.09375]\n",
            "  [-0.363281 0.953125 -1.13281 ... 1.38281 -0.0644531 -1.09375]]\n",
            "\n",
            " [[-0.363281 0.953125 -1.14062 ... 1.36719 -0.0563965 -1.09375]\n",
            "  [-0.363281 0.953125 -1.14062 ... 1.36719 -0.0563965 -1.09375]\n",
            "  [-0.363281 0.953125 -1.125 ... 1.38281 -0.0598145 -1.08594]\n",
            "  ...\n",
            "  [-0.369141 0.953125 -1.13281 ... 1.38281 -0.0673828 -1.10156]\n",
            "  [-0.363281 0.953125 -1.13281 ... 1.375 -0.0629883 -1.09375]\n",
            "  [-0.363281 0.953125 -1.13281 ... 1.38281 -0.0644531 -1.09375]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.363281 0.953125 -1.14062 ... 1.36719 -0.0563965 -1.09375]\n",
            "  [-0.363281 0.953125 -1.14062 ... 1.36719 -0.0563965 -1.09375]\n",
            "  [-0.363281 0.953125 -1.125 ... 1.38281 -0.0598145 -1.08594]\n",
            "  ...\n",
            "  [-0.369141 0.953125 -1.13281 ... 1.38281 -0.0673828 -1.10156]\n",
            "  [-0.363281 0.953125 -1.13281 ... 1.375 -0.0629883 -1.09375]\n",
            "  [-0.363281 0.953125 -1.13281 ... 1.38281 -0.0644531 -1.09375]]\n",
            "\n",
            " [[-0.363281 0.953125 -1.14062 ... 1.36719 -0.0563965 -1.09375]\n",
            "  [-0.363281 0.953125 -1.14062 ... 1.36719 -0.0563965 -1.09375]\n",
            "  [-0.363281 0.953125 -1.125 ... 1.38281 -0.0598145 -1.08594]\n",
            "  ...\n",
            "  [-0.369141 0.953125 -1.13281 ... 1.38281 -0.0673828 -1.10156]\n",
            "  [-0.363281 0.953125 -1.13281 ... 1.375 -0.0629883 -1.09375]\n",
            "  [-0.363281 0.953125 -1.13281 ... 1.38281 -0.0644531 -1.09375]]\n",
            "\n",
            " [[-0.363281 0.953125 -1.14062 ... 1.36719 -0.0563965 -1.09375]\n",
            "  [-0.363281 0.953125 -1.14062 ... 1.36719 -0.0563965 -1.09375]\n",
            "  [-0.363281 0.953125 -1.125 ... 1.38281 -0.0598145 -1.08594]\n",
            "  ...\n",
            "  [-0.369141 0.953125 -1.13281 ... 1.38281 -0.0673828 -1.10156]\n",
            "  [-0.363281 0.953125 -1.13281 ... 1.375 -0.0629883 -1.09375]\n",
            "  [-0.363281 0.953125 -1.13281 ... 1.38281 -0.0644531 -1.09375]]]\n",
            "next_layer_addition_dropped_out=[[[-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.74219 -2.64062 ... 3.60938 -0.181641 -1.78906]\n",
            "  ...\n",
            "  [-0.800781 1.74219 -2.64062 ... 3.625 -0.181641 -1.82812]\n",
            "  [-0.792969 1.74219 -2.64062 ... 3.57812 -0.174805 -1.82031]\n",
            "  [-0.78125 1.74219 -2.65625 ... 3.59375 -0.183594 -1.8125]]\n",
            "\n",
            " [[-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.74219 -2.64062 ... 3.60938 -0.181641 -1.78906]\n",
            "  ...\n",
            "  [-0.800781 1.74219 -2.64062 ... 3.625 -0.181641 -1.82812]\n",
            "  [-0.792969 1.74219 -2.64062 ... 3.57812 -0.174805 -1.82031]\n",
            "  [-0.78125 1.74219 -2.65625 ... 3.59375 -0.183594 -1.8125]]\n",
            "\n",
            " [[-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.74219 -2.64062 ... 3.60938 -0.181641 -1.78906]\n",
            "  ...\n",
            "  [-0.800781 1.74219 -2.64062 ... 3.625 -0.181641 -1.82812]\n",
            "  [-0.792969 1.74219 -2.64062 ... 3.57812 -0.174805 -1.82031]\n",
            "  [-0.78125 1.74219 -2.65625 ... 3.59375 -0.183594 -1.8125]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.74219 -2.64062 ... 3.60938 -0.181641 -1.78906]\n",
            "  ...\n",
            "  [-0.800781 1.74219 -2.64062 ... 3.625 -0.181641 -1.82812]\n",
            "  [-0.792969 1.74219 -2.64062 ... 3.57812 -0.174805 -1.82031]\n",
            "  [-0.78125 1.74219 -2.65625 ... 3.59375 -0.183594 -1.8125]]\n",
            "\n",
            " [[-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.74219 -2.64062 ... 3.60938 -0.181641 -1.78906]\n",
            "  ...\n",
            "  [-0.800781 1.74219 -2.64062 ... 3.625 -0.181641 -1.82812]\n",
            "  [-0.792969 1.74219 -2.64062 ... 3.57812 -0.174805 -1.82031]\n",
            "  [-0.78125 1.74219 -2.65625 ... 3.59375 -0.183594 -1.8125]]\n",
            "\n",
            " [[-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.74219 -2.64062 ... 3.60938 -0.181641 -1.78906]\n",
            "  ...\n",
            "  [-0.800781 1.74219 -2.64062 ... 3.625 -0.181641 -1.82812]\n",
            "  [-0.792969 1.74219 -2.64062 ... 3.57812 -0.174805 -1.82031]\n",
            "  [-0.78125 1.74219 -2.65625 ... 3.59375 -0.183594 -1.8125]]]\n",
            "inputs=[[[-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.74219 -2.64062 ... 3.60938 -0.181641 -1.78906]\n",
            "  ...\n",
            "  [-0.800781 1.74219 -2.64062 ... 3.625 -0.181641 -1.82812]\n",
            "  [-0.792969 1.74219 -2.64062 ... 3.57812 -0.174805 -1.82031]\n",
            "  [-0.78125 1.74219 -2.65625 ... 3.59375 -0.183594 -1.8125]]\n",
            "\n",
            " [[-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.74219 -2.64062 ... 3.60938 -0.181641 -1.78906]\n",
            "  ...\n",
            "  [-0.800781 1.74219 -2.64062 ... 3.625 -0.181641 -1.82812]\n",
            "  [-0.792969 1.74219 -2.64062 ... 3.57812 -0.174805 -1.82031]\n",
            "  [-0.78125 1.74219 -2.65625 ... 3.59375 -0.183594 -1.8125]]\n",
            "\n",
            " [[-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.74219 -2.64062 ... 3.60938 -0.181641 -1.78906]\n",
            "  ...\n",
            "  [-0.800781 1.74219 -2.64062 ... 3.625 -0.181641 -1.82812]\n",
            "  [-0.792969 1.74219 -2.64062 ... 3.57812 -0.174805 -1.82031]\n",
            "  [-0.78125 1.74219 -2.65625 ... 3.59375 -0.183594 -1.8125]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.74219 -2.64062 ... 3.60938 -0.181641 -1.78906]\n",
            "  ...\n",
            "  [-0.800781 1.74219 -2.64062 ... 3.625 -0.181641 -1.82812]\n",
            "  [-0.792969 1.74219 -2.64062 ... 3.57812 -0.174805 -1.82031]\n",
            "  [-0.78125 1.74219 -2.65625 ... 3.59375 -0.183594 -1.8125]]\n",
            "\n",
            " [[-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.74219 -2.64062 ... 3.60938 -0.181641 -1.78906]\n",
            "  ...\n",
            "  [-0.800781 1.74219 -2.64062 ... 3.625 -0.181641 -1.82812]\n",
            "  [-0.792969 1.74219 -2.64062 ... 3.57812 -0.174805 -1.82031]\n",
            "  [-0.78125 1.74219 -2.65625 ... 3.59375 -0.183594 -1.8125]]\n",
            "\n",
            " [[-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.75 -2.67188 ... 3.5625 -0.154297 -1.8125]\n",
            "  [-0.789062 1.74219 -2.64062 ... 3.60938 -0.181641 -1.78906]\n",
            "  ...\n",
            "  [-0.800781 1.74219 -2.64062 ... 3.625 -0.181641 -1.82812]\n",
            "  [-0.792969 1.74219 -2.64062 ... 3.57812 -0.174805 -1.82031]\n",
            "  [-0.78125 1.74219 -2.65625 ... 3.59375 -0.183594 -1.8125]]]\n",
            "lnx=[[[-0.310547 0.6875 -1.05469 ... 1.40625 -0.060791 -0.714844]\n",
            "  [-0.310547 0.6875 -1.05469 ... 1.40625 -0.060791 -0.714844]\n",
            "  [-0.310547 0.6875 -1.03906 ... 1.42188 -0.0712891 -0.703125]\n",
            "  ...\n",
            "  [-0.314453 0.6875 -1.03906 ... 1.42969 -0.0712891 -0.71875]\n",
            "  [-0.3125 0.6875 -1.03906 ... 1.40625 -0.0688477 -0.714844]\n",
            "  [-0.306641 0.6875 -1.04688 ... 1.41406 -0.0722656 -0.714844]]\n",
            "\n",
            " [[-0.310547 0.6875 -1.05469 ... 1.40625 -0.060791 -0.714844]\n",
            "  [-0.310547 0.6875 -1.05469 ... 1.40625 -0.060791 -0.714844]\n",
            "  [-0.310547 0.6875 -1.03906 ... 1.42188 -0.0712891 -0.703125]\n",
            "  ...\n",
            "  [-0.314453 0.6875 -1.03906 ... 1.42969 -0.0712891 -0.71875]\n",
            "  [-0.3125 0.6875 -1.03906 ... 1.40625 -0.0688477 -0.714844]\n",
            "  [-0.306641 0.6875 -1.04688 ... 1.41406 -0.0722656 -0.714844]]\n",
            "\n",
            " [[-0.310547 0.6875 -1.05469 ... 1.40625 -0.060791 -0.714844]\n",
            "  [-0.310547 0.6875 -1.05469 ... 1.40625 -0.060791 -0.714844]\n",
            "  [-0.310547 0.6875 -1.03906 ... 1.42188 -0.0712891 -0.703125]\n",
            "  ...\n",
            "  [-0.314453 0.6875 -1.03906 ... 1.42969 -0.0712891 -0.71875]\n",
            "  [-0.3125 0.6875 -1.03906 ... 1.40625 -0.0688477 -0.714844]\n",
            "  [-0.306641 0.6875 -1.04688 ... 1.41406 -0.0722656 -0.714844]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.310547 0.6875 -1.05469 ... 1.40625 -0.060791 -0.714844]\n",
            "  [-0.310547 0.6875 -1.05469 ... 1.40625 -0.060791 -0.714844]\n",
            "  [-0.310547 0.6875 -1.03906 ... 1.42188 -0.0712891 -0.703125]\n",
            "  ...\n",
            "  [-0.314453 0.6875 -1.03906 ... 1.42969 -0.0712891 -0.71875]\n",
            "  [-0.3125 0.6875 -1.03906 ... 1.40625 -0.0688477 -0.714844]\n",
            "  [-0.306641 0.6875 -1.04688 ... 1.41406 -0.0722656 -0.714844]]\n",
            "\n",
            " [[-0.310547 0.6875 -1.05469 ... 1.40625 -0.060791 -0.714844]\n",
            "  [-0.310547 0.6875 -1.05469 ... 1.40625 -0.060791 -0.714844]\n",
            "  [-0.310547 0.6875 -1.03906 ... 1.42188 -0.0712891 -0.703125]\n",
            "  ...\n",
            "  [-0.314453 0.6875 -1.03906 ... 1.42969 -0.0712891 -0.71875]\n",
            "  [-0.3125 0.6875 -1.03906 ... 1.40625 -0.0688477 -0.714844]\n",
            "  [-0.306641 0.6875 -1.04688 ... 1.41406 -0.0722656 -0.714844]]\n",
            "\n",
            " [[-0.310547 0.6875 -1.05469 ... 1.40625 -0.060791 -0.714844]\n",
            "  [-0.310547 0.6875 -1.05469 ... 1.40625 -0.060791 -0.714844]\n",
            "  [-0.310547 0.6875 -1.03906 ... 1.42188 -0.0712891 -0.703125]\n",
            "  ...\n",
            "  [-0.314453 0.6875 -1.03906 ... 1.42969 -0.0712891 -0.71875]\n",
            "  [-0.3125 0.6875 -1.03906 ... 1.40625 -0.0688477 -0.714844]\n",
            "  [-0.306641 0.6875 -1.04688 ... 1.41406 -0.0722656 -0.714844]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    ...\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00]]]]]\n",
            "attention_lnx=[[[0.369141 -0.0883789 1.14062 ... -0.277344 -0.15625 2]\n",
            "  [0.369141 -0.0883789 1.14062 ... -0.277344 -0.15625 2]\n",
            "  [0.367188 -0.0874023 1.14062 ... -0.277344 -0.148438 2]\n",
            "  ...\n",
            "  [0.373047 -0.0908203 1.14844 ... -0.287109 -0.155273 2.01562]\n",
            "  [0.376953 -0.0893555 1.14062 ... -0.287109 -0.155273 2.01562]\n",
            "  [0.376953 -0.090332 1.14844 ... -0.285156 -0.155273 2.01562]]\n",
            "\n",
            " [[0.369141 -0.0883789 1.14062 ... -0.277344 -0.15625 2]\n",
            "  [0.369141 -0.0883789 1.14062 ... -0.277344 -0.15625 2]\n",
            "  [0.367188 -0.0874023 1.14062 ... -0.277344 -0.148438 2]\n",
            "  ...\n",
            "  [0.373047 -0.0908203 1.14844 ... -0.287109 -0.155273 2.01562]\n",
            "  [0.376953 -0.0893555 1.14062 ... -0.287109 -0.155273 2.01562]\n",
            "  [0.376953 -0.090332 1.14844 ... -0.285156 -0.155273 2.01562]]\n",
            "\n",
            " [[0.369141 -0.0883789 1.14062 ... -0.277344 -0.15625 2]\n",
            "  [0.369141 -0.0883789 1.14062 ... -0.277344 -0.15625 2]\n",
            "  [0.367188 -0.0874023 1.14062 ... -0.277344 -0.148438 2]\n",
            "  ...\n",
            "  [0.373047 -0.0908203 1.14844 ... -0.287109 -0.155273 2.01562]\n",
            "  [0.376953 -0.0893555 1.14062 ... -0.287109 -0.155273 2.01562]\n",
            "  [0.376953 -0.090332 1.14844 ... -0.285156 -0.155273 2.01562]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.369141 -0.0883789 1.14062 ... -0.277344 -0.15625 2]\n",
            "  [0.369141 -0.0883789 1.14062 ... -0.277344 -0.15625 2]\n",
            "  [0.367188 -0.0874023 1.14062 ... -0.277344 -0.148438 2]\n",
            "  ...\n",
            "  [0.373047 -0.0908203 1.14844 ... -0.287109 -0.155273 2.01562]\n",
            "  [0.376953 -0.0893555 1.14062 ... -0.287109 -0.155273 2.01562]\n",
            "  [0.376953 -0.090332 1.14844 ... -0.285156 -0.155273 2.01562]]\n",
            "\n",
            " [[0.369141 -0.0883789 1.14062 ... -0.277344 -0.15625 2]\n",
            "  [0.369141 -0.0883789 1.14062 ... -0.277344 -0.15625 2]\n",
            "  [0.367188 -0.0874023 1.14062 ... -0.277344 -0.148438 2]\n",
            "  ...\n",
            "  [0.373047 -0.0908203 1.14844 ... -0.287109 -0.155273 2.01562]\n",
            "  [0.376953 -0.0893555 1.14062 ... -0.287109 -0.155273 2.01562]\n",
            "  [0.376953 -0.090332 1.14844 ... -0.285156 -0.155273 2.01562]]\n",
            "\n",
            " [[0.369141 -0.0883789 1.14062 ... -0.277344 -0.15625 2]\n",
            "  [0.369141 -0.0883789 1.14062 ... -0.277344 -0.15625 2]\n",
            "  [0.367188 -0.0874023 1.14062 ... -0.277344 -0.148438 2]\n",
            "  ...\n",
            "  [0.373047 -0.0908203 1.14844 ... -0.287109 -0.155273 2.01562]\n",
            "  [0.376953 -0.0893555 1.14062 ... -0.287109 -0.155273 2.01562]\n",
            "  [0.376953 -0.090332 1.14844 ... -0.285156 -0.155273 2.01562]]]\n",
            "attn_output=[[[-0.15332 0.605469 -0.558594 ... 1.19531 -0.113281 0.0703125]\n",
            "  [-0.15332 0.605469 -0.558594 ... 1.19531 -0.113281 0.0703125]\n",
            "  [-0.154297 0.605469 -0.546875 ... 1.21094 -0.120605 0.078125]\n",
            "  ...\n",
            "  [-0.15625 0.601562 -0.542969 ... 1.21875 -0.122559 0.0683594]\n",
            "  [-0.151367 0.605469 -0.546875 ... 1.20312 -0.120605 0.0698242]\n",
            "  [-0.147461 0.601562 -0.550781 ... 1.21094 -0.123535 0.0712891]]\n",
            "\n",
            " [[-0.15332 0.605469 -0.558594 ... 1.19531 -0.113281 0.0703125]\n",
            "  [-0.15332 0.605469 -0.558594 ... 1.19531 -0.113281 0.0703125]\n",
            "  [-0.154297 0.605469 -0.546875 ... 1.21094 -0.120605 0.078125]\n",
            "  ...\n",
            "  [-0.15625 0.601562 -0.542969 ... 1.21875 -0.122559 0.0683594]\n",
            "  [-0.151367 0.605469 -0.546875 ... 1.20312 -0.120605 0.0698242]\n",
            "  [-0.147461 0.601562 -0.550781 ... 1.21094 -0.123535 0.0712891]]\n",
            "\n",
            " [[-0.15332 0.605469 -0.558594 ... 1.19531 -0.113281 0.0703125]\n",
            "  [-0.15332 0.605469 -0.558594 ... 1.19531 -0.113281 0.0703125]\n",
            "  [-0.154297 0.605469 -0.546875 ... 1.21094 -0.120605 0.078125]\n",
            "  ...\n",
            "  [-0.15625 0.601562 -0.542969 ... 1.21875 -0.122559 0.0683594]\n",
            "  [-0.151367 0.605469 -0.546875 ... 1.20312 -0.120605 0.0698242]\n",
            "  [-0.147461 0.601562 -0.550781 ... 1.21094 -0.123535 0.0712891]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.15332 0.605469 -0.558594 ... 1.19531 -0.113281 0.0703125]\n",
            "  [-0.15332 0.605469 -0.558594 ... 1.19531 -0.113281 0.0703125]\n",
            "  [-0.154297 0.605469 -0.546875 ... 1.21094 -0.120605 0.078125]\n",
            "  ...\n",
            "  [-0.15625 0.601562 -0.542969 ... 1.21875 -0.122559 0.0683594]\n",
            "  [-0.151367 0.605469 -0.546875 ... 1.20312 -0.120605 0.0698242]\n",
            "  [-0.147461 0.601562 -0.550781 ... 1.21094 -0.123535 0.0712891]]\n",
            "\n",
            " [[-0.15332 0.605469 -0.558594 ... 1.19531 -0.113281 0.0703125]\n",
            "  [-0.15332 0.605469 -0.558594 ... 1.19531 -0.113281 0.0703125]\n",
            "  [-0.154297 0.605469 -0.546875 ... 1.21094 -0.120605 0.078125]\n",
            "  ...\n",
            "  [-0.15625 0.601562 -0.542969 ... 1.21875 -0.122559 0.0683594]\n",
            "  [-0.151367 0.605469 -0.546875 ... 1.20312 -0.120605 0.0698242]\n",
            "  [-0.147461 0.601562 -0.550781 ... 1.21094 -0.123535 0.0712891]]\n",
            "\n",
            " [[-0.15332 0.605469 -0.558594 ... 1.19531 -0.113281 0.0703125]\n",
            "  [-0.15332 0.605469 -0.558594 ... 1.19531 -0.113281 0.0703125]\n",
            "  [-0.154297 0.605469 -0.546875 ... 1.21094 -0.120605 0.078125]\n",
            "  ...\n",
            "  [-0.15625 0.601562 -0.542969 ... 1.21875 -0.122559 0.0683594]\n",
            "  [-0.151367 0.605469 -0.546875 ... 1.20312 -0.120605 0.0698242]\n",
            "  [-0.147461 0.601562 -0.550781 ... 1.21094 -0.123535 0.0712891]]]\n",
            "next_layer_addition_dropped_out=[[[-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.19043 1.1875 -2.4375 ... 4.0625 -0.273438 2.89062]\n",
            "  ...\n",
            "  [-0.197266 1.17969 -2.42188 ... 4.0625 -0.28125 2.875]\n",
            "  [-0.18457 1.1875 -2.4375 ... 4.03125 -0.275391 2.875]\n",
            "  [-0.173828 1.17969 -2.4375 ... 4.03125 -0.287109 2.89062]]\n",
            "\n",
            " [[-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.19043 1.1875 -2.4375 ... 4.0625 -0.273438 2.89062]\n",
            "  ...\n",
            "  [-0.197266 1.17969 -2.42188 ... 4.0625 -0.28125 2.875]\n",
            "  [-0.18457 1.1875 -2.4375 ... 4.03125 -0.275391 2.875]\n",
            "  [-0.173828 1.17969 -2.4375 ... 4.03125 -0.287109 2.89062]]\n",
            "\n",
            " [[-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.19043 1.1875 -2.4375 ... 4.0625 -0.273438 2.89062]\n",
            "  ...\n",
            "  [-0.197266 1.17969 -2.42188 ... 4.0625 -0.28125 2.875]\n",
            "  [-0.18457 1.1875 -2.4375 ... 4.03125 -0.275391 2.875]\n",
            "  [-0.173828 1.17969 -2.4375 ... 4.03125 -0.287109 2.89062]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.19043 1.1875 -2.4375 ... 4.0625 -0.273438 2.89062]\n",
            "  ...\n",
            "  [-0.197266 1.17969 -2.42188 ... 4.0625 -0.28125 2.875]\n",
            "  [-0.18457 1.1875 -2.4375 ... 4.03125 -0.275391 2.875]\n",
            "  [-0.173828 1.17969 -2.4375 ... 4.03125 -0.287109 2.89062]]\n",
            "\n",
            " [[-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.19043 1.1875 -2.4375 ... 4.0625 -0.273438 2.89062]\n",
            "  ...\n",
            "  [-0.197266 1.17969 -2.42188 ... 4.0625 -0.28125 2.875]\n",
            "  [-0.18457 1.1875 -2.4375 ... 4.03125 -0.275391 2.875]\n",
            "  [-0.173828 1.17969 -2.4375 ... 4.03125 -0.287109 2.89062]]\n",
            "\n",
            " [[-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.19043 1.1875 -2.4375 ... 4.0625 -0.273438 2.89062]\n",
            "  ...\n",
            "  [-0.197266 1.17969 -2.42188 ... 4.0625 -0.28125 2.875]\n",
            "  [-0.18457 1.1875 -2.4375 ... 4.03125 -0.275391 2.875]\n",
            "  [-0.173828 1.17969 -2.4375 ... 4.03125 -0.287109 2.89062]]]\n",
            "inputs=[[[-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.19043 1.1875 -2.4375 ... 4.0625 -0.273438 2.89062]\n",
            "  ...\n",
            "  [-0.197266 1.17969 -2.42188 ... 4.0625 -0.28125 2.875]\n",
            "  [-0.18457 1.1875 -2.4375 ... 4.03125 -0.275391 2.875]\n",
            "  [-0.173828 1.17969 -2.4375 ... 4.03125 -0.287109 2.89062]]\n",
            "\n",
            " [[-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.19043 1.1875 -2.4375 ... 4.0625 -0.273438 2.89062]\n",
            "  ...\n",
            "  [-0.197266 1.17969 -2.42188 ... 4.0625 -0.28125 2.875]\n",
            "  [-0.18457 1.1875 -2.4375 ... 4.03125 -0.275391 2.875]\n",
            "  [-0.173828 1.17969 -2.4375 ... 4.03125 -0.287109 2.89062]]\n",
            "\n",
            " [[-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.19043 1.1875 -2.4375 ... 4.0625 -0.273438 2.89062]\n",
            "  ...\n",
            "  [-0.197266 1.17969 -2.42188 ... 4.0625 -0.28125 2.875]\n",
            "  [-0.18457 1.1875 -2.4375 ... 4.03125 -0.275391 2.875]\n",
            "  [-0.173828 1.17969 -2.4375 ... 4.03125 -0.287109 2.89062]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.19043 1.1875 -2.4375 ... 4.0625 -0.273438 2.89062]\n",
            "  ...\n",
            "  [-0.197266 1.17969 -2.42188 ... 4.0625 -0.28125 2.875]\n",
            "  [-0.18457 1.1875 -2.4375 ... 4.03125 -0.275391 2.875]\n",
            "  [-0.173828 1.17969 -2.4375 ... 4.03125 -0.287109 2.89062]]\n",
            "\n",
            " [[-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.19043 1.1875 -2.4375 ... 4.0625 -0.273438 2.89062]\n",
            "  ...\n",
            "  [-0.197266 1.17969 -2.42188 ... 4.0625 -0.28125 2.875]\n",
            "  [-0.18457 1.1875 -2.4375 ... 4.03125 -0.275391 2.875]\n",
            "  [-0.173828 1.17969 -2.4375 ... 4.03125 -0.287109 2.89062]]\n",
            "\n",
            " [[-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.1875 1.19531 -2.48438 ... 4.03125 -0.259766 2.875]\n",
            "  [-0.19043 1.1875 -2.4375 ... 4.0625 -0.273438 2.89062]\n",
            "  ...\n",
            "  [-0.197266 1.17969 -2.42188 ... 4.0625 -0.28125 2.875]\n",
            "  [-0.18457 1.1875 -2.4375 ... 4.03125 -0.275391 2.875]\n",
            "  [-0.173828 1.17969 -2.4375 ... 4.03125 -0.287109 2.89062]]]\n",
            "lnx=[[[-0.065918 0.421875 -0.875 ... 1.42188 -0.0917969 1.01562]\n",
            "  [-0.065918 0.421875 -0.875 ... 1.42188 -0.0917969 1.01562]\n",
            "  [-0.0673828 0.417969 -0.859375 ... 1.42969 -0.0966797 1.02344]\n",
            "  ...\n",
            "  [-0.0698242 0.416016 -0.855469 ... 1.42969 -0.0991211 1.01562]\n",
            "  [-0.0649414 0.419922 -0.859375 ... 1.42188 -0.097168 1.01562]\n",
            "  [-0.0612793 0.416016 -0.859375 ... 1.42188 -0.101074 1.02344]]\n",
            "\n",
            " [[-0.065918 0.421875 -0.875 ... 1.42188 -0.0917969 1.01562]\n",
            "  [-0.065918 0.421875 -0.875 ... 1.42188 -0.0917969 1.01562]\n",
            "  [-0.0673828 0.417969 -0.859375 ... 1.42969 -0.0966797 1.02344]\n",
            "  ...\n",
            "  [-0.0698242 0.416016 -0.855469 ... 1.42969 -0.0991211 1.01562]\n",
            "  [-0.0649414 0.419922 -0.859375 ... 1.42188 -0.097168 1.01562]\n",
            "  [-0.0612793 0.416016 -0.859375 ... 1.42188 -0.101074 1.02344]]\n",
            "\n",
            " [[-0.065918 0.421875 -0.875 ... 1.42188 -0.0917969 1.01562]\n",
            "  [-0.065918 0.421875 -0.875 ... 1.42188 -0.0917969 1.01562]\n",
            "  [-0.0673828 0.417969 -0.859375 ... 1.42969 -0.0966797 1.02344]\n",
            "  ...\n",
            "  [-0.0698242 0.416016 -0.855469 ... 1.42969 -0.0991211 1.01562]\n",
            "  [-0.0649414 0.419922 -0.859375 ... 1.42188 -0.097168 1.01562]\n",
            "  [-0.0612793 0.416016 -0.859375 ... 1.42188 -0.101074 1.02344]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.065918 0.421875 -0.875 ... 1.42188 -0.0917969 1.01562]\n",
            "  [-0.065918 0.421875 -0.875 ... 1.42188 -0.0917969 1.01562]\n",
            "  [-0.0673828 0.417969 -0.859375 ... 1.42969 -0.0966797 1.02344]\n",
            "  ...\n",
            "  [-0.0698242 0.416016 -0.855469 ... 1.42969 -0.0991211 1.01562]\n",
            "  [-0.0649414 0.419922 -0.859375 ... 1.42188 -0.097168 1.01562]\n",
            "  [-0.0612793 0.416016 -0.859375 ... 1.42188 -0.101074 1.02344]]\n",
            "\n",
            " [[-0.065918 0.421875 -0.875 ... 1.42188 -0.0917969 1.01562]\n",
            "  [-0.065918 0.421875 -0.875 ... 1.42188 -0.0917969 1.01562]\n",
            "  [-0.0673828 0.417969 -0.859375 ... 1.42969 -0.0966797 1.02344]\n",
            "  ...\n",
            "  [-0.0698242 0.416016 -0.855469 ... 1.42969 -0.0991211 1.01562]\n",
            "  [-0.0649414 0.419922 -0.859375 ... 1.42188 -0.097168 1.01562]\n",
            "  [-0.0612793 0.416016 -0.859375 ... 1.42188 -0.101074 1.02344]]\n",
            "\n",
            " [[-0.065918 0.421875 -0.875 ... 1.42188 -0.0917969 1.01562]\n",
            "  [-0.065918 0.421875 -0.875 ... 1.42188 -0.0917969 1.01562]\n",
            "  [-0.0673828 0.417969 -0.859375 ... 1.42969 -0.0966797 1.02344]\n",
            "  ...\n",
            "  [-0.0698242 0.416016 -0.855469 ... 1.42969 -0.0991211 1.01562]\n",
            "  [-0.0649414 0.419922 -0.859375 ... 1.42188 -0.097168 1.01562]\n",
            "  [-0.0612793 0.416016 -0.859375 ... 1.42188 -0.101074 1.02344]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    ...\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00]]]]]\n",
            "attention_lnx=[[[-0.832031 0.558594 0.345703 ... 2.45312 -0.19043 -1.64062]\n",
            "  [-0.832031 0.558594 0.345703 ... 2.45312 -0.19043 -1.64062]\n",
            "  [-0.832031 0.554688 0.339844 ... 2.45312 -0.191406 -1.63281]\n",
            "  ...\n",
            "  [-0.835938 0.554688 0.351562 ... 2.45312 -0.192383 -1.625]\n",
            "  [-0.835938 0.554688 0.351562 ... 2.45312 -0.18457 -1.625]\n",
            "  [-0.835938 0.558594 0.353516 ... 2.45312 -0.19043 -1.63281]]\n",
            "\n",
            " [[-0.832031 0.558594 0.345703 ... 2.45312 -0.19043 -1.64062]\n",
            "  [-0.832031 0.558594 0.345703 ... 2.45312 -0.19043 -1.64062]\n",
            "  [-0.832031 0.554688 0.339844 ... 2.45312 -0.191406 -1.63281]\n",
            "  ...\n",
            "  [-0.835938 0.554688 0.351562 ... 2.45312 -0.192383 -1.625]\n",
            "  [-0.835938 0.554688 0.351562 ... 2.45312 -0.18457 -1.625]\n",
            "  [-0.835938 0.558594 0.353516 ... 2.45312 -0.19043 -1.63281]]\n",
            "\n",
            " [[-0.832031 0.558594 0.345703 ... 2.45312 -0.19043 -1.64062]\n",
            "  [-0.832031 0.558594 0.345703 ... 2.45312 -0.19043 -1.64062]\n",
            "  [-0.832031 0.554688 0.339844 ... 2.45312 -0.191406 -1.63281]\n",
            "  ...\n",
            "  [-0.835938 0.554688 0.351562 ... 2.45312 -0.192383 -1.625]\n",
            "  [-0.835938 0.554688 0.351562 ... 2.45312 -0.18457 -1.625]\n",
            "  [-0.835938 0.558594 0.353516 ... 2.45312 -0.19043 -1.63281]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.832031 0.558594 0.345703 ... 2.45312 -0.19043 -1.64062]\n",
            "  [-0.832031 0.558594 0.345703 ... 2.45312 -0.19043 -1.64062]\n",
            "  [-0.832031 0.554688 0.339844 ... 2.45312 -0.191406 -1.63281]\n",
            "  ...\n",
            "  [-0.835938 0.554688 0.351562 ... 2.45312 -0.192383 -1.625]\n",
            "  [-0.835938 0.554688 0.351562 ... 2.45312 -0.18457 -1.625]\n",
            "  [-0.835938 0.558594 0.353516 ... 2.45312 -0.19043 -1.63281]]\n",
            "\n",
            " [[-0.832031 0.558594 0.345703 ... 2.45312 -0.19043 -1.64062]\n",
            "  [-0.832031 0.558594 0.345703 ... 2.45312 -0.19043 -1.64062]\n",
            "  [-0.832031 0.554688 0.339844 ... 2.45312 -0.191406 -1.63281]\n",
            "  ...\n",
            "  [-0.835938 0.554688 0.351562 ... 2.45312 -0.192383 -1.625]\n",
            "  [-0.835938 0.554688 0.351562 ... 2.45312 -0.18457 -1.625]\n",
            "  [-0.835938 0.558594 0.353516 ... 2.45312 -0.19043 -1.63281]]\n",
            "\n",
            " [[-0.832031 0.558594 0.345703 ... 2.45312 -0.19043 -1.64062]\n",
            "  [-0.832031 0.558594 0.345703 ... 2.45312 -0.19043 -1.64062]\n",
            "  [-0.832031 0.554688 0.339844 ... 2.45312 -0.191406 -1.63281]\n",
            "  ...\n",
            "  [-0.835938 0.554688 0.351562 ... 2.45312 -0.192383 -1.625]\n",
            "  [-0.835938 0.554688 0.351562 ... 2.45312 -0.18457 -1.625]\n",
            "  [-0.835938 0.558594 0.353516 ... 2.45312 -0.19043 -1.63281]]]\n",
            "attn_output=[[[-0.341797 0.585938 -0.714844 ... 2.17188 -0.150391 0.412109]\n",
            "  [-0.341797 0.585938 -0.714844 ... 2.17188 -0.150391 0.412109]\n",
            "  [-0.341797 0.582031 -0.699219 ... 2.1875 -0.155273 0.419922]\n",
            "  ...\n",
            "  [-0.34375 0.578125 -0.6875 ... 2.17188 -0.158203 0.417969]\n",
            "  [-0.339844 0.582031 -0.695312 ... 2.15625 -0.15332 0.417969]\n",
            "  [-0.335938 0.582031 -0.695312 ... 2.17188 -0.160156 0.419922]]\n",
            "\n",
            " [[-0.341797 0.585938 -0.714844 ... 2.17188 -0.150391 0.412109]\n",
            "  [-0.341797 0.585938 -0.714844 ... 2.17188 -0.150391 0.412109]\n",
            "  [-0.341797 0.582031 -0.699219 ... 2.1875 -0.155273 0.419922]\n",
            "  ...\n",
            "  [-0.34375 0.578125 -0.6875 ... 2.17188 -0.158203 0.417969]\n",
            "  [-0.339844 0.582031 -0.695312 ... 2.15625 -0.15332 0.417969]\n",
            "  [-0.335938 0.582031 -0.695312 ... 2.17188 -0.160156 0.419922]]\n",
            "\n",
            " [[-0.341797 0.585938 -0.714844 ... 2.17188 -0.150391 0.412109]\n",
            "  [-0.341797 0.585938 -0.714844 ... 2.17188 -0.150391 0.412109]\n",
            "  [-0.341797 0.582031 -0.699219 ... 2.1875 -0.155273 0.419922]\n",
            "  ...\n",
            "  [-0.34375 0.578125 -0.6875 ... 2.17188 -0.158203 0.417969]\n",
            "  [-0.339844 0.582031 -0.695312 ... 2.15625 -0.15332 0.417969]\n",
            "  [-0.335938 0.582031 -0.695312 ... 2.17188 -0.160156 0.419922]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.341797 0.585938 -0.714844 ... 2.17188 -0.150391 0.412109]\n",
            "  [-0.341797 0.585938 -0.714844 ... 2.17188 -0.150391 0.412109]\n",
            "  [-0.341797 0.582031 -0.699219 ... 2.1875 -0.155273 0.419922]\n",
            "  ...\n",
            "  [-0.34375 0.578125 -0.6875 ... 2.17188 -0.158203 0.417969]\n",
            "  [-0.339844 0.582031 -0.695312 ... 2.15625 -0.15332 0.417969]\n",
            "  [-0.335938 0.582031 -0.695312 ... 2.17188 -0.160156 0.419922]]\n",
            "\n",
            " [[-0.341797 0.585938 -0.714844 ... 2.17188 -0.150391 0.412109]\n",
            "  [-0.341797 0.585938 -0.714844 ... 2.17188 -0.150391 0.412109]\n",
            "  [-0.341797 0.582031 -0.699219 ... 2.1875 -0.155273 0.419922]\n",
            "  ...\n",
            "  [-0.34375 0.578125 -0.6875 ... 2.17188 -0.158203 0.417969]\n",
            "  [-0.339844 0.582031 -0.695312 ... 2.15625 -0.15332 0.417969]\n",
            "  [-0.335938 0.582031 -0.695312 ... 2.17188 -0.160156 0.419922]]\n",
            "\n",
            " [[-0.341797 0.585938 -0.714844 ... 2.17188 -0.150391 0.412109]\n",
            "  [-0.341797 0.585938 -0.714844 ... 2.17188 -0.150391 0.412109]\n",
            "  [-0.341797 0.582031 -0.699219 ... 2.1875 -0.155273 0.419922]\n",
            "  ...\n",
            "  [-0.34375 0.578125 -0.6875 ... 2.17188 -0.158203 0.417969]\n",
            "  [-0.339844 0.582031 -0.695312 ... 2.15625 -0.15332 0.417969]\n",
            "  [-0.335938 0.582031 -0.695312 ... 2.17188 -0.160156 0.419922]]]\n",
            "next_layer_addition_dropped_out=[[[-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.929688 -2.57812 ... 6.5 -0.566406 1]\n",
            "  ...\n",
            "  [-2.85938 0.929688 -2.5625 ... 6.46875 -0.589844 0.992188]\n",
            "  [-2.82812 0.941406 -2.5625 ... 6.4375 -0.582031 0.996094]\n",
            "  [-2.82812 0.933594 -2.5625 ... 6.46875 -0.589844 1]]\n",
            "\n",
            " [[-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.929688 -2.57812 ... 6.5 -0.566406 1]\n",
            "  ...\n",
            "  [-2.85938 0.929688 -2.5625 ... 6.46875 -0.589844 0.992188]\n",
            "  [-2.82812 0.941406 -2.5625 ... 6.4375 -0.582031 0.996094]\n",
            "  [-2.82812 0.933594 -2.5625 ... 6.46875 -0.589844 1]]\n",
            "\n",
            " [[-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.929688 -2.57812 ... 6.5 -0.566406 1]\n",
            "  ...\n",
            "  [-2.85938 0.929688 -2.5625 ... 6.46875 -0.589844 0.992188]\n",
            "  [-2.82812 0.941406 -2.5625 ... 6.4375 -0.582031 0.996094]\n",
            "  [-2.82812 0.933594 -2.5625 ... 6.46875 -0.589844 1]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.929688 -2.57812 ... 6.5 -0.566406 1]\n",
            "  ...\n",
            "  [-2.85938 0.929688 -2.5625 ... 6.46875 -0.589844 0.992188]\n",
            "  [-2.82812 0.941406 -2.5625 ... 6.4375 -0.582031 0.996094]\n",
            "  [-2.82812 0.933594 -2.5625 ... 6.46875 -0.589844 1]]\n",
            "\n",
            " [[-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.929688 -2.57812 ... 6.5 -0.566406 1]\n",
            "  ...\n",
            "  [-2.85938 0.929688 -2.5625 ... 6.46875 -0.589844 0.992188]\n",
            "  [-2.82812 0.941406 -2.5625 ... 6.4375 -0.582031 0.996094]\n",
            "  [-2.82812 0.933594 -2.5625 ... 6.46875 -0.589844 1]]\n",
            "\n",
            " [[-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.929688 -2.57812 ... 6.5 -0.566406 1]\n",
            "  ...\n",
            "  [-2.85938 0.929688 -2.5625 ... 6.46875 -0.589844 0.992188]\n",
            "  [-2.82812 0.941406 -2.5625 ... 6.4375 -0.582031 0.996094]\n",
            "  [-2.82812 0.933594 -2.5625 ... 6.46875 -0.589844 1]]]\n",
            "inputs=[[[-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.929688 -2.57812 ... 6.5 -0.566406 1]\n",
            "  ...\n",
            "  [-2.85938 0.929688 -2.5625 ... 6.46875 -0.589844 0.992188]\n",
            "  [-2.82812 0.941406 -2.5625 ... 6.4375 -0.582031 0.996094]\n",
            "  [-2.82812 0.933594 -2.5625 ... 6.46875 -0.589844 1]]\n",
            "\n",
            " [[-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.929688 -2.57812 ... 6.5 -0.566406 1]\n",
            "  ...\n",
            "  [-2.85938 0.929688 -2.5625 ... 6.46875 -0.589844 0.992188]\n",
            "  [-2.82812 0.941406 -2.5625 ... 6.4375 -0.582031 0.996094]\n",
            "  [-2.82812 0.933594 -2.5625 ... 6.46875 -0.589844 1]]\n",
            "\n",
            " [[-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.929688 -2.57812 ... 6.5 -0.566406 1]\n",
            "  ...\n",
            "  [-2.85938 0.929688 -2.5625 ... 6.46875 -0.589844 0.992188]\n",
            "  [-2.82812 0.941406 -2.5625 ... 6.4375 -0.582031 0.996094]\n",
            "  [-2.82812 0.933594 -2.5625 ... 6.46875 -0.589844 1]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.929688 -2.57812 ... 6.5 -0.566406 1]\n",
            "  ...\n",
            "  [-2.85938 0.929688 -2.5625 ... 6.46875 -0.589844 0.992188]\n",
            "  [-2.82812 0.941406 -2.5625 ... 6.4375 -0.582031 0.996094]\n",
            "  [-2.82812 0.933594 -2.5625 ... 6.46875 -0.589844 1]]\n",
            "\n",
            " [[-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.929688 -2.57812 ... 6.5 -0.566406 1]\n",
            "  ...\n",
            "  [-2.85938 0.929688 -2.5625 ... 6.46875 -0.589844 0.992188]\n",
            "  [-2.82812 0.941406 -2.5625 ... 6.4375 -0.582031 0.996094]\n",
            "  [-2.82812 0.933594 -2.5625 ... 6.46875 -0.589844 1]]\n",
            "\n",
            " [[-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.949219 -2.625 ... 6.46875 -0.558594 0.980469]\n",
            "  [-2.84375 0.929688 -2.57812 ... 6.5 -0.566406 1]\n",
            "  ...\n",
            "  [-2.85938 0.929688 -2.5625 ... 6.46875 -0.589844 0.992188]\n",
            "  [-2.82812 0.941406 -2.5625 ... 6.4375 -0.582031 0.996094]\n",
            "  [-2.82812 0.933594 -2.5625 ... 6.46875 -0.589844 1]]]\n",
            "lnx=[[[-0.925781 0.308594 -0.855469 ... 2.10938 -0.181641 0.320312]\n",
            "  [-0.925781 0.308594 -0.855469 ... 2.10938 -0.181641 0.320312]\n",
            "  [-0.925781 0.302734 -0.839844 ... 2.125 -0.18457 0.326172]\n",
            "  ...\n",
            "  [-0.929688 0.302734 -0.835938 ... 2.10938 -0.192383 0.324219]\n",
            "  [-0.921875 0.306641 -0.835938 ... 2.09375 -0.189453 0.324219]\n",
            "  [-0.921875 0.304688 -0.835938 ... 2.10938 -0.192383 0.326172]]\n",
            "\n",
            " [[-0.925781 0.308594 -0.855469 ... 2.10938 -0.181641 0.320312]\n",
            "  [-0.925781 0.308594 -0.855469 ... 2.10938 -0.181641 0.320312]\n",
            "  [-0.925781 0.302734 -0.839844 ... 2.125 -0.18457 0.326172]\n",
            "  ...\n",
            "  [-0.929688 0.302734 -0.835938 ... 2.10938 -0.192383 0.324219]\n",
            "  [-0.921875 0.306641 -0.835938 ... 2.09375 -0.189453 0.324219]\n",
            "  [-0.921875 0.304688 -0.835938 ... 2.10938 -0.192383 0.326172]]\n",
            "\n",
            " [[-0.925781 0.308594 -0.855469 ... 2.10938 -0.181641 0.320312]\n",
            "  [-0.925781 0.308594 -0.855469 ... 2.10938 -0.181641 0.320312]\n",
            "  [-0.925781 0.302734 -0.839844 ... 2.125 -0.18457 0.326172]\n",
            "  ...\n",
            "  [-0.929688 0.302734 -0.835938 ... 2.10938 -0.192383 0.324219]\n",
            "  [-0.921875 0.306641 -0.835938 ... 2.09375 -0.189453 0.324219]\n",
            "  [-0.921875 0.304688 -0.835938 ... 2.10938 -0.192383 0.326172]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.925781 0.308594 -0.855469 ... 2.10938 -0.181641 0.320312]\n",
            "  [-0.925781 0.308594 -0.855469 ... 2.10938 -0.181641 0.320312]\n",
            "  [-0.925781 0.302734 -0.839844 ... 2.125 -0.18457 0.326172]\n",
            "  ...\n",
            "  [-0.929688 0.302734 -0.835938 ... 2.10938 -0.192383 0.324219]\n",
            "  [-0.921875 0.306641 -0.835938 ... 2.09375 -0.189453 0.324219]\n",
            "  [-0.921875 0.304688 -0.835938 ... 2.10938 -0.192383 0.326172]]\n",
            "\n",
            " [[-0.925781 0.308594 -0.855469 ... 2.10938 -0.181641 0.320312]\n",
            "  [-0.925781 0.308594 -0.855469 ... 2.10938 -0.181641 0.320312]\n",
            "  [-0.925781 0.302734 -0.839844 ... 2.125 -0.18457 0.326172]\n",
            "  ...\n",
            "  [-0.929688 0.302734 -0.835938 ... 2.10938 -0.192383 0.324219]\n",
            "  [-0.921875 0.306641 -0.835938 ... 2.09375 -0.189453 0.324219]\n",
            "  [-0.921875 0.304688 -0.835938 ... 2.10938 -0.192383 0.326172]]\n",
            "\n",
            " [[-0.925781 0.308594 -0.855469 ... 2.10938 -0.181641 0.320312]\n",
            "  [-0.925781 0.308594 -0.855469 ... 2.10938 -0.181641 0.320312]\n",
            "  [-0.925781 0.302734 -0.839844 ... 2.125 -0.18457 0.326172]\n",
            "  ...\n",
            "  [-0.929688 0.302734 -0.835938 ... 2.10938 -0.192383 0.324219]\n",
            "  [-0.921875 0.306641 -0.835938 ... 2.09375 -0.189453 0.324219]\n",
            "  [-0.921875 0.304688 -0.835938 ... 2.10938 -0.192383 0.326172]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    ...\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00]]]]]\n",
            "attention_lnx=[[[0.337891 -0.613281 -1.6875 ... 1.58594 -0.769531 -0.605469]\n",
            "  [0.337891 -0.613281 -1.6875 ... 1.58594 -0.769531 -0.605469]\n",
            "  [0.339844 -0.617188 -1.6875 ... 1.59375 -0.769531 -0.613281]\n",
            "  ...\n",
            "  [0.335938 -0.609375 -1.6875 ... 1.58594 -0.765625 -0.613281]\n",
            "  [0.335938 -0.609375 -1.6875 ... 1.59375 -0.765625 -0.613281]\n",
            "  [0.335938 -0.609375 -1.6875 ... 1.59375 -0.765625 -0.613281]]\n",
            "\n",
            " [[0.337891 -0.613281 -1.6875 ... 1.58594 -0.769531 -0.605469]\n",
            "  [0.337891 -0.613281 -1.6875 ... 1.58594 -0.769531 -0.605469]\n",
            "  [0.339844 -0.617188 -1.6875 ... 1.59375 -0.769531 -0.613281]\n",
            "  ...\n",
            "  [0.335938 -0.609375 -1.6875 ... 1.58594 -0.765625 -0.613281]\n",
            "  [0.335938 -0.609375 -1.6875 ... 1.59375 -0.765625 -0.613281]\n",
            "  [0.335938 -0.609375 -1.6875 ... 1.59375 -0.765625 -0.613281]]\n",
            "\n",
            " [[0.337891 -0.613281 -1.6875 ... 1.58594 -0.769531 -0.605469]\n",
            "  [0.337891 -0.613281 -1.6875 ... 1.58594 -0.769531 -0.605469]\n",
            "  [0.339844 -0.617188 -1.6875 ... 1.59375 -0.769531 -0.613281]\n",
            "  ...\n",
            "  [0.335938 -0.609375 -1.6875 ... 1.58594 -0.765625 -0.613281]\n",
            "  [0.335938 -0.609375 -1.6875 ... 1.59375 -0.765625 -0.613281]\n",
            "  [0.335938 -0.609375 -1.6875 ... 1.59375 -0.765625 -0.613281]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.337891 -0.613281 -1.6875 ... 1.58594 -0.769531 -0.605469]\n",
            "  [0.337891 -0.613281 -1.6875 ... 1.58594 -0.769531 -0.605469]\n",
            "  [0.339844 -0.617188 -1.6875 ... 1.59375 -0.769531 -0.613281]\n",
            "  ...\n",
            "  [0.335938 -0.609375 -1.6875 ... 1.58594 -0.765625 -0.613281]\n",
            "  [0.335938 -0.609375 -1.6875 ... 1.59375 -0.765625 -0.613281]\n",
            "  [0.335938 -0.609375 -1.6875 ... 1.59375 -0.765625 -0.613281]]\n",
            "\n",
            " [[0.337891 -0.613281 -1.6875 ... 1.58594 -0.769531 -0.605469]\n",
            "  [0.337891 -0.613281 -1.6875 ... 1.58594 -0.769531 -0.605469]\n",
            "  [0.339844 -0.617188 -1.6875 ... 1.59375 -0.769531 -0.613281]\n",
            "  ...\n",
            "  [0.335938 -0.609375 -1.6875 ... 1.58594 -0.765625 -0.613281]\n",
            "  [0.335938 -0.609375 -1.6875 ... 1.59375 -0.765625 -0.613281]\n",
            "  [0.335938 -0.609375 -1.6875 ... 1.59375 -0.765625 -0.613281]]\n",
            "\n",
            " [[0.337891 -0.613281 -1.6875 ... 1.58594 -0.769531 -0.605469]\n",
            "  [0.337891 -0.613281 -1.6875 ... 1.58594 -0.769531 -0.605469]\n",
            "  [0.339844 -0.617188 -1.6875 ... 1.59375 -0.769531 -0.613281]\n",
            "  ...\n",
            "  [0.335938 -0.609375 -1.6875 ... 1.58594 -0.765625 -0.613281]\n",
            "  [0.335938 -0.609375 -1.6875 ... 1.59375 -0.765625 -0.613281]\n",
            "  [0.335938 -0.609375 -1.6875 ... 1.59375 -0.765625 -0.613281]]]\n",
            "attn_output=[[[-0.777344 0.104004 -1.34375 ... 2.5 -0.412109 0.116211]\n",
            "  [-0.777344 0.104004 -1.34375 ... 2.5 -0.412109 0.116211]\n",
            "  [-0.777344 0.097168 -1.32031 ... 2.5 -0.416016 0.120117]\n",
            "  ...\n",
            "  [-0.789062 0.0996094 -1.32031 ... 2.5 -0.419922 0.117676]\n",
            "  [-0.777344 0.104004 -1.32031 ... 2.48438 -0.417969 0.118652]\n",
            "  [-0.777344 0.100586 -1.32031 ... 2.5 -0.421875 0.120117]]\n",
            "\n",
            " [[-0.777344 0.104004 -1.34375 ... 2.5 -0.412109 0.116211]\n",
            "  [-0.777344 0.104004 -1.34375 ... 2.5 -0.412109 0.116211]\n",
            "  [-0.777344 0.097168 -1.32031 ... 2.5 -0.416016 0.120117]\n",
            "  ...\n",
            "  [-0.789062 0.0996094 -1.32031 ... 2.5 -0.419922 0.117676]\n",
            "  [-0.777344 0.104004 -1.32031 ... 2.48438 -0.417969 0.118652]\n",
            "  [-0.777344 0.100586 -1.32031 ... 2.5 -0.421875 0.120117]]\n",
            "\n",
            " [[-0.777344 0.104004 -1.34375 ... 2.5 -0.412109 0.116211]\n",
            "  [-0.777344 0.104004 -1.34375 ... 2.5 -0.412109 0.116211]\n",
            "  [-0.777344 0.097168 -1.32031 ... 2.5 -0.416016 0.120117]\n",
            "  ...\n",
            "  [-0.789062 0.0996094 -1.32031 ... 2.5 -0.419922 0.117676]\n",
            "  [-0.777344 0.104004 -1.32031 ... 2.48438 -0.417969 0.118652]\n",
            "  [-0.777344 0.100586 -1.32031 ... 2.5 -0.421875 0.120117]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.777344 0.104004 -1.34375 ... 2.5 -0.412109 0.116211]\n",
            "  [-0.777344 0.104004 -1.34375 ... 2.5 -0.412109 0.116211]\n",
            "  [-0.777344 0.097168 -1.32031 ... 2.5 -0.416016 0.120117]\n",
            "  ...\n",
            "  [-0.789062 0.0996094 -1.32031 ... 2.5 -0.419922 0.117676]\n",
            "  [-0.777344 0.104004 -1.32031 ... 2.48438 -0.417969 0.118652]\n",
            "  [-0.777344 0.100586 -1.32031 ... 2.5 -0.421875 0.120117]]\n",
            "\n",
            " [[-0.777344 0.104004 -1.34375 ... 2.5 -0.412109 0.116211]\n",
            "  [-0.777344 0.104004 -1.34375 ... 2.5 -0.412109 0.116211]\n",
            "  [-0.777344 0.097168 -1.32031 ... 2.5 -0.416016 0.120117]\n",
            "  ...\n",
            "  [-0.789062 0.0996094 -1.32031 ... 2.5 -0.419922 0.117676]\n",
            "  [-0.777344 0.104004 -1.32031 ... 2.48438 -0.417969 0.118652]\n",
            "  [-0.777344 0.100586 -1.32031 ... 2.5 -0.421875 0.120117]]\n",
            "\n",
            " [[-0.777344 0.104004 -1.34375 ... 2.5 -0.412109 0.116211]\n",
            "  [-0.777344 0.104004 -1.34375 ... 2.5 -0.412109 0.116211]\n",
            "  [-0.777344 0.097168 -1.32031 ... 2.5 -0.416016 0.120117]\n",
            "  ...\n",
            "  [-0.789062 0.0996094 -1.32031 ... 2.5 -0.419922 0.117676]\n",
            "  [-0.777344 0.104004 -1.32031 ... 2.48438 -0.417969 0.118652]\n",
            "  [-0.777344 0.100586 -1.32031 ... 2.5 -0.421875 0.120117]]]\n",
            "next_layer_addition_dropped_out=[[[-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.217773 -4.65625 ... 8.0625 -1.07812 1.25781]\n",
            "  ...\n",
            "  [-2.29688 -0.201172 -4.65625 ... 8.0625 -1.08594 1.25]\n",
            "  [-2.26562 -0.201172 -4.65625 ... 8 -1.08594 1.24219]\n",
            "  [-2.26562 -0.198242 -4.65625 ... 8.0625 -1.10938 1.25]]\n",
            "\n",
            " [[-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.217773 -4.65625 ... 8.0625 -1.07812 1.25781]\n",
            "  ...\n",
            "  [-2.29688 -0.201172 -4.65625 ... 8.0625 -1.08594 1.25]\n",
            "  [-2.26562 -0.201172 -4.65625 ... 8 -1.08594 1.24219]\n",
            "  [-2.26562 -0.198242 -4.65625 ... 8.0625 -1.10938 1.25]]\n",
            "\n",
            " [[-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.217773 -4.65625 ... 8.0625 -1.07812 1.25781]\n",
            "  ...\n",
            "  [-2.29688 -0.201172 -4.65625 ... 8.0625 -1.08594 1.25]\n",
            "  [-2.26562 -0.201172 -4.65625 ... 8 -1.08594 1.24219]\n",
            "  [-2.26562 -0.198242 -4.65625 ... 8.0625 -1.10938 1.25]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.217773 -4.65625 ... 8.0625 -1.07812 1.25781]\n",
            "  ...\n",
            "  [-2.29688 -0.201172 -4.65625 ... 8.0625 -1.08594 1.25]\n",
            "  [-2.26562 -0.201172 -4.65625 ... 8 -1.08594 1.24219]\n",
            "  [-2.26562 -0.198242 -4.65625 ... 8.0625 -1.10938 1.25]]\n",
            "\n",
            " [[-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.217773 -4.65625 ... 8.0625 -1.07812 1.25781]\n",
            "  ...\n",
            "  [-2.29688 -0.201172 -4.65625 ... 8.0625 -1.08594 1.25]\n",
            "  [-2.26562 -0.201172 -4.65625 ... 8 -1.08594 1.24219]\n",
            "  [-2.26562 -0.198242 -4.65625 ... 8.0625 -1.10938 1.25]]\n",
            "\n",
            " [[-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.217773 -4.65625 ... 8.0625 -1.07812 1.25781]\n",
            "  ...\n",
            "  [-2.29688 -0.201172 -4.65625 ... 8.0625 -1.08594 1.25]\n",
            "  [-2.26562 -0.201172 -4.65625 ... 8 -1.08594 1.24219]\n",
            "  [-2.26562 -0.198242 -4.65625 ... 8.0625 -1.10938 1.25]]]\n",
            "inputs=[[[-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.217773 -4.65625 ... 8.0625 -1.07812 1.25781]\n",
            "  ...\n",
            "  [-2.29688 -0.201172 -4.65625 ... 8.0625 -1.08594 1.25]\n",
            "  [-2.26562 -0.201172 -4.65625 ... 8 -1.08594 1.24219]\n",
            "  [-2.26562 -0.198242 -4.65625 ... 8.0625 -1.10938 1.25]]\n",
            "\n",
            " [[-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.217773 -4.65625 ... 8.0625 -1.07812 1.25781]\n",
            "  ...\n",
            "  [-2.29688 -0.201172 -4.65625 ... 8.0625 -1.08594 1.25]\n",
            "  [-2.26562 -0.201172 -4.65625 ... 8 -1.08594 1.24219]\n",
            "  [-2.26562 -0.198242 -4.65625 ... 8.0625 -1.10938 1.25]]\n",
            "\n",
            " [[-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.217773 -4.65625 ... 8.0625 -1.07812 1.25781]\n",
            "  ...\n",
            "  [-2.29688 -0.201172 -4.65625 ... 8.0625 -1.08594 1.25]\n",
            "  [-2.26562 -0.201172 -4.65625 ... 8 -1.08594 1.24219]\n",
            "  [-2.26562 -0.198242 -4.65625 ... 8.0625 -1.10938 1.25]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.217773 -4.65625 ... 8.0625 -1.07812 1.25781]\n",
            "  ...\n",
            "  [-2.29688 -0.201172 -4.65625 ... 8.0625 -1.08594 1.25]\n",
            "  [-2.26562 -0.201172 -4.65625 ... 8 -1.08594 1.24219]\n",
            "  [-2.26562 -0.198242 -4.65625 ... 8.0625 -1.10938 1.25]]\n",
            "\n",
            " [[-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.217773 -4.65625 ... 8.0625 -1.07812 1.25781]\n",
            "  ...\n",
            "  [-2.29688 -0.201172 -4.65625 ... 8.0625 -1.08594 1.25]\n",
            "  [-2.26562 -0.201172 -4.65625 ... 8 -1.08594 1.24219]\n",
            "  [-2.26562 -0.198242 -4.65625 ... 8.0625 -1.10938 1.25]]\n",
            "\n",
            " [[-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.189453 -4.71875 ... 8.0625 -1.0625 1.24219]\n",
            "  [-2.25 -0.217773 -4.65625 ... 8.0625 -1.07812 1.25781]\n",
            "  ...\n",
            "  [-2.29688 -0.201172 -4.65625 ... 8.0625 -1.08594 1.25]\n",
            "  [-2.26562 -0.201172 -4.65625 ... 8 -1.08594 1.24219]\n",
            "  [-2.26562 -0.198242 -4.65625 ... 8.0625 -1.10938 1.25]]]\n",
            "lnx=[[[-0.6875 -0.0578613 -1.4375 ... 2.46875 -0.324219 0.378906]\n",
            "  [-0.6875 -0.0578613 -1.4375 ... 2.46875 -0.324219 0.378906]\n",
            "  [-0.6875 -0.0664062 -1.42188 ... 2.46875 -0.330078 0.384766]\n",
            "  ...\n",
            "  [-0.703125 -0.0615234 -1.42188 ... 2.46875 -0.332031 0.380859]\n",
            "  [-0.691406 -0.0615234 -1.42188 ... 2.4375 -0.332031 0.378906]\n",
            "  [-0.691406 -0.0605469 -1.42188 ... 2.46875 -0.337891 0.380859]]\n",
            "\n",
            " [[-0.6875 -0.0578613 -1.4375 ... 2.46875 -0.324219 0.378906]\n",
            "  [-0.6875 -0.0578613 -1.4375 ... 2.46875 -0.324219 0.378906]\n",
            "  [-0.6875 -0.0664062 -1.42188 ... 2.46875 -0.330078 0.384766]\n",
            "  ...\n",
            "  [-0.703125 -0.0615234 -1.42188 ... 2.46875 -0.332031 0.380859]\n",
            "  [-0.691406 -0.0615234 -1.42188 ... 2.4375 -0.332031 0.378906]\n",
            "  [-0.691406 -0.0605469 -1.42188 ... 2.46875 -0.337891 0.380859]]\n",
            "\n",
            " [[-0.6875 -0.0578613 -1.4375 ... 2.46875 -0.324219 0.378906]\n",
            "  [-0.6875 -0.0578613 -1.4375 ... 2.46875 -0.324219 0.378906]\n",
            "  [-0.6875 -0.0664062 -1.42188 ... 2.46875 -0.330078 0.384766]\n",
            "  ...\n",
            "  [-0.703125 -0.0615234 -1.42188 ... 2.46875 -0.332031 0.380859]\n",
            "  [-0.691406 -0.0615234 -1.42188 ... 2.4375 -0.332031 0.378906]\n",
            "  [-0.691406 -0.0605469 -1.42188 ... 2.46875 -0.337891 0.380859]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.6875 -0.0578613 -1.4375 ... 2.46875 -0.324219 0.378906]\n",
            "  [-0.6875 -0.0578613 -1.4375 ... 2.46875 -0.324219 0.378906]\n",
            "  [-0.6875 -0.0664062 -1.42188 ... 2.46875 -0.330078 0.384766]\n",
            "  ...\n",
            "  [-0.703125 -0.0615234 -1.42188 ... 2.46875 -0.332031 0.380859]\n",
            "  [-0.691406 -0.0615234 -1.42188 ... 2.4375 -0.332031 0.378906]\n",
            "  [-0.691406 -0.0605469 -1.42188 ... 2.46875 -0.337891 0.380859]]\n",
            "\n",
            " [[-0.6875 -0.0578613 -1.4375 ... 2.46875 -0.324219 0.378906]\n",
            "  [-0.6875 -0.0578613 -1.4375 ... 2.46875 -0.324219 0.378906]\n",
            "  [-0.6875 -0.0664062 -1.42188 ... 2.46875 -0.330078 0.384766]\n",
            "  ...\n",
            "  [-0.703125 -0.0615234 -1.42188 ... 2.46875 -0.332031 0.380859]\n",
            "  [-0.691406 -0.0615234 -1.42188 ... 2.4375 -0.332031 0.378906]\n",
            "  [-0.691406 -0.0605469 -1.42188 ... 2.46875 -0.337891 0.380859]]\n",
            "\n",
            " [[-0.6875 -0.0578613 -1.4375 ... 2.46875 -0.324219 0.378906]\n",
            "  [-0.6875 -0.0578613 -1.4375 ... 2.46875 -0.324219 0.378906]\n",
            "  [-0.6875 -0.0664062 -1.42188 ... 2.46875 -0.330078 0.384766]\n",
            "  ...\n",
            "  [-0.703125 -0.0615234 -1.42188 ... 2.46875 -0.332031 0.380859]\n",
            "  [-0.691406 -0.0615234 -1.42188 ... 2.4375 -0.332031 0.378906]\n",
            "  [-0.691406 -0.0605469 -1.42188 ... 2.46875 -0.337891 0.380859]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    ...\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00]]]]]\n",
            "attention_lnx=[[[0.186523 -0.546875 0.0664062 ... 1.72656 -0.25 -1.55469]\n",
            "  [0.186523 -0.546875 0.0664062 ... 1.72656 -0.25 -1.55469]\n",
            "  [0.19043 -0.550781 0.0617676 ... 1.72656 -0.248047 -1.55469]\n",
            "  ...\n",
            "  [0.180664 -0.542969 0.0603027 ... 1.72656 -0.25 -1.53906]\n",
            "  [0.181641 -0.542969 0.0603027 ... 1.73438 -0.255859 -1.53906]\n",
            "  [0.182617 -0.542969 0.0615234 ... 1.72656 -0.253906 -1.53906]]\n",
            "\n",
            " [[0.186523 -0.546875 0.0664062 ... 1.72656 -0.25 -1.55469]\n",
            "  [0.186523 -0.546875 0.0664062 ... 1.72656 -0.25 -1.55469]\n",
            "  [0.19043 -0.550781 0.0617676 ... 1.72656 -0.248047 -1.55469]\n",
            "  ...\n",
            "  [0.180664 -0.542969 0.0603027 ... 1.72656 -0.25 -1.53906]\n",
            "  [0.181641 -0.542969 0.0603027 ... 1.73438 -0.255859 -1.53906]\n",
            "  [0.182617 -0.542969 0.0615234 ... 1.72656 -0.253906 -1.53906]]\n",
            "\n",
            " [[0.186523 -0.546875 0.0664062 ... 1.72656 -0.25 -1.55469]\n",
            "  [0.186523 -0.546875 0.0664062 ... 1.72656 -0.25 -1.55469]\n",
            "  [0.19043 -0.550781 0.0617676 ... 1.72656 -0.248047 -1.55469]\n",
            "  ...\n",
            "  [0.180664 -0.542969 0.0603027 ... 1.72656 -0.25 -1.53906]\n",
            "  [0.181641 -0.542969 0.0603027 ... 1.73438 -0.255859 -1.53906]\n",
            "  [0.182617 -0.542969 0.0615234 ... 1.72656 -0.253906 -1.53906]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.186523 -0.546875 0.0664062 ... 1.72656 -0.25 -1.55469]\n",
            "  [0.186523 -0.546875 0.0664062 ... 1.72656 -0.25 -1.55469]\n",
            "  [0.19043 -0.550781 0.0617676 ... 1.72656 -0.248047 -1.55469]\n",
            "  ...\n",
            "  [0.180664 -0.542969 0.0603027 ... 1.72656 -0.25 -1.53906]\n",
            "  [0.181641 -0.542969 0.0603027 ... 1.73438 -0.255859 -1.53906]\n",
            "  [0.182617 -0.542969 0.0615234 ... 1.72656 -0.253906 -1.53906]]\n",
            "\n",
            " [[0.186523 -0.546875 0.0664062 ... 1.72656 -0.25 -1.55469]\n",
            "  [0.186523 -0.546875 0.0664062 ... 1.72656 -0.25 -1.55469]\n",
            "  [0.19043 -0.550781 0.0617676 ... 1.72656 -0.248047 -1.55469]\n",
            "  ...\n",
            "  [0.180664 -0.542969 0.0603027 ... 1.72656 -0.25 -1.53906]\n",
            "  [0.181641 -0.542969 0.0603027 ... 1.73438 -0.255859 -1.53906]\n",
            "  [0.182617 -0.542969 0.0615234 ... 1.72656 -0.253906 -1.53906]]\n",
            "\n",
            " [[0.186523 -0.546875 0.0664062 ... 1.72656 -0.25 -1.55469]\n",
            "  [0.186523 -0.546875 0.0664062 ... 1.72656 -0.25 -1.55469]\n",
            "  [0.19043 -0.550781 0.0617676 ... 1.72656 -0.248047 -1.55469]\n",
            "  ...\n",
            "  [0.180664 -0.542969 0.0603027 ... 1.72656 -0.25 -1.53906]\n",
            "  [0.181641 -0.542969 0.0603027 ... 1.73438 -0.255859 -1.53906]\n",
            "  [0.182617 -0.542969 0.0615234 ... 1.72656 -0.253906 -1.53906]]]\n",
            "attn_output=[[[-0.601562 -0.214844 -1.35938 ... 2.85938 -0.382812 -0.0917969]\n",
            "  [-0.601562 -0.214844 -1.35938 ... 2.85938 -0.382812 -0.0917969]\n",
            "  [-0.601562 -0.222656 -1.33594 ... 2.85938 -0.386719 -0.0869141]\n",
            "  ...\n",
            "  [-0.613281 -0.21582 -1.33594 ... 2.85938 -0.388672 -0.0834961]\n",
            "  [-0.605469 -0.21582 -1.33594 ... 2.84375 -0.390625 -0.0864258]\n",
            "  [-0.605469 -0.21582 -1.33594 ... 2.85938 -0.396484 -0.0844727]]\n",
            "\n",
            " [[-0.601562 -0.214844 -1.35938 ... 2.85938 -0.382812 -0.0917969]\n",
            "  [-0.601562 -0.214844 -1.35938 ... 2.85938 -0.382812 -0.0917969]\n",
            "  [-0.601562 -0.222656 -1.33594 ... 2.85938 -0.386719 -0.0869141]\n",
            "  ...\n",
            "  [-0.613281 -0.21582 -1.33594 ... 2.85938 -0.388672 -0.0834961]\n",
            "  [-0.605469 -0.21582 -1.33594 ... 2.84375 -0.390625 -0.0864258]\n",
            "  [-0.605469 -0.21582 -1.33594 ... 2.85938 -0.396484 -0.0844727]]\n",
            "\n",
            " [[-0.601562 -0.214844 -1.35938 ... 2.85938 -0.382812 -0.0917969]\n",
            "  [-0.601562 -0.214844 -1.35938 ... 2.85938 -0.382812 -0.0917969]\n",
            "  [-0.601562 -0.222656 -1.33594 ... 2.85938 -0.386719 -0.0869141]\n",
            "  ...\n",
            "  [-0.613281 -0.21582 -1.33594 ... 2.85938 -0.388672 -0.0834961]\n",
            "  [-0.605469 -0.21582 -1.33594 ... 2.84375 -0.390625 -0.0864258]\n",
            "  [-0.605469 -0.21582 -1.33594 ... 2.85938 -0.396484 -0.0844727]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.601562 -0.214844 -1.35938 ... 2.85938 -0.382812 -0.0917969]\n",
            "  [-0.601562 -0.214844 -1.35938 ... 2.85938 -0.382812 -0.0917969]\n",
            "  [-0.601562 -0.222656 -1.33594 ... 2.85938 -0.386719 -0.0869141]\n",
            "  ...\n",
            "  [-0.613281 -0.21582 -1.33594 ... 2.85938 -0.388672 -0.0834961]\n",
            "  [-0.605469 -0.21582 -1.33594 ... 2.84375 -0.390625 -0.0864258]\n",
            "  [-0.605469 -0.21582 -1.33594 ... 2.85938 -0.396484 -0.0844727]]\n",
            "\n",
            " [[-0.601562 -0.214844 -1.35938 ... 2.85938 -0.382812 -0.0917969]\n",
            "  [-0.601562 -0.214844 -1.35938 ... 2.85938 -0.382812 -0.0917969]\n",
            "  [-0.601562 -0.222656 -1.33594 ... 2.85938 -0.386719 -0.0869141]\n",
            "  ...\n",
            "  [-0.613281 -0.21582 -1.33594 ... 2.85938 -0.388672 -0.0834961]\n",
            "  [-0.605469 -0.21582 -1.33594 ... 2.84375 -0.390625 -0.0864258]\n",
            "  [-0.605469 -0.21582 -1.33594 ... 2.85938 -0.396484 -0.0844727]]\n",
            "\n",
            " [[-0.601562 -0.214844 -1.35938 ... 2.85938 -0.382812 -0.0917969]\n",
            "  [-0.601562 -0.214844 -1.35938 ... 2.85938 -0.382812 -0.0917969]\n",
            "  [-0.601562 -0.222656 -1.33594 ... 2.85938 -0.386719 -0.0869141]\n",
            "  ...\n",
            "  [-0.613281 -0.21582 -1.33594 ... 2.85938 -0.388672 -0.0834961]\n",
            "  [-0.605469 -0.21582 -1.33594 ... 2.84375 -0.390625 -0.0864258]\n",
            "  [-0.605469 -0.21582 -1.33594 ... 2.85938 -0.396484 -0.0844727]]]\n",
            "next_layer_addition_dropped_out=[[[-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.3125 -0.988281 -6.3125 ... 10.375 -1.29688 0.257812]\n",
            "  ...\n",
            "  [-2.375 -0.992188 -6.3125 ... 10.3125 -1.29688 0.259766]\n",
            "  [-2.32812 -0.976562 -6.3125 ... 10.25 -1.3125 0.255859]\n",
            "  [-2.32812 -0.984375 -6.3125 ... 10.375 -1.32812 0.261719]]\n",
            "\n",
            " [[-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.3125 -0.988281 -6.3125 ... 10.375 -1.29688 0.257812]\n",
            "  ...\n",
            "  [-2.375 -0.992188 -6.3125 ... 10.3125 -1.29688 0.259766]\n",
            "  [-2.32812 -0.976562 -6.3125 ... 10.25 -1.3125 0.255859]\n",
            "  [-2.32812 -0.984375 -6.3125 ... 10.375 -1.32812 0.261719]]\n",
            "\n",
            " [[-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.3125 -0.988281 -6.3125 ... 10.375 -1.29688 0.257812]\n",
            "  ...\n",
            "  [-2.375 -0.992188 -6.3125 ... 10.3125 -1.29688 0.259766]\n",
            "  [-2.32812 -0.976562 -6.3125 ... 10.25 -1.3125 0.255859]\n",
            "  [-2.32812 -0.984375 -6.3125 ... 10.375 -1.32812 0.261719]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.3125 -0.988281 -6.3125 ... 10.375 -1.29688 0.257812]\n",
            "  ...\n",
            "  [-2.375 -0.992188 -6.3125 ... 10.3125 -1.29688 0.259766]\n",
            "  [-2.32812 -0.976562 -6.3125 ... 10.25 -1.3125 0.255859]\n",
            "  [-2.32812 -0.984375 -6.3125 ... 10.375 -1.32812 0.261719]]\n",
            "\n",
            " [[-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.3125 -0.988281 -6.3125 ... 10.375 -1.29688 0.257812]\n",
            "  ...\n",
            "  [-2.375 -0.992188 -6.3125 ... 10.3125 -1.29688 0.259766]\n",
            "  [-2.32812 -0.976562 -6.3125 ... 10.25 -1.3125 0.255859]\n",
            "  [-2.32812 -0.984375 -6.3125 ... 10.375 -1.32812 0.261719]]\n",
            "\n",
            " [[-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.3125 -0.988281 -6.3125 ... 10.375 -1.29688 0.257812]\n",
            "  ...\n",
            "  [-2.375 -0.992188 -6.3125 ... 10.3125 -1.29688 0.259766]\n",
            "  [-2.32812 -0.976562 -6.3125 ... 10.25 -1.3125 0.255859]\n",
            "  [-2.32812 -0.984375 -6.3125 ... 10.375 -1.32812 0.261719]]]\n",
            "inputs=[[[-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.3125 -0.988281 -6.3125 ... 10.375 -1.29688 0.257812]\n",
            "  ...\n",
            "  [-2.375 -0.992188 -6.3125 ... 10.3125 -1.29688 0.259766]\n",
            "  [-2.32812 -0.976562 -6.3125 ... 10.25 -1.3125 0.255859]\n",
            "  [-2.32812 -0.984375 -6.3125 ... 10.375 -1.32812 0.261719]]\n",
            "\n",
            " [[-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.3125 -0.988281 -6.3125 ... 10.375 -1.29688 0.257812]\n",
            "  ...\n",
            "  [-2.375 -0.992188 -6.3125 ... 10.3125 -1.29688 0.259766]\n",
            "  [-2.32812 -0.976562 -6.3125 ... 10.25 -1.3125 0.255859]\n",
            "  [-2.32812 -0.984375 -6.3125 ... 10.375 -1.32812 0.261719]]\n",
            "\n",
            " [[-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.3125 -0.988281 -6.3125 ... 10.375 -1.29688 0.257812]\n",
            "  ...\n",
            "  [-2.375 -0.992188 -6.3125 ... 10.3125 -1.29688 0.259766]\n",
            "  [-2.32812 -0.976562 -6.3125 ... 10.25 -1.3125 0.255859]\n",
            "  [-2.32812 -0.984375 -6.3125 ... 10.375 -1.32812 0.261719]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.3125 -0.988281 -6.3125 ... 10.375 -1.29688 0.257812]\n",
            "  ...\n",
            "  [-2.375 -0.992188 -6.3125 ... 10.3125 -1.29688 0.259766]\n",
            "  [-2.32812 -0.976562 -6.3125 ... 10.25 -1.3125 0.255859]\n",
            "  [-2.32812 -0.984375 -6.3125 ... 10.375 -1.32812 0.261719]]\n",
            "\n",
            " [[-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.3125 -0.988281 -6.3125 ... 10.375 -1.29688 0.257812]\n",
            "  ...\n",
            "  [-2.375 -0.992188 -6.3125 ... 10.3125 -1.29688 0.259766]\n",
            "  [-2.32812 -0.976562 -6.3125 ... 10.25 -1.3125 0.255859]\n",
            "  [-2.32812 -0.984375 -6.3125 ... 10.375 -1.32812 0.261719]]\n",
            "\n",
            " [[-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.32812 -0.96875 -6.40625 ... 10.3125 -1.28125 0.241211]\n",
            "  [-2.3125 -0.988281 -6.3125 ... 10.375 -1.29688 0.257812]\n",
            "  ...\n",
            "  [-2.375 -0.992188 -6.3125 ... 10.3125 -1.29688 0.259766]\n",
            "  [-2.32812 -0.976562 -6.3125 ... 10.25 -1.3125 0.255859]\n",
            "  [-2.32812 -0.984375 -6.3125 ... 10.375 -1.32812 0.261719]]]\n",
            "lnx=[[[-0.664062 -0.277344 -1.82812 ... 2.95312 -0.367188 0.0688477]\n",
            "  [-0.664062 -0.277344 -1.82812 ... 2.95312 -0.367188 0.0688477]\n",
            "  [-0.660156 -0.283203 -1.80469 ... 2.96875 -0.371094 0.0737305]\n",
            "  ...\n",
            "  [-0.679688 -0.283203 -1.80469 ... 2.95312 -0.371094 0.0742188]\n",
            "  [-0.664062 -0.279297 -1.80469 ... 2.9375 -0.375 0.0732422]\n",
            "  [-0.664062 -0.28125 -1.80469 ... 2.96875 -0.378906 0.074707]]\n",
            "\n",
            " [[-0.664062 -0.277344 -1.82812 ... 2.95312 -0.367188 0.0688477]\n",
            "  [-0.664062 -0.277344 -1.82812 ... 2.95312 -0.367188 0.0688477]\n",
            "  [-0.660156 -0.283203 -1.80469 ... 2.96875 -0.371094 0.0737305]\n",
            "  ...\n",
            "  [-0.679688 -0.283203 -1.80469 ... 2.95312 -0.371094 0.0742188]\n",
            "  [-0.664062 -0.279297 -1.80469 ... 2.9375 -0.375 0.0732422]\n",
            "  [-0.664062 -0.28125 -1.80469 ... 2.96875 -0.378906 0.074707]]\n",
            "\n",
            " [[-0.664062 -0.277344 -1.82812 ... 2.95312 -0.367188 0.0688477]\n",
            "  [-0.664062 -0.277344 -1.82812 ... 2.95312 -0.367188 0.0688477]\n",
            "  [-0.660156 -0.283203 -1.80469 ... 2.96875 -0.371094 0.0737305]\n",
            "  ...\n",
            "  [-0.679688 -0.283203 -1.80469 ... 2.95312 -0.371094 0.0742188]\n",
            "  [-0.664062 -0.279297 -1.80469 ... 2.9375 -0.375 0.0732422]\n",
            "  [-0.664062 -0.28125 -1.80469 ... 2.96875 -0.378906 0.074707]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.664062 -0.277344 -1.82812 ... 2.95312 -0.367188 0.0688477]\n",
            "  [-0.664062 -0.277344 -1.82812 ... 2.95312 -0.367188 0.0688477]\n",
            "  [-0.660156 -0.283203 -1.80469 ... 2.96875 -0.371094 0.0737305]\n",
            "  ...\n",
            "  [-0.679688 -0.283203 -1.80469 ... 2.95312 -0.371094 0.0742188]\n",
            "  [-0.664062 -0.279297 -1.80469 ... 2.9375 -0.375 0.0732422]\n",
            "  [-0.664062 -0.28125 -1.80469 ... 2.96875 -0.378906 0.074707]]\n",
            "\n",
            " [[-0.664062 -0.277344 -1.82812 ... 2.95312 -0.367188 0.0688477]\n",
            "  [-0.664062 -0.277344 -1.82812 ... 2.95312 -0.367188 0.0688477]\n",
            "  [-0.660156 -0.283203 -1.80469 ... 2.96875 -0.371094 0.0737305]\n",
            "  ...\n",
            "  [-0.679688 -0.283203 -1.80469 ... 2.95312 -0.371094 0.0742188]\n",
            "  [-0.664062 -0.279297 -1.80469 ... 2.9375 -0.375 0.0732422]\n",
            "  [-0.664062 -0.28125 -1.80469 ... 2.96875 -0.378906 0.074707]]\n",
            "\n",
            " [[-0.664062 -0.277344 -1.82812 ... 2.95312 -0.367188 0.0688477]\n",
            "  [-0.664062 -0.277344 -1.82812 ... 2.95312 -0.367188 0.0688477]\n",
            "  [-0.660156 -0.283203 -1.80469 ... 2.96875 -0.371094 0.0737305]\n",
            "  ...\n",
            "  [-0.679688 -0.283203 -1.80469 ... 2.95312 -0.371094 0.0742188]\n",
            "  [-0.664062 -0.279297 -1.80469 ... 2.9375 -0.375 0.0732422]\n",
            "  [-0.664062 -0.28125 -1.80469 ... 2.96875 -0.378906 0.074707]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    ...\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00]]]]]\n",
            "attention_lnx=[[[-0.757812 0.578125 -0.259766 ... 1.47656 0.683594 0.000896454]\n",
            "  [-0.757812 0.578125 -0.259766 ... 1.47656 0.683594 0.000896454]\n",
            "  [-0.753906 0.570312 -0.265625 ... 1.46875 0.683594 -0.00184631]\n",
            "  ...\n",
            "  [-0.761719 0.5625 -0.261719 ... 1.47656 0.6875 0.00595093]\n",
            "  [-0.75 0.566406 -0.259766 ... 1.47656 0.691406 0.00646973]\n",
            "  [-0.757812 0.570312 -0.259766 ... 1.47656 0.691406 0.00604248]]\n",
            "\n",
            " [[-0.757812 0.578125 -0.259766 ... 1.47656 0.683594 0.000896454]\n",
            "  [-0.757812 0.578125 -0.259766 ... 1.47656 0.683594 0.000896454]\n",
            "  [-0.753906 0.570312 -0.265625 ... 1.46875 0.683594 -0.00184631]\n",
            "  ...\n",
            "  [-0.761719 0.5625 -0.261719 ... 1.47656 0.6875 0.00595093]\n",
            "  [-0.75 0.566406 -0.259766 ... 1.47656 0.691406 0.00646973]\n",
            "  [-0.757812 0.570312 -0.259766 ... 1.47656 0.691406 0.00604248]]\n",
            "\n",
            " [[-0.757812 0.578125 -0.259766 ... 1.47656 0.683594 0.000896454]\n",
            "  [-0.757812 0.578125 -0.259766 ... 1.47656 0.683594 0.000896454]\n",
            "  [-0.753906 0.570312 -0.265625 ... 1.46875 0.683594 -0.00184631]\n",
            "  ...\n",
            "  [-0.761719 0.5625 -0.261719 ... 1.47656 0.6875 0.00595093]\n",
            "  [-0.75 0.566406 -0.259766 ... 1.47656 0.691406 0.00646973]\n",
            "  [-0.757812 0.570312 -0.259766 ... 1.47656 0.691406 0.00604248]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.757812 0.578125 -0.259766 ... 1.47656 0.683594 0.000896454]\n",
            "  [-0.757812 0.578125 -0.259766 ... 1.47656 0.683594 0.000896454]\n",
            "  [-0.753906 0.570312 -0.265625 ... 1.46875 0.683594 -0.00184631]\n",
            "  ...\n",
            "  [-0.761719 0.5625 -0.261719 ... 1.47656 0.6875 0.00595093]\n",
            "  [-0.75 0.566406 -0.259766 ... 1.47656 0.691406 0.00646973]\n",
            "  [-0.757812 0.570312 -0.259766 ... 1.47656 0.691406 0.00604248]]\n",
            "\n",
            " [[-0.757812 0.578125 -0.259766 ... 1.47656 0.683594 0.000896454]\n",
            "  [-0.757812 0.578125 -0.259766 ... 1.47656 0.683594 0.000896454]\n",
            "  [-0.753906 0.570312 -0.265625 ... 1.46875 0.683594 -0.00184631]\n",
            "  ...\n",
            "  [-0.761719 0.5625 -0.261719 ... 1.47656 0.6875 0.00595093]\n",
            "  [-0.75 0.566406 -0.259766 ... 1.47656 0.691406 0.00646973]\n",
            "  [-0.757812 0.570312 -0.259766 ... 1.47656 0.691406 0.00604248]]\n",
            "\n",
            " [[-0.757812 0.578125 -0.259766 ... 1.47656 0.683594 0.000896454]\n",
            "  [-0.757812 0.578125 -0.259766 ... 1.47656 0.683594 0.000896454]\n",
            "  [-0.753906 0.570312 -0.265625 ... 1.46875 0.683594 -0.00184631]\n",
            "  ...\n",
            "  [-0.761719 0.5625 -0.261719 ... 1.47656 0.6875 0.00595093]\n",
            "  [-0.75 0.566406 -0.259766 ... 1.47656 0.691406 0.00646973]\n",
            "  [-0.757812 0.570312 -0.259766 ... 1.47656 0.691406 0.00604248]]]\n",
            "attn_output=[[[-0.84375 -0.106445 -1.8125 ... 3.21875 -0.163086 0.065918]\n",
            "  [-0.84375 -0.106445 -1.8125 ... 3.21875 -0.163086 0.065918]\n",
            "  [-0.835938 -0.114258 -1.79688 ... 3.23438 -0.166992 0.0698242]\n",
            "  ...\n",
            "  [-0.855469 -0.116699 -1.78906 ... 3.21875 -0.166016 0.0722656]\n",
            "  [-0.839844 -0.111816 -1.78906 ... 3.20312 -0.168945 0.0712891]\n",
            "  [-0.839844 -0.113281 -1.78906 ... 3.23438 -0.173828 0.0727539]]\n",
            "\n",
            " [[-0.84375 -0.106445 -1.8125 ... 3.21875 -0.163086 0.065918]\n",
            "  [-0.84375 -0.106445 -1.8125 ... 3.21875 -0.163086 0.065918]\n",
            "  [-0.835938 -0.114258 -1.79688 ... 3.23438 -0.166992 0.0698242]\n",
            "  ...\n",
            "  [-0.855469 -0.116699 -1.78906 ... 3.21875 -0.166016 0.0722656]\n",
            "  [-0.839844 -0.111816 -1.78906 ... 3.20312 -0.168945 0.0712891]\n",
            "  [-0.839844 -0.113281 -1.78906 ... 3.23438 -0.173828 0.0727539]]\n",
            "\n",
            " [[-0.84375 -0.106445 -1.8125 ... 3.21875 -0.163086 0.065918]\n",
            "  [-0.84375 -0.106445 -1.8125 ... 3.21875 -0.163086 0.065918]\n",
            "  [-0.835938 -0.114258 -1.79688 ... 3.23438 -0.166992 0.0698242]\n",
            "  ...\n",
            "  [-0.855469 -0.116699 -1.78906 ... 3.21875 -0.166016 0.0722656]\n",
            "  [-0.839844 -0.111816 -1.78906 ... 3.20312 -0.168945 0.0712891]\n",
            "  [-0.839844 -0.113281 -1.78906 ... 3.23438 -0.173828 0.0727539]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.84375 -0.106445 -1.8125 ... 3.21875 -0.163086 0.065918]\n",
            "  [-0.84375 -0.106445 -1.8125 ... 3.21875 -0.163086 0.065918]\n",
            "  [-0.835938 -0.114258 -1.79688 ... 3.23438 -0.166992 0.0698242]\n",
            "  ...\n",
            "  [-0.855469 -0.116699 -1.78906 ... 3.21875 -0.166016 0.0722656]\n",
            "  [-0.839844 -0.111816 -1.78906 ... 3.20312 -0.168945 0.0712891]\n",
            "  [-0.839844 -0.113281 -1.78906 ... 3.23438 -0.173828 0.0727539]]\n",
            "\n",
            " [[-0.84375 -0.106445 -1.8125 ... 3.21875 -0.163086 0.065918]\n",
            "  [-0.84375 -0.106445 -1.8125 ... 3.21875 -0.163086 0.065918]\n",
            "  [-0.835938 -0.114258 -1.79688 ... 3.23438 -0.166992 0.0698242]\n",
            "  ...\n",
            "  [-0.855469 -0.116699 -1.78906 ... 3.21875 -0.166016 0.0722656]\n",
            "  [-0.839844 -0.111816 -1.78906 ... 3.20312 -0.168945 0.0712891]\n",
            "  [-0.839844 -0.113281 -1.78906 ... 3.23438 -0.173828 0.0727539]]\n",
            "\n",
            " [[-0.84375 -0.106445 -1.8125 ... 3.21875 -0.163086 0.065918]\n",
            "  [-0.84375 -0.106445 -1.8125 ... 3.21875 -0.163086 0.065918]\n",
            "  [-0.835938 -0.114258 -1.79688 ... 3.23438 -0.166992 0.0698242]\n",
            "  ...\n",
            "  [-0.855469 -0.116699 -1.78906 ... 3.21875 -0.166016 0.0722656]\n",
            "  [-0.839844 -0.111816 -1.78906 ... 3.20312 -0.168945 0.0712891]\n",
            "  [-0.839844 -0.113281 -1.78906 ... 3.23438 -0.173828 0.0727539]]]\n",
            "next_layer_addition_dropped_out=[[[-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.46875 0.124023 -7.5 ... 12.5625 -1.29688 0.373047]\n",
            "  ...\n",
            "  [-4.5625 0.132812 -7.46875 ... 12.5 -1.28125 0.388672]\n",
            "  [-4.5 0.139648 -7.46875 ... 12.4375 -1.28906 0.392578]\n",
            "  [-4.5 0.132812 -7.46875 ... 12.5625 -1.29688 0.396484]]\n",
            "\n",
            " [[-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.46875 0.124023 -7.5 ... 12.5625 -1.29688 0.373047]\n",
            "  ...\n",
            "  [-4.5625 0.132812 -7.46875 ... 12.5 -1.28125 0.388672]\n",
            "  [-4.5 0.139648 -7.46875 ... 12.4375 -1.28906 0.392578]\n",
            "  [-4.5 0.132812 -7.46875 ... 12.5625 -1.29688 0.396484]]\n",
            "\n",
            " [[-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.46875 0.124023 -7.5 ... 12.5625 -1.29688 0.373047]\n",
            "  ...\n",
            "  [-4.5625 0.132812 -7.46875 ... 12.5 -1.28125 0.388672]\n",
            "  [-4.5 0.139648 -7.46875 ... 12.4375 -1.28906 0.392578]\n",
            "  [-4.5 0.132812 -7.46875 ... 12.5625 -1.29688 0.396484]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.46875 0.124023 -7.5 ... 12.5625 -1.29688 0.373047]\n",
            "  ...\n",
            "  [-4.5625 0.132812 -7.46875 ... 12.5 -1.28125 0.388672]\n",
            "  [-4.5 0.139648 -7.46875 ... 12.4375 -1.28906 0.392578]\n",
            "  [-4.5 0.132812 -7.46875 ... 12.5625 -1.29688 0.396484]]\n",
            "\n",
            " [[-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.46875 0.124023 -7.5 ... 12.5625 -1.29688 0.373047]\n",
            "  ...\n",
            "  [-4.5625 0.132812 -7.46875 ... 12.5 -1.28125 0.388672]\n",
            "  [-4.5 0.139648 -7.46875 ... 12.4375 -1.28906 0.392578]\n",
            "  [-4.5 0.132812 -7.46875 ... 12.5625 -1.29688 0.396484]]\n",
            "\n",
            " [[-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.46875 0.124023 -7.5 ... 12.5625 -1.29688 0.373047]\n",
            "  ...\n",
            "  [-4.5625 0.132812 -7.46875 ... 12.5 -1.28125 0.388672]\n",
            "  [-4.5 0.139648 -7.46875 ... 12.4375 -1.28906 0.392578]\n",
            "  [-4.5 0.132812 -7.46875 ... 12.5625 -1.29688 0.396484]]]\n",
            "inputs=[[[-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.46875 0.124023 -7.5 ... 12.5625 -1.29688 0.373047]\n",
            "  ...\n",
            "  [-4.5625 0.132812 -7.46875 ... 12.5 -1.28125 0.388672]\n",
            "  [-4.5 0.139648 -7.46875 ... 12.4375 -1.28906 0.392578]\n",
            "  [-4.5 0.132812 -7.46875 ... 12.5625 -1.29688 0.396484]]\n",
            "\n",
            " [[-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.46875 0.124023 -7.5 ... 12.5625 -1.29688 0.373047]\n",
            "  ...\n",
            "  [-4.5625 0.132812 -7.46875 ... 12.5 -1.28125 0.388672]\n",
            "  [-4.5 0.139648 -7.46875 ... 12.4375 -1.28906 0.392578]\n",
            "  [-4.5 0.132812 -7.46875 ... 12.5625 -1.29688 0.396484]]\n",
            "\n",
            " [[-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.46875 0.124023 -7.5 ... 12.5625 -1.29688 0.373047]\n",
            "  ...\n",
            "  [-4.5625 0.132812 -7.46875 ... 12.5 -1.28125 0.388672]\n",
            "  [-4.5 0.139648 -7.46875 ... 12.4375 -1.28906 0.392578]\n",
            "  [-4.5 0.132812 -7.46875 ... 12.5625 -1.29688 0.396484]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.46875 0.124023 -7.5 ... 12.5625 -1.29688 0.373047]\n",
            "  ...\n",
            "  [-4.5625 0.132812 -7.46875 ... 12.5 -1.28125 0.388672]\n",
            "  [-4.5 0.139648 -7.46875 ... 12.4375 -1.28906 0.392578]\n",
            "  [-4.5 0.132812 -7.46875 ... 12.5625 -1.29688 0.396484]]\n",
            "\n",
            " [[-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.46875 0.124023 -7.5 ... 12.5625 -1.29688 0.373047]\n",
            "  ...\n",
            "  [-4.5625 0.132812 -7.46875 ... 12.5 -1.28125 0.388672]\n",
            "  [-4.5 0.139648 -7.46875 ... 12.4375 -1.28906 0.392578]\n",
            "  [-4.5 0.132812 -7.46875 ... 12.5625 -1.29688 0.396484]]\n",
            "\n",
            " [[-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.5 0.157227 -7.5625 ... 12.5 -1.26562 0.363281]\n",
            "  [-4.46875 0.124023 -7.5 ... 12.5625 -1.29688 0.373047]\n",
            "  ...\n",
            "  [-4.5625 0.132812 -7.46875 ... 12.5 -1.28125 0.388672]\n",
            "  [-4.5 0.139648 -7.46875 ... 12.4375 -1.28906 0.392578]\n",
            "  [-4.5 0.132812 -7.46875 ... 12.5625 -1.29688 0.396484]]]\n",
            "lnx=[[[-1.20312 0.0419922 -2.01562 ... 3.34375 -0.337891 0.097168]\n",
            "  [-1.20312 0.0419922 -2.01562 ... 3.34375 -0.337891 0.097168]\n",
            "  [-1.19531 0.0332031 -2 ... 3.35938 -0.345703 0.0996094]\n",
            "  ...\n",
            "  [-1.21875 0.0354004 -1.99219 ... 3.34375 -0.341797 0.103516]\n",
            "  [-1.20312 0.0373535 -1.99219 ... 3.3125 -0.34375 0.10498]\n",
            "  [-1.20312 0.0354004 -1.99219 ... 3.35938 -0.345703 0.105957]]\n",
            "\n",
            " [[-1.20312 0.0419922 -2.01562 ... 3.34375 -0.337891 0.097168]\n",
            "  [-1.20312 0.0419922 -2.01562 ... 3.34375 -0.337891 0.097168]\n",
            "  [-1.19531 0.0332031 -2 ... 3.35938 -0.345703 0.0996094]\n",
            "  ...\n",
            "  [-1.21875 0.0354004 -1.99219 ... 3.34375 -0.341797 0.103516]\n",
            "  [-1.20312 0.0373535 -1.99219 ... 3.3125 -0.34375 0.10498]\n",
            "  [-1.20312 0.0354004 -1.99219 ... 3.35938 -0.345703 0.105957]]\n",
            "\n",
            " [[-1.20312 0.0419922 -2.01562 ... 3.34375 -0.337891 0.097168]\n",
            "  [-1.20312 0.0419922 -2.01562 ... 3.34375 -0.337891 0.097168]\n",
            "  [-1.19531 0.0332031 -2 ... 3.35938 -0.345703 0.0996094]\n",
            "  ...\n",
            "  [-1.21875 0.0354004 -1.99219 ... 3.34375 -0.341797 0.103516]\n",
            "  [-1.20312 0.0373535 -1.99219 ... 3.3125 -0.34375 0.10498]\n",
            "  [-1.20312 0.0354004 -1.99219 ... 3.35938 -0.345703 0.105957]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.20312 0.0419922 -2.01562 ... 3.34375 -0.337891 0.097168]\n",
            "  [-1.20312 0.0419922 -2.01562 ... 3.34375 -0.337891 0.097168]\n",
            "  [-1.19531 0.0332031 -2 ... 3.35938 -0.345703 0.0996094]\n",
            "  ...\n",
            "  [-1.21875 0.0354004 -1.99219 ... 3.34375 -0.341797 0.103516]\n",
            "  [-1.20312 0.0373535 -1.99219 ... 3.3125 -0.34375 0.10498]\n",
            "  [-1.20312 0.0354004 -1.99219 ... 3.35938 -0.345703 0.105957]]\n",
            "\n",
            " [[-1.20312 0.0419922 -2.01562 ... 3.34375 -0.337891 0.097168]\n",
            "  [-1.20312 0.0419922 -2.01562 ... 3.34375 -0.337891 0.097168]\n",
            "  [-1.19531 0.0332031 -2 ... 3.35938 -0.345703 0.0996094]\n",
            "  ...\n",
            "  [-1.21875 0.0354004 -1.99219 ... 3.34375 -0.341797 0.103516]\n",
            "  [-1.20312 0.0373535 -1.99219 ... 3.3125 -0.34375 0.10498]\n",
            "  [-1.20312 0.0354004 -1.99219 ... 3.35938 -0.345703 0.105957]]\n",
            "\n",
            " [[-1.20312 0.0419922 -2.01562 ... 3.34375 -0.337891 0.097168]\n",
            "  [-1.20312 0.0419922 -2.01562 ... 3.34375 -0.337891 0.097168]\n",
            "  [-1.19531 0.0332031 -2 ... 3.35938 -0.345703 0.0996094]\n",
            "  ...\n",
            "  [-1.21875 0.0354004 -1.99219 ... 3.34375 -0.341797 0.103516]\n",
            "  [-1.20312 0.0373535 -1.99219 ... 3.3125 -0.34375 0.10498]\n",
            "  [-1.20312 0.0354004 -1.99219 ... 3.35938 -0.345703 0.105957]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    ...\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00]]]]]\n",
            "attention_lnx=[[[1.71094 1.57031 -0.353516 ... -0.890625 0.613281 1.125]\n",
            "  [1.71094 1.57031 -0.353516 ... -0.890625 0.613281 1.125]\n",
            "  [1.71094 1.57031 -0.353516 ... -0.886719 0.609375 1.125]\n",
            "  ...\n",
            "  [1.71094 1.5625 -0.359375 ... -0.882812 0.601562 1.13281]\n",
            "  [1.70312 1.5625 -0.357422 ... -0.882812 0.601562 1.13281]\n",
            "  [1.71094 1.5625 -0.353516 ... -0.886719 0.601562 1.13281]]\n",
            "\n",
            " [[1.71094 1.57031 -0.353516 ... -0.890625 0.613281 1.125]\n",
            "  [1.71094 1.57031 -0.353516 ... -0.890625 0.613281 1.125]\n",
            "  [1.71094 1.57031 -0.353516 ... -0.886719 0.609375 1.125]\n",
            "  ...\n",
            "  [1.71094 1.5625 -0.359375 ... -0.882812 0.601562 1.13281]\n",
            "  [1.70312 1.5625 -0.357422 ... -0.882812 0.601562 1.13281]\n",
            "  [1.71094 1.5625 -0.353516 ... -0.886719 0.601562 1.13281]]\n",
            "\n",
            " [[1.71094 1.57031 -0.353516 ... -0.890625 0.613281 1.125]\n",
            "  [1.71094 1.57031 -0.353516 ... -0.890625 0.613281 1.125]\n",
            "  [1.71094 1.57031 -0.353516 ... -0.886719 0.609375 1.125]\n",
            "  ...\n",
            "  [1.71094 1.5625 -0.359375 ... -0.882812 0.601562 1.13281]\n",
            "  [1.70312 1.5625 -0.357422 ... -0.882812 0.601562 1.13281]\n",
            "  [1.71094 1.5625 -0.353516 ... -0.886719 0.601562 1.13281]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1.71094 1.57031 -0.353516 ... -0.890625 0.613281 1.125]\n",
            "  [1.71094 1.57031 -0.353516 ... -0.890625 0.613281 1.125]\n",
            "  [1.71094 1.57031 -0.353516 ... -0.886719 0.609375 1.125]\n",
            "  ...\n",
            "  [1.71094 1.5625 -0.359375 ... -0.882812 0.601562 1.13281]\n",
            "  [1.70312 1.5625 -0.357422 ... -0.882812 0.601562 1.13281]\n",
            "  [1.71094 1.5625 -0.353516 ... -0.886719 0.601562 1.13281]]\n",
            "\n",
            " [[1.71094 1.57031 -0.353516 ... -0.890625 0.613281 1.125]\n",
            "  [1.71094 1.57031 -0.353516 ... -0.890625 0.613281 1.125]\n",
            "  [1.71094 1.57031 -0.353516 ... -0.886719 0.609375 1.125]\n",
            "  ...\n",
            "  [1.71094 1.5625 -0.359375 ... -0.882812 0.601562 1.13281]\n",
            "  [1.70312 1.5625 -0.357422 ... -0.882812 0.601562 1.13281]\n",
            "  [1.71094 1.5625 -0.353516 ... -0.886719 0.601562 1.13281]]\n",
            "\n",
            " [[1.71094 1.57031 -0.353516 ... -0.890625 0.613281 1.125]\n",
            "  [1.71094 1.57031 -0.353516 ... -0.890625 0.613281 1.125]\n",
            "  [1.71094 1.57031 -0.353516 ... -0.886719 0.609375 1.125]\n",
            "  ...\n",
            "  [1.71094 1.5625 -0.359375 ... -0.882812 0.601562 1.13281]\n",
            "  [1.70312 1.5625 -0.357422 ... -0.882812 0.601562 1.13281]\n",
            "  [1.71094 1.5625 -0.353516 ... -0.886719 0.601562 1.13281]]]\n",
            "attn_output=[[[-0.71875 0.443359 -2.03125 ... 2.98438 -0.167969 0.382812]\n",
            "  [-0.71875 0.443359 -2.03125 ... 2.98438 -0.167969 0.382812]\n",
            "  [-0.710938 0.435547 -2.01562 ... 3 -0.176758 0.386719]\n",
            "  ...\n",
            "  [-0.734375 0.435547 -2.01562 ... 2.98438 -0.174805 0.388672]\n",
            "  [-0.71875 0.4375 -2.01562 ... 2.96875 -0.176758 0.392578]\n",
            "  [-0.71875 0.435547 -2 ... 3 -0.178711 0.390625]]\n",
            "\n",
            " [[-0.71875 0.443359 -2.03125 ... 2.98438 -0.167969 0.382812]\n",
            "  [-0.71875 0.443359 -2.03125 ... 2.98438 -0.167969 0.382812]\n",
            "  [-0.710938 0.435547 -2.01562 ... 3 -0.176758 0.386719]\n",
            "  ...\n",
            "  [-0.734375 0.435547 -2.01562 ... 2.98438 -0.174805 0.388672]\n",
            "  [-0.71875 0.4375 -2.01562 ... 2.96875 -0.176758 0.392578]\n",
            "  [-0.71875 0.435547 -2 ... 3 -0.178711 0.390625]]\n",
            "\n",
            " [[-0.71875 0.443359 -2.03125 ... 2.98438 -0.167969 0.382812]\n",
            "  [-0.71875 0.443359 -2.03125 ... 2.98438 -0.167969 0.382812]\n",
            "  [-0.710938 0.435547 -2.01562 ... 3 -0.176758 0.386719]\n",
            "  ...\n",
            "  [-0.734375 0.435547 -2.01562 ... 2.98438 -0.174805 0.388672]\n",
            "  [-0.71875 0.4375 -2.01562 ... 2.96875 -0.176758 0.392578]\n",
            "  [-0.71875 0.435547 -2 ... 3 -0.178711 0.390625]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.71875 0.443359 -2.03125 ... 2.98438 -0.167969 0.382812]\n",
            "  [-0.71875 0.443359 -2.03125 ... 2.98438 -0.167969 0.382812]\n",
            "  [-0.710938 0.435547 -2.01562 ... 3 -0.176758 0.386719]\n",
            "  ...\n",
            "  [-0.734375 0.435547 -2.01562 ... 2.98438 -0.174805 0.388672]\n",
            "  [-0.71875 0.4375 -2.01562 ... 2.96875 -0.176758 0.392578]\n",
            "  [-0.71875 0.435547 -2 ... 3 -0.178711 0.390625]]\n",
            "\n",
            " [[-0.71875 0.443359 -2.03125 ... 2.98438 -0.167969 0.382812]\n",
            "  [-0.71875 0.443359 -2.03125 ... 2.98438 -0.167969 0.382812]\n",
            "  [-0.710938 0.435547 -2.01562 ... 3 -0.176758 0.386719]\n",
            "  ...\n",
            "  [-0.734375 0.435547 -2.01562 ... 2.98438 -0.174805 0.388672]\n",
            "  [-0.71875 0.4375 -2.01562 ... 2.96875 -0.176758 0.392578]\n",
            "  [-0.71875 0.435547 -2 ... 3 -0.178711 0.390625]]\n",
            "\n",
            " [[-0.71875 0.443359 -2.03125 ... 2.98438 -0.167969 0.382812]\n",
            "  [-0.71875 0.443359 -2.03125 ... 2.98438 -0.167969 0.382812]\n",
            "  [-0.710938 0.435547 -2.01562 ... 3 -0.176758 0.386719]\n",
            "  ...\n",
            "  [-0.734375 0.435547 -2.01562 ... 2.98438 -0.174805 0.388672]\n",
            "  [-0.71875 0.4375 -2.01562 ... 2.96875 -0.176758 0.392578]\n",
            "  [-0.71875 0.435547 -2 ... 3 -0.178711 0.390625]]]\n",
            "next_layer_addition_dropped_out=[[[-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.23438 1.90625 -8 ... 11.875 -2.10938 1.47656]\n",
            "  ...\n",
            "  [-3.34375 1.92188 -7.96875 ... 11.8125 -2.125 1.50781]\n",
            "  [-3.28125 1.92969 -7.96875 ... 11.75 -2.125 1.51562]\n",
            "  [-3.29688 1.92188 -8 ... 11.875 -2.14062 1.51562]]\n",
            "\n",
            " [[-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.23438 1.90625 -8 ... 11.875 -2.10938 1.47656]\n",
            "  ...\n",
            "  [-3.34375 1.92188 -7.96875 ... 11.8125 -2.125 1.50781]\n",
            "  [-3.28125 1.92969 -7.96875 ... 11.75 -2.125 1.51562]\n",
            "  [-3.29688 1.92188 -8 ... 11.875 -2.14062 1.51562]]\n",
            "\n",
            " [[-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.23438 1.90625 -8 ... 11.875 -2.10938 1.47656]\n",
            "  ...\n",
            "  [-3.34375 1.92188 -7.96875 ... 11.8125 -2.125 1.50781]\n",
            "  [-3.28125 1.92969 -7.96875 ... 11.75 -2.125 1.51562]\n",
            "  [-3.29688 1.92188 -8 ... 11.875 -2.14062 1.51562]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.23438 1.90625 -8 ... 11.875 -2.10938 1.47656]\n",
            "  ...\n",
            "  [-3.34375 1.92188 -7.96875 ... 11.8125 -2.125 1.50781]\n",
            "  [-3.28125 1.92969 -7.96875 ... 11.75 -2.125 1.51562]\n",
            "  [-3.29688 1.92188 -8 ... 11.875 -2.14062 1.51562]]\n",
            "\n",
            " [[-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.23438 1.90625 -8 ... 11.875 -2.10938 1.47656]\n",
            "  ...\n",
            "  [-3.34375 1.92188 -7.96875 ... 11.8125 -2.125 1.50781]\n",
            "  [-3.28125 1.92969 -7.96875 ... 11.75 -2.125 1.51562]\n",
            "  [-3.29688 1.92188 -8 ... 11.875 -2.14062 1.51562]]\n",
            "\n",
            " [[-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.23438 1.90625 -8 ... 11.875 -2.10938 1.47656]\n",
            "  ...\n",
            "  [-3.34375 1.92188 -7.96875 ... 11.8125 -2.125 1.50781]\n",
            "  [-3.28125 1.92969 -7.96875 ... 11.75 -2.125 1.51562]\n",
            "  [-3.29688 1.92188 -8 ... 11.875 -2.14062 1.51562]]]\n",
            "inputs=[[[-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.23438 1.90625 -8 ... 11.875 -2.10938 1.47656]\n",
            "  ...\n",
            "  [-3.34375 1.92188 -7.96875 ... 11.8125 -2.125 1.50781]\n",
            "  [-3.28125 1.92969 -7.96875 ... 11.75 -2.125 1.51562]\n",
            "  [-3.29688 1.92188 -8 ... 11.875 -2.14062 1.51562]]\n",
            "\n",
            " [[-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.23438 1.90625 -8 ... 11.875 -2.10938 1.47656]\n",
            "  ...\n",
            "  [-3.34375 1.92188 -7.96875 ... 11.8125 -2.125 1.50781]\n",
            "  [-3.28125 1.92969 -7.96875 ... 11.75 -2.125 1.51562]\n",
            "  [-3.29688 1.92188 -8 ... 11.875 -2.14062 1.51562]]\n",
            "\n",
            " [[-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.23438 1.90625 -8 ... 11.875 -2.10938 1.47656]\n",
            "  ...\n",
            "  [-3.34375 1.92188 -7.96875 ... 11.8125 -2.125 1.50781]\n",
            "  [-3.28125 1.92969 -7.96875 ... 11.75 -2.125 1.51562]\n",
            "  [-3.29688 1.92188 -8 ... 11.875 -2.14062 1.51562]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.23438 1.90625 -8 ... 11.875 -2.10938 1.47656]\n",
            "  ...\n",
            "  [-3.34375 1.92188 -7.96875 ... 11.8125 -2.125 1.50781]\n",
            "  [-3.28125 1.92969 -7.96875 ... 11.75 -2.125 1.51562]\n",
            "  [-3.29688 1.92188 -8 ... 11.875 -2.14062 1.51562]]\n",
            "\n",
            " [[-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.23438 1.90625 -8 ... 11.875 -2.10938 1.47656]\n",
            "  ...\n",
            "  [-3.34375 1.92188 -7.96875 ... 11.8125 -2.125 1.50781]\n",
            "  [-3.28125 1.92969 -7.96875 ... 11.75 -2.125 1.51562]\n",
            "  [-3.29688 1.92188 -8 ... 11.875 -2.14062 1.51562]]\n",
            "\n",
            " [[-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.28125 1.94531 -8.0625 ... 11.8125 -2.07812 1.46875]\n",
            "  [-3.23438 1.90625 -8 ... 11.875 -2.10938 1.47656]\n",
            "  ...\n",
            "  [-3.34375 1.92188 -7.96875 ... 11.8125 -2.125 1.50781]\n",
            "  [-3.28125 1.92969 -7.96875 ... 11.75 -2.125 1.51562]\n",
            "  [-3.29688 1.92188 -8 ... 11.875 -2.14062 1.51562]]]\n",
            "lnx=[[[-0.828125 0.492188 -2.03125 ... 2.98438 -0.523438 0.371094]\n",
            "  [-0.828125 0.492188 -2.03125 ... 2.98438 -0.523438 0.371094]\n",
            "  [-0.816406 0.482422 -2.01562 ... 3 -0.53125 0.373047]\n",
            "  ...\n",
            "  [-0.84375 0.484375 -2.01562 ... 2.98438 -0.535156 0.380859]\n",
            "  [-0.828125 0.486328 -2.01562 ... 2.96875 -0.535156 0.382812]\n",
            "  [-0.832031 0.484375 -2.01562 ... 3 -0.539062 0.382812]]\n",
            "\n",
            " [[-0.828125 0.492188 -2.03125 ... 2.98438 -0.523438 0.371094]\n",
            "  [-0.828125 0.492188 -2.03125 ... 2.98438 -0.523438 0.371094]\n",
            "  [-0.816406 0.482422 -2.01562 ... 3 -0.53125 0.373047]\n",
            "  ...\n",
            "  [-0.84375 0.484375 -2.01562 ... 2.98438 -0.535156 0.380859]\n",
            "  [-0.828125 0.486328 -2.01562 ... 2.96875 -0.535156 0.382812]\n",
            "  [-0.832031 0.484375 -2.01562 ... 3 -0.539062 0.382812]]\n",
            "\n",
            " [[-0.828125 0.492188 -2.03125 ... 2.98438 -0.523438 0.371094]\n",
            "  [-0.828125 0.492188 -2.03125 ... 2.98438 -0.523438 0.371094]\n",
            "  [-0.816406 0.482422 -2.01562 ... 3 -0.53125 0.373047]\n",
            "  ...\n",
            "  [-0.84375 0.484375 -2.01562 ... 2.98438 -0.535156 0.380859]\n",
            "  [-0.828125 0.486328 -2.01562 ... 2.96875 -0.535156 0.382812]\n",
            "  [-0.832031 0.484375 -2.01562 ... 3 -0.539062 0.382812]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.828125 0.492188 -2.03125 ... 2.98438 -0.523438 0.371094]\n",
            "  [-0.828125 0.492188 -2.03125 ... 2.98438 -0.523438 0.371094]\n",
            "  [-0.816406 0.482422 -2.01562 ... 3 -0.53125 0.373047]\n",
            "  ...\n",
            "  [-0.84375 0.484375 -2.01562 ... 2.98438 -0.535156 0.380859]\n",
            "  [-0.828125 0.486328 -2.01562 ... 2.96875 -0.535156 0.382812]\n",
            "  [-0.832031 0.484375 -2.01562 ... 3 -0.539062 0.382812]]\n",
            "\n",
            " [[-0.828125 0.492188 -2.03125 ... 2.98438 -0.523438 0.371094]\n",
            "  [-0.828125 0.492188 -2.03125 ... 2.98438 -0.523438 0.371094]\n",
            "  [-0.816406 0.482422 -2.01562 ... 3 -0.53125 0.373047]\n",
            "  ...\n",
            "  [-0.84375 0.484375 -2.01562 ... 2.98438 -0.535156 0.380859]\n",
            "  [-0.828125 0.486328 -2.01562 ... 2.96875 -0.535156 0.382812]\n",
            "  [-0.832031 0.484375 -2.01562 ... 3 -0.539062 0.382812]]\n",
            "\n",
            " [[-0.828125 0.492188 -2.03125 ... 2.98438 -0.523438 0.371094]\n",
            "  [-0.828125 0.492188 -2.03125 ... 2.98438 -0.523438 0.371094]\n",
            "  [-0.816406 0.482422 -2.01562 ... 3 -0.53125 0.373047]\n",
            "  ...\n",
            "  [-0.84375 0.484375 -2.01562 ... 2.98438 -0.535156 0.380859]\n",
            "  [-0.828125 0.486328 -2.01562 ... 2.96875 -0.535156 0.382812]\n",
            "  [-0.832031 0.484375 -2.01562 ... 3 -0.539062 0.382812]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    ...\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00]]]]]\n",
            "attention_lnx=[[[-0.882812 -0.1875 -1.11719 ... -0.0235596 0.664062 0.0427246]\n",
            "  [-0.882812 -0.1875 -1.11719 ... -0.0235596 0.664062 0.0427246]\n",
            "  [-0.882812 -0.180664 -1.11719 ... -0.0339355 0.667969 0.0388184]\n",
            "  ...\n",
            "  [-0.878906 -0.174805 -1.11719 ... -0.036377 0.675781 0.0388184]\n",
            "  [-0.882812 -0.176758 -1.125 ... -0.0405273 0.675781 0.0397949]\n",
            "  [-0.878906 -0.173828 -1.125 ... -0.0373535 0.679688 0.0366211]]\n",
            "\n",
            " [[-0.882812 -0.1875 -1.11719 ... -0.0235596 0.664062 0.0427246]\n",
            "  [-0.882812 -0.1875 -1.11719 ... -0.0235596 0.664062 0.0427246]\n",
            "  [-0.882812 -0.180664 -1.11719 ... -0.0339355 0.667969 0.0388184]\n",
            "  ...\n",
            "  [-0.878906 -0.174805 -1.11719 ... -0.036377 0.675781 0.0388184]\n",
            "  [-0.882812 -0.176758 -1.125 ... -0.0405273 0.675781 0.0397949]\n",
            "  [-0.878906 -0.173828 -1.125 ... -0.0373535 0.679688 0.0366211]]\n",
            "\n",
            " [[-0.882812 -0.1875 -1.11719 ... -0.0235596 0.664062 0.0427246]\n",
            "  [-0.882812 -0.1875 -1.11719 ... -0.0235596 0.664062 0.0427246]\n",
            "  [-0.882812 -0.180664 -1.11719 ... -0.0339355 0.667969 0.0388184]\n",
            "  ...\n",
            "  [-0.878906 -0.174805 -1.11719 ... -0.036377 0.675781 0.0388184]\n",
            "  [-0.882812 -0.176758 -1.125 ... -0.0405273 0.675781 0.0397949]\n",
            "  [-0.878906 -0.173828 -1.125 ... -0.0373535 0.679688 0.0366211]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.882812 -0.1875 -1.11719 ... -0.0235596 0.664062 0.0427246]\n",
            "  [-0.882812 -0.1875 -1.11719 ... -0.0235596 0.664062 0.0427246]\n",
            "  [-0.882812 -0.180664 -1.11719 ... -0.0339355 0.667969 0.0388184]\n",
            "  ...\n",
            "  [-0.878906 -0.174805 -1.11719 ... -0.036377 0.675781 0.0388184]\n",
            "  [-0.882812 -0.176758 -1.125 ... -0.0405273 0.675781 0.0397949]\n",
            "  [-0.878906 -0.173828 -1.125 ... -0.0373535 0.679688 0.0366211]]\n",
            "\n",
            " [[-0.882812 -0.1875 -1.11719 ... -0.0235596 0.664062 0.0427246]\n",
            "  [-0.882812 -0.1875 -1.11719 ... -0.0235596 0.664062 0.0427246]\n",
            "  [-0.882812 -0.180664 -1.11719 ... -0.0339355 0.667969 0.0388184]\n",
            "  ...\n",
            "  [-0.878906 -0.174805 -1.11719 ... -0.036377 0.675781 0.0388184]\n",
            "  [-0.882812 -0.176758 -1.125 ... -0.0405273 0.675781 0.0397949]\n",
            "  [-0.878906 -0.173828 -1.125 ... -0.0373535 0.679688 0.0366211]]\n",
            "\n",
            " [[-0.882812 -0.1875 -1.11719 ... -0.0235596 0.664062 0.0427246]\n",
            "  [-0.882812 -0.1875 -1.11719 ... -0.0235596 0.664062 0.0427246]\n",
            "  [-0.882812 -0.180664 -1.11719 ... -0.0339355 0.667969 0.0388184]\n",
            "  ...\n",
            "  [-0.878906 -0.174805 -1.11719 ... -0.036377 0.675781 0.0388184]\n",
            "  [-0.882812 -0.176758 -1.125 ... -0.0405273 0.675781 0.0397949]\n",
            "  [-0.878906 -0.173828 -1.125 ... -0.0373535 0.679688 0.0366211]]]\n",
            "attn_output=[[[-1.00781 0.427734 -2.23438 ... 2.875 -0.34375 0.367188]\n",
            "  [-1.00781 0.427734 -2.23438 ... 2.875 -0.34375 0.367188]\n",
            "  [-1 0.419922 -2.21875 ... 2.875 -0.349609 0.369141]\n",
            "  ...\n",
            "  [-1.02344 0.425781 -2.20312 ... 2.85938 -0.351562 0.375]\n",
            "  [-1.00781 0.425781 -2.21875 ... 2.84375 -0.351562 0.376953]\n",
            "  [-1.01562 0.425781 -2.21875 ... 2.875 -0.355469 0.376953]]\n",
            "\n",
            " [[-1.00781 0.427734 -2.23438 ... 2.875 -0.34375 0.367188]\n",
            "  [-1.00781 0.427734 -2.23438 ... 2.875 -0.34375 0.367188]\n",
            "  [-1 0.419922 -2.21875 ... 2.875 -0.349609 0.369141]\n",
            "  ...\n",
            "  [-1.02344 0.425781 -2.20312 ... 2.85938 -0.351562 0.375]\n",
            "  [-1.00781 0.425781 -2.21875 ... 2.84375 -0.351562 0.376953]\n",
            "  [-1.01562 0.425781 -2.21875 ... 2.875 -0.355469 0.376953]]\n",
            "\n",
            " [[-1.00781 0.427734 -2.23438 ... 2.875 -0.34375 0.367188]\n",
            "  [-1.00781 0.427734 -2.23438 ... 2.875 -0.34375 0.367188]\n",
            "  [-1 0.419922 -2.21875 ... 2.875 -0.349609 0.369141]\n",
            "  ...\n",
            "  [-1.02344 0.425781 -2.20312 ... 2.85938 -0.351562 0.375]\n",
            "  [-1.00781 0.425781 -2.21875 ... 2.84375 -0.351562 0.376953]\n",
            "  [-1.01562 0.425781 -2.21875 ... 2.875 -0.355469 0.376953]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.00781 0.427734 -2.23438 ... 2.875 -0.34375 0.367188]\n",
            "  [-1.00781 0.427734 -2.23438 ... 2.875 -0.34375 0.367188]\n",
            "  [-1 0.419922 -2.21875 ... 2.875 -0.349609 0.369141]\n",
            "  ...\n",
            "  [-1.02344 0.425781 -2.20312 ... 2.85938 -0.351562 0.375]\n",
            "  [-1.00781 0.425781 -2.21875 ... 2.84375 -0.351562 0.376953]\n",
            "  [-1.01562 0.425781 -2.21875 ... 2.875 -0.355469 0.376953]]\n",
            "\n",
            " [[-1.00781 0.427734 -2.23438 ... 2.875 -0.34375 0.367188]\n",
            "  [-1.00781 0.427734 -2.23438 ... 2.875 -0.34375 0.367188]\n",
            "  [-1 0.419922 -2.21875 ... 2.875 -0.349609 0.369141]\n",
            "  ...\n",
            "  [-1.02344 0.425781 -2.20312 ... 2.85938 -0.351562 0.375]\n",
            "  [-1.00781 0.425781 -2.21875 ... 2.84375 -0.351562 0.376953]\n",
            "  [-1.01562 0.425781 -2.21875 ... 2.875 -0.355469 0.376953]]\n",
            "\n",
            " [[-1.00781 0.427734 -2.23438 ... 2.875 -0.34375 0.367188]\n",
            "  [-1.00781 0.427734 -2.23438 ... 2.875 -0.34375 0.367188]\n",
            "  [-1 0.419922 -2.21875 ... 2.875 -0.349609 0.369141]\n",
            "  ...\n",
            "  [-1.02344 0.425781 -2.20312 ... 2.85938 -0.351562 0.375]\n",
            "  [-1.00781 0.425781 -2.21875 ... 2.84375 -0.351562 0.376953]\n",
            "  [-1.01562 0.425781 -2.21875 ... 2.875 -0.355469 0.376953]]]\n",
            "next_layer_addition_dropped_out=[[[-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.59375 1.79688 -9.625 ... 12.4375 -0.828125 -0.0996094]\n",
            "  ...\n",
            "  [-4.6875 1.82812 -9.5625 ... 12.375 -0.839844 -0.0820312]\n",
            "  [-4.625 1.82812 -9.6875 ... 12.3125 -0.828125 -0.0522461]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.855469 -0.0698242]]\n",
            "\n",
            " [[-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.59375 1.79688 -9.625 ... 12.4375 -0.828125 -0.0996094]\n",
            "  ...\n",
            "  [-4.6875 1.82812 -9.5625 ... 12.375 -0.839844 -0.0820312]\n",
            "  [-4.625 1.82812 -9.6875 ... 12.3125 -0.828125 -0.0522461]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.855469 -0.0698242]]\n",
            "\n",
            " [[-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.59375 1.79688 -9.625 ... 12.4375 -0.828125 -0.0996094]\n",
            "  ...\n",
            "  [-4.6875 1.82812 -9.5625 ... 12.375 -0.839844 -0.0820312]\n",
            "  [-4.625 1.82812 -9.6875 ... 12.3125 -0.828125 -0.0522461]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.855469 -0.0698242]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.59375 1.79688 -9.625 ... 12.4375 -0.828125 -0.0996094]\n",
            "  ...\n",
            "  [-4.6875 1.82812 -9.5625 ... 12.375 -0.839844 -0.0820312]\n",
            "  [-4.625 1.82812 -9.6875 ... 12.3125 -0.828125 -0.0522461]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.855469 -0.0698242]]\n",
            "\n",
            " [[-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.59375 1.79688 -9.625 ... 12.4375 -0.828125 -0.0996094]\n",
            "  ...\n",
            "  [-4.6875 1.82812 -9.5625 ... 12.375 -0.839844 -0.0820312]\n",
            "  [-4.625 1.82812 -9.6875 ... 12.3125 -0.828125 -0.0522461]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.855469 -0.0698242]]\n",
            "\n",
            " [[-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.59375 1.79688 -9.625 ... 12.4375 -0.828125 -0.0996094]\n",
            "  ...\n",
            "  [-4.6875 1.82812 -9.5625 ... 12.375 -0.839844 -0.0820312]\n",
            "  [-4.625 1.82812 -9.6875 ... 12.3125 -0.828125 -0.0522461]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.855469 -0.0698242]]]\n",
            "inputs=[[[-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.59375 1.79688 -9.625 ... 12.4375 -0.828125 -0.0996094]\n",
            "  ...\n",
            "  [-4.6875 1.82812 -9.5625 ... 12.375 -0.839844 -0.0820312]\n",
            "  [-4.625 1.82812 -9.6875 ... 12.3125 -0.828125 -0.0522461]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.855469 -0.0698242]]\n",
            "\n",
            " [[-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.59375 1.79688 -9.625 ... 12.4375 -0.828125 -0.0996094]\n",
            "  ...\n",
            "  [-4.6875 1.82812 -9.5625 ... 12.375 -0.839844 -0.0820312]\n",
            "  [-4.625 1.82812 -9.6875 ... 12.3125 -0.828125 -0.0522461]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.855469 -0.0698242]]\n",
            "\n",
            " [[-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.59375 1.79688 -9.625 ... 12.4375 -0.828125 -0.0996094]\n",
            "  ...\n",
            "  [-4.6875 1.82812 -9.5625 ... 12.375 -0.839844 -0.0820312]\n",
            "  [-4.625 1.82812 -9.6875 ... 12.3125 -0.828125 -0.0522461]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.855469 -0.0698242]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.59375 1.79688 -9.625 ... 12.4375 -0.828125 -0.0996094]\n",
            "  ...\n",
            "  [-4.6875 1.82812 -9.5625 ... 12.375 -0.839844 -0.0820312]\n",
            "  [-4.625 1.82812 -9.6875 ... 12.3125 -0.828125 -0.0522461]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.855469 -0.0698242]]\n",
            "\n",
            " [[-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.59375 1.79688 -9.625 ... 12.4375 -0.828125 -0.0996094]\n",
            "  ...\n",
            "  [-4.6875 1.82812 -9.5625 ... 12.375 -0.839844 -0.0820312]\n",
            "  [-4.625 1.82812 -9.6875 ... 12.3125 -0.828125 -0.0522461]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.855469 -0.0698242]]\n",
            "\n",
            " [[-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.808594 -0.12207]\n",
            "  [-4.59375 1.79688 -9.625 ... 12.4375 -0.828125 -0.0996094]\n",
            "  ...\n",
            "  [-4.6875 1.82812 -9.5625 ... 12.375 -0.839844 -0.0820312]\n",
            "  [-4.625 1.82812 -9.6875 ... 12.3125 -0.828125 -0.0522461]\n",
            "  [-4.625 1.82031 -9.6875 ... 12.4375 -0.855469 -0.0698242]]]\n",
            "lnx=[[[-1.10156 0.435547 -2.3125 ... 2.96875 -0.193359 -0.0291748]\n",
            "  [-1.10156 0.435547 -2.3125 ... 2.96875 -0.193359 -0.0291748]\n",
            "  [-1.09375 0.429688 -2.29688 ... 2.96875 -0.198242 -0.0238037]\n",
            "  ...\n",
            "  [-1.11719 0.4375 -2.28125 ... 2.95312 -0.200195 -0.0196533]\n",
            "  [-1.10156 0.4375 -2.3125 ... 2.9375 -0.198242 -0.0124512]\n",
            "  [-1.10156 0.433594 -2.3125 ... 2.96875 -0.204102 -0.0167236]]\n",
            "\n",
            " [[-1.10156 0.435547 -2.3125 ... 2.96875 -0.193359 -0.0291748]\n",
            "  [-1.10156 0.435547 -2.3125 ... 2.96875 -0.193359 -0.0291748]\n",
            "  [-1.09375 0.429688 -2.29688 ... 2.96875 -0.198242 -0.0238037]\n",
            "  ...\n",
            "  [-1.11719 0.4375 -2.28125 ... 2.95312 -0.200195 -0.0196533]\n",
            "  [-1.10156 0.4375 -2.3125 ... 2.9375 -0.198242 -0.0124512]\n",
            "  [-1.10156 0.433594 -2.3125 ... 2.96875 -0.204102 -0.0167236]]\n",
            "\n",
            " [[-1.10156 0.435547 -2.3125 ... 2.96875 -0.193359 -0.0291748]\n",
            "  [-1.10156 0.435547 -2.3125 ... 2.96875 -0.193359 -0.0291748]\n",
            "  [-1.09375 0.429688 -2.29688 ... 2.96875 -0.198242 -0.0238037]\n",
            "  ...\n",
            "  [-1.11719 0.4375 -2.28125 ... 2.95312 -0.200195 -0.0196533]\n",
            "  [-1.10156 0.4375 -2.3125 ... 2.9375 -0.198242 -0.0124512]\n",
            "  [-1.10156 0.433594 -2.3125 ... 2.96875 -0.204102 -0.0167236]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.10156 0.435547 -2.3125 ... 2.96875 -0.193359 -0.0291748]\n",
            "  [-1.10156 0.435547 -2.3125 ... 2.96875 -0.193359 -0.0291748]\n",
            "  [-1.09375 0.429688 -2.29688 ... 2.96875 -0.198242 -0.0238037]\n",
            "  ...\n",
            "  [-1.11719 0.4375 -2.28125 ... 2.95312 -0.200195 -0.0196533]\n",
            "  [-1.10156 0.4375 -2.3125 ... 2.9375 -0.198242 -0.0124512]\n",
            "  [-1.10156 0.433594 -2.3125 ... 2.96875 -0.204102 -0.0167236]]\n",
            "\n",
            " [[-1.10156 0.435547 -2.3125 ... 2.96875 -0.193359 -0.0291748]\n",
            "  [-1.10156 0.435547 -2.3125 ... 2.96875 -0.193359 -0.0291748]\n",
            "  [-1.09375 0.429688 -2.29688 ... 2.96875 -0.198242 -0.0238037]\n",
            "  ...\n",
            "  [-1.11719 0.4375 -2.28125 ... 2.95312 -0.200195 -0.0196533]\n",
            "  [-1.10156 0.4375 -2.3125 ... 2.9375 -0.198242 -0.0124512]\n",
            "  [-1.10156 0.433594 -2.3125 ... 2.96875 -0.204102 -0.0167236]]\n",
            "\n",
            " [[-1.10156 0.435547 -2.3125 ... 2.96875 -0.193359 -0.0291748]\n",
            "  [-1.10156 0.435547 -2.3125 ... 2.96875 -0.193359 -0.0291748]\n",
            "  [-1.09375 0.429688 -2.29688 ... 2.96875 -0.198242 -0.0238037]\n",
            "  ...\n",
            "  [-1.11719 0.4375 -2.28125 ... 2.95312 -0.200195 -0.0196533]\n",
            "  [-1.10156 0.4375 -2.3125 ... 2.9375 -0.198242 -0.0124512]\n",
            "  [-1.10156 0.433594 -2.3125 ... 2.96875 -0.204102 -0.0167236]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    ...\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00]]]]]\n",
            "attention_lnx=[[[0.773438 -0.921875 0.921875 ... -0.671875 -0.546875 -0.652344]\n",
            "  [0.773438 -0.921875 0.921875 ... -0.671875 -0.546875 -0.652344]\n",
            "  [0.773438 -0.921875 0.917969 ... -0.675781 -0.542969 -0.660156]\n",
            "  ...\n",
            "  [0.777344 -0.910156 0.910156 ... -0.667969 -0.535156 -0.65625]\n",
            "  [0.78125 -0.90625 0.910156 ... -0.671875 -0.527344 -0.65625]\n",
            "  [0.78125 -0.910156 0.914062 ... -0.667969 -0.535156 -0.65625]]\n",
            "\n",
            " [[0.773438 -0.921875 0.921875 ... -0.671875 -0.546875 -0.652344]\n",
            "  [0.773438 -0.921875 0.921875 ... -0.671875 -0.546875 -0.652344]\n",
            "  [0.773438 -0.921875 0.917969 ... -0.675781 -0.542969 -0.660156]\n",
            "  ...\n",
            "  [0.777344 -0.910156 0.910156 ... -0.667969 -0.535156 -0.65625]\n",
            "  [0.78125 -0.90625 0.910156 ... -0.671875 -0.527344 -0.65625]\n",
            "  [0.78125 -0.910156 0.914062 ... -0.667969 -0.535156 -0.65625]]\n",
            "\n",
            " [[0.773438 -0.921875 0.921875 ... -0.671875 -0.546875 -0.652344]\n",
            "  [0.773438 -0.921875 0.921875 ... -0.671875 -0.546875 -0.652344]\n",
            "  [0.773438 -0.921875 0.917969 ... -0.675781 -0.542969 -0.660156]\n",
            "  ...\n",
            "  [0.777344 -0.910156 0.910156 ... -0.667969 -0.535156 -0.65625]\n",
            "  [0.78125 -0.90625 0.910156 ... -0.671875 -0.527344 -0.65625]\n",
            "  [0.78125 -0.910156 0.914062 ... -0.667969 -0.535156 -0.65625]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.773438 -0.921875 0.921875 ... -0.671875 -0.546875 -0.652344]\n",
            "  [0.773438 -0.921875 0.921875 ... -0.671875 -0.546875 -0.652344]\n",
            "  [0.773438 -0.921875 0.917969 ... -0.675781 -0.542969 -0.660156]\n",
            "  ...\n",
            "  [0.777344 -0.910156 0.910156 ... -0.667969 -0.535156 -0.65625]\n",
            "  [0.78125 -0.90625 0.910156 ... -0.671875 -0.527344 -0.65625]\n",
            "  [0.78125 -0.910156 0.914062 ... -0.667969 -0.535156 -0.65625]]\n",
            "\n",
            " [[0.773438 -0.921875 0.921875 ... -0.671875 -0.546875 -0.652344]\n",
            "  [0.773438 -0.921875 0.921875 ... -0.671875 -0.546875 -0.652344]\n",
            "  [0.773438 -0.921875 0.917969 ... -0.675781 -0.542969 -0.660156]\n",
            "  ...\n",
            "  [0.777344 -0.910156 0.910156 ... -0.667969 -0.535156 -0.65625]\n",
            "  [0.78125 -0.90625 0.910156 ... -0.671875 -0.527344 -0.65625]\n",
            "  [0.78125 -0.910156 0.914062 ... -0.667969 -0.535156 -0.65625]]\n",
            "\n",
            " [[0.773438 -0.921875 0.921875 ... -0.671875 -0.546875 -0.652344]\n",
            "  [0.773438 -0.921875 0.921875 ... -0.671875 -0.546875 -0.652344]\n",
            "  [0.773438 -0.921875 0.917969 ... -0.675781 -0.542969 -0.660156]\n",
            "  ...\n",
            "  [0.777344 -0.910156 0.910156 ... -0.667969 -0.535156 -0.65625]\n",
            "  [0.78125 -0.90625 0.910156 ... -0.671875 -0.527344 -0.65625]\n",
            "  [0.78125 -0.910156 0.914062 ... -0.667969 -0.535156 -0.65625]]]\n",
            "attn_output=[[[-0.894531 0.208008 -2.03125 ... 2.71875 -0.3125 -0.179688]\n",
            "  [-0.894531 0.208008 -2.03125 ... 2.71875 -0.3125 -0.179688]\n",
            "  [-0.882812 0.203125 -2.01562 ... 2.71875 -0.318359 -0.176758]\n",
            "  ...\n",
            "  [-0.90625 0.212891 -2 ... 2.70312 -0.318359 -0.170898]\n",
            "  [-0.890625 0.213867 -2.03125 ... 2.6875 -0.3125 -0.165039]\n",
            "  [-0.890625 0.210938 -2.03125 ... 2.71875 -0.322266 -0.167969]]\n",
            "\n",
            " [[-0.894531 0.208008 -2.03125 ... 2.71875 -0.3125 -0.179688]\n",
            "  [-0.894531 0.208008 -2.03125 ... 2.71875 -0.3125 -0.179688]\n",
            "  [-0.882812 0.203125 -2.01562 ... 2.71875 -0.318359 -0.176758]\n",
            "  ...\n",
            "  [-0.90625 0.212891 -2 ... 2.70312 -0.318359 -0.170898]\n",
            "  [-0.890625 0.213867 -2.03125 ... 2.6875 -0.3125 -0.165039]\n",
            "  [-0.890625 0.210938 -2.03125 ... 2.71875 -0.322266 -0.167969]]\n",
            "\n",
            " [[-0.894531 0.208008 -2.03125 ... 2.71875 -0.3125 -0.179688]\n",
            "  [-0.894531 0.208008 -2.03125 ... 2.71875 -0.3125 -0.179688]\n",
            "  [-0.882812 0.203125 -2.01562 ... 2.71875 -0.318359 -0.176758]\n",
            "  ...\n",
            "  [-0.90625 0.212891 -2 ... 2.70312 -0.318359 -0.170898]\n",
            "  [-0.890625 0.213867 -2.03125 ... 2.6875 -0.3125 -0.165039]\n",
            "  [-0.890625 0.210938 -2.03125 ... 2.71875 -0.322266 -0.167969]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.894531 0.208008 -2.03125 ... 2.71875 -0.3125 -0.179688]\n",
            "  [-0.894531 0.208008 -2.03125 ... 2.71875 -0.3125 -0.179688]\n",
            "  [-0.882812 0.203125 -2.01562 ... 2.71875 -0.318359 -0.176758]\n",
            "  ...\n",
            "  [-0.90625 0.212891 -2 ... 2.70312 -0.318359 -0.170898]\n",
            "  [-0.890625 0.213867 -2.03125 ... 2.6875 -0.3125 -0.165039]\n",
            "  [-0.890625 0.210938 -2.03125 ... 2.71875 -0.322266 -0.167969]]\n",
            "\n",
            " [[-0.894531 0.208008 -2.03125 ... 2.71875 -0.3125 -0.179688]\n",
            "  [-0.894531 0.208008 -2.03125 ... 2.71875 -0.3125 -0.179688]\n",
            "  [-0.882812 0.203125 -2.01562 ... 2.71875 -0.318359 -0.176758]\n",
            "  ...\n",
            "  [-0.90625 0.212891 -2 ... 2.70312 -0.318359 -0.170898]\n",
            "  [-0.890625 0.213867 -2.03125 ... 2.6875 -0.3125 -0.165039]\n",
            "  [-0.890625 0.210938 -2.03125 ... 2.71875 -0.322266 -0.167969]]\n",
            "\n",
            " [[-0.894531 0.208008 -2.03125 ... 2.71875 -0.3125 -0.179688]\n",
            "  [-0.894531 0.208008 -2.03125 ... 2.71875 -0.3125 -0.179688]\n",
            "  [-0.882812 0.203125 -2.01562 ... 2.71875 -0.318359 -0.176758]\n",
            "  ...\n",
            "  [-0.90625 0.212891 -2 ... 2.70312 -0.318359 -0.170898]\n",
            "  [-0.890625 0.213867 -2.03125 ... 2.6875 -0.3125 -0.165039]\n",
            "  [-0.890625 0.210938 -2.03125 ... 2.71875 -0.322266 -0.167969]]]\n",
            "next_layer_addition_dropped_out=[[[-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.26562 0.816406 -9.5 ... 13.25 -1.15625 -0.8125]\n",
            "  ...\n",
            "  [-3.35938 0.867188 -9.4375 ... 13.1875 -1.16406 -0.792969]\n",
            "  [-3.28125 0.871094 -9.5625 ... 13.125 -1.14844 -0.761719]\n",
            "  [-3.28125 0.863281 -9.5625 ... 13.25 -1.17188 -0.773438]]\n",
            "\n",
            " [[-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.26562 0.816406 -9.5 ... 13.25 -1.15625 -0.8125]\n",
            "  ...\n",
            "  [-3.35938 0.867188 -9.4375 ... 13.1875 -1.16406 -0.792969]\n",
            "  [-3.28125 0.871094 -9.5625 ... 13.125 -1.14844 -0.761719]\n",
            "  [-3.28125 0.863281 -9.5625 ... 13.25 -1.17188 -0.773438]]\n",
            "\n",
            " [[-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.26562 0.816406 -9.5 ... 13.25 -1.15625 -0.8125]\n",
            "  ...\n",
            "  [-3.35938 0.867188 -9.4375 ... 13.1875 -1.16406 -0.792969]\n",
            "  [-3.28125 0.871094 -9.5625 ... 13.125 -1.14844 -0.761719]\n",
            "  [-3.28125 0.863281 -9.5625 ... 13.25 -1.17188 -0.773438]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.26562 0.816406 -9.5 ... 13.25 -1.15625 -0.8125]\n",
            "  ...\n",
            "  [-3.35938 0.867188 -9.4375 ... 13.1875 -1.16406 -0.792969]\n",
            "  [-3.28125 0.871094 -9.5625 ... 13.125 -1.14844 -0.761719]\n",
            "  [-3.28125 0.863281 -9.5625 ... 13.25 -1.17188 -0.773438]]\n",
            "\n",
            " [[-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.26562 0.816406 -9.5 ... 13.25 -1.15625 -0.8125]\n",
            "  ...\n",
            "  [-3.35938 0.867188 -9.4375 ... 13.1875 -1.16406 -0.792969]\n",
            "  [-3.28125 0.871094 -9.5625 ... 13.125 -1.14844 -0.761719]\n",
            "  [-3.28125 0.863281 -9.5625 ... 13.25 -1.17188 -0.773438]]\n",
            "\n",
            " [[-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.26562 0.816406 -9.5 ... 13.25 -1.15625 -0.8125]\n",
            "  ...\n",
            "  [-3.35938 0.867188 -9.4375 ... 13.1875 -1.16406 -0.792969]\n",
            "  [-3.28125 0.871094 -9.5625 ... 13.125 -1.14844 -0.761719]\n",
            "  [-3.28125 0.863281 -9.5625 ... 13.25 -1.17188 -0.773438]]]\n",
            "inputs=[[[-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.26562 0.816406 -9.5 ... 13.25 -1.15625 -0.8125]\n",
            "  ...\n",
            "  [-3.35938 0.867188 -9.4375 ... 13.1875 -1.16406 -0.792969]\n",
            "  [-3.28125 0.871094 -9.5625 ... 13.125 -1.14844 -0.761719]\n",
            "  [-3.28125 0.863281 -9.5625 ... 13.25 -1.17188 -0.773438]]\n",
            "\n",
            " [[-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.26562 0.816406 -9.5 ... 13.25 -1.15625 -0.8125]\n",
            "  ...\n",
            "  [-3.35938 0.867188 -9.4375 ... 13.1875 -1.16406 -0.792969]\n",
            "  [-3.28125 0.871094 -9.5625 ... 13.125 -1.14844 -0.761719]\n",
            "  [-3.28125 0.863281 -9.5625 ... 13.25 -1.17188 -0.773438]]\n",
            "\n",
            " [[-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.26562 0.816406 -9.5 ... 13.25 -1.15625 -0.8125]\n",
            "  ...\n",
            "  [-3.35938 0.867188 -9.4375 ... 13.1875 -1.16406 -0.792969]\n",
            "  [-3.28125 0.871094 -9.5625 ... 13.125 -1.14844 -0.761719]\n",
            "  [-3.28125 0.863281 -9.5625 ... 13.25 -1.17188 -0.773438]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.26562 0.816406 -9.5 ... 13.25 -1.15625 -0.8125]\n",
            "  ...\n",
            "  [-3.35938 0.867188 -9.4375 ... 13.1875 -1.16406 -0.792969]\n",
            "  [-3.28125 0.871094 -9.5625 ... 13.125 -1.14844 -0.761719]\n",
            "  [-3.28125 0.863281 -9.5625 ... 13.25 -1.17188 -0.773438]]\n",
            "\n",
            " [[-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.26562 0.816406 -9.5 ... 13.25 -1.15625 -0.8125]\n",
            "  ...\n",
            "  [-3.35938 0.867188 -9.4375 ... 13.1875 -1.16406 -0.792969]\n",
            "  [-3.28125 0.871094 -9.5625 ... 13.125 -1.14844 -0.761719]\n",
            "  [-3.28125 0.863281 -9.5625 ... 13.25 -1.17188 -0.773438]]\n",
            "\n",
            " [[-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.3125 0.847656 -9.5625 ... 13.25 -1.13281 -0.832031]\n",
            "  [-3.26562 0.816406 -9.5 ... 13.25 -1.15625 -0.8125]\n",
            "  ...\n",
            "  [-3.35938 0.867188 -9.4375 ... 13.1875 -1.16406 -0.792969]\n",
            "  [-3.28125 0.871094 -9.5625 ... 13.125 -1.14844 -0.761719]\n",
            "  [-3.28125 0.863281 -9.5625 ... 13.25 -1.17188 -0.773438]]]\n",
            "lnx=[[[-0.757812 0.193359 -2.1875 ... 3.03125 -0.257812 -0.189453]\n",
            "  [-0.757812 0.193359 -2.1875 ... 3.03125 -0.257812 -0.189453]\n",
            "  [-0.746094 0.186523 -2.17188 ... 3.03125 -0.263672 -0.185547]\n",
            "  ...\n",
            "  [-0.765625 0.198242 -2.15625 ... 3.01562 -0.265625 -0.180664]\n",
            "  [-0.75 0.199219 -2.1875 ... 3 -0.261719 -0.173828]\n",
            "  [-0.75 0.197266 -2.1875 ... 3.01562 -0.267578 -0.176758]]\n",
            "\n",
            " [[-0.757812 0.193359 -2.1875 ... 3.03125 -0.257812 -0.189453]\n",
            "  [-0.757812 0.193359 -2.1875 ... 3.03125 -0.257812 -0.189453]\n",
            "  [-0.746094 0.186523 -2.17188 ... 3.03125 -0.263672 -0.185547]\n",
            "  ...\n",
            "  [-0.765625 0.198242 -2.15625 ... 3.01562 -0.265625 -0.180664]\n",
            "  [-0.75 0.199219 -2.1875 ... 3 -0.261719 -0.173828]\n",
            "  [-0.75 0.197266 -2.1875 ... 3.01562 -0.267578 -0.176758]]\n",
            "\n",
            " [[-0.757812 0.193359 -2.1875 ... 3.03125 -0.257812 -0.189453]\n",
            "  [-0.757812 0.193359 -2.1875 ... 3.03125 -0.257812 -0.189453]\n",
            "  [-0.746094 0.186523 -2.17188 ... 3.03125 -0.263672 -0.185547]\n",
            "  ...\n",
            "  [-0.765625 0.198242 -2.15625 ... 3.01562 -0.265625 -0.180664]\n",
            "  [-0.75 0.199219 -2.1875 ... 3 -0.261719 -0.173828]\n",
            "  [-0.75 0.197266 -2.1875 ... 3.01562 -0.267578 -0.176758]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.757812 0.193359 -2.1875 ... 3.03125 -0.257812 -0.189453]\n",
            "  [-0.757812 0.193359 -2.1875 ... 3.03125 -0.257812 -0.189453]\n",
            "  [-0.746094 0.186523 -2.17188 ... 3.03125 -0.263672 -0.185547]\n",
            "  ...\n",
            "  [-0.765625 0.198242 -2.15625 ... 3.01562 -0.265625 -0.180664]\n",
            "  [-0.75 0.199219 -2.1875 ... 3 -0.261719 -0.173828]\n",
            "  [-0.75 0.197266 -2.1875 ... 3.01562 -0.267578 -0.176758]]\n",
            "\n",
            " [[-0.757812 0.193359 -2.1875 ... 3.03125 -0.257812 -0.189453]\n",
            "  [-0.757812 0.193359 -2.1875 ... 3.03125 -0.257812 -0.189453]\n",
            "  [-0.746094 0.186523 -2.17188 ... 3.03125 -0.263672 -0.185547]\n",
            "  ...\n",
            "  [-0.765625 0.198242 -2.15625 ... 3.01562 -0.265625 -0.180664]\n",
            "  [-0.75 0.199219 -2.1875 ... 3 -0.261719 -0.173828]\n",
            "  [-0.75 0.197266 -2.1875 ... 3.01562 -0.267578 -0.176758]]\n",
            "\n",
            " [[-0.757812 0.193359 -2.1875 ... 3.03125 -0.257812 -0.189453]\n",
            "  [-0.757812 0.193359 -2.1875 ... 3.03125 -0.257812 -0.189453]\n",
            "  [-0.746094 0.186523 -2.17188 ... 3.03125 -0.263672 -0.185547]\n",
            "  ...\n",
            "  [-0.765625 0.198242 -2.15625 ... 3.01562 -0.265625 -0.180664]\n",
            "  [-0.75 0.199219 -2.1875 ... 3 -0.261719 -0.173828]\n",
            "  [-0.75 0.197266 -2.1875 ... 3.01562 -0.267578 -0.176758]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    ...\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00]]]]]\n",
            "attention_lnx=[[[-1.19531 0.585938 0.683594 ... -0.515625 -0.289062 -1.64844]\n",
            "  [-1.19531 0.585938 0.683594 ... -0.515625 -0.289062 -1.64844]\n",
            "  [-1.19531 0.585938 0.679688 ... -0.515625 -0.283203 -1.64844]\n",
            "  ...\n",
            "  [-1.17969 0.582031 0.667969 ... -0.527344 -0.277344 -1.64844]\n",
            "  [-1.1875 0.578125 0.667969 ... -0.527344 -0.277344 -1.64062]\n",
            "  [-1.1875 0.578125 0.664062 ... -0.523438 -0.279297 -1.64844]]\n",
            "\n",
            " [[-1.19531 0.585938 0.683594 ... -0.515625 -0.289062 -1.64844]\n",
            "  [-1.19531 0.585938 0.683594 ... -0.515625 -0.289062 -1.64844]\n",
            "  [-1.19531 0.585938 0.679688 ... -0.515625 -0.283203 -1.64844]\n",
            "  ...\n",
            "  [-1.17969 0.582031 0.667969 ... -0.527344 -0.277344 -1.64844]\n",
            "  [-1.1875 0.578125 0.667969 ... -0.527344 -0.277344 -1.64062]\n",
            "  [-1.1875 0.578125 0.664062 ... -0.523438 -0.279297 -1.64844]]\n",
            "\n",
            " [[-1.19531 0.585938 0.683594 ... -0.515625 -0.289062 -1.64844]\n",
            "  [-1.19531 0.585938 0.683594 ... -0.515625 -0.289062 -1.64844]\n",
            "  [-1.19531 0.585938 0.679688 ... -0.515625 -0.283203 -1.64844]\n",
            "  ...\n",
            "  [-1.17969 0.582031 0.667969 ... -0.527344 -0.277344 -1.64844]\n",
            "  [-1.1875 0.578125 0.667969 ... -0.527344 -0.277344 -1.64062]\n",
            "  [-1.1875 0.578125 0.664062 ... -0.523438 -0.279297 -1.64844]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.19531 0.585938 0.683594 ... -0.515625 -0.289062 -1.64844]\n",
            "  [-1.19531 0.585938 0.683594 ... -0.515625 -0.289062 -1.64844]\n",
            "  [-1.19531 0.585938 0.679688 ... -0.515625 -0.283203 -1.64844]\n",
            "  ...\n",
            "  [-1.17969 0.582031 0.667969 ... -0.527344 -0.277344 -1.64844]\n",
            "  [-1.1875 0.578125 0.667969 ... -0.527344 -0.277344 -1.64062]\n",
            "  [-1.1875 0.578125 0.664062 ... -0.523438 -0.279297 -1.64844]]\n",
            "\n",
            " [[-1.19531 0.585938 0.683594 ... -0.515625 -0.289062 -1.64844]\n",
            "  [-1.19531 0.585938 0.683594 ... -0.515625 -0.289062 -1.64844]\n",
            "  [-1.19531 0.585938 0.679688 ... -0.515625 -0.283203 -1.64844]\n",
            "  ...\n",
            "  [-1.17969 0.582031 0.667969 ... -0.527344 -0.277344 -1.64844]\n",
            "  [-1.1875 0.578125 0.667969 ... -0.527344 -0.277344 -1.64062]\n",
            "  [-1.1875 0.578125 0.664062 ... -0.523438 -0.279297 -1.64844]]\n",
            "\n",
            " [[-1.19531 0.585938 0.683594 ... -0.515625 -0.289062 -1.64844]\n",
            "  [-1.19531 0.585938 0.683594 ... -0.515625 -0.289062 -1.64844]\n",
            "  [-1.19531 0.585938 0.679688 ... -0.515625 -0.283203 -1.64844]\n",
            "  ...\n",
            "  [-1.17969 0.582031 0.667969 ... -0.527344 -0.277344 -1.64844]\n",
            "  [-1.1875 0.578125 0.667969 ... -0.527344 -0.277344 -1.64062]\n",
            "  [-1.1875 0.578125 0.664062 ... -0.523438 -0.279297 -1.64844]]]\n",
            "attn_output=[[[-0.996094 0.316406 -1.96094 ... 2.82812 -0.314453 -0.550781]\n",
            "  [-0.996094 0.316406 -1.96094 ... 2.82812 -0.314453 -0.550781]\n",
            "  [-0.988281 0.310547 -1.95312 ... 2.82812 -0.318359 -0.546875]\n",
            "  ...\n",
            "  [-1 0.322266 -1.9375 ... 2.8125 -0.318359 -0.539062]\n",
            "  [-0.988281 0.322266 -1.96094 ... 2.79688 -0.316406 -0.53125]\n",
            "  [-0.988281 0.320312 -1.96094 ... 2.82812 -0.322266 -0.535156]]\n",
            "\n",
            " [[-0.996094 0.316406 -1.96094 ... 2.82812 -0.314453 -0.550781]\n",
            "  [-0.996094 0.316406 -1.96094 ... 2.82812 -0.314453 -0.550781]\n",
            "  [-0.988281 0.310547 -1.95312 ... 2.82812 -0.318359 -0.546875]\n",
            "  ...\n",
            "  [-1 0.322266 -1.9375 ... 2.8125 -0.318359 -0.539062]\n",
            "  [-0.988281 0.322266 -1.96094 ... 2.79688 -0.316406 -0.53125]\n",
            "  [-0.988281 0.320312 -1.96094 ... 2.82812 -0.322266 -0.535156]]\n",
            "\n",
            " [[-0.996094 0.316406 -1.96094 ... 2.82812 -0.314453 -0.550781]\n",
            "  [-0.996094 0.316406 -1.96094 ... 2.82812 -0.314453 -0.550781]\n",
            "  [-0.988281 0.310547 -1.95312 ... 2.82812 -0.318359 -0.546875]\n",
            "  ...\n",
            "  [-1 0.322266 -1.9375 ... 2.8125 -0.318359 -0.539062]\n",
            "  [-0.988281 0.322266 -1.96094 ... 2.79688 -0.316406 -0.53125]\n",
            "  [-0.988281 0.320312 -1.96094 ... 2.82812 -0.322266 -0.535156]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.996094 0.316406 -1.96094 ... 2.82812 -0.314453 -0.550781]\n",
            "  [-0.996094 0.316406 -1.96094 ... 2.82812 -0.314453 -0.550781]\n",
            "  [-0.988281 0.310547 -1.95312 ... 2.82812 -0.318359 -0.546875]\n",
            "  ...\n",
            "  [-1 0.322266 -1.9375 ... 2.8125 -0.318359 -0.539062]\n",
            "  [-0.988281 0.322266 -1.96094 ... 2.79688 -0.316406 -0.53125]\n",
            "  [-0.988281 0.320312 -1.96094 ... 2.82812 -0.322266 -0.535156]]\n",
            "\n",
            " [[-0.996094 0.316406 -1.96094 ... 2.82812 -0.314453 -0.550781]\n",
            "  [-0.996094 0.316406 -1.96094 ... 2.82812 -0.314453 -0.550781]\n",
            "  [-0.988281 0.310547 -1.95312 ... 2.82812 -0.318359 -0.546875]\n",
            "  ...\n",
            "  [-1 0.322266 -1.9375 ... 2.8125 -0.318359 -0.539062]\n",
            "  [-0.988281 0.322266 -1.96094 ... 2.79688 -0.316406 -0.53125]\n",
            "  [-0.988281 0.320312 -1.96094 ... 2.82812 -0.322266 -0.535156]]\n",
            "\n",
            " [[-0.996094 0.316406 -1.96094 ... 2.82812 -0.314453 -0.550781]\n",
            "  [-0.996094 0.316406 -1.96094 ... 2.82812 -0.314453 -0.550781]\n",
            "  [-0.988281 0.310547 -1.95312 ... 2.82812 -0.318359 -0.546875]\n",
            "  ...\n",
            "  [-1 0.322266 -1.9375 ... 2.8125 -0.318359 -0.539062]\n",
            "  [-0.988281 0.322266 -1.96094 ... 2.79688 -0.316406 -0.53125]\n",
            "  [-0.988281 0.320312 -1.96094 ... 2.82812 -0.322266 -0.535156]]]\n",
            "next_layer_addition_dropped_out=[[[-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.92188 2.78125 -8.8125 ... 13.0625 -1.35156 -2.73438]\n",
            "  ...\n",
            "  [-3.98438 2.8125 -8.75 ... 13.0625 -1.35156 -2.6875]\n",
            "  [-3.9375 2.8125 -8.875 ... 12.9375 -1.33594 -2.65625]\n",
            "  [-3.90625 2.8125 -8.875 ... 13.0625 -1.35938 -2.67188]]\n",
            "\n",
            " [[-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.92188 2.78125 -8.8125 ... 13.0625 -1.35156 -2.73438]\n",
            "  ...\n",
            "  [-3.98438 2.8125 -8.75 ... 13.0625 -1.35156 -2.6875]\n",
            "  [-3.9375 2.8125 -8.875 ... 12.9375 -1.33594 -2.65625]\n",
            "  [-3.90625 2.8125 -8.875 ... 13.0625 -1.35938 -2.67188]]\n",
            "\n",
            " [[-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.92188 2.78125 -8.8125 ... 13.0625 -1.35156 -2.73438]\n",
            "  ...\n",
            "  [-3.98438 2.8125 -8.75 ... 13.0625 -1.35156 -2.6875]\n",
            "  [-3.9375 2.8125 -8.875 ... 12.9375 -1.33594 -2.65625]\n",
            "  [-3.90625 2.8125 -8.875 ... 13.0625 -1.35938 -2.67188]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.92188 2.78125 -8.8125 ... 13.0625 -1.35156 -2.73438]\n",
            "  ...\n",
            "  [-3.98438 2.8125 -8.75 ... 13.0625 -1.35156 -2.6875]\n",
            "  [-3.9375 2.8125 -8.875 ... 12.9375 -1.33594 -2.65625]\n",
            "  [-3.90625 2.8125 -8.875 ... 13.0625 -1.35938 -2.67188]]\n",
            "\n",
            " [[-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.92188 2.78125 -8.8125 ... 13.0625 -1.35156 -2.73438]\n",
            "  ...\n",
            "  [-3.98438 2.8125 -8.75 ... 13.0625 -1.35156 -2.6875]\n",
            "  [-3.9375 2.8125 -8.875 ... 12.9375 -1.33594 -2.65625]\n",
            "  [-3.90625 2.8125 -8.875 ... 13.0625 -1.35938 -2.67188]]\n",
            "\n",
            " [[-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.92188 2.78125 -8.8125 ... 13.0625 -1.35156 -2.73438]\n",
            "  ...\n",
            "  [-3.98438 2.8125 -8.75 ... 13.0625 -1.35156 -2.6875]\n",
            "  [-3.9375 2.8125 -8.875 ... 12.9375 -1.33594 -2.65625]\n",
            "  [-3.90625 2.8125 -8.875 ... 13.0625 -1.35938 -2.67188]]]\n",
            "inputs=[[[-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.92188 2.78125 -8.8125 ... 13.0625 -1.35156 -2.73438]\n",
            "  ...\n",
            "  [-3.98438 2.8125 -8.75 ... 13.0625 -1.35156 -2.6875]\n",
            "  [-3.9375 2.8125 -8.875 ... 12.9375 -1.33594 -2.65625]\n",
            "  [-3.90625 2.8125 -8.875 ... 13.0625 -1.35938 -2.67188]]\n",
            "\n",
            " [[-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.92188 2.78125 -8.8125 ... 13.0625 -1.35156 -2.73438]\n",
            "  ...\n",
            "  [-3.98438 2.8125 -8.75 ... 13.0625 -1.35156 -2.6875]\n",
            "  [-3.9375 2.8125 -8.875 ... 12.9375 -1.33594 -2.65625]\n",
            "  [-3.90625 2.8125 -8.875 ... 13.0625 -1.35938 -2.67188]]\n",
            "\n",
            " [[-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.92188 2.78125 -8.8125 ... 13.0625 -1.35156 -2.73438]\n",
            "  ...\n",
            "  [-3.98438 2.8125 -8.75 ... 13.0625 -1.35156 -2.6875]\n",
            "  [-3.9375 2.8125 -8.875 ... 12.9375 -1.33594 -2.65625]\n",
            "  [-3.90625 2.8125 -8.875 ... 13.0625 -1.35938 -2.67188]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.92188 2.78125 -8.8125 ... 13.0625 -1.35156 -2.73438]\n",
            "  ...\n",
            "  [-3.98438 2.8125 -8.75 ... 13.0625 -1.35156 -2.6875]\n",
            "  [-3.9375 2.8125 -8.875 ... 12.9375 -1.33594 -2.65625]\n",
            "  [-3.90625 2.8125 -8.875 ... 13.0625 -1.35938 -2.67188]]\n",
            "\n",
            " [[-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.92188 2.78125 -8.8125 ... 13.0625 -1.35156 -2.73438]\n",
            "  ...\n",
            "  [-3.98438 2.8125 -8.75 ... 13.0625 -1.35156 -2.6875]\n",
            "  [-3.9375 2.8125 -8.875 ... 12.9375 -1.33594 -2.65625]\n",
            "  [-3.90625 2.8125 -8.875 ... 13.0625 -1.35938 -2.67188]]\n",
            "\n",
            " [[-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.95312 2.8125 -8.875 ... 13.0625 -1.33594 -2.75]\n",
            "  [-3.92188 2.78125 -8.8125 ... 13.0625 -1.35156 -2.73438]\n",
            "  ...\n",
            "  [-3.98438 2.8125 -8.75 ... 13.0625 -1.35156 -2.6875]\n",
            "  [-3.9375 2.8125 -8.875 ... 12.9375 -1.33594 -2.65625]\n",
            "  [-3.90625 2.8125 -8.875 ... 13.0625 -1.35938 -2.67188]]]\n",
            "lnx=[[[-0.859375 0.613281 -1.92969 ... 2.84375 -0.291016 -0.597656]\n",
            "  [-0.859375 0.613281 -1.92969 ... 2.84375 -0.291016 -0.597656]\n",
            "  [-0.855469 0.605469 -1.92188 ... 2.84375 -0.294922 -0.59375]\n",
            "  ...\n",
            "  [-0.867188 0.613281 -1.90625 ... 2.84375 -0.294922 -0.585938]\n",
            "  [-0.859375 0.613281 -1.92969 ... 2.8125 -0.291016 -0.578125]\n",
            "  [-0.851562 0.613281 -1.92969 ... 2.84375 -0.296875 -0.582031]]\n",
            "\n",
            " [[-0.859375 0.613281 -1.92969 ... 2.84375 -0.291016 -0.597656]\n",
            "  [-0.859375 0.613281 -1.92969 ... 2.84375 -0.291016 -0.597656]\n",
            "  [-0.855469 0.605469 -1.92188 ... 2.84375 -0.294922 -0.59375]\n",
            "  ...\n",
            "  [-0.867188 0.613281 -1.90625 ... 2.84375 -0.294922 -0.585938]\n",
            "  [-0.859375 0.613281 -1.92969 ... 2.8125 -0.291016 -0.578125]\n",
            "  [-0.851562 0.613281 -1.92969 ... 2.84375 -0.296875 -0.582031]]\n",
            "\n",
            " [[-0.859375 0.613281 -1.92969 ... 2.84375 -0.291016 -0.597656]\n",
            "  [-0.859375 0.613281 -1.92969 ... 2.84375 -0.291016 -0.597656]\n",
            "  [-0.855469 0.605469 -1.92188 ... 2.84375 -0.294922 -0.59375]\n",
            "  ...\n",
            "  [-0.867188 0.613281 -1.90625 ... 2.84375 -0.294922 -0.585938]\n",
            "  [-0.859375 0.613281 -1.92969 ... 2.8125 -0.291016 -0.578125]\n",
            "  [-0.851562 0.613281 -1.92969 ... 2.84375 -0.296875 -0.582031]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.859375 0.613281 -1.92969 ... 2.84375 -0.291016 -0.597656]\n",
            "  [-0.859375 0.613281 -1.92969 ... 2.84375 -0.291016 -0.597656]\n",
            "  [-0.855469 0.605469 -1.92188 ... 2.84375 -0.294922 -0.59375]\n",
            "  ...\n",
            "  [-0.867188 0.613281 -1.90625 ... 2.84375 -0.294922 -0.585938]\n",
            "  [-0.859375 0.613281 -1.92969 ... 2.8125 -0.291016 -0.578125]\n",
            "  [-0.851562 0.613281 -1.92969 ... 2.84375 -0.296875 -0.582031]]\n",
            "\n",
            " [[-0.859375 0.613281 -1.92969 ... 2.84375 -0.291016 -0.597656]\n",
            "  [-0.859375 0.613281 -1.92969 ... 2.84375 -0.291016 -0.597656]\n",
            "  [-0.855469 0.605469 -1.92188 ... 2.84375 -0.294922 -0.59375]\n",
            "  ...\n",
            "  [-0.867188 0.613281 -1.90625 ... 2.84375 -0.294922 -0.585938]\n",
            "  [-0.859375 0.613281 -1.92969 ... 2.8125 -0.291016 -0.578125]\n",
            "  [-0.851562 0.613281 -1.92969 ... 2.84375 -0.296875 -0.582031]]\n",
            "\n",
            " [[-0.859375 0.613281 -1.92969 ... 2.84375 -0.291016 -0.597656]\n",
            "  [-0.859375 0.613281 -1.92969 ... 2.84375 -0.291016 -0.597656]\n",
            "  [-0.855469 0.605469 -1.92188 ... 2.84375 -0.294922 -0.59375]\n",
            "  ...\n",
            "  [-0.867188 0.613281 -1.90625 ... 2.84375 -0.294922 -0.585938]\n",
            "  [-0.859375 0.613281 -1.92969 ... 2.8125 -0.291016 -0.578125]\n",
            "  [-0.851562 0.613281 -1.92969 ... 2.84375 -0.296875 -0.582031]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    ...\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00]]]]]\n",
            "attention_lnx=[[[0.769531 -0.208984 1.98438 ... -0.382812 -1.22656 -0.730469]\n",
            "  [0.769531 -0.208984 1.98438 ... -0.382812 -1.22656 -0.730469]\n",
            "  [0.769531 -0.210938 1.98438 ... -0.384766 -1.22656 -0.734375]\n",
            "  ...\n",
            "  [0.765625 -0.211914 2 ... -0.380859 -1.23438 -0.738281]\n",
            "  [0.765625 -0.212891 2 ... -0.380859 -1.23438 -0.734375]\n",
            "  [0.765625 -0.212891 1.99219 ... -0.376953 -1.23438 -0.734375]]\n",
            "\n",
            " [[0.769531 -0.208984 1.98438 ... -0.382812 -1.22656 -0.730469]\n",
            "  [0.769531 -0.208984 1.98438 ... -0.382812 -1.22656 -0.730469]\n",
            "  [0.769531 -0.210938 1.98438 ... -0.384766 -1.22656 -0.734375]\n",
            "  ...\n",
            "  [0.765625 -0.211914 2 ... -0.380859 -1.23438 -0.738281]\n",
            "  [0.765625 -0.212891 2 ... -0.380859 -1.23438 -0.734375]\n",
            "  [0.765625 -0.212891 1.99219 ... -0.376953 -1.23438 -0.734375]]\n",
            "\n",
            " [[0.769531 -0.208984 1.98438 ... -0.382812 -1.22656 -0.730469]\n",
            "  [0.769531 -0.208984 1.98438 ... -0.382812 -1.22656 -0.730469]\n",
            "  [0.769531 -0.210938 1.98438 ... -0.384766 -1.22656 -0.734375]\n",
            "  ...\n",
            "  [0.765625 -0.211914 2 ... -0.380859 -1.23438 -0.738281]\n",
            "  [0.765625 -0.212891 2 ... -0.380859 -1.23438 -0.734375]\n",
            "  [0.765625 -0.212891 1.99219 ... -0.376953 -1.23438 -0.734375]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.769531 -0.208984 1.98438 ... -0.382812 -1.22656 -0.730469]\n",
            "  [0.769531 -0.208984 1.98438 ... -0.382812 -1.22656 -0.730469]\n",
            "  [0.769531 -0.210938 1.98438 ... -0.384766 -1.22656 -0.734375]\n",
            "  ...\n",
            "  [0.765625 -0.211914 2 ... -0.380859 -1.23438 -0.738281]\n",
            "  [0.765625 -0.212891 2 ... -0.380859 -1.23438 -0.734375]\n",
            "  [0.765625 -0.212891 1.99219 ... -0.376953 -1.23438 -0.734375]]\n",
            "\n",
            " [[0.769531 -0.208984 1.98438 ... -0.382812 -1.22656 -0.730469]\n",
            "  [0.769531 -0.208984 1.98438 ... -0.382812 -1.22656 -0.730469]\n",
            "  [0.769531 -0.210938 1.98438 ... -0.384766 -1.22656 -0.734375]\n",
            "  ...\n",
            "  [0.765625 -0.211914 2 ... -0.380859 -1.23438 -0.738281]\n",
            "  [0.765625 -0.212891 2 ... -0.380859 -1.23438 -0.734375]\n",
            "  [0.765625 -0.212891 1.99219 ... -0.376953 -1.23438 -0.734375]]\n",
            "\n",
            " [[0.769531 -0.208984 1.98438 ... -0.382812 -1.22656 -0.730469]\n",
            "  [0.769531 -0.208984 1.98438 ... -0.382812 -1.22656 -0.730469]\n",
            "  [0.769531 -0.210938 1.98438 ... -0.384766 -1.22656 -0.734375]\n",
            "  ...\n",
            "  [0.765625 -0.211914 2 ... -0.380859 -1.23438 -0.738281]\n",
            "  [0.765625 -0.212891 2 ... -0.380859 -1.23438 -0.734375]\n",
            "  [0.765625 -0.212891 1.99219 ... -0.376953 -1.23438 -0.734375]]]\n",
            "attn_output=[[[-0.679688 0.554688 -1.46875 ... 2.70312 -0.546875 -0.742188]\n",
            "  [-0.679688 0.554688 -1.46875 ... 2.70312 -0.546875 -0.742188]\n",
            "  [-0.671875 0.550781 -1.46094 ... 2.70312 -0.550781 -0.738281]\n",
            "  ...\n",
            "  [-0.6875 0.554688 -1.4375 ... 2.70312 -0.554688 -0.730469]\n",
            "  [-0.675781 0.554688 -1.46875 ... 2.67188 -0.550781 -0.722656]\n",
            "  [-0.667969 0.550781 -1.46094 ... 2.70312 -0.550781 -0.726562]]\n",
            "\n",
            " [[-0.679688 0.554688 -1.46875 ... 2.70312 -0.546875 -0.742188]\n",
            "  [-0.679688 0.554688 -1.46875 ... 2.70312 -0.546875 -0.742188]\n",
            "  [-0.671875 0.550781 -1.46094 ... 2.70312 -0.550781 -0.738281]\n",
            "  ...\n",
            "  [-0.6875 0.554688 -1.4375 ... 2.70312 -0.554688 -0.730469]\n",
            "  [-0.675781 0.554688 -1.46875 ... 2.67188 -0.550781 -0.722656]\n",
            "  [-0.667969 0.550781 -1.46094 ... 2.70312 -0.550781 -0.726562]]\n",
            "\n",
            " [[-0.679688 0.554688 -1.46875 ... 2.70312 -0.546875 -0.742188]\n",
            "  [-0.679688 0.554688 -1.46875 ... 2.70312 -0.546875 -0.742188]\n",
            "  [-0.671875 0.550781 -1.46094 ... 2.70312 -0.550781 -0.738281]\n",
            "  ...\n",
            "  [-0.6875 0.554688 -1.4375 ... 2.70312 -0.554688 -0.730469]\n",
            "  [-0.675781 0.554688 -1.46875 ... 2.67188 -0.550781 -0.722656]\n",
            "  [-0.667969 0.550781 -1.46094 ... 2.70312 -0.550781 -0.726562]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.679688 0.554688 -1.46875 ... 2.70312 -0.546875 -0.742188]\n",
            "  [-0.679688 0.554688 -1.46875 ... 2.70312 -0.546875 -0.742188]\n",
            "  [-0.671875 0.550781 -1.46094 ... 2.70312 -0.550781 -0.738281]\n",
            "  ...\n",
            "  [-0.6875 0.554688 -1.4375 ... 2.70312 -0.554688 -0.730469]\n",
            "  [-0.675781 0.554688 -1.46875 ... 2.67188 -0.550781 -0.722656]\n",
            "  [-0.667969 0.550781 -1.46094 ... 2.70312 -0.550781 -0.726562]]\n",
            "\n",
            " [[-0.679688 0.554688 -1.46875 ... 2.70312 -0.546875 -0.742188]\n",
            "  [-0.679688 0.554688 -1.46875 ... 2.70312 -0.546875 -0.742188]\n",
            "  [-0.671875 0.550781 -1.46094 ... 2.70312 -0.550781 -0.738281]\n",
            "  ...\n",
            "  [-0.6875 0.554688 -1.4375 ... 2.70312 -0.554688 -0.730469]\n",
            "  [-0.675781 0.554688 -1.46875 ... 2.67188 -0.550781 -0.722656]\n",
            "  [-0.667969 0.550781 -1.46094 ... 2.70312 -0.550781 -0.726562]]\n",
            "\n",
            " [[-0.679688 0.554688 -1.46875 ... 2.70312 -0.546875 -0.742188]\n",
            "  [-0.679688 0.554688 -1.46875 ... 2.70312 -0.546875 -0.742188]\n",
            "  [-0.671875 0.550781 -1.46094 ... 2.70312 -0.550781 -0.738281]\n",
            "  ...\n",
            "  [-0.6875 0.554688 -1.4375 ... 2.70312 -0.554688 -0.730469]\n",
            "  [-0.675781 0.554688 -1.46875 ... 2.67188 -0.550781 -0.722656]\n",
            "  [-0.667969 0.550781 -1.46094 ... 2.70312 -0.550781 -0.726562]]]\n",
            "next_layer_addition_dropped_out=[[[-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.1875 2.5 -6.4375 ... 12.75 -3.0625 -4.25]\n",
            "  ...\n",
            "  [-3.25 2.5 -6.34375 ... 12.75 -3.07812 -4.1875]\n",
            "  [-3.20312 2.5 -6.46875 ... 12.625 -3.0625 -4.15625]\n",
            "  [-3.17188 2.48438 -6.5 ... 12.75 -3.0625 -4.1875]]\n",
            "\n",
            " [[-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.1875 2.5 -6.4375 ... 12.75 -3.0625 -4.25]\n",
            "  ...\n",
            "  [-3.25 2.5 -6.34375 ... 12.75 -3.07812 -4.1875]\n",
            "  [-3.20312 2.5 -6.46875 ... 12.625 -3.0625 -4.15625]\n",
            "  [-3.17188 2.48438 -6.5 ... 12.75 -3.0625 -4.1875]]\n",
            "\n",
            " [[-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.1875 2.5 -6.4375 ... 12.75 -3.0625 -4.25]\n",
            "  ...\n",
            "  [-3.25 2.5 -6.34375 ... 12.75 -3.07812 -4.1875]\n",
            "  [-3.20312 2.5 -6.46875 ... 12.625 -3.0625 -4.15625]\n",
            "  [-3.17188 2.48438 -6.5 ... 12.75 -3.0625 -4.1875]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.1875 2.5 -6.4375 ... 12.75 -3.0625 -4.25]\n",
            "  ...\n",
            "  [-3.25 2.5 -6.34375 ... 12.75 -3.07812 -4.1875]\n",
            "  [-3.20312 2.5 -6.46875 ... 12.625 -3.0625 -4.15625]\n",
            "  [-3.17188 2.48438 -6.5 ... 12.75 -3.0625 -4.1875]]\n",
            "\n",
            " [[-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.1875 2.5 -6.4375 ... 12.75 -3.0625 -4.25]\n",
            "  ...\n",
            "  [-3.25 2.5 -6.34375 ... 12.75 -3.07812 -4.1875]\n",
            "  [-3.20312 2.5 -6.46875 ... 12.625 -3.0625 -4.15625]\n",
            "  [-3.17188 2.48438 -6.5 ... 12.75 -3.0625 -4.1875]]\n",
            "\n",
            " [[-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.1875 2.5 -6.4375 ... 12.75 -3.0625 -4.25]\n",
            "  ...\n",
            "  [-3.25 2.5 -6.34375 ... 12.75 -3.07812 -4.1875]\n",
            "  [-3.20312 2.5 -6.46875 ... 12.625 -3.0625 -4.15625]\n",
            "  [-3.17188 2.48438 -6.5 ... 12.75 -3.0625 -4.1875]]]\n",
            "inputs=[[[-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.1875 2.5 -6.4375 ... 12.75 -3.0625 -4.25]\n",
            "  ...\n",
            "  [-3.25 2.5 -6.34375 ... 12.75 -3.07812 -4.1875]\n",
            "  [-3.20312 2.5 -6.46875 ... 12.625 -3.0625 -4.15625]\n",
            "  [-3.17188 2.48438 -6.5 ... 12.75 -3.0625 -4.1875]]\n",
            "\n",
            " [[-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.1875 2.5 -6.4375 ... 12.75 -3.0625 -4.25]\n",
            "  ...\n",
            "  [-3.25 2.5 -6.34375 ... 12.75 -3.07812 -4.1875]\n",
            "  [-3.20312 2.5 -6.46875 ... 12.625 -3.0625 -4.15625]\n",
            "  [-3.17188 2.48438 -6.5 ... 12.75 -3.0625 -4.1875]]\n",
            "\n",
            " [[-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.1875 2.5 -6.4375 ... 12.75 -3.0625 -4.25]\n",
            "  ...\n",
            "  [-3.25 2.5 -6.34375 ... 12.75 -3.07812 -4.1875]\n",
            "  [-3.20312 2.5 -6.46875 ... 12.625 -3.0625 -4.15625]\n",
            "  [-3.17188 2.48438 -6.5 ... 12.75 -3.0625 -4.1875]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.1875 2.5 -6.4375 ... 12.75 -3.0625 -4.25]\n",
            "  ...\n",
            "  [-3.25 2.5 -6.34375 ... 12.75 -3.07812 -4.1875]\n",
            "  [-3.20312 2.5 -6.46875 ... 12.625 -3.0625 -4.15625]\n",
            "  [-3.17188 2.48438 -6.5 ... 12.75 -3.0625 -4.1875]]\n",
            "\n",
            " [[-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.1875 2.5 -6.4375 ... 12.75 -3.0625 -4.25]\n",
            "  ...\n",
            "  [-3.25 2.5 -6.34375 ... 12.75 -3.07812 -4.1875]\n",
            "  [-3.20312 2.5 -6.46875 ... 12.625 -3.0625 -4.15625]\n",
            "  [-3.17188 2.48438 -6.5 ... 12.75 -3.0625 -4.1875]]\n",
            "\n",
            " [[-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.21875 2.53125 -6.5 ... 12.75 -3.04688 -4.25]\n",
            "  [-3.1875 2.5 -6.4375 ... 12.75 -3.0625 -4.25]\n",
            "  ...\n",
            "  [-3.25 2.5 -6.34375 ... 12.75 -3.07812 -4.1875]\n",
            "  [-3.20312 2.5 -6.46875 ... 12.625 -3.0625 -4.15625]\n",
            "  [-3.17188 2.48438 -6.5 ... 12.75 -3.0625 -4.1875]]]\n",
            "lnx=[[[-0.675781 0.53125 -1.36719 ... 2.67188 -0.640625 -0.890625]\n",
            "  [-0.675781 0.53125 -1.36719 ... 2.67188 -0.640625 -0.890625]\n",
            "  [-0.667969 0.523438 -1.35156 ... 2.67188 -0.644531 -0.894531]\n",
            "  ...\n",
            "  [-0.683594 0.523438 -1.33594 ... 2.67188 -0.648438 -0.878906]\n",
            "  [-0.671875 0.523438 -1.35938 ... 2.65625 -0.644531 -0.875]\n",
            "  [-0.667969 0.523438 -1.36719 ... 2.67188 -0.644531 -0.878906]]\n",
            "\n",
            " [[-0.675781 0.53125 -1.36719 ... 2.67188 -0.640625 -0.890625]\n",
            "  [-0.675781 0.53125 -1.36719 ... 2.67188 -0.640625 -0.890625]\n",
            "  [-0.667969 0.523438 -1.35156 ... 2.67188 -0.644531 -0.894531]\n",
            "  ...\n",
            "  [-0.683594 0.523438 -1.33594 ... 2.67188 -0.648438 -0.878906]\n",
            "  [-0.671875 0.523438 -1.35938 ... 2.65625 -0.644531 -0.875]\n",
            "  [-0.667969 0.523438 -1.36719 ... 2.67188 -0.644531 -0.878906]]\n",
            "\n",
            " [[-0.675781 0.53125 -1.36719 ... 2.67188 -0.640625 -0.890625]\n",
            "  [-0.675781 0.53125 -1.36719 ... 2.67188 -0.640625 -0.890625]\n",
            "  [-0.667969 0.523438 -1.35156 ... 2.67188 -0.644531 -0.894531]\n",
            "  ...\n",
            "  [-0.683594 0.523438 -1.33594 ... 2.67188 -0.648438 -0.878906]\n",
            "  [-0.671875 0.523438 -1.35938 ... 2.65625 -0.644531 -0.875]\n",
            "  [-0.667969 0.523438 -1.36719 ... 2.67188 -0.644531 -0.878906]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.675781 0.53125 -1.36719 ... 2.67188 -0.640625 -0.890625]\n",
            "  [-0.675781 0.53125 -1.36719 ... 2.67188 -0.640625 -0.890625]\n",
            "  [-0.667969 0.523438 -1.35156 ... 2.67188 -0.644531 -0.894531]\n",
            "  ...\n",
            "  [-0.683594 0.523438 -1.33594 ... 2.67188 -0.648438 -0.878906]\n",
            "  [-0.671875 0.523438 -1.35938 ... 2.65625 -0.644531 -0.875]\n",
            "  [-0.667969 0.523438 -1.36719 ... 2.67188 -0.644531 -0.878906]]\n",
            "\n",
            " [[-0.675781 0.53125 -1.36719 ... 2.67188 -0.640625 -0.890625]\n",
            "  [-0.675781 0.53125 -1.36719 ... 2.67188 -0.640625 -0.890625]\n",
            "  [-0.667969 0.523438 -1.35156 ... 2.67188 -0.644531 -0.894531]\n",
            "  ...\n",
            "  [-0.683594 0.523438 -1.33594 ... 2.67188 -0.648438 -0.878906]\n",
            "  [-0.671875 0.523438 -1.35938 ... 2.65625 -0.644531 -0.875]\n",
            "  [-0.667969 0.523438 -1.36719 ... 2.67188 -0.644531 -0.878906]]\n",
            "\n",
            " [[-0.675781 0.53125 -1.36719 ... 2.67188 -0.640625 -0.890625]\n",
            "  [-0.675781 0.53125 -1.36719 ... 2.67188 -0.640625 -0.890625]\n",
            "  [-0.667969 0.523438 -1.35156 ... 2.67188 -0.644531 -0.894531]\n",
            "  ...\n",
            "  [-0.683594 0.523438 -1.33594 ... 2.67188 -0.648438 -0.878906]\n",
            "  [-0.671875 0.523438 -1.35938 ... 2.65625 -0.644531 -0.875]\n",
            "  [-0.667969 0.523438 -1.36719 ... 2.67188 -0.644531 -0.878906]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    ...\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00]]]]]\n",
            "attention_lnx=[[[1.32031 0.0961914 1.22656 ... 1.60156 -0.455078 -0.570312]\n",
            "  [1.32031 0.0961914 1.22656 ... 1.60156 -0.455078 -0.570312]\n",
            "  [1.3125 0.109863 1.22656 ... 1.60156 -0.449219 -0.566406]\n",
            "  ...\n",
            "  [1.32031 0.0986328 1.21875 ... 1.60156 -0.457031 -0.546875]\n",
            "  [1.32812 0.0986328 1.22656 ... 1.60156 -0.458984 -0.546875]\n",
            "  [1.32812 0.0966797 1.22656 ... 1.59375 -0.458984 -0.542969]]\n",
            "\n",
            " [[1.32031 0.0961914 1.22656 ... 1.60156 -0.455078 -0.570312]\n",
            "  [1.32031 0.0961914 1.22656 ... 1.60156 -0.455078 -0.570312]\n",
            "  [1.3125 0.109863 1.22656 ... 1.60156 -0.449219 -0.566406]\n",
            "  ...\n",
            "  [1.32031 0.0986328 1.21875 ... 1.60156 -0.457031 -0.546875]\n",
            "  [1.32812 0.0986328 1.22656 ... 1.60156 -0.458984 -0.546875]\n",
            "  [1.32812 0.0966797 1.22656 ... 1.59375 -0.458984 -0.542969]]\n",
            "\n",
            " [[1.32031 0.0961914 1.22656 ... 1.60156 -0.455078 -0.570312]\n",
            "  [1.32031 0.0961914 1.22656 ... 1.60156 -0.455078 -0.570312]\n",
            "  [1.3125 0.109863 1.22656 ... 1.60156 -0.449219 -0.566406]\n",
            "  ...\n",
            "  [1.32031 0.0986328 1.21875 ... 1.60156 -0.457031 -0.546875]\n",
            "  [1.32812 0.0986328 1.22656 ... 1.60156 -0.458984 -0.546875]\n",
            "  [1.32812 0.0966797 1.22656 ... 1.59375 -0.458984 -0.542969]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1.32031 0.0961914 1.22656 ... 1.60156 -0.455078 -0.570312]\n",
            "  [1.32031 0.0961914 1.22656 ... 1.60156 -0.455078 -0.570312]\n",
            "  [1.3125 0.109863 1.22656 ... 1.60156 -0.449219 -0.566406]\n",
            "  ...\n",
            "  [1.32031 0.0986328 1.21875 ... 1.60156 -0.457031 -0.546875]\n",
            "  [1.32812 0.0986328 1.22656 ... 1.60156 -0.458984 -0.546875]\n",
            "  [1.32812 0.0966797 1.22656 ... 1.59375 -0.458984 -0.542969]]\n",
            "\n",
            " [[1.32031 0.0961914 1.22656 ... 1.60156 -0.455078 -0.570312]\n",
            "  [1.32031 0.0961914 1.22656 ... 1.60156 -0.455078 -0.570312]\n",
            "  [1.3125 0.109863 1.22656 ... 1.60156 -0.449219 -0.566406]\n",
            "  ...\n",
            "  [1.32031 0.0986328 1.21875 ... 1.60156 -0.457031 -0.546875]\n",
            "  [1.32812 0.0986328 1.22656 ... 1.60156 -0.458984 -0.546875]\n",
            "  [1.32812 0.0966797 1.22656 ... 1.59375 -0.458984 -0.542969]]\n",
            "\n",
            " [[1.32031 0.0961914 1.22656 ... 1.60156 -0.455078 -0.570312]\n",
            "  [1.32031 0.0961914 1.22656 ... 1.60156 -0.455078 -0.570312]\n",
            "  [1.3125 0.109863 1.22656 ... 1.60156 -0.449219 -0.566406]\n",
            "  ...\n",
            "  [1.32031 0.0986328 1.21875 ... 1.60156 -0.457031 -0.546875]\n",
            "  [1.32812 0.0986328 1.22656 ... 1.60156 -0.458984 -0.546875]\n",
            "  [1.32812 0.0966797 1.22656 ... 1.59375 -0.458984 -0.542969]]]\n",
            "attn_output=[[[-0.390625 0.539062 -1.08594 ... 2.95312 -0.71875 -0.992188]\n",
            "  [-0.390625 0.539062 -1.08594 ... 2.95312 -0.71875 -0.992188]\n",
            "  [-0.386719 0.539062 -1.07812 ... 2.95312 -0.722656 -0.992188]\n",
            "  ...\n",
            "  [-0.396484 0.535156 -1.05469 ... 2.95312 -0.726562 -0.976562]\n",
            "  [-0.386719 0.535156 -1.07812 ... 2.9375 -0.722656 -0.964844]\n",
            "  [-0.378906 0.53125 -1.08594 ... 2.95312 -0.722656 -0.972656]]\n",
            "\n",
            " [[-0.390625 0.539062 -1.08594 ... 2.95312 -0.71875 -0.992188]\n",
            "  [-0.390625 0.539062 -1.08594 ... 2.95312 -0.71875 -0.992188]\n",
            "  [-0.386719 0.539062 -1.07812 ... 2.95312 -0.722656 -0.992188]\n",
            "  ...\n",
            "  [-0.396484 0.535156 -1.05469 ... 2.95312 -0.726562 -0.976562]\n",
            "  [-0.386719 0.535156 -1.07812 ... 2.9375 -0.722656 -0.964844]\n",
            "  [-0.378906 0.53125 -1.08594 ... 2.95312 -0.722656 -0.972656]]\n",
            "\n",
            " [[-0.390625 0.539062 -1.08594 ... 2.95312 -0.71875 -0.992188]\n",
            "  [-0.390625 0.539062 -1.08594 ... 2.95312 -0.71875 -0.992188]\n",
            "  [-0.386719 0.539062 -1.07812 ... 2.95312 -0.722656 -0.992188]\n",
            "  ...\n",
            "  [-0.396484 0.535156 -1.05469 ... 2.95312 -0.726562 -0.976562]\n",
            "  [-0.386719 0.535156 -1.07812 ... 2.9375 -0.722656 -0.964844]\n",
            "  [-0.378906 0.53125 -1.08594 ... 2.95312 -0.722656 -0.972656]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.390625 0.539062 -1.08594 ... 2.95312 -0.71875 -0.992188]\n",
            "  [-0.390625 0.539062 -1.08594 ... 2.95312 -0.71875 -0.992188]\n",
            "  [-0.386719 0.539062 -1.07812 ... 2.95312 -0.722656 -0.992188]\n",
            "  ...\n",
            "  [-0.396484 0.535156 -1.05469 ... 2.95312 -0.726562 -0.976562]\n",
            "  [-0.386719 0.535156 -1.07812 ... 2.9375 -0.722656 -0.964844]\n",
            "  [-0.378906 0.53125 -1.08594 ... 2.95312 -0.722656 -0.972656]]\n",
            "\n",
            " [[-0.390625 0.539062 -1.08594 ... 2.95312 -0.71875 -0.992188]\n",
            "  [-0.390625 0.539062 -1.08594 ... 2.95312 -0.71875 -0.992188]\n",
            "  [-0.386719 0.539062 -1.07812 ... 2.95312 -0.722656 -0.992188]\n",
            "  ...\n",
            "  [-0.396484 0.535156 -1.05469 ... 2.95312 -0.726562 -0.976562]\n",
            "  [-0.386719 0.535156 -1.07812 ... 2.9375 -0.722656 -0.964844]\n",
            "  [-0.378906 0.53125 -1.08594 ... 2.95312 -0.722656 -0.972656]]\n",
            "\n",
            " [[-0.390625 0.539062 -1.08594 ... 2.95312 -0.71875 -0.992188]\n",
            "  [-0.390625 0.539062 -1.08594 ... 2.95312 -0.71875 -0.992188]\n",
            "  [-0.386719 0.539062 -1.07812 ... 2.95312 -0.722656 -0.992188]\n",
            "  ...\n",
            "  [-0.396484 0.535156 -1.05469 ... 2.95312 -0.726562 -0.976562]\n",
            "  [-0.386719 0.535156 -1.07812 ... 2.9375 -0.722656 -0.964844]\n",
            "  [-0.378906 0.53125 -1.08594 ... 2.95312 -0.722656 -0.972656]]]\n",
            "next_layer_addition_dropped_out=[[[-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.3125 1.26562 -6.25 ... 13.1875 -4.5625 -7.125]\n",
            "  ...\n",
            "  [-2.35938 1.23438 -6.15625 ... 13.1875 -4.5625 -7.0625]\n",
            "  [-2.32812 1.25 -6.28125 ... 13.0625 -4.5625 -7]\n",
            "  [-2.29688 1.21875 -6.3125 ... 13.1875 -4.5625 -7.0625]]\n",
            "\n",
            " [[-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.3125 1.26562 -6.25 ... 13.1875 -4.5625 -7.125]\n",
            "  ...\n",
            "  [-2.35938 1.23438 -6.15625 ... 13.1875 -4.5625 -7.0625]\n",
            "  [-2.32812 1.25 -6.28125 ... 13.0625 -4.5625 -7]\n",
            "  [-2.29688 1.21875 -6.3125 ... 13.1875 -4.5625 -7.0625]]\n",
            "\n",
            " [[-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.3125 1.26562 -6.25 ... 13.1875 -4.5625 -7.125]\n",
            "  ...\n",
            "  [-2.35938 1.23438 -6.15625 ... 13.1875 -4.5625 -7.0625]\n",
            "  [-2.32812 1.25 -6.28125 ... 13.0625 -4.5625 -7]\n",
            "  [-2.29688 1.21875 -6.3125 ... 13.1875 -4.5625 -7.0625]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.3125 1.26562 -6.25 ... 13.1875 -4.5625 -7.125]\n",
            "  ...\n",
            "  [-2.35938 1.23438 -6.15625 ... 13.1875 -4.5625 -7.0625]\n",
            "  [-2.32812 1.25 -6.28125 ... 13.0625 -4.5625 -7]\n",
            "  [-2.29688 1.21875 -6.3125 ... 13.1875 -4.5625 -7.0625]]\n",
            "\n",
            " [[-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.3125 1.26562 -6.25 ... 13.1875 -4.5625 -7.125]\n",
            "  ...\n",
            "  [-2.35938 1.23438 -6.15625 ... 13.1875 -4.5625 -7.0625]\n",
            "  [-2.32812 1.25 -6.28125 ... 13.0625 -4.5625 -7]\n",
            "  [-2.29688 1.21875 -6.3125 ... 13.1875 -4.5625 -7.0625]]\n",
            "\n",
            " [[-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.3125 1.26562 -6.25 ... 13.1875 -4.5625 -7.125]\n",
            "  ...\n",
            "  [-2.35938 1.23438 -6.15625 ... 13.1875 -4.5625 -7.0625]\n",
            "  [-2.32812 1.25 -6.28125 ... 13.0625 -4.5625 -7]\n",
            "  [-2.29688 1.21875 -6.3125 ... 13.1875 -4.5625 -7.0625]]]\n",
            "inputs=[[[-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.3125 1.26562 -6.25 ... 13.1875 -4.5625 -7.125]\n",
            "  ...\n",
            "  [-2.35938 1.23438 -6.15625 ... 13.1875 -4.5625 -7.0625]\n",
            "  [-2.32812 1.25 -6.28125 ... 13.0625 -4.5625 -7]\n",
            "  [-2.29688 1.21875 -6.3125 ... 13.1875 -4.5625 -7.0625]]\n",
            "\n",
            " [[-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.3125 1.26562 -6.25 ... 13.1875 -4.5625 -7.125]\n",
            "  ...\n",
            "  [-2.35938 1.23438 -6.15625 ... 13.1875 -4.5625 -7.0625]\n",
            "  [-2.32812 1.25 -6.28125 ... 13.0625 -4.5625 -7]\n",
            "  [-2.29688 1.21875 -6.3125 ... 13.1875 -4.5625 -7.0625]]\n",
            "\n",
            " [[-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.3125 1.26562 -6.25 ... 13.1875 -4.5625 -7.125]\n",
            "  ...\n",
            "  [-2.35938 1.23438 -6.15625 ... 13.1875 -4.5625 -7.0625]\n",
            "  [-2.32812 1.25 -6.28125 ... 13.0625 -4.5625 -7]\n",
            "  [-2.29688 1.21875 -6.3125 ... 13.1875 -4.5625 -7.0625]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.3125 1.26562 -6.25 ... 13.1875 -4.5625 -7.125]\n",
            "  ...\n",
            "  [-2.35938 1.23438 -6.15625 ... 13.1875 -4.5625 -7.0625]\n",
            "  [-2.32812 1.25 -6.28125 ... 13.0625 -4.5625 -7]\n",
            "  [-2.29688 1.21875 -6.3125 ... 13.1875 -4.5625 -7.0625]]\n",
            "\n",
            " [[-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.3125 1.26562 -6.25 ... 13.1875 -4.5625 -7.125]\n",
            "  ...\n",
            "  [-2.35938 1.23438 -6.15625 ... 13.1875 -4.5625 -7.0625]\n",
            "  [-2.32812 1.25 -6.28125 ... 13.0625 -4.5625 -7]\n",
            "  [-2.29688 1.21875 -6.3125 ... 13.1875 -4.5625 -7.0625]]\n",
            "\n",
            " [[-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.34375 1.27344 -6.3125 ... 13.1875 -4.5625 -7.125]\n",
            "  [-2.3125 1.26562 -6.25 ... 13.1875 -4.5625 -7.125]\n",
            "  ...\n",
            "  [-2.35938 1.23438 -6.15625 ... 13.1875 -4.5625 -7.0625]\n",
            "  [-2.32812 1.25 -6.28125 ... 13.0625 -4.5625 -7]\n",
            "  [-2.29688 1.21875 -6.3125 ... 13.1875 -4.5625 -7.0625]]]\n",
            "lnx=[[[-0.474609 0.257812 -1.28125 ... 2.67188 -0.925781 -1.44531]\n",
            "  [-0.474609 0.257812 -1.28125 ... 2.67188 -0.925781 -1.44531]\n",
            "  [-0.46875 0.257812 -1.26562 ... 2.67188 -0.925781 -1.44531]\n",
            "  ...\n",
            "  [-0.478516 0.25 -1.25 ... 2.67188 -0.925781 -1.42969]\n",
            "  [-0.472656 0.253906 -1.27344 ... 2.65625 -0.925781 -1.42188]\n",
            "  [-0.466797 0.24707 -1.28125 ... 2.67188 -0.925781 -1.42969]]\n",
            "\n",
            " [[-0.474609 0.257812 -1.28125 ... 2.67188 -0.925781 -1.44531]\n",
            "  [-0.474609 0.257812 -1.28125 ... 2.67188 -0.925781 -1.44531]\n",
            "  [-0.46875 0.257812 -1.26562 ... 2.67188 -0.925781 -1.44531]\n",
            "  ...\n",
            "  [-0.478516 0.25 -1.25 ... 2.67188 -0.925781 -1.42969]\n",
            "  [-0.472656 0.253906 -1.27344 ... 2.65625 -0.925781 -1.42188]\n",
            "  [-0.466797 0.24707 -1.28125 ... 2.67188 -0.925781 -1.42969]]\n",
            "\n",
            " [[-0.474609 0.257812 -1.28125 ... 2.67188 -0.925781 -1.44531]\n",
            "  [-0.474609 0.257812 -1.28125 ... 2.67188 -0.925781 -1.44531]\n",
            "  [-0.46875 0.257812 -1.26562 ... 2.67188 -0.925781 -1.44531]\n",
            "  ...\n",
            "  [-0.478516 0.25 -1.25 ... 2.67188 -0.925781 -1.42969]\n",
            "  [-0.472656 0.253906 -1.27344 ... 2.65625 -0.925781 -1.42188]\n",
            "  [-0.466797 0.24707 -1.28125 ... 2.67188 -0.925781 -1.42969]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.474609 0.257812 -1.28125 ... 2.67188 -0.925781 -1.44531]\n",
            "  [-0.474609 0.257812 -1.28125 ... 2.67188 -0.925781 -1.44531]\n",
            "  [-0.46875 0.257812 -1.26562 ... 2.67188 -0.925781 -1.44531]\n",
            "  ...\n",
            "  [-0.478516 0.25 -1.25 ... 2.67188 -0.925781 -1.42969]\n",
            "  [-0.472656 0.253906 -1.27344 ... 2.65625 -0.925781 -1.42188]\n",
            "  [-0.466797 0.24707 -1.28125 ... 2.67188 -0.925781 -1.42969]]\n",
            "\n",
            " [[-0.474609 0.257812 -1.28125 ... 2.67188 -0.925781 -1.44531]\n",
            "  [-0.474609 0.257812 -1.28125 ... 2.67188 -0.925781 -1.44531]\n",
            "  [-0.46875 0.257812 -1.26562 ... 2.67188 -0.925781 -1.44531]\n",
            "  ...\n",
            "  [-0.478516 0.25 -1.25 ... 2.67188 -0.925781 -1.42969]\n",
            "  [-0.472656 0.253906 -1.27344 ... 2.65625 -0.925781 -1.42188]\n",
            "  [-0.466797 0.24707 -1.28125 ... 2.67188 -0.925781 -1.42969]]\n",
            "\n",
            " [[-0.474609 0.257812 -1.28125 ... 2.67188 -0.925781 -1.44531]\n",
            "  [-0.474609 0.257812 -1.28125 ... 2.67188 -0.925781 -1.44531]\n",
            "  [-0.46875 0.257812 -1.26562 ... 2.67188 -0.925781 -1.44531]\n",
            "  ...\n",
            "  [-0.478516 0.25 -1.25 ... 2.67188 -0.925781 -1.42969]\n",
            "  [-0.472656 0.253906 -1.27344 ... 2.65625 -0.925781 -1.42188]\n",
            "  [-0.466797 0.24707 -1.28125 ... 2.67188 -0.925781 -1.42969]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    ...\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00]]]]]\n",
            "attention_lnx=[[[-1.5625 0.832031 -1.10156 ... -2.0625 -0.785156 0.206055]\n",
            "  [-1.5625 0.832031 -1.10156 ... -2.0625 -0.785156 0.206055]\n",
            "  [-1.5625 0.835938 -1.10156 ... -2.0625 -0.78125 0.203125]\n",
            "  ...\n",
            "  [-1.5625 0.824219 -1.08594 ... -2.04688 -0.796875 0.208008]\n",
            "  [-1.5625 0.820312 -1.08594 ... -2.04688 -0.796875 0.209961]\n",
            "  [-1.5625 0.824219 -1.08594 ... -2.04688 -0.800781 0.207031]]\n",
            "\n",
            " [[-1.5625 0.832031 -1.10156 ... -2.0625 -0.785156 0.206055]\n",
            "  [-1.5625 0.832031 -1.10156 ... -2.0625 -0.785156 0.206055]\n",
            "  [-1.5625 0.835938 -1.10156 ... -2.0625 -0.78125 0.203125]\n",
            "  ...\n",
            "  [-1.5625 0.824219 -1.08594 ... -2.04688 -0.796875 0.208008]\n",
            "  [-1.5625 0.820312 -1.08594 ... -2.04688 -0.796875 0.209961]\n",
            "  [-1.5625 0.824219 -1.08594 ... -2.04688 -0.800781 0.207031]]\n",
            "\n",
            " [[-1.5625 0.832031 -1.10156 ... -2.0625 -0.785156 0.206055]\n",
            "  [-1.5625 0.832031 -1.10156 ... -2.0625 -0.785156 0.206055]\n",
            "  [-1.5625 0.835938 -1.10156 ... -2.0625 -0.78125 0.203125]\n",
            "  ...\n",
            "  [-1.5625 0.824219 -1.08594 ... -2.04688 -0.796875 0.208008]\n",
            "  [-1.5625 0.820312 -1.08594 ... -2.04688 -0.796875 0.209961]\n",
            "  [-1.5625 0.824219 -1.08594 ... -2.04688 -0.800781 0.207031]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.5625 0.832031 -1.10156 ... -2.0625 -0.785156 0.206055]\n",
            "  [-1.5625 0.832031 -1.10156 ... -2.0625 -0.785156 0.206055]\n",
            "  [-1.5625 0.835938 -1.10156 ... -2.0625 -0.78125 0.203125]\n",
            "  ...\n",
            "  [-1.5625 0.824219 -1.08594 ... -2.04688 -0.796875 0.208008]\n",
            "  [-1.5625 0.820312 -1.08594 ... -2.04688 -0.796875 0.209961]\n",
            "  [-1.5625 0.824219 -1.08594 ... -2.04688 -0.800781 0.207031]]\n",
            "\n",
            " [[-1.5625 0.832031 -1.10156 ... -2.0625 -0.785156 0.206055]\n",
            "  [-1.5625 0.832031 -1.10156 ... -2.0625 -0.785156 0.206055]\n",
            "  [-1.5625 0.835938 -1.10156 ... -2.0625 -0.78125 0.203125]\n",
            "  ...\n",
            "  [-1.5625 0.824219 -1.08594 ... -2.04688 -0.796875 0.208008]\n",
            "  [-1.5625 0.820312 -1.08594 ... -2.04688 -0.796875 0.209961]\n",
            "  [-1.5625 0.824219 -1.08594 ... -2.04688 -0.800781 0.207031]]\n",
            "\n",
            " [[-1.5625 0.832031 -1.10156 ... -2.0625 -0.785156 0.206055]\n",
            "  [-1.5625 0.832031 -1.10156 ... -2.0625 -0.785156 0.206055]\n",
            "  [-1.5625 0.835938 -1.10156 ... -2.0625 -0.78125 0.203125]\n",
            "  ...\n",
            "  [-1.5625 0.824219 -1.08594 ... -2.04688 -0.796875 0.208008]\n",
            "  [-1.5625 0.820312 -1.08594 ... -2.04688 -0.796875 0.209961]\n",
            "  [-1.5625 0.824219 -1.08594 ... -2.04688 -0.800781 0.207031]]]\n",
            "attn_output=[[[-0.777344 0.419922 -1.46875 ... 2.20312 -1.0625 -1.375]\n",
            "  [-0.777344 0.419922 -1.46875 ... 2.20312 -1.0625 -1.375]\n",
            "  [-0.769531 0.416016 -1.46094 ... 2.21875 -1.0625 -1.375]\n",
            "  ...\n",
            "  [-0.78125 0.410156 -1.4375 ... 2.20312 -1.0625 -1.35938]\n",
            "  [-0.773438 0.414062 -1.46875 ... 2.1875 -1.07031 -1.35156]\n",
            "  [-0.765625 0.40625 -1.46875 ... 2.20312 -1.07031 -1.35938]]\n",
            "\n",
            " [[-0.777344 0.419922 -1.46875 ... 2.20312 -1.0625 -1.375]\n",
            "  [-0.777344 0.419922 -1.46875 ... 2.20312 -1.0625 -1.375]\n",
            "  [-0.769531 0.416016 -1.46094 ... 2.21875 -1.0625 -1.375]\n",
            "  ...\n",
            "  [-0.78125 0.410156 -1.4375 ... 2.20312 -1.0625 -1.35938]\n",
            "  [-0.773438 0.414062 -1.46875 ... 2.1875 -1.07031 -1.35156]\n",
            "  [-0.765625 0.40625 -1.46875 ... 2.20312 -1.07031 -1.35938]]\n",
            "\n",
            " [[-0.777344 0.419922 -1.46875 ... 2.20312 -1.0625 -1.375]\n",
            "  [-0.777344 0.419922 -1.46875 ... 2.20312 -1.0625 -1.375]\n",
            "  [-0.769531 0.416016 -1.46094 ... 2.21875 -1.0625 -1.375]\n",
            "  ...\n",
            "  [-0.78125 0.410156 -1.4375 ... 2.20312 -1.0625 -1.35938]\n",
            "  [-0.773438 0.414062 -1.46875 ... 2.1875 -1.07031 -1.35156]\n",
            "  [-0.765625 0.40625 -1.46875 ... 2.20312 -1.07031 -1.35938]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.777344 0.419922 -1.46875 ... 2.20312 -1.0625 -1.375]\n",
            "  [-0.777344 0.419922 -1.46875 ... 2.20312 -1.0625 -1.375]\n",
            "  [-0.769531 0.416016 -1.46094 ... 2.21875 -1.0625 -1.375]\n",
            "  ...\n",
            "  [-0.78125 0.410156 -1.4375 ... 2.20312 -1.0625 -1.35938]\n",
            "  [-0.773438 0.414062 -1.46875 ... 2.1875 -1.07031 -1.35156]\n",
            "  [-0.765625 0.40625 -1.46875 ... 2.20312 -1.07031 -1.35938]]\n",
            "\n",
            " [[-0.777344 0.419922 -1.46875 ... 2.20312 -1.0625 -1.375]\n",
            "  [-0.777344 0.419922 -1.46875 ... 2.20312 -1.0625 -1.375]\n",
            "  [-0.769531 0.416016 -1.46094 ... 2.21875 -1.0625 -1.375]\n",
            "  ...\n",
            "  [-0.78125 0.410156 -1.4375 ... 2.20312 -1.0625 -1.35938]\n",
            "  [-0.773438 0.414062 -1.46875 ... 2.1875 -1.07031 -1.35156]\n",
            "  [-0.765625 0.40625 -1.46875 ... 2.20312 -1.07031 -1.35938]]\n",
            "\n",
            " [[-0.777344 0.419922 -1.46875 ... 2.20312 -1.0625 -1.375]\n",
            "  [-0.777344 0.419922 -1.46875 ... 2.20312 -1.0625 -1.375]\n",
            "  [-0.769531 0.416016 -1.46094 ... 2.21875 -1.0625 -1.375]\n",
            "  ...\n",
            "  [-0.78125 0.410156 -1.4375 ... 2.20312 -1.0625 -1.35938]\n",
            "  [-0.773438 0.414062 -1.46875 ... 2.1875 -1.07031 -1.35156]\n",
            "  [-0.765625 0.40625 -1.46875 ... 2.20312 -1.07031 -1.35938]]]\n",
            "next_layer_addition_dropped_out=[[[-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.625 2.82812 -7.5625 ... 10.6875 -5.15625 -6.1875]\n",
            "  ...\n",
            "  [-4.65625 2.79688 -7.46875 ... 10.6875 -5.15625 -6.125]\n",
            "  [-4.625 2.82812 -7.625 ... 10.5625 -5.1875 -6.0625]\n",
            "  [-4.59375 2.79688 -7.625 ... 10.6875 -5.1875 -6.125]]\n",
            "\n",
            " [[-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.625 2.82812 -7.5625 ... 10.6875 -5.15625 -6.1875]\n",
            "  ...\n",
            "  [-4.65625 2.79688 -7.46875 ... 10.6875 -5.15625 -6.125]\n",
            "  [-4.625 2.82812 -7.625 ... 10.5625 -5.1875 -6.0625]\n",
            "  [-4.59375 2.79688 -7.625 ... 10.6875 -5.1875 -6.125]]\n",
            "\n",
            " [[-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.625 2.82812 -7.5625 ... 10.6875 -5.15625 -6.1875]\n",
            "  ...\n",
            "  [-4.65625 2.79688 -7.46875 ... 10.6875 -5.15625 -6.125]\n",
            "  [-4.625 2.82812 -7.625 ... 10.5625 -5.1875 -6.0625]\n",
            "  [-4.59375 2.79688 -7.625 ... 10.6875 -5.1875 -6.125]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.625 2.82812 -7.5625 ... 10.6875 -5.15625 -6.1875]\n",
            "  ...\n",
            "  [-4.65625 2.79688 -7.46875 ... 10.6875 -5.15625 -6.125]\n",
            "  [-4.625 2.82812 -7.625 ... 10.5625 -5.1875 -6.0625]\n",
            "  [-4.59375 2.79688 -7.625 ... 10.6875 -5.1875 -6.125]]\n",
            "\n",
            " [[-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.625 2.82812 -7.5625 ... 10.6875 -5.15625 -6.1875]\n",
            "  ...\n",
            "  [-4.65625 2.79688 -7.46875 ... 10.6875 -5.15625 -6.125]\n",
            "  [-4.625 2.82812 -7.625 ... 10.5625 -5.1875 -6.0625]\n",
            "  [-4.59375 2.79688 -7.625 ... 10.6875 -5.1875 -6.125]]\n",
            "\n",
            " [[-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.625 2.82812 -7.5625 ... 10.6875 -5.15625 -6.1875]\n",
            "  ...\n",
            "  [-4.65625 2.79688 -7.46875 ... 10.6875 -5.15625 -6.125]\n",
            "  [-4.625 2.82812 -7.625 ... 10.5625 -5.1875 -6.0625]\n",
            "  [-4.59375 2.79688 -7.625 ... 10.6875 -5.1875 -6.125]]]\n",
            "inputs=[[[-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.625 2.82812 -7.5625 ... 10.6875 -5.15625 -6.1875]\n",
            "  ...\n",
            "  [-4.65625 2.79688 -7.46875 ... 10.6875 -5.15625 -6.125]\n",
            "  [-4.625 2.82812 -7.625 ... 10.5625 -5.1875 -6.0625]\n",
            "  [-4.59375 2.79688 -7.625 ... 10.6875 -5.1875 -6.125]]\n",
            "\n",
            " [[-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.625 2.82812 -7.5625 ... 10.6875 -5.15625 -6.1875]\n",
            "  ...\n",
            "  [-4.65625 2.79688 -7.46875 ... 10.6875 -5.15625 -6.125]\n",
            "  [-4.625 2.82812 -7.625 ... 10.5625 -5.1875 -6.0625]\n",
            "  [-4.59375 2.79688 -7.625 ... 10.6875 -5.1875 -6.125]]\n",
            "\n",
            " [[-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.625 2.82812 -7.5625 ... 10.6875 -5.15625 -6.1875]\n",
            "  ...\n",
            "  [-4.65625 2.79688 -7.46875 ... 10.6875 -5.15625 -6.125]\n",
            "  [-4.625 2.82812 -7.625 ... 10.5625 -5.1875 -6.0625]\n",
            "  [-4.59375 2.79688 -7.625 ... 10.6875 -5.1875 -6.125]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.625 2.82812 -7.5625 ... 10.6875 -5.15625 -6.1875]\n",
            "  ...\n",
            "  [-4.65625 2.79688 -7.46875 ... 10.6875 -5.15625 -6.125]\n",
            "  [-4.625 2.82812 -7.625 ... 10.5625 -5.1875 -6.0625]\n",
            "  [-4.59375 2.79688 -7.625 ... 10.6875 -5.1875 -6.125]]\n",
            "\n",
            " [[-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.625 2.82812 -7.5625 ... 10.6875 -5.15625 -6.1875]\n",
            "  ...\n",
            "  [-4.65625 2.79688 -7.46875 ... 10.6875 -5.15625 -6.125]\n",
            "  [-4.625 2.82812 -7.625 ... 10.5625 -5.1875 -6.0625]\n",
            "  [-4.59375 2.79688 -7.625 ... 10.6875 -5.1875 -6.125]]\n",
            "\n",
            " [[-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.65625 2.82812 -7.625 ... 10.6875 -5.15625 -6.1875]\n",
            "  [-4.625 2.82812 -7.5625 ... 10.6875 -5.15625 -6.1875]\n",
            "  ...\n",
            "  [-4.65625 2.79688 -7.46875 ... 10.6875 -5.15625 -6.125]\n",
            "  [-4.625 2.82812 -7.625 ... 10.5625 -5.1875 -6.0625]\n",
            "  [-4.59375 2.79688 -7.625 ... 10.6875 -5.1875 -6.125]]]\n",
            "lnx=[[[-0.910156 0.550781 -1.48438 ... 2.07812 -1.00781 -1.21094]\n",
            "  [-0.910156 0.550781 -1.48438 ... 2.07812 -1.00781 -1.21094]\n",
            "  [-0.902344 0.550781 -1.47656 ... 2.07812 -1.00781 -1.21094]\n",
            "  ...\n",
            "  [-0.910156 0.546875 -1.46094 ... 2.07812 -1.00781 -1.19531]\n",
            "  [-0.902344 0.550781 -1.49219 ... 2.0625 -1.01562 -1.1875]\n",
            "  [-0.894531 0.546875 -1.48438 ... 2.07812 -1.01562 -1.19531]]\n",
            "\n",
            " [[-0.910156 0.550781 -1.48438 ... 2.07812 -1.00781 -1.21094]\n",
            "  [-0.910156 0.550781 -1.48438 ... 2.07812 -1.00781 -1.21094]\n",
            "  [-0.902344 0.550781 -1.47656 ... 2.07812 -1.00781 -1.21094]\n",
            "  ...\n",
            "  [-0.910156 0.546875 -1.46094 ... 2.07812 -1.00781 -1.19531]\n",
            "  [-0.902344 0.550781 -1.49219 ... 2.0625 -1.01562 -1.1875]\n",
            "  [-0.894531 0.546875 -1.48438 ... 2.07812 -1.01562 -1.19531]]\n",
            "\n",
            " [[-0.910156 0.550781 -1.48438 ... 2.07812 -1.00781 -1.21094]\n",
            "  [-0.910156 0.550781 -1.48438 ... 2.07812 -1.00781 -1.21094]\n",
            "  [-0.902344 0.550781 -1.47656 ... 2.07812 -1.00781 -1.21094]\n",
            "  ...\n",
            "  [-0.910156 0.546875 -1.46094 ... 2.07812 -1.00781 -1.19531]\n",
            "  [-0.902344 0.550781 -1.49219 ... 2.0625 -1.01562 -1.1875]\n",
            "  [-0.894531 0.546875 -1.48438 ... 2.07812 -1.01562 -1.19531]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.910156 0.550781 -1.48438 ... 2.07812 -1.00781 -1.21094]\n",
            "  [-0.910156 0.550781 -1.48438 ... 2.07812 -1.00781 -1.21094]\n",
            "  [-0.902344 0.550781 -1.47656 ... 2.07812 -1.00781 -1.21094]\n",
            "  ...\n",
            "  [-0.910156 0.546875 -1.46094 ... 2.07812 -1.00781 -1.19531]\n",
            "  [-0.902344 0.550781 -1.49219 ... 2.0625 -1.01562 -1.1875]\n",
            "  [-0.894531 0.546875 -1.48438 ... 2.07812 -1.01562 -1.19531]]\n",
            "\n",
            " [[-0.910156 0.550781 -1.48438 ... 2.07812 -1.00781 -1.21094]\n",
            "  [-0.910156 0.550781 -1.48438 ... 2.07812 -1.00781 -1.21094]\n",
            "  [-0.902344 0.550781 -1.47656 ... 2.07812 -1.00781 -1.21094]\n",
            "  ...\n",
            "  [-0.910156 0.546875 -1.46094 ... 2.07812 -1.00781 -1.19531]\n",
            "  [-0.902344 0.550781 -1.49219 ... 2.0625 -1.01562 -1.1875]\n",
            "  [-0.894531 0.546875 -1.48438 ... 2.07812 -1.01562 -1.19531]]\n",
            "\n",
            " [[-0.910156 0.550781 -1.48438 ... 2.07812 -1.00781 -1.21094]\n",
            "  [-0.910156 0.550781 -1.48438 ... 2.07812 -1.00781 -1.21094]\n",
            "  [-0.902344 0.550781 -1.47656 ... 2.07812 -1.00781 -1.21094]\n",
            "  ...\n",
            "  [-0.910156 0.546875 -1.46094 ... 2.07812 -1.00781 -1.19531]\n",
            "  [-0.902344 0.550781 -1.49219 ... 2.0625 -1.01562 -1.1875]\n",
            "  [-0.894531 0.546875 -1.48438 ... 2.07812 -1.01562 -1.19531]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    ...\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00]]]]]\n",
            "attention_lnx=[[[0.353516 1.01562 -0.0849609 ... -0.640625 -1.08594 -0.53125]\n",
            "  [0.353516 1.01562 -0.0849609 ... -0.640625 -1.08594 -0.53125]\n",
            "  [0.359375 1.02344 -0.0859375 ... -0.636719 -1.09375 -0.53125]\n",
            "  ...\n",
            "  [0.369141 1.03906 -0.0751953 ... -0.648438 -1.07031 -0.542969]\n",
            "  [0.363281 1.03125 -0.0761719 ... -0.648438 -1.07812 -0.546875]\n",
            "  [0.367188 1.03906 -0.0771484 ... -0.648438 -1.07812 -0.542969]]\n",
            "\n",
            " [[0.353516 1.01562 -0.0849609 ... -0.640625 -1.08594 -0.53125]\n",
            "  [0.353516 1.01562 -0.0849609 ... -0.640625 -1.08594 -0.53125]\n",
            "  [0.359375 1.02344 -0.0859375 ... -0.636719 -1.09375 -0.53125]\n",
            "  ...\n",
            "  [0.369141 1.03906 -0.0751953 ... -0.648438 -1.07031 -0.542969]\n",
            "  [0.363281 1.03125 -0.0761719 ... -0.648438 -1.07812 -0.546875]\n",
            "  [0.367188 1.03906 -0.0771484 ... -0.648438 -1.07812 -0.542969]]\n",
            "\n",
            " [[0.353516 1.01562 -0.0849609 ... -0.640625 -1.08594 -0.53125]\n",
            "  [0.353516 1.01562 -0.0849609 ... -0.640625 -1.08594 -0.53125]\n",
            "  [0.359375 1.02344 -0.0859375 ... -0.636719 -1.09375 -0.53125]\n",
            "  ...\n",
            "  [0.369141 1.03906 -0.0751953 ... -0.648438 -1.07031 -0.542969]\n",
            "  [0.363281 1.03125 -0.0761719 ... -0.648438 -1.07812 -0.546875]\n",
            "  [0.367188 1.03906 -0.0771484 ... -0.648438 -1.07812 -0.542969]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.353516 1.01562 -0.0849609 ... -0.640625 -1.08594 -0.53125]\n",
            "  [0.353516 1.01562 -0.0849609 ... -0.640625 -1.08594 -0.53125]\n",
            "  [0.359375 1.02344 -0.0859375 ... -0.636719 -1.09375 -0.53125]\n",
            "  ...\n",
            "  [0.369141 1.03906 -0.0751953 ... -0.648438 -1.07031 -0.542969]\n",
            "  [0.363281 1.03125 -0.0761719 ... -0.648438 -1.07812 -0.546875]\n",
            "  [0.367188 1.03906 -0.0771484 ... -0.648438 -1.07812 -0.542969]]\n",
            "\n",
            " [[0.353516 1.01562 -0.0849609 ... -0.640625 -1.08594 -0.53125]\n",
            "  [0.353516 1.01562 -0.0849609 ... -0.640625 -1.08594 -0.53125]\n",
            "  [0.359375 1.02344 -0.0859375 ... -0.636719 -1.09375 -0.53125]\n",
            "  ...\n",
            "  [0.369141 1.03906 -0.0751953 ... -0.648438 -1.07031 -0.542969]\n",
            "  [0.363281 1.03125 -0.0761719 ... -0.648438 -1.07812 -0.546875]\n",
            "  [0.367188 1.03906 -0.0771484 ... -0.648438 -1.07812 -0.542969]]\n",
            "\n",
            " [[0.353516 1.01562 -0.0849609 ... -0.640625 -1.08594 -0.53125]\n",
            "  [0.353516 1.01562 -0.0849609 ... -0.640625 -1.08594 -0.53125]\n",
            "  [0.359375 1.02344 -0.0859375 ... -0.636719 -1.09375 -0.53125]\n",
            "  ...\n",
            "  [0.369141 1.03906 -0.0751953 ... -0.648438 -1.07031 -0.542969]\n",
            "  [0.363281 1.03125 -0.0761719 ... -0.648438 -1.07812 -0.546875]\n",
            "  [0.367188 1.03906 -0.0771484 ... -0.648438 -1.07812 -0.542969]]]\n",
            "attn_output=[[[-0.824219 0.734375 -1.47656 ... 1.92188 -1.19531 -1.28125]\n",
            "  [-0.824219 0.734375 -1.47656 ... 1.92188 -1.19531 -1.28125]\n",
            "  [-0.8125 0.738281 -1.46094 ... 1.92188 -1.19531 -1.28125]\n",
            "  ...\n",
            "  [-0.820312 0.730469 -1.4375 ... 1.92188 -1.1875 -1.27344]\n",
            "  [-0.8125 0.738281 -1.46875 ... 1.89844 -1.20312 -1.26562]\n",
            "  [-0.804688 0.730469 -1.46875 ... 1.92188 -1.19531 -1.27344]]\n",
            "\n",
            " [[-0.824219 0.734375 -1.47656 ... 1.92188 -1.19531 -1.28125]\n",
            "  [-0.824219 0.734375 -1.47656 ... 1.92188 -1.19531 -1.28125]\n",
            "  [-0.8125 0.738281 -1.46094 ... 1.92188 -1.19531 -1.28125]\n",
            "  ...\n",
            "  [-0.820312 0.730469 -1.4375 ... 1.92188 -1.1875 -1.27344]\n",
            "  [-0.8125 0.738281 -1.46875 ... 1.89844 -1.20312 -1.26562]\n",
            "  [-0.804688 0.730469 -1.46875 ... 1.92188 -1.19531 -1.27344]]\n",
            "\n",
            " [[-0.824219 0.734375 -1.47656 ... 1.92188 -1.19531 -1.28125]\n",
            "  [-0.824219 0.734375 -1.47656 ... 1.92188 -1.19531 -1.28125]\n",
            "  [-0.8125 0.738281 -1.46094 ... 1.92188 -1.19531 -1.28125]\n",
            "  ...\n",
            "  [-0.820312 0.730469 -1.4375 ... 1.92188 -1.1875 -1.27344]\n",
            "  [-0.8125 0.738281 -1.46875 ... 1.89844 -1.20312 -1.26562]\n",
            "  [-0.804688 0.730469 -1.46875 ... 1.92188 -1.19531 -1.27344]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.824219 0.734375 -1.47656 ... 1.92188 -1.19531 -1.28125]\n",
            "  [-0.824219 0.734375 -1.47656 ... 1.92188 -1.19531 -1.28125]\n",
            "  [-0.8125 0.738281 -1.46094 ... 1.92188 -1.19531 -1.28125]\n",
            "  ...\n",
            "  [-0.820312 0.730469 -1.4375 ... 1.92188 -1.1875 -1.27344]\n",
            "  [-0.8125 0.738281 -1.46875 ... 1.89844 -1.20312 -1.26562]\n",
            "  [-0.804688 0.730469 -1.46875 ... 1.92188 -1.19531 -1.27344]]\n",
            "\n",
            " [[-0.824219 0.734375 -1.47656 ... 1.92188 -1.19531 -1.28125]\n",
            "  [-0.824219 0.734375 -1.47656 ... 1.92188 -1.19531 -1.28125]\n",
            "  [-0.8125 0.738281 -1.46094 ... 1.92188 -1.19531 -1.28125]\n",
            "  ...\n",
            "  [-0.820312 0.730469 -1.4375 ... 1.92188 -1.1875 -1.27344]\n",
            "  [-0.8125 0.738281 -1.46875 ... 1.89844 -1.20312 -1.26562]\n",
            "  [-0.804688 0.730469 -1.46875 ... 1.92188 -1.19531 -1.27344]]\n",
            "\n",
            " [[-0.824219 0.734375 -1.47656 ... 1.92188 -1.19531 -1.28125]\n",
            "  [-0.824219 0.734375 -1.47656 ... 1.92188 -1.19531 -1.28125]\n",
            "  [-0.8125 0.738281 -1.46094 ... 1.92188 -1.19531 -1.28125]\n",
            "  ...\n",
            "  [-0.820312 0.730469 -1.4375 ... 1.92188 -1.1875 -1.27344]\n",
            "  [-0.8125 0.738281 -1.46875 ... 1.89844 -1.20312 -1.26562]\n",
            "  [-0.804688 0.730469 -1.46875 ... 1.92188 -1.19531 -1.27344]]]\n",
            "next_layer_addition_dropped_out=[[[-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.53125 3.51562 -7.65625 ... 8.625 -5.59375 -7.53125]\n",
            "  ...\n",
            "  [-5.5625 3.46875 -7.53125 ... 8.625 -5.5625 -7.4375]\n",
            "  [-5.53125 3.5 -7.6875 ... 8.5 -5.625 -7.375]\n",
            "  [-5.5 3.5 -7.71875 ... 8.625 -5.59375 -7.4375]]\n",
            "\n",
            " [[-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.53125 3.51562 -7.65625 ... 8.625 -5.59375 -7.53125]\n",
            "  ...\n",
            "  [-5.5625 3.46875 -7.53125 ... 8.625 -5.5625 -7.4375]\n",
            "  [-5.53125 3.5 -7.6875 ... 8.5 -5.625 -7.375]\n",
            "  [-5.5 3.5 -7.71875 ... 8.625 -5.59375 -7.4375]]\n",
            "\n",
            " [[-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.53125 3.51562 -7.65625 ... 8.625 -5.59375 -7.53125]\n",
            "  ...\n",
            "  [-5.5625 3.46875 -7.53125 ... 8.625 -5.5625 -7.4375]\n",
            "  [-5.53125 3.5 -7.6875 ... 8.5 -5.625 -7.375]\n",
            "  [-5.5 3.5 -7.71875 ... 8.625 -5.59375 -7.4375]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.53125 3.51562 -7.65625 ... 8.625 -5.59375 -7.53125]\n",
            "  ...\n",
            "  [-5.5625 3.46875 -7.53125 ... 8.625 -5.5625 -7.4375]\n",
            "  [-5.53125 3.5 -7.6875 ... 8.5 -5.625 -7.375]\n",
            "  [-5.5 3.5 -7.71875 ... 8.625 -5.59375 -7.4375]]\n",
            "\n",
            " [[-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.53125 3.51562 -7.65625 ... 8.625 -5.59375 -7.53125]\n",
            "  ...\n",
            "  [-5.5625 3.46875 -7.53125 ... 8.625 -5.5625 -7.4375]\n",
            "  [-5.53125 3.5 -7.6875 ... 8.5 -5.625 -7.375]\n",
            "  [-5.5 3.5 -7.71875 ... 8.625 -5.59375 -7.4375]]\n",
            "\n",
            " [[-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.53125 3.51562 -7.65625 ... 8.625 -5.59375 -7.53125]\n",
            "  ...\n",
            "  [-5.5625 3.46875 -7.53125 ... 8.625 -5.5625 -7.4375]\n",
            "  [-5.53125 3.5 -7.6875 ... 8.5 -5.625 -7.375]\n",
            "  [-5.5 3.5 -7.71875 ... 8.625 -5.59375 -7.4375]]]\n",
            "inputs=[[[-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.53125 3.51562 -7.65625 ... 8.625 -5.59375 -7.53125]\n",
            "  ...\n",
            "  [-5.5625 3.46875 -7.53125 ... 8.625 -5.5625 -7.4375]\n",
            "  [-5.53125 3.5 -7.6875 ... 8.5 -5.625 -7.375]\n",
            "  [-5.5 3.5 -7.71875 ... 8.625 -5.59375 -7.4375]]\n",
            "\n",
            " [[-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.53125 3.51562 -7.65625 ... 8.625 -5.59375 -7.53125]\n",
            "  ...\n",
            "  [-5.5625 3.46875 -7.53125 ... 8.625 -5.5625 -7.4375]\n",
            "  [-5.53125 3.5 -7.6875 ... 8.5 -5.625 -7.375]\n",
            "  [-5.5 3.5 -7.71875 ... 8.625 -5.59375 -7.4375]]\n",
            "\n",
            " [[-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.53125 3.51562 -7.65625 ... 8.625 -5.59375 -7.53125]\n",
            "  ...\n",
            "  [-5.5625 3.46875 -7.53125 ... 8.625 -5.5625 -7.4375]\n",
            "  [-5.53125 3.5 -7.6875 ... 8.5 -5.625 -7.375]\n",
            "  [-5.5 3.5 -7.71875 ... 8.625 -5.59375 -7.4375]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.53125 3.51562 -7.65625 ... 8.625 -5.59375 -7.53125]\n",
            "  ...\n",
            "  [-5.5625 3.46875 -7.53125 ... 8.625 -5.5625 -7.4375]\n",
            "  [-5.53125 3.5 -7.6875 ... 8.5 -5.625 -7.375]\n",
            "  [-5.5 3.5 -7.71875 ... 8.625 -5.59375 -7.4375]]\n",
            "\n",
            " [[-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.53125 3.51562 -7.65625 ... 8.625 -5.59375 -7.53125]\n",
            "  ...\n",
            "  [-5.5625 3.46875 -7.53125 ... 8.625 -5.5625 -7.4375]\n",
            "  [-5.53125 3.5 -7.6875 ... 8.5 -5.625 -7.375]\n",
            "  [-5.5 3.5 -7.71875 ... 8.625 -5.59375 -7.4375]]\n",
            "\n",
            " [[-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.59375 3.51562 -7.71875 ... 8.625 -5.59375 -7.53125]\n",
            "  [-5.53125 3.51562 -7.65625 ... 8.625 -5.59375 -7.53125]\n",
            "  ...\n",
            "  [-5.5625 3.46875 -7.53125 ... 8.625 -5.5625 -7.4375]\n",
            "  [-5.53125 3.5 -7.6875 ... 8.5 -5.625 -7.375]\n",
            "  [-5.5 3.5 -7.71875 ... 8.625 -5.59375 -7.4375]]]\n",
            "lnx=[[[-1.05469 0.660156 -1.45312 ... 1.625 -1.05469 -1.41406]\n",
            "  [-1.05469 0.660156 -1.45312 ... 1.625 -1.05469 -1.41406]\n",
            "  [-1.03906 0.660156 -1.4375 ... 1.625 -1.05469 -1.41406]\n",
            "  ...\n",
            "  [-1.04688 0.652344 -1.41406 ... 1.625 -1.04688 -1.39844]\n",
            "  [-1.03906 0.660156 -1.44531 ... 1.60156 -1.0625 -1.39062]\n",
            "  [-1.03125 0.660156 -1.45312 ... 1.625 -1.05469 -1.39844]]\n",
            "\n",
            " [[-1.05469 0.660156 -1.45312 ... 1.625 -1.05469 -1.41406]\n",
            "  [-1.05469 0.660156 -1.45312 ... 1.625 -1.05469 -1.41406]\n",
            "  [-1.03906 0.660156 -1.4375 ... 1.625 -1.05469 -1.41406]\n",
            "  ...\n",
            "  [-1.04688 0.652344 -1.41406 ... 1.625 -1.04688 -1.39844]\n",
            "  [-1.03906 0.660156 -1.44531 ... 1.60156 -1.0625 -1.39062]\n",
            "  [-1.03125 0.660156 -1.45312 ... 1.625 -1.05469 -1.39844]]\n",
            "\n",
            " [[-1.05469 0.660156 -1.45312 ... 1.625 -1.05469 -1.41406]\n",
            "  [-1.05469 0.660156 -1.45312 ... 1.625 -1.05469 -1.41406]\n",
            "  [-1.03906 0.660156 -1.4375 ... 1.625 -1.05469 -1.41406]\n",
            "  ...\n",
            "  [-1.04688 0.652344 -1.41406 ... 1.625 -1.04688 -1.39844]\n",
            "  [-1.03906 0.660156 -1.44531 ... 1.60156 -1.0625 -1.39062]\n",
            "  [-1.03125 0.660156 -1.45312 ... 1.625 -1.05469 -1.39844]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.05469 0.660156 -1.45312 ... 1.625 -1.05469 -1.41406]\n",
            "  [-1.05469 0.660156 -1.45312 ... 1.625 -1.05469 -1.41406]\n",
            "  [-1.03906 0.660156 -1.4375 ... 1.625 -1.05469 -1.41406]\n",
            "  ...\n",
            "  [-1.04688 0.652344 -1.41406 ... 1.625 -1.04688 -1.39844]\n",
            "  [-1.03906 0.660156 -1.44531 ... 1.60156 -1.0625 -1.39062]\n",
            "  [-1.03125 0.660156 -1.45312 ... 1.625 -1.05469 -1.39844]]\n",
            "\n",
            " [[-1.05469 0.660156 -1.45312 ... 1.625 -1.05469 -1.41406]\n",
            "  [-1.05469 0.660156 -1.45312 ... 1.625 -1.05469 -1.41406]\n",
            "  [-1.03906 0.660156 -1.4375 ... 1.625 -1.05469 -1.41406]\n",
            "  ...\n",
            "  [-1.04688 0.652344 -1.41406 ... 1.625 -1.04688 -1.39844]\n",
            "  [-1.03906 0.660156 -1.44531 ... 1.60156 -1.0625 -1.39062]\n",
            "  [-1.03125 0.660156 -1.45312 ... 1.625 -1.05469 -1.39844]]\n",
            "\n",
            " [[-1.05469 0.660156 -1.45312 ... 1.625 -1.05469 -1.41406]\n",
            "  [-1.05469 0.660156 -1.45312 ... 1.625 -1.05469 -1.41406]\n",
            "  [-1.03906 0.660156 -1.4375 ... 1.625 -1.05469 -1.41406]\n",
            "  ...\n",
            "  [-1.04688 0.652344 -1.41406 ... 1.625 -1.04688 -1.39844]\n",
            "  [-1.03906 0.660156 -1.44531 ... 1.60156 -1.0625 -1.39062]\n",
            "  [-1.03125 0.660156 -1.45312 ... 1.625 -1.05469 -1.39844]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    ...\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00]]]]]\n",
            "attention_lnx=[[[-1.39844 -0.158203 -2.25 ... 0.535156 -1.03906 0.244141]\n",
            "  [-1.39844 -0.158203 -2.25 ... 0.535156 -1.03906 0.244141]\n",
            "  [-1.39062 -0.158203 -2.25 ... 0.539062 -1.03906 0.239258]\n",
            "  ...\n",
            "  [-1.38281 -0.138672 -2.26562 ... 0.539062 -1.05469 0.249023]\n",
            "  [-1.38281 -0.141602 -2.25 ... 0.539062 -1.05469 0.249023]\n",
            "  [-1.38281 -0.137695 -2.25 ... 0.542969 -1.05469 0.251953]]\n",
            "\n",
            " [[-1.39844 -0.158203 -2.25 ... 0.535156 -1.03906 0.244141]\n",
            "  [-1.39844 -0.158203 -2.25 ... 0.535156 -1.03906 0.244141]\n",
            "  [-1.39062 -0.158203 -2.25 ... 0.539062 -1.03906 0.239258]\n",
            "  ...\n",
            "  [-1.38281 -0.138672 -2.26562 ... 0.539062 -1.05469 0.249023]\n",
            "  [-1.38281 -0.141602 -2.25 ... 0.539062 -1.05469 0.249023]\n",
            "  [-1.38281 -0.137695 -2.25 ... 0.542969 -1.05469 0.251953]]\n",
            "\n",
            " [[-1.39844 -0.158203 -2.25 ... 0.535156 -1.03906 0.244141]\n",
            "  [-1.39844 -0.158203 -2.25 ... 0.535156 -1.03906 0.244141]\n",
            "  [-1.39062 -0.158203 -2.25 ... 0.539062 -1.03906 0.239258]\n",
            "  ...\n",
            "  [-1.38281 -0.138672 -2.26562 ... 0.539062 -1.05469 0.249023]\n",
            "  [-1.38281 -0.141602 -2.25 ... 0.539062 -1.05469 0.249023]\n",
            "  [-1.38281 -0.137695 -2.25 ... 0.542969 -1.05469 0.251953]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.39844 -0.158203 -2.25 ... 0.535156 -1.03906 0.244141]\n",
            "  [-1.39844 -0.158203 -2.25 ... 0.535156 -1.03906 0.244141]\n",
            "  [-1.39062 -0.158203 -2.25 ... 0.539062 -1.03906 0.239258]\n",
            "  ...\n",
            "  [-1.38281 -0.138672 -2.26562 ... 0.539062 -1.05469 0.249023]\n",
            "  [-1.38281 -0.141602 -2.25 ... 0.539062 -1.05469 0.249023]\n",
            "  [-1.38281 -0.137695 -2.25 ... 0.542969 -1.05469 0.251953]]\n",
            "\n",
            " [[-1.39844 -0.158203 -2.25 ... 0.535156 -1.03906 0.244141]\n",
            "  [-1.39844 -0.158203 -2.25 ... 0.535156 -1.03906 0.244141]\n",
            "  [-1.39062 -0.158203 -2.25 ... 0.539062 -1.03906 0.239258]\n",
            "  ...\n",
            "  [-1.38281 -0.138672 -2.26562 ... 0.539062 -1.05469 0.249023]\n",
            "  [-1.38281 -0.141602 -2.25 ... 0.539062 -1.05469 0.249023]\n",
            "  [-1.38281 -0.137695 -2.25 ... 0.542969 -1.05469 0.251953]]\n",
            "\n",
            " [[-1.39844 -0.158203 -2.25 ... 0.535156 -1.03906 0.244141]\n",
            "  [-1.39844 -0.158203 -2.25 ... 0.535156 -1.03906 0.244141]\n",
            "  [-1.39062 -0.158203 -2.25 ... 0.539062 -1.03906 0.239258]\n",
            "  ...\n",
            "  [-1.38281 -0.138672 -2.26562 ... 0.539062 -1.05469 0.249023]\n",
            "  [-1.38281 -0.141602 -2.25 ... 0.539062 -1.05469 0.249023]\n",
            "  [-1.38281 -0.137695 -2.25 ... 0.542969 -1.05469 0.251953]]]\n",
            "attn_output=[[[-1.28906 0.617188 -1.84375 ... 1.6875 -1.21875 -1.34375]\n",
            "  [-1.28906 0.617188 -1.84375 ... 1.6875 -1.21875 -1.34375]\n",
            "  [-1.27344 0.617188 -1.82812 ... 1.6875 -1.21875 -1.34375]\n",
            "  ...\n",
            "  [-1.27344 0.613281 -1.80469 ... 1.6875 -1.21875 -1.32031]\n",
            "  [-1.27344 0.617188 -1.82812 ... 1.67188 -1.23438 -1.3125]\n",
            "  [-1.26562 0.617188 -1.84375 ... 1.6875 -1.22656 -1.32031]]\n",
            "\n",
            " [[-1.28906 0.617188 -1.84375 ... 1.6875 -1.21875 -1.34375]\n",
            "  [-1.28906 0.617188 -1.84375 ... 1.6875 -1.21875 -1.34375]\n",
            "  [-1.27344 0.617188 -1.82812 ... 1.6875 -1.21875 -1.34375]\n",
            "  ...\n",
            "  [-1.27344 0.613281 -1.80469 ... 1.6875 -1.21875 -1.32031]\n",
            "  [-1.27344 0.617188 -1.82812 ... 1.67188 -1.23438 -1.3125]\n",
            "  [-1.26562 0.617188 -1.84375 ... 1.6875 -1.22656 -1.32031]]\n",
            "\n",
            " [[-1.28906 0.617188 -1.84375 ... 1.6875 -1.21875 -1.34375]\n",
            "  [-1.28906 0.617188 -1.84375 ... 1.6875 -1.21875 -1.34375]\n",
            "  [-1.27344 0.617188 -1.82812 ... 1.6875 -1.21875 -1.34375]\n",
            "  ...\n",
            "  [-1.27344 0.613281 -1.80469 ... 1.6875 -1.21875 -1.32031]\n",
            "  [-1.27344 0.617188 -1.82812 ... 1.67188 -1.23438 -1.3125]\n",
            "  [-1.26562 0.617188 -1.84375 ... 1.6875 -1.22656 -1.32031]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.28906 0.617188 -1.84375 ... 1.6875 -1.21875 -1.34375]\n",
            "  [-1.28906 0.617188 -1.84375 ... 1.6875 -1.21875 -1.34375]\n",
            "  [-1.27344 0.617188 -1.82812 ... 1.6875 -1.21875 -1.34375]\n",
            "  ...\n",
            "  [-1.27344 0.613281 -1.80469 ... 1.6875 -1.21875 -1.32031]\n",
            "  [-1.27344 0.617188 -1.82812 ... 1.67188 -1.23438 -1.3125]\n",
            "  [-1.26562 0.617188 -1.84375 ... 1.6875 -1.22656 -1.32031]]\n",
            "\n",
            " [[-1.28906 0.617188 -1.84375 ... 1.6875 -1.21875 -1.34375]\n",
            "  [-1.28906 0.617188 -1.84375 ... 1.6875 -1.21875 -1.34375]\n",
            "  [-1.27344 0.617188 -1.82812 ... 1.6875 -1.21875 -1.34375]\n",
            "  ...\n",
            "  [-1.27344 0.613281 -1.80469 ... 1.6875 -1.21875 -1.32031]\n",
            "  [-1.27344 0.617188 -1.82812 ... 1.67188 -1.23438 -1.3125]\n",
            "  [-1.26562 0.617188 -1.84375 ... 1.6875 -1.22656 -1.32031]]\n",
            "\n",
            " [[-1.28906 0.617188 -1.84375 ... 1.6875 -1.21875 -1.34375]\n",
            "  [-1.28906 0.617188 -1.84375 ... 1.6875 -1.21875 -1.34375]\n",
            "  [-1.27344 0.617188 -1.82812 ... 1.6875 -1.21875 -1.34375]\n",
            "  ...\n",
            "  [-1.27344 0.613281 -1.80469 ... 1.6875 -1.21875 -1.32031]\n",
            "  [-1.27344 0.617188 -1.82812 ... 1.67188 -1.23438 -1.3125]\n",
            "  [-1.26562 0.617188 -1.84375 ... 1.6875 -1.22656 -1.32031]]]\n",
            "next_layer_addition_dropped_out=[[[-6.6875 2.65625 -9.9375 ... 8.125 -6.65625 -7.875]\n",
            "  [-6.6875 2.65625 -9.9375 ... 8.125 -6.65625 -7.875]\n",
            "  [-6.625 2.64062 -9.875 ... 8.125 -6.65625 -7.84375]\n",
            "  ...\n",
            "  [-6.65625 2.60938 -9.75 ... 8.125 -6.65625 -7.75]\n",
            "  [-6.625 2.64062 -9.875 ... 8 -6.71875 -7.6875]\n",
            "  [-6.5625 2.625 -9.9375 ... 8.125 -6.6875 -7.75]]\n",
            "\n",
            " [[-6.6875 2.65625 -9.9375 ... 8.125 -6.65625 -7.875]\n",
            "  [-6.6875 2.65625 -9.9375 ... 8.125 -6.65625 -7.875]\n",
            "  [-6.625 2.64062 -9.875 ... 8.125 -6.65625 -7.84375]\n",
            "  ...\n",
            "  [-6.65625 2.60938 -9.75 ... 8.125 -6.65625 -7.75]\n",
            "  [-6.625 2.64062 -9.875 ... 8 -6.71875 -7.6875]\n",
            "  [-6.5625 2.625 -9.9375 ... 8.125 -6.6875 -7.75]]\n",
            "\n",
            " [[-6.6875 2.65625 -9.9375 ... 8.125 -6.65625 -7.875]\n",
            "  [-6.6875 2.65625 -9.9375 ... 8.125 -6.65625 -7.875]\n",
            "  [-6.625 2.64062 -9.875 ... 8.125 -6.65625 -7.84375]\n",
            "  ...\n",
            "  [-6.65625 2.60938 -9.75 ... 8.125 -6.65625 -7.75]\n",
            "  [-6.625 2.64062 -9.875 ... 8 -6.71875 -7.6875]\n",
            "  [-6.5625 2.625 -9.9375 ... 8.125 -6.6875 -7.75]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-6.6875 2.65625 -9.9375 ... 8.125 -6.65625 -7.875]\n",
            "  [-6.6875 2.65625 -9.9375 ... 8.125 -6.65625 -7.875]\n",
            "  [-6.625 2.64062 -9.875 ... 8.125 -6.65625 -7.84375]\n",
            "  ...\n",
            "  [-6.65625 2.60938 -9.75 ... 8.125 -6.65625 -7.75]\n",
            "  [-6.625 2.64062 -9.875 ... 8 -6.71875 -7.6875]\n",
            "  [-6.5625 2.625 -9.9375 ... 8.125 -6.6875 -7.75]]\n",
            "\n",
            " [[-6.6875 2.65625 -9.9375 ... 8.125 -6.65625 -7.875]\n",
            "  [-6.6875 2.65625 -9.9375 ... 8.125 -6.65625 -7.875]\n",
            "  [-6.625 2.64062 -9.875 ... 8.125 -6.65625 -7.84375]\n",
            "  ...\n",
            "  [-6.65625 2.60938 -9.75 ... 8.125 -6.65625 -7.75]\n",
            "  [-6.625 2.64062 -9.875 ... 8 -6.71875 -7.6875]\n",
            "  [-6.5625 2.625 -9.9375 ... 8.125 -6.6875 -7.75]]\n",
            "\n",
            " [[-6.6875 2.65625 -9.9375 ... 8.125 -6.65625 -7.875]\n",
            "  [-6.6875 2.65625 -9.9375 ... 8.125 -6.65625 -7.875]\n",
            "  [-6.625 2.64062 -9.875 ... 8.125 -6.65625 -7.84375]\n",
            "  ...\n",
            "  [-6.65625 2.60938 -9.75 ... 8.125 -6.65625 -7.75]\n",
            "  [-6.625 2.64062 -9.875 ... 8 -6.71875 -7.6875]\n",
            "  [-6.5625 2.625 -9.9375 ... 8.125 -6.6875 -7.75]]]\n",
            "attn_mask=[[[[[ 0.0000000e+00 -2.3819763e+38 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00 -2.3819763e+38 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -2.3819763e+38\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    ...\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "     -2.3819763e+38 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00 -2.3819763e+38]\n",
            "    [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "      0.0000000e+00  0.0000000e+00]]]]]\n",
            "\u001b[38;2;79;201;177mTransformer\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # Param: 2,506,434,560 (5.0 GB), RngState: 4 (24 B), Total: 2,506,434,564 (5.0 GB)\u001b[0m\n",
            "  \u001b[38;2;156;220;254mconfig\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m<MaxText.pyconfig.HyperParameters object at 0x7d8908d09710>,\n",
            "  \u001b[38;2;156;220;254mdecoder\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mToNNX\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # Param: 1,981,884,416 (4.0 GB), RngState: 4 (24 B), Total: 1,981,884,420 (4.0 GB)\u001b[0m\n",
            "    \u001b[38;2;156;220;254mdecoder_norm\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'scale'\u001b[0m: \u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 2,048 (4.1 KB)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mArray\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;156;220;254mshape\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;182;207;169m2048\u001b[0m,\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16)\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mmesh\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'norm'\u001b[0m,\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding_rules\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mlinen_meta_type\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mLogicallyPartitioned\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m,\n",
            "    \u001b[38;2;156;220;254mlayers\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'mlp'\u001b[0m: \u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'wi_0'\u001b[0m: \u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'kernel'\u001b[0m: \u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 603,979,776 (1.2 GB)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mArray\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;156;220;254mshape\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;182;207;169m2048\u001b[0m, \u001b[38;2;182;207;169m18\u001b[0m, \u001b[38;2;182;207;169m16384\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16)\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mmesh\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'embed'\u001b[0m, \u001b[38;2;207;144;120m'layers'\u001b[0m, \u001b[38;2;207;144;120m'mlp'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding_rules\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mlinen_meta_type\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mLogicallyPartitioned\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m, \u001b[38;2;207;144;120m'wi_1'\u001b[0m: \u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'kernel'\u001b[0m: \u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 603,979,776 (1.2 GB)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mArray\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;156;220;254mshape\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;182;207;169m2048\u001b[0m, \u001b[38;2;182;207;169m18\u001b[0m, \u001b[38;2;182;207;169m16384\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16)\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mmesh\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'embed'\u001b[0m, \u001b[38;2;207;144;120m'layers'\u001b[0m, \u001b[38;2;207;144;120m'mlp'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding_rules\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mlinen_meta_type\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mLogicallyPartitioned\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m, \u001b[38;2;207;144;120m'wo'\u001b[0m: \u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'kernel'\u001b[0m: \u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 603,979,776 (1.2 GB)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mArray\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;156;220;254mshape\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;182;207;169m16384\u001b[0m, \u001b[38;2;182;207;169m18\u001b[0m, \u001b[38;2;182;207;169m2048\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16)\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mmesh\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'mlp'\u001b[0m, \u001b[38;2;207;144;120m'layers'\u001b[0m, \u001b[38;2;207;144;120m'embed'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding_rules\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mlinen_meta_type\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mLogicallyPartitioned\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m, \u001b[38;2;207;144;120m'pre_ffw_norm'\u001b[0m: \u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'scale'\u001b[0m: \u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 36,864 (73.7 KB)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mArray\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;156;220;254mshape\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;182;207;169m2048\u001b[0m, \u001b[38;2;182;207;169m18\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16)\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mmesh\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'norm'\u001b[0m, \u001b[38;2;207;144;120m'layers'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding_rules\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mlinen_meta_type\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mLogicallyPartitioned\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m, \u001b[38;2;207;144;120m'pre_self_attention_norm'\u001b[0m: \u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'scale'\u001b[0m: \u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 36,864 (73.7 KB)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mArray\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;156;220;254mshape\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;182;207;169m2048\u001b[0m, \u001b[38;2;182;207;169m18\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16)\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mmesh\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'norm'\u001b[0m, \u001b[38;2;207;144;120m'layers'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding_rules\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mlinen_meta_type\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mLogicallyPartitioned\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m, \u001b[38;2;207;144;120m'self_attention'\u001b[0m: \u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'key'\u001b[0m: \u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'kernel'\u001b[0m: \u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 9,437,184 (18.9 MB)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mArray\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;156;220;254mshape\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;182;207;169m2048\u001b[0m, \u001b[38;2;182;207;169m18\u001b[0m, \u001b[38;2;182;207;169m1\u001b[0m, \u001b[38;2;182;207;169m256\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16)\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mmesh\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'embed'\u001b[0m, \u001b[38;2;207;144;120m'layers'\u001b[0m, \u001b[38;2;207;144;120m'kv_heads'\u001b[0m, \u001b[38;2;207;144;120m'kv_head_dim'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding_rules\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mlinen_meta_type\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mLogicallyPartitioned\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m, \u001b[38;2;207;144;120m'out'\u001b[0m: \u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'kernel'\u001b[0m: \u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 75,497,472 (151.0 MB)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mArray\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;156;220;254mshape\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;182;207;169m8\u001b[0m, \u001b[38;2;182;207;169m18\u001b[0m, \u001b[38;2;182;207;169m256\u001b[0m, \u001b[38;2;182;207;169m2048\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16)\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mmesh\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'heads'\u001b[0m, \u001b[38;2;207;144;120m'layers'\u001b[0m, \u001b[38;2;207;144;120m'kv'\u001b[0m, \u001b[38;2;207;144;120m'embed'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding_rules\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mlinen_meta_type\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mLogicallyPartitioned\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m, \u001b[38;2;207;144;120m'query'\u001b[0m: \u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'kernel'\u001b[0m: \u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 75,497,472 (151.0 MB)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mArray\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;156;220;254mshape\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;182;207;169m2048\u001b[0m, \u001b[38;2;182;207;169m18\u001b[0m, \u001b[38;2;182;207;169m8\u001b[0m, \u001b[38;2;182;207;169m256\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16)\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mmesh\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'embed'\u001b[0m, \u001b[38;2;207;144;120m'layers'\u001b[0m, \u001b[38;2;207;144;120m'q_heads'\u001b[0m, \u001b[38;2;207;144;120m'kv'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding_rules\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mlinen_meta_type\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mLogicallyPartitioned\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m, \u001b[38;2;207;144;120m'value'\u001b[0m: \u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;207;144;120m'kernel'\u001b[0m: \u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 9,437,184 (18.9 MB)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mArray\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;156;220;254mshape\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;182;207;169m2048\u001b[0m, \u001b[38;2;182;207;169m18\u001b[0m, \u001b[38;2;182;207;169m1\u001b[0m, \u001b[38;2;182;207;169m256\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16)\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mmesh\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'embed'\u001b[0m, \u001b[38;2;207;144;120m'layers'\u001b[0m, \u001b[38;2;207;144;120m'kv_heads'\u001b[0m, \u001b[38;2;207;144;120m'kv_head_dim'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding_rules\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mlinen_meta_type\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mLogicallyPartitioned\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m\u001b[38;2;255;213;3m}\u001b[0m,\n",
            "    \u001b[38;2;156;220;254mto_nnx__module\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mDecoder(\n",
            "        # attributes\n",
            "        config = <MaxText.pyconfig.HyperParameters object at 0x7d8908d09710>\n",
            "        mesh = Mesh(device_ids=array([[[[[[[[[[[[0]]]]]]]]],\n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "                 [[[[[[[[[1]]]]]]]]],\n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "                 [[[[[[[[[2]]]]]]]]],\n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "                 [[[[[[[[[3]]]]]]]]],\n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "                 [[[[[[[[[7]]]]]]]]],\n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "                 [[[[[[[[[6]]]]]]]]],\n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "                 [[[[[[[[[5]]]]]]]]],\n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "                 [[[[[[[[[4]]]]]]]]]]]]), axis_names=('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'), axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto))\n",
            "        quant = None\n",
            "    ),\n",
            "    \u001b[38;2;156;220;254mto_nnx__rngs\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mRngs\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # RngState: 4 (24 B)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mdropout\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mRngStream\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # RngState: 2 (12 B)\u001b[0m\n",
            "        \u001b[38;2;156;220;254mcount\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mRngCount\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (4 B)\u001b[0m\n",
            "          \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(1, dtype=uint32),\n",
            "          \u001b[38;2;156;220;254mtag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'dropout'\u001b[0m\n",
            "        \u001b[38;2;255;213;3m)\u001b[0m,\n",
            "        \u001b[38;2;156;220;254mkey\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mRngKey\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n",
            "          \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray((), dtype=key<fry>) overlaying:\n",
            "          [ 507451445 1853169794],\n",
            "          \u001b[38;2;156;220;254mtag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'dropout'\u001b[0m\n",
            "        \u001b[38;2;255;213;3m)\u001b[0m,\n",
            "        \u001b[38;2;156;220;254mtag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'dropout'\u001b[0m\n",
            "      \u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254mparams\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mRngStream\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # RngState: 2 (12 B)\u001b[0m\n",
            "        \u001b[38;2;156;220;254mcount\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mRngCount\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (4 B)\u001b[0m\n",
            "          \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(1, dtype=uint32),\n",
            "          \u001b[38;2;156;220;254mtag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'params'\u001b[0m\n",
            "        \u001b[38;2;255;213;3m)\u001b[0m,\n",
            "        \u001b[38;2;156;220;254mkey\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mRngKey\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n",
            "          \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray((), dtype=key<fry>) overlaying:\n",
            "          [ 928981903 3453687069],\n",
            "          \u001b[38;2;156;220;254mtag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'params'\u001b[0m\n",
            "        \u001b[38;2;255;213;3m)\u001b[0m,\n",
            "        \u001b[38;2;156;220;254mtag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'params'\u001b[0m\n",
            "      \u001b[38;2;255;213;3m)\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m\n",
            "  \u001b[38;2;255;213;3m)\u001b[0m,\n",
            "  \u001b[38;2;156;220;254mmesh\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mMesh(device_ids=array([[[[[[[[[[[[0]]]]]]]]],\n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "           [[[[[[[[[1]]]]]]]]],\n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "           [[[[[[[[[2]]]]]]]]],\n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "           [[[[[[[[[3]]]]]]]]],\n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "           [[[[[[[[[7]]]]]]]]],\n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "           [[[[[[[[[6]]]]]]]]],\n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "           [[[[[[[[[5]]]]]]]]],\n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "           [[[[[[[[[4]]]]]]]]]]]]), axis_names=('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'), axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)),\n",
            "  \u001b[38;2;156;220;254mmodel_mode\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'train'\u001b[0m,\n",
            "  \u001b[38;2;156;220;254mquant\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "  \u001b[38;2;156;220;254mtoken_embedder\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mEmbed\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # Param: 524,550,144 (1.0 GB)\u001b[0m\n",
            "    \u001b[38;2;156;220;254mattend_dtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16),\n",
            "    \u001b[38;2;156;220;254mcast_input_dtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n",
            "    \u001b[38;2;156;220;254mconfig\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m<MaxText.pyconfig.HyperParameters object at 0x7d8908d09710>,\n",
            "    \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16),\n",
            "    \u001b[38;2;156;220;254membedding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 524,550,144 (1.0 GB)\u001b[0m\n",
            "      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mArray\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;156;220;254mshape\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;182;207;169m256128\u001b[0m, \u001b[38;2;182;207;169m2048\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;156;220;254mdtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mdtype(bfloat16)\u001b[38;2;255;213;3m)\u001b[0m,\n",
            "      \u001b[38;2;156;220;254msharding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'vocab'\u001b[0m, \u001b[38;2;207;144;120m'embed'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m\n",
            "    \u001b[38;2;255;213;3m)\u001b[0m,\n",
            "    \u001b[38;2;156;220;254mnum_embeddings\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;182;207;169m256128\u001b[0m,\n",
            "    \u001b[38;2;156;220;254mnum_features\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;182;207;169m2048\u001b[0m\n",
            "  \u001b[38;2;255;213;3m)\u001b[0m,\n",
            "  \u001b[38;2;156;220;254mvision_encoder\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m\n",
            "\u001b[38;2;255;213;3m)\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<script> (()=>{ if (customElements.get('treescope-container') === undefined) { class TreescopeContainer extends HTMLElement { constructor() { super(); this.attachShadow({mode: \"open\"}); this.defns = {}; this.state = {}; } } customElements.define(\"treescope-container\", TreescopeContainer); } if (customElements.get('treescope-run-here') === undefined) { class RunHere extends HTMLElement { constructor() { super() } connectedCallback() { const run = child => { const fn = new Function(child.textContent); child.textContent = \"\"; fn.call(this); this.remove(); }; const child = this.querySelector(\"script\"); if (child) { run(child); } else { new MutationObserver(()=>{ run(this.querySelector(\"script\")); }).observe(this, {childList: true}); } } } customElements.define(\"treescope-run-here\", RunHere); } })(); </script> <treescope-container class=\"treescope_out_c71a3f37e94d4b6aae96cf21fbf76090\" style=\"display:block\"></treescope-container> <treescope-run-here><script type=\"application/octet-stream\"> const root = ( Array.from(document.getElementsByClassName( \"treescope_out_c71a3f37e94d4b6aae96cf21fbf76090\")) .filter((elt) => !elt.dataset.setup) )[0]; root.dataset.setup = 1; const msg = document.createElement(\"span\"); msg.style = \"color: #cccccc; font-family: monospace;\"; msg.textContent = \"(Loading...)\"; root.state.loadingMsg = msg; root.shadowRoot.appendChild(msg); root.state.chain = new Promise((resolve, reject) => { const observer = new IntersectionObserver((entries) => { for (const entry of entries) { if (entry.isIntersecting) { resolve(); observer.disconnect(); return; } } }, {rootMargin: \"1000px\"}); window.setTimeout(() => { observer.observe(root); }, 0); }); root.state.deferring = false; const _insertNode = (node) => { for (let oldScript of node.querySelectorAll(\"script\")) { let newScript = document.createElement(\"script\"); newScript.type = oldScript.type; newScript.textContent = oldScript.textContent; oldScript.parentNode.replaceChild(newScript, oldScript); } if (root.state.loadingMsg) { root.state.loadingMsg.remove(); root.state.loadingMsg = null; } root.shadowRoot.appendChild(node); }; root.defns.insertContent = ((contentNode, compressed) => { if (compressed) { root.state.deferring = true; } if (root.state.deferring) { root.state.chain = (async () => { await root.state.chain; if (compressed) { const encoded = contentNode.textContent; const blob = new Blob([ Uint8Array.from(atob(encoded), (m) => m.codePointAt(0)) ]); const reader = blob.stream().pipeThrough( new DecompressionStream(\"deflate\") ).pipeThrough( new TextDecoderStream(\"utf-8\") ).getReader(); const parts = []; while (true) { const step = await reader.read(); if (step.done) { break; } parts.push(step.value); } const tpl = document.createElement('template'); tpl.innerHTML = parts.join(\"\"); _insertNode(tpl.content); } else { _insertNode(contentNode.content); } })(); } else { _insertNode(contentNode.content); } }); </script></treescope-run-here><div style=\"display:none\"> <script type=\"application/octet-stream\" >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</script> <treescope-run-here><script type=\"application/octet-stream\"> const root = ( Array.from(document.getElementsByClassName( \"treescope_out_c71a3f37e94d4b6aae96cf21fbf76090\")) .filter((elt) => !elt.dataset['step0']) )[0]; root.dataset['step0'] = 1; root.defns.insertContent( this.parentNode.querySelector('script[type=\"application/octet-stream\"]'), true ); this.parentNode.remove(); </script></treescope-run-here> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div style=\"display:none\"> <script type=\"application/octet-stream\" >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</script> <treescope-run-here><script type=\"application/octet-stream\"> const root = ( Array.from(document.getElementsByClassName( \"treescope_out_c71a3f37e94d4b6aae96cf21fbf76090\")) .filter((elt) => !elt.dataset['step1']) )[0]; root.dataset['step1'] = 1; root.defns.insertContent( this.parentNode.querySelector('script[type=\"application/octet-stream\"]'), true ); this.parentNode.remove(); </script></treescope-run-here> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized successfully\n",
            "Model mesh shape: OrderedDict([('data', 1), ('stage', 1), ('fsdp', 8), ('fsdp_transpose', 1), ('sequence', 1), ('context', 1), ('context_autoregressive', 1), ('tensor', 1), ('tensor_transpose', 1), ('tensor_sequence', 1), ('expert', 1), ('autoregressive', 1)])\n",
            "Model config: TransformerConfig(num_layers=26, num_embed=256128, embed_dim=2304, hidden_dim=9216, num_heads=8, head_dim=256, num_kv_heads=4, final_logit_softcap=30.0, use_post_attn_norm=True, use_post_ffw_norm=True, attention_types=(<AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>, <AttentionType.LOCAL_SLIDING: 2>, <AttentionType.GLOBAL: 1>), attn_logits_soft_cap=50.0, sliding_window_size=4096, shd_config=ShardingConfig(emb_vd=('tp', 'fsdp'), q_weight_ndh=('tp', 'fsdp', None), kv_weight_cndh=(None, 'tp', 'fsdp', None), qkv_weight_cndh=(None, 'tp', 'fsdp', None), o_weight_nhd=('tp', None, 'fsdp'), ffw_weight_df=('fsdp', 'tp'), ffw_weight_fd=('tp', 'fsdp'), rms_norm_weight=('tp',), act_btd=('fsdp', None, 'tp'), act_btf=('fsdp', None, 'tp'), act_btnh=('fsdp', None, 'tp', None)))\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# # Policy model\n",
        "# lora_gemma = get_lora_model(gemma, mesh=mesh)\n",
        "# nnx.display(lora_gemma)\n",
        "\n",
        "\n",
        "# Policy model\n",
        "# This can remain unchanged from default Tunix's colab\n",
        "# lora_gemma = get_lora_model(gemma, mesh=mesh)\n",
        "\n",
        "# TODO: @mazumdera: change this to use lora\n",
        "# lora_gemma = get_lora_model(gemma, mesh=mesh)\n",
        "# nnx.display(lora_gemma)\n",
        "\n",
        "gemma_policy, mesh_policy, model_config_policy = get_ref_maxtext_model()\n",
        "\n",
        "# gemma_maxtext_nnx = nnx.bridge.ToNNX(gemma)\n",
        "# Instead of:\n",
        "nnx.display(gemma_policy)\n",
        "\n",
        "# Use:\n",
        "print(\"Model initialized successfully\")\n",
        "print(f\"Model mesh shape: {mesh_policy.shape}\")\n",
        "print(f\"Model config: {model_config_policy}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zLzR1tJfTn1k",
      "metadata": {},
      "source": [
        "## Define reward functions\n",
        "\n",
        "We define four reward functions:\n",
        "\n",
        "- reward if the format of the output exactly matches the instruction given in\n",
        "`TEMPLATE`;\n",
        "- reward if the format of the output approximately matches the instruction given\n",
        "in `TEMPLATE`;\n",
        "- reward if the answer is correct/partially correct;\n",
        "- Sometimes, the text between `<answer>`, `</answer>` might not be one\n",
        "  number. So, extract the number, and reward the model if the answer is correct.\n",
        "\n",
        "The reward functions are inspired from\n",
        "[here](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb).\n",
        "\n",
        "First off, let's define a RegEx for checking whether the format matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C7Beft8wTn1k",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 54), match='<reasoning>Let me think!</reasoning><answer>2</an>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "match_format = re.compile(\n",
        "    rf\"^[\\s]{{0,}}\"\n",
        "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\n",
        "    rf\"{solution_start}(.+?){solution_end}\"\n",
        "    rf\"[\\s]{{0,}}$\",\n",
        "    flags=re.MULTILINE | re.DOTALL,\n",
        ")\n",
        "\n",
        "match_format.search(\n",
        "    f\"{reasoning_start}Let me\"\n",
        "    f\" think!{reasoning_end}{solution_start}2{solution_end}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fe1rF15zTn1k",
      "metadata": {},
      "source": [
        "Give the model a reward of 3 points if the format matches exactly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_fhQ6pY2Tn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def match_format_exactly(prompts, completions, **kargs):\n",
        "  scores = []\n",
        "  for completion in completions:\n",
        "    score = 0\n",
        "    response = completion\n",
        "    # Match if format is seen exactly!\n",
        "    if match_format.search(response) is not None:\n",
        "      score += 3.0\n",
        "    scores.append(score)\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sWdAdUHuTn1k",
      "metadata": {},
      "source": [
        "We also reward the model if the format of the output matches partially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uOhO4f3-Tn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def match_format_approximately(prompts, completions, **kargs):\n",
        "  scores = []\n",
        "\n",
        "  for completion in completions:\n",
        "    score = 0\n",
        "    response = completion\n",
        "    # Count how many keywords are seen - we penalize if too many!\n",
        "    # If we see 1, then plus some points!\n",
        "    score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n",
        "    score += 0.5 if response.count(reasoning_end) == 1 else -0.5\n",
        "    score += 0.5 if response.count(solution_start) == 1 else -0.5\n",
        "    score += 0.5 if response.count(solution_end) == 1 else -0.5\n",
        "    scores.append(score)\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A2fNZDgTTn1k",
      "metadata": {},
      "source": [
        "Reward the model if the answer is correct. A reward is also given if the answer\n",
        "does not match exactly, i.e., based on how close the answer is to the correct\n",
        "value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S8zcWsmhTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_answer(prompts, completions, answer, **kargs):\n",
        "  responses = completions\n",
        "\n",
        "  extracted_responses = [\n",
        "      guess.group(1) if (guess := match_format.search(r)) is not None else None\n",
        "      for r in responses\n",
        "  ]\n",
        "\n",
        "  scores = []\n",
        "  for guess, true_answer in zip(extracted_responses, answer):\n",
        "    score = 0\n",
        "    if guess is None:\n",
        "      scores.append(0)\n",
        "      continue\n",
        "    # Correct answer gets 3 points!\n",
        "    if guess == true_answer:\n",
        "      score += 3.0\n",
        "    # Match if spaces are seen\n",
        "    elif guess.strip() == true_answer.strip():\n",
        "      score += 1.5\n",
        "    else:\n",
        "      # We also reward it if the answer is close via ratios!\n",
        "      # Ie if the answer is within some range, reward it!\n",
        "      try:\n",
        "        ratio = float(guess) / float(true_answer)\n",
        "        if ratio >= 0.9 and ratio <= 1.1:\n",
        "          score += 0.5\n",
        "        elif ratio >= 0.8 and ratio <= 1.2:\n",
        "          score += 0.25\n",
        "        else:\n",
        "          score -= 1.0  # Penalize wrong answers\n",
        "      except:\n",
        "        score -= 0.5  # Penalize\n",
        "    scores.append(score)\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nIpOVv78Tn1k",
      "metadata": {},
      "source": [
        "Sometimes, the text between `<answer>` and `</answer>` might not be one\n",
        "number; it can be a sentence. So, we extract the number and compare the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NXvRtbk8Tn1k",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['0.34']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "match_numbers = re.compile(\n",
        "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\", flags=re.MULTILINE | re.DOTALL\n",
        ")\n",
        "match_numbers.findall(f\"{solution_start}  0.34  {solution_end}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oxZQAFKOTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_numbers(prompts, completions, answer, **kargs):\n",
        "  question = kargs[\"question\"]\n",
        "  responses = completions\n",
        "\n",
        "  extracted_responses = [\n",
        "      guess.group(1) if (guess := match_numbers.search(r)) is not None else None\n",
        "      for r in responses\n",
        "  ]\n",
        "\n",
        "  scores = []\n",
        "  print(\"START ============================\")\n",
        "  print(f\"Question: {question[0]}\")\n",
        "  print(f\"Answer: {answer[0]}\")\n",
        "  print(f\"Response: {responses[0]}\")\n",
        "  print(f\"Extracted: {extracted_responses[0]}\")\n",
        "  print(\"END ==============================\")\n",
        "  for guess, true_answer in zip(extracted_responses, answer):\n",
        "    if guess is None:\n",
        "      scores.append(0)\n",
        "      continue\n",
        "    # Convert to numbers\n",
        "    try:\n",
        "      true_answer = float(true_answer.strip())\n",
        "      guess = float(guess.strip())\n",
        "      scores.append(1.5 if guess == true_answer else 0.0)\n",
        "    except:\n",
        "      scores.append(0)\n",
        "      continue\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AaiYMJxFTn1k",
      "metadata": {},
      "source": [
        "## Evaluate\n",
        "\n",
        "\n",
        "Before we train the model, let's evaluate the model on the test set so we can\n",
        "see the improvement post training.\n",
        "\n",
        "We evaluate it in two ways:\n",
        "\n",
        "**Quantitative**\n",
        "\n",
        "* **Answer Accuracy**: percentage of samples for which the model predicts the\n",
        "correct final numerical answer  \n",
        "* **Answer (Partial) Accuracy**: percentage of samples for which the model\n",
        "predicts a final numerical answer such that the \\`model answer / answer\\`\n",
        "ratio lies between 0.9 and 1.1.  \n",
        "* **Format Accuracy**: percentage of samples for which the model outputs the\n",
        "correct format, i.e., reasoning between the reasoning special tokens, and the\n",
        "final answer between the \\`\\<start\\_answer\\>\\`, \\`\\<end\\_answer\\>\\` tokens.\n",
        "\n",
        "**Qualitative**\n",
        "\n",
        "We'll also print outputs for a few given questions so that we can compare the generated output later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_k58bOicUHJy",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate(\n",
        "    question, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None\n",
        "):\n",
        "  \"\"\"Given prompt, generates text.\"\"\"\n",
        "\n",
        "  if isinstance(question, str):\n",
        "    input_batch = [\n",
        "        TEMPLATE.format(\n",
        "            system_prompt=SYSTEM_PROMPT,\n",
        "            question=question,\n",
        "        ),\n",
        "    ]\n",
        "  else:\n",
        "    input_batch = [\n",
        "        TEMPLATE.format(\n",
        "            system_prompt=SYSTEM_PROMPT,\n",
        "            question=q,\n",
        "        )\n",
        "        for q in question\n",
        "    ]\n",
        "\n",
        "  out_data = sampler(\n",
        "      input_strings=input_batch,\n",
        "      total_generation_steps=768,\n",
        "      temperature=temperature,\n",
        "      top_k=top_k,\n",
        "      top_p=top_p,\n",
        "      echo=False,\n",
        "      seed=seed if seed is not None else None,\n",
        "  )\n",
        "\n",
        "  output = out_data.text\n",
        "  if isinstance(question, str):\n",
        "    return output[0]\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yJo2nuKB-wlw",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    dataset,\n",
        "    sampler,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    num_passes=1,\n",
        "    corr_lst=False,\n",
        "    make_lst=False,\n",
        "):\n",
        "  \"\"\"Computes accuracy and percentage of outputs matching the format.\"\"\"\n",
        "\n",
        "  response_lst = []\n",
        "  corr = 0\n",
        "  partially_corr = 0\n",
        "  corr_format = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch in tqdm(dataset):\n",
        "    answers = batch[\"answer\"]\n",
        "    questions = batch[\"question\"]\n",
        "\n",
        "    multiple_call_responses = [[] for _ in range(len(questions))]\n",
        "    for p in range(num_passes):\n",
        "      responses = generate(\n",
        "          questions, sampler, temperature, top_k, top_p, seed=p\n",
        "      )\n",
        "      for idx, response in enumerate(responses):\n",
        "        multiple_call_responses[idx].append(response)\n",
        "\n",
        "    for question, multiple_call_response, answer in zip(\n",
        "        questions, multiple_call_responses, answers\n",
        "    ):\n",
        "      # check answer\n",
        "      corr_ctr_per_question = 0\n",
        "      partially_corr_per_question = 0\n",
        "      corr_format_per_question = 0\n",
        "      for response in multiple_call_response:\n",
        "        extracted_response = (\n",
        "            guess.group(1)\n",
        "            if (guess := match_numbers.search(response)) is not None\n",
        "            else \"-1000000\"\n",
        "        )\n",
        "        try:\n",
        "          if float(extracted_response.strip()) == float(answer.strip()):\n",
        "            corr_ctr_per_question += 1\n",
        "\n",
        "          ratio = float(extracted_response.strip()) / float(answer.strip())\n",
        "          if ratio >= 0.9 and ratio <= 1.1:\n",
        "            partially_corr_per_question += 1\n",
        "        except:\n",
        "          print(\"SKIPPED\")\n",
        "\n",
        "        # check format\n",
        "        if match_format.search(response) is not None:\n",
        "          corr_format_per_question += 1\n",
        "\n",
        "        if (\n",
        "            corr_ctr_per_question > 0\n",
        "            and partially_corr_per_question > 0\n",
        "            and corr_format_per_question > 0\n",
        "        ):\n",
        "          break\n",
        "\n",
        "      if corr_ctr_per_question > 0:\n",
        "        corr += 1\n",
        "        if corr_lst and make_lst:\n",
        "          response_lst.append((question, answer, multiple_call_response))\n",
        "      else:\n",
        "        if not corr_lst and make_lst:\n",
        "          response_lst.append((question, answer, multiple_call_response))\n",
        "      if partially_corr_per_question > 0:\n",
        "        partially_corr += 1\n",
        "      if corr_format_per_question > 0:\n",
        "        corr_format += 1\n",
        "\n",
        "      total += 1\n",
        "      if total % 10 == 0:\n",
        "        print(\n",
        "            f\"===> {corr=}, {total=}, {corr / total * 100=}, \"\n",
        "            f\"{partially_corr / total * 100=}, {corr_format / total * 100=}\"\n",
        "        )\n",
        "\n",
        "  to_return = (\n",
        "      corr,\n",
        "      total,\n",
        "      corr / total * 100,\n",
        "      partially_corr / total * 100,\n",
        "      corr_format / total * 100,\n",
        "  )\n",
        "  if make_lst:\n",
        "    return to_return, response_lst\n",
        "  return to_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HZMO-KflTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "gemma_tokenizer = data_lib.GemmaTokenizer()\n",
        "sampler = sampler_lib.Sampler(\n",
        "    # transformer=lora_gemma,\n",
        "    transformer=gemma_policy,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YQM-tzXWUmoE",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e08a5b3ab7ce482182a537ae49b660be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "ValueError",
          "evalue": "Cannot infer collection name from value: frozenset({'layers', '_pytree__state', 'to_nnx__rngs', 'decoder_norm'})",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m (corr, total, accuracy, partial_accuracy, format_accuracy) = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mGENERATION_CONFIGS\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgreedy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m      7\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorr\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m%, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpartial_accuracy\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m%,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformat_accuracy\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# TODO: @mazumdera: why is this 0?\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# corr=0, total=5, accuracy=0.0%, partial_accuracy=0.0%, format_accuracy=0.0%\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(dataset, sampler, temperature, top_k, top_p, num_passes, corr_lst, make_lst)\u001b[39m\n\u001b[32m     23\u001b[39m multiple_call_responses = [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(questions))]\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_passes):\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m   responses = \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m      \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m idx, response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(responses):\n\u001b[32m     29\u001b[39m     multiple_call_responses[idx].append(response)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mgenerate\u001b[39m\u001b[34m(question, sampler, temperature, top_k, top_p, seed)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     14\u001b[39m   input_batch = [\n\u001b[32m     15\u001b[39m       TEMPLATE.format(\n\u001b[32m     16\u001b[39m           system_prompt=SYSTEM_PROMPT,\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m       \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m question\n\u001b[32m     20\u001b[39m   ]\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m out_data = \u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_strings\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_generation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m768\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mecho\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m output = out_data.text\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(question, \u001b[38;5;28mstr\u001b[39m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/tunix/generate/sampler.py:708\u001b[39m, in \u001b[36mSampler.__call__\u001b[39m\u001b[34m(self, input_strings, total_generation_steps, max_prompt_length, echo, return_logits, forbidden_tokens, temperature, top_p, top_k, beam_size, seed, pad_output)\u001b[39m\n\u001b[32m    696\u001b[39m   seed = jax.random.PRNGKey(seed)\n\u001b[32m    697\u001b[39m sampling_state = \u001b[38;5;28mself\u001b[39m.init_sample_state(\n\u001b[32m    698\u001b[39m     all_input_ids,\n\u001b[32m    699\u001b[39m     include_logits=return_logits,\n\u001b[32m   (...)\u001b[39m\u001b[32m    706\u001b[39m     beam_size=beam_size,\n\u001b[32m    707\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m sampling_state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compiled_prefill_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flattened_transformer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_state\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    712\u001b[39m sampling_state = \u001b[38;5;28mself\u001b[39m._compiled_decode_fn(\n\u001b[32m    713\u001b[39m     \u001b[38;5;28mself\u001b[39m._flattened_transformer_state, sampling_state\n\u001b[32m    714\u001b[39m )\n\u001b[32m    716\u001b[39m token_buffers = sampling_state.token_buffer\n",
            "    \u001b[31m[... skipping hidden 14 frame]\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/tunix/generate/sampler.py:489\u001b[39m, in \u001b[36mSampler._prefill_fn\u001b[39m\u001b[34m(self, params, sampler_state)\u001b[39m\n\u001b[32m    484\u001b[39m attention_mask = utils.make_causal_attn_mask(\n\u001b[32m    485\u001b[39m     input_mask, \u001b[38;5;28mself\u001b[39m.cache_config.cache_size\n\u001b[32m    486\u001b[39m )\n\u001b[32m    488\u001b[39m transformer = nnx.merge(\u001b[38;5;28mself\u001b[39m._transformer_graphdef, params)\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m logits, cache = \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstep_positions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampler_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    495\u001b[39m token_buffer = sampler_state.token_buffer\n\u001b[32m    496\u001b[39m done = sampler_state.done\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/maxtext/MaxText/integration/tunix/tunix_adaptor.py:46\u001b[39m, in \u001b[36mTunixMaxTextLlama.__call__\u001b[39m\u001b[34m(self, input_tokens, positions, cache, attention_mask, output_hidden_states)\u001b[39m\n\u001b[32m     44\u001b[39m attn = attention_mask \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_attention_mask \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# logits = self.base.forward_train(\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_positions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_segment_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# attention_mask=attn,\u001b[39;49;00m\n\u001b[32m     51\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logits, \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/maxtext/MaxText/layers/models.py:217\u001b[39m, in \u001b[36mTransformer.__call__\u001b[39m\u001b[34m(self, decoder_input_tokens, decoder_positions, decoder_segment_ids, cache, encoder_images, enable_dropout, previous_chunk, true_length, slot, page_state, decoder_target_tokens, decoder_target_mask)\u001b[39m\n\u001b[32m    214\u001b[39m   \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.decoder_block == DecoderBlockType.LLAMA4:\n\u001b[32m    215\u001b[39m     bidirectional_mask = decoder_input_tokens == multimodal_utils.LLAMA4_PATCH_TOKEN\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m logits, hidden_state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshared_embedding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken_embedder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_positions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_positions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_segment_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_segment_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43menable_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprevious_chunk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprevious_chunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43mslot\u001b[49m\u001b[43m=\u001b[49m\u001b[43mslot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpage_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbidirectional_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbidirectional_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[38;5;66;03m# If we are initializing the model AND MTP is enabled, we must create\u001b[39;00m\n\u001b[32m    232\u001b[39m \u001b[38;5;66;03m# dummy target tensors. This allows Flax to trace the MTPBlock and create\u001b[39;00m\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# all its necessary parameters, without requiring the main training pipeline\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    248\u001b[39m \u001b[38;5;66;03m#      logit projection.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# Its only effect is to \"sow\" these losses; it does not alter the primary logits output.\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.mtp_num_layers > \u001b[32m0\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/maxtext/MaxText/layers/nnx_wrappers.py:248\u001b[39m, in \u001b[36mToNNX.__call__\u001b[39m\u001b[34m(self, rngs, method, *args, **kwargs)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    243\u001b[39m   nnx_attrs = {\n\u001b[32m    244\u001b[39m     k: v\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(\u001b[38;5;28mself\u001b[39m).items()\n\u001b[32m    246\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m k.startswith(\u001b[33m\"\u001b[39m\u001b[33mto_nnx__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m k.startswith(\u001b[33m\"\u001b[39m\u001b[33m_object__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    247\u001b[39m   }\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m   variables = \u001b[43mnnx_attrs_to_linen_vars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnnx_attrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m   \u001b[38;5;66;03m# Get `mutable` from top level bridge.Module context if any\u001b[39;00m\n\u001b[32m    251\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m (m := bdg_module.current_module()) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/maxtext/MaxText/layers/nnx_wrappers.py:129\u001b[39m, in \u001b[36mnnx_attrs_to_linen_vars\u001b[39m\u001b[34m(nnx_attrs)\u001b[39m\n\u001b[32m    127\u001b[39m     v = to_linen_var(v)\n\u001b[32m    128\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot infer collection name from value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    130\u001b[39m   linen_structured[(col_name, *kp)] = v\n\u001b[32m    131\u001b[39m variables = nnx.traversals.unflatten_mapping(linen_structured)\n",
            "\u001b[31mValueError\u001b[39m: Cannot infer collection name from value: frozenset({'layers', '_pytree__state', 'to_nnx__rngs', 'decoder_norm'})"
          ]
        }
      ],
      "source": [
        "(corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n",
        "    test_dataset,\n",
        "    sampler,\n",
        "    **GENERATION_CONFIGS[\"greedy\"],\n",
        ")\n",
        "print(\n",
        "    f\"{corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n",
        "    f\" {format_accuracy=}%\"\n",
        ")\n",
        "\n",
        "# TODO: @mazumdera: why is this 0?\n",
        "# corr=0, total=5, accuracy=0.0%, partial_accuracy=0.0%, format_accuracy=0.0%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PJV6wNGY-3PG",
      "metadata": {},
      "outputs": [],
      "source": [
        "# for eval_example in QUALITATIVE_EVAL_EXAMPLES:\n",
        "#   question = eval_example[\"question\"]\n",
        "#   answer = eval_example[\"answer\"]\n",
        "#   response = generate(\n",
        "#       question,\n",
        "#       sampler,\n",
        "#       temperature=INFERENCE_TEMPERATURE,\n",
        "#       top_k=INFERENCE_TOP_K,\n",
        "#       top_p=INFERENCE_TOP_P,\n",
        "#   )\n",
        "\n",
        "#   print(f\"Question:\\n{question}\")\n",
        "#   print(f\"Answer:\\n{answer}\")\n",
        "#   print(f\"Response:\\n{response}\")\n",
        "#   print(\"===============\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-CmB2ZT9Tn1l",
      "metadata": {},
      "source": [
        "## Train\n",
        "\n",
        "Let's set up all the configs first - checkpointing, metric logging and training.\n",
        "We then train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mHzdsYsGTn1l",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ckpt saving\n",
        "checkpointing_options = ocp.CheckpointManagerOptions(\n",
        "    save_interval_steps=SAVE_INTERVAL_STEPS, max_to_keep=MAX_TO_KEEP\n",
        ")\n",
        "\n",
        "# Metrics logger\n",
        "metrics_logging_options = metrics_logger.MetricsLoggerOptions(\n",
        "    log_dir=\"/home/mazumdera_google_com/content/tmp/tensorboard/grpo\", flush_every_n_steps=20\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u1Sc1fNC_CJ7",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-f0681a2d168fa6e3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-f0681a2d168fa6e3\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 39723;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Logs\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /home/mazumdera_google_com/content/tmp/tensorboard/grpo --port=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_6VxFW1ZTn1l",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Training config\n",
        "# training_config = GrpoTrainingConfig(\n",
        "#     max_prompt_length=MAX_PROMPT_LENGTH,\n",
        "#     total_generation_steps=TOTAL_GENERATION_STEPS,\n",
        "#     num_generations=NUM_GENERATIONS,\n",
        "#     num_iterations=NUM_ITERATIONS,\n",
        "#     beta=BETA,\n",
        "#     epsilon=EPSILON,\n",
        "#     temperature=TEMPERATURE,\n",
        "#     top_p=TOP_P,\n",
        "#     top_k=TOP_K,\n",
        "#     eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "#     max_steps=MAX_STEPS,\n",
        "#     # metrics logging\n",
        "#     metrics_logging_options=metrics_logging_options,\n",
        "#     # checkpoint saving\n",
        "#     checkpoint_root_directory=CKPT_DIR,\n",
        "#     checkpointing_options=checkpointing_options,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OIe1lO08Tn1l",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimizer, learning rate scheduler, gradient clipping\n",
        "# optimizer = optax.adamw(\n",
        "#     learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
        "#         init_value=0.0,\n",
        "#         peak_value=LEARNING_RATE,\n",
        "#         warmup_steps=WARMUP_STEPS,\n",
        "#         decay_steps=MAX_STEPS,\n",
        "#         end_value=0.0,\n",
        "#     ),\n",
        "#     b1=B1,\n",
        "#     b2=B2,\n",
        "#     weight_decay=WEIGHT_DECAY,\n",
        "# )\n",
        "#TODO: @mazumdera: try optimizer offloading with adamw\n",
        "optimizer = optax.adafactor(\n",
        "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
        "        init_value=0.0,\n",
        "        peak_value=LEARNING_RATE,\n",
        "        warmup_steps=WARMUP_STEPS,\n",
        "        decay_steps=MAX_STEPS,\n",
        "        end_value=0.0,\n",
        "    ),\n",
        ")\n",
        "if MAX_GRAD_NORM is not None:\n",
        "  optimizer = optax.chain(\n",
        "      optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n",
        "      optimizer,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_6VxFW1ZTn1l",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training config\n",
        "cluster_config = rl_cluster_lib.ClusterConfig(\n",
        "    role_to_mesh={\n",
        "        rl_cluster_lib.Role.ACTOR: mesh,\n",
        "        rl_cluster_lib.Role.REFERENCE: mesh,\n",
        "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
        "    },\n",
        "    rollout_engine='vanilla',\n",
        "    offload_to_cpu=False,\n",
        "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
        "        actor_optimizer=optimizer,\n",
        "        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "        max_steps=MAX_STEPS,\n",
        "        gradient_accumulation_steps=1,\n",
        "        # metrics logging\n",
        "        metrics_logging_options=metrics_logging_options,\n",
        "        # checkpoint saving\n",
        "        checkpoint_root_directory=CKPT_DIR,\n",
        "        checkpointing_options=checkpointing_options,\n",
        "    ),\n",
        "    rollout_config=base_rollout.RolloutConfig(\n",
        "        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n",
        "        max_prompt_length=MAX_PROMPT_LENGTH,\n",
        "        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_p=TOP_P,\n",
        "        top_k=TOP_K,\n",
        "    ),\n",
        ")\n",
        "\n",
        "grpo_config = GrpoConfig(\n",
        "    num_generations=NUM_GENERATIONS,\n",
        "    num_iterations=NUM_ITERATIONS,\n",
        "    beta=BETA,\n",
        "    epsilon=EPSILON,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2517d77",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sharding of embed kernel:\n",
            "\u001b[38;2;79;201;177mParam\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 524,550,144 (1.0 GB)\u001b[0m\n",
            "  \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray([[1.15625, -0.355469, 1.42969, ..., -1.58594, 0.0245361, -0.644531],\n",
            "         [1.78125, 0.707031, 1.24219, ..., 1.05469, 0.0441895, 1.375],\n",
            "         [-0.550781, 1.32812, 0.302734, ..., -0.691406, 0.925781, -1.74219],\n",
            "         ...,\n",
            "         [-0.96875, -0.851562, -1, ..., 0.0441895, -1.74219, 1.42969],\n",
            "         [0.00488281, 0.302734, 0.65625, ..., -1.83594, -0.192383,\n",
            "          0.162109],\n",
            "         [-0.251953, -1.65625, 0.730469, ..., -1.25781, -0.273438,\n",
            "          -0.878906]], dtype=bfloat16),\n",
            "  \u001b[38;2;156;220;254msharding\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'vocab'\u001b[0m, \u001b[38;2;207;144;120m'embed'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m\n",
            "\u001b[38;2;255;213;3m)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Now lora_gemma's parameters are annotated with the specified sharding.\n",
        "# When lora_gemma is used inside a jitted function, JAX will respect these\n",
        "# shardings.\n",
        "\n",
        "# You can inspect the sharding of a parameter's value.\n",
        "# The sharding will be concrete after being passed through a jitted function.\n",
        "@jax.jit\n",
        "def get_sharded_kernel(model):\n",
        "    return model.base.token_embedder.embedding\n",
        "\n",
        "with mesh:\n",
        "    sharded_kernel_value = get_sharded_kernel(gemma_policy)\n",
        "\n",
        "print(\"Sharding of embed kernel:\")\n",
        "print(sharded_kernel_value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef184e65",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster config: ClusterConfig(role_to_mesh={<Role.ACTOR: 'actor'>: Mesh(device_ids=array([[[[[[[[[[[[0]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[1]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[2]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[3]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[7]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[6]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[5]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[4]]]]]]]]]]]]), axis_names=('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'), axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), <Role.REFERENCE: 'reference'>: Mesh(device_ids=array([[[[[[[[[[[[0]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[1]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[2]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[3]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[7]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[6]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[5]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[4]]]]]]]]]]]]), axis_names=('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'), axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), <Role.ROLLOUT: 'rollout'>: Mesh(device_ids=array([[[[[[[[[[[[0]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[1]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[2]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[3]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[7]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[6]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[5]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[4]]]]]]]]]]]]), axis_names=('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'), axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto))}, rollout_engine='vanilla', offload_to_cpu=False, training_config=RLTrainingConfig(eval_every_n_steps=10, max_steps=3738, gradient_accumulation_steps=1, checkpoint_root_directory='/home/mazumdera_google_com/content/ckpts/', checkpointing_options=CheckpointManagerOptions(save_interval_steps=500, max_to_keep=4, keep_time_interval=None, keep_period=None, should_keep_fn=None, best_fn=None, best_mode='max', keep_checkpoints_without_metrics=True, step_prefix=None, step_format_fixed_length=None, step_name_format=None, create=True, cleanup_tmp_directories=False, save_on_steps=frozenset(), single_host_load_and_broadcast=False, todelete_subdir=None, todelete_full_path=None, enable_hns=False, enable_background_delete=False, read_only=False, enable_async_checkpointing=True, async_options=None, multiprocessing_options=MultiprocessingOptions(primary_host=0, active_processes=None, barrier_sync_key_prefix=None), should_save_fn=None, file_options=FileOptions(path_permission_mode=None), save_root_metadata=True, temporary_path_class=None, save_decision_policy=None, preservation_policy=None, prevent_write_metrics=False, enable_should_save_is_saving_in_progress_check=True), metrics_logging_options=MetricsLoggerOptions(log_dir='/home/mazumdera_google_com/content/tmp/tensorboard/grpo', flush_every_n_steps=20), profiler_options=None, data_sharding_axis=('fsdp',), max_inflight_computations=2, actor_optimizer=GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x735d24289760>, update=<function chain.<locals>.update_fn at 0x735dc0de2480>), critic_optimizer=None, actor_critic_share_backbone=False), rollout_config=RolloutConfig(n=1, max_tokens_to_generate=768, temperature=0.9, top_p=1.0, top_k=50, seed=None, max_prompt_length=256, kv_cache_size=1280))\n",
            "Role to mesh mapping: {<Role.ACTOR: 'actor'>: Mesh(device_ids=array([[[[[[[[[[[[0]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[1]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[2]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[3]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[7]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[6]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[5]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[4]]]]]]]]]]]]), axis_names=('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'), axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), <Role.REFERENCE: 'reference'>: Mesh(device_ids=array([[[[[[[[[[[[0]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[1]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[2]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[3]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[7]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[6]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[5]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[4]]]]]]]]]]]]), axis_names=('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'), axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), <Role.ROLLOUT: 'rollout'>: Mesh(device_ids=array([[[[[[[[[[[[0]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[1]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[2]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[3]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[7]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[6]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[5]]]]]]]]],\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "         [[[[[[[[[4]]]]]]]]]]]]), axis_names=('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'), axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto))}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manony-mouse-863749125460230603\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/mazumdera_google_com/tunix/examples/wandb/run-20250807_085924-mn1bc5da</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/anony-mouse-863749125460230603/tunix/runs/mn1bc5da?apiKey=b4d00f0db746a7c14cc080c4b44a878fb3515769' target=\"_blank\">2025-08-07_08-59-24</a></strong> to <a href='https://wandb.ai/anony-mouse-863749125460230603/tunix?apiKey=b4d00f0db746a7c14cc080c4b44a878fb3515769' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/anony-mouse-863749125460230603/tunix?apiKey=b4d00f0db746a7c14cc080c4b44a878fb3515769' target=\"_blank\">https://wandb.ai/anony-mouse-863749125460230603/tunix?apiKey=b4d00f0db746a7c14cc080c4b44a878fb3515769</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/anony-mouse-863749125460230603/tunix/runs/mn1bc5da?apiKey=b4d00f0db746a7c14cc080c4b44a878fb3515769' target=\"_blank\">https://wandb.ai/anony-mouse-863749125460230603/tunix/runs/mn1bc5da?apiKey=b4d00f0db746a7c14cc080c4b44a878fb3515769</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Do NOT share these links with anyone. They can be used to claim your runs."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# RL cluster\n",
        "rl_cluster = rl_cluster_lib.RLCluster(\n",
        "    actor=gemma_policy,\n",
        "    reference=gemma,\n",
        "    tokenizer=data_lib.GemmaTokenizer(),\n",
        "    cluster_config=cluster_config,\n",
        ")\n",
        "\n",
        "# GRPO Trainer\n",
        "grpo_trainer = GrpoLearner(\n",
        "    rl_cluster=rl_cluster,\n",
        "    reward_fns=[\n",
        "        match_format_exactly,\n",
        "        match_format_approximately,\n",
        "        check_answer,\n",
        "        check_numbers,\n",
        "    ],\n",
        "    grpo_config=grpo_config,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S27XDebYTn1l",
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Cannot infer collection name from value: frozenset({'layers', 'decoder_norm', '_pytree__state', 'to_nnx__rngs'})",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m mesh:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m   \u001b[43mgrpo_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/tunix/rl/grpo/grpo_learner.py:511\u001b[39m, in \u001b[36mGrpoLearner.train\u001b[39m\u001b[34m(self, train_ds, eval_ds, skip_jit)\u001b[39m\n\u001b[32m    505\u001b[39m     \u001b[38;5;28mself\u001b[39m.rl_cluster.update_actor(\n\u001b[32m    506\u001b[39m         curr_train_ds,\n\u001b[32m    507\u001b[39m         curr_eval_ds,\n\u001b[32m    508\u001b[39m         skip_jit,\n\u001b[32m    509\u001b[39m     )  \u001b[38;5;66;03m# loop over Œº\u001b[39;00m\n\u001b[32m    510\u001b[39m \u001b[38;5;66;03m# call to throw stop iteration as a singal to break the loop\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[38;5;66;03m# sync the train steps with internel trainer, this is based on the\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[38;5;66;03m# assumption that the trainer internally doesn't reset the train steps.\u001b[39;00m\n\u001b[32m    514\u001b[39m \u001b[38;5;66;03m# there is current a unit test to ensure this assumption.\u001b[39;00m\n\u001b[32m    515\u001b[39m \u001b[38;5;28mself\u001b[39m._train_steps = \u001b[38;5;28mself\u001b[39m.rl_cluster.actor_trainer.train_steps\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/concurrent/futures/thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/tunix/rl/grpo/grpo_learner.py:419\u001b[39m, in \u001b[36mGrpoLearner.prepare_dataset\u001b[39m\u001b[34m(self, iterator, proceed_num_steps, sample_repeat, batch_repeat, data_queue, async_loading, mode)\u001b[39m\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    421\u001b[39m   \u001b[38;5;66;03m# Signal no more iterable to be loaded.\u001b[39;00m\n\u001b[32m    422\u001b[39m   data_queue.put(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/tunix/rl/grpo/grpo_learner.py:399\u001b[39m, in \u001b[36mGrpoLearner.prepare_dataset\u001b[39m\u001b[34m(self, iterator, proceed_num_steps, sample_repeat, batch_repeat, data_queue, async_loading, mode)\u001b[39m\n\u001b[32m    388\u001b[39m example = jax.tree.map(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m x: np.repeat(x, sample_repeat, axis=\u001b[32m0\u001b[39m),\n\u001b[32m    390\u001b[39m     example,\n\u001b[32m    391\u001b[39m )  \u001b[38;5;66;03m# [B] -> [B * G]\u001b[39;00m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m jax.profiler.StepTraceAnnotation(\n\u001b[32m    394\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msampler\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    395\u001b[39m     step_num=\u001b[38;5;28mself\u001b[39m._train_steps\n\u001b[32m    396\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mode == metrics_logger.Mode.TRAIN\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._eval_steps,\n\u001b[32m    398\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m   advantage = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_and_compute_advantage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m async_loading:\n\u001b[32m    401\u001b[39m   data_queue.put([advantage])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/tunix/rl/grpo/grpo_learner.py:216\u001b[39m, in \u001b[36mGrpoLearner._generate_and_compute_advantage\u001b[39m\u001b[34m(self, training_input, mode)\u001b[39m\n\u001b[32m    213\u001b[39m eos_value = \u001b[38;5;28mself\u001b[39m.rl_cluster.rollout.eos_id()\n\u001b[32m    215\u001b[39m \u001b[38;5;66;03m# Generate, and pad output.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m completion_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrl_cluster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_input\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompts\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m completion_ids = completion_output.tokens\n\u001b[32m    220\u001b[39m prompt_ids = completion_output.left_padded_prompt_tokens\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/tunix/rl/rl_cluster.py:253\u001b[39m, in \u001b[36mRLCluster.generate\u001b[39m\u001b[34m(self, prompts)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]):\n\u001b[32m    252\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cluster_config.role_to_mesh[Role.ROLLOUT]:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrollout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcluster_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrollout_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/tunix/rl/rollout/vanilla_rollout.py:59\u001b[39m, in \u001b[36mVanillaRollout.generate\u001b[39m\u001b[34m(self, prompts, rollout_config, **kwargs)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\n\u001b[32m     53\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     54\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m     55\u001b[39m     rollout_config: base_rollout.RolloutConfig,\n\u001b[32m     56\u001b[39m     **kwargs,\n\u001b[32m     57\u001b[39m ) -> base_rollout.RolloutOutput:\n\u001b[32m     58\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Generates samples from the model.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m   output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m      \u001b[49m\u001b[43minput_strings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtotal_generation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrollout_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_tokens_to_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmax_prompt_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrollout_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_prompt_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m      \u001b[49m\u001b[43mecho\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrollout_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrollout_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrollout_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m      \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrollout_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m      \u001b[49m\u001b[43mpad_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m base_rollout.RolloutOutput(\n\u001b[32m     71\u001b[39m       text=output.text,\n\u001b[32m     72\u001b[39m       logits=output.logits,\n\u001b[32m     73\u001b[39m       tokens=output.tokens,\n\u001b[32m     74\u001b[39m       left_padded_prompt_tokens=output.padded_prompt_tokens,\n\u001b[32m     75\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/tunix/generate/sampler.py:708\u001b[39m, in \u001b[36mSampler.__call__\u001b[39m\u001b[34m(self, input_strings, total_generation_steps, max_prompt_length, echo, return_logits, forbidden_tokens, temperature, top_p, top_k, beam_size, seed, pad_output)\u001b[39m\n\u001b[32m    696\u001b[39m   seed = jax.random.PRNGKey(seed)\n\u001b[32m    697\u001b[39m sampling_state = \u001b[38;5;28mself\u001b[39m.init_sample_state(\n\u001b[32m    698\u001b[39m     all_input_ids,\n\u001b[32m    699\u001b[39m     include_logits=return_logits,\n\u001b[32m   (...)\u001b[39m\u001b[32m    706\u001b[39m     beam_size=beam_size,\n\u001b[32m    707\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m sampling_state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compiled_prefill_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flattened_transformer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_state\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    712\u001b[39m sampling_state = \u001b[38;5;28mself\u001b[39m._compiled_decode_fn(\n\u001b[32m    713\u001b[39m     \u001b[38;5;28mself\u001b[39m._flattened_transformer_state, sampling_state\n\u001b[32m    714\u001b[39m )\n\u001b[32m    716\u001b[39m token_buffers = sampling_state.token_buffer\n",
            "    \u001b[31m[... skipping hidden 14 frame]\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/tunix/generate/sampler.py:489\u001b[39m, in \u001b[36mSampler._prefill_fn\u001b[39m\u001b[34m(self, params, sampler_state)\u001b[39m\n\u001b[32m    484\u001b[39m attention_mask = utils.make_causal_attn_mask(\n\u001b[32m    485\u001b[39m     input_mask, \u001b[38;5;28mself\u001b[39m.cache_config.cache_size\n\u001b[32m    486\u001b[39m )\n\u001b[32m    488\u001b[39m transformer = nnx.merge(\u001b[38;5;28mself\u001b[39m._transformer_graphdef, params)\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m logits, cache = \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstep_positions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampler_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    495\u001b[39m token_buffer = sampler_state.token_buffer\n\u001b[32m    496\u001b[39m done = sampler_state.done\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/maxtext/MaxText/integration/tunix/tunix_adaptor.py:46\u001b[39m, in \u001b[36mTunixMaxTextLlama.__call__\u001b[39m\u001b[34m(self, input_tokens, positions, cache, attention_mask, output_hidden_states)\u001b[39m\n\u001b[32m     44\u001b[39m attn = attention_mask \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_attention_mask \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# logits = self.base.forward_train(\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_positions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_segment_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# attention_mask=attn,\u001b[39;49;00m\n\u001b[32m     51\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logits, \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/maxtext/MaxText/layers/models.py:274\u001b[39m, in \u001b[36mTransformerNNX.__call__\u001b[39m\u001b[34m(self, decoder_input_tokens, decoder_positions, decoder_segment_ids, cache, encoder_images, enable_dropout, model_mode, previous_chunk, true_length, slot, page_state, decoder_target_tokens, decoder_target_mask)\u001b[39m\n\u001b[32m    271\u001b[39m   \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.decoder_block == DecoderBlockType.LLAMA4:\n\u001b[32m    272\u001b[39m     bidirectional_mask = decoder_input_tokens == multimodal_utils.LLAMA4_PATCH_TOKEN\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m logits, hidden_state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshared_embedding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken_embedder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_positions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_positions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_segment_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_segment_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43menable_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprevious_chunk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprevious_chunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mslot\u001b[49m\u001b[43m=\u001b[49m\u001b[43mslot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpage_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbidirectional_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbidirectional_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# If we are initializing the model AND MTP is enabled, we must create\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;66;03m# dummy target tensors. This allows Flax to trace the MTPBlock and create\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[38;5;66;03m# all its necessary parameters, without requiring the main training pipeline\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    305\u001b[39m \u001b[38;5;66;03m#      logit projection.\u001b[39;00m\n\u001b[32m    306\u001b[39m \u001b[38;5;66;03m# Its only effect is to \"sow\" these losses; it does not alter the primary logits output.\u001b[39;00m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.mtp_num_layers > \u001b[32m0\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/maxtext/MaxText/layers/nnx_wrappers.py:257\u001b[39m, in \u001b[36mToNNX.__call__\u001b[39m\u001b[34m(self, rngs, method, *args, **kwargs)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    252\u001b[39m   nnx_attrs = {\n\u001b[32m    253\u001b[39m     k: v\n\u001b[32m    254\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(\u001b[38;5;28mself\u001b[39m).items()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m k.startswith(\u001b[33m\"\u001b[39m\u001b[33mto_nnx__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m k.startswith(\u001b[33m\"\u001b[39m\u001b[33m_object__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    256\u001b[39m   }\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m   variables = \u001b[43mnnx_attrs_to_linen_vars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnnx_attrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m   \u001b[38;5;66;03m# Get `mutable` from top level bridge.Module context if any\u001b[39;00m\n\u001b[32m    260\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m (m := bdg_module.current_module()) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/maxtext/MaxText/layers/nnx_wrappers.py:135\u001b[39m, in \u001b[36mnnx_attrs_to_linen_vars\u001b[39m\u001b[34m(nnx_attrs)\u001b[39m\n\u001b[32m    133\u001b[39m     v = to_linen_var(v.to_state())\n\u001b[32m    134\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot infer collection name from value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    136\u001b[39m   linen_structured[(col_name, *kp)] = v\n\u001b[32m    137\u001b[39m variables = nnx.traversals.unflatten_mapping(linen_structured)\n",
            "\u001b[31mValueError\u001b[39m: Cannot infer collection name from value: frozenset({'layers', 'decoder_norm', '_pytree__state', 'to_nnx__rngs'})"
          ]
        }
      ],
      "source": [
        "with mesh:\n",
        "  grpo_trainer.train(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FzIP8glkTn1l",
      "metadata": {},
      "source": [
        "## Evaluate\n",
        "\n",
        "Let's evaluate our model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V-73HfP1Tn1l",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load checkpoint first.\n",
        "\n",
        "trained_ckpt_path = os.path.join(CKPT_DIR, str(MAX_STEPS), \"model_params\")\n",
        "\n",
        "abs_params = jax.tree.map(\n",
        "    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
        "    nnx.state(lora_gemma, nnx.LoRAParam),\n",
        ")\n",
        "checkpointer = ocp.StandardCheckpointer()\n",
        "trained_lora_params = checkpointer.restore(trained_ckpt_path, target=abs_params)\n",
        "\n",
        "nnx.update(\n",
        "    lora_gemma,\n",
        "    jax.tree.map(\n",
        "        lambda a, b: b,\n",
        "        nnx.state(lora_gemma, nnx.LoRAParam),\n",
        "        trained_lora_params,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1vY9kl-ITn1l",
      "metadata": {},
      "outputs": [],
      "source": [
        "gemma_tokenizer = data_lib.GemmaTokenizer()\n",
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_gemma,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nz0q_gGHqYz6",
      "metadata": {},
      "outputs": [],
      "source": [
        "(corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n",
        "    test_dataset,\n",
        "    sampler,\n",
        "    **GENERATION_CONFIGS[\"greedy\"],\n",
        ")\n",
        "print(\n",
        "    f\"{corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n",
        "    f\" {format_accuracy=}%\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jd9gpYVpUd3_",
      "metadata": {},
      "outputs": [],
      "source": [
        "# for eval_example in QUALITATIVE_EVAL_EXAMPLES:\n",
        "#   question = eval_example[\"question\"]\n",
        "#   answer = eval_example[\"answer\"]\n",
        "#   response = generate(\n",
        "#       question,\n",
        "#       sampler,\n",
        "#       temperature=INFERENCE_TEMPERATURE,\n",
        "#       top_k=INFERENCE_TOP_K,\n",
        "#       top_p=INFERENCE_TOP_P,\n",
        "#   )\n",
        "\n",
        "#   print(f\"Question:\\n{question}\")\n",
        "#   print(f\"Answer:\\n{answer}\")\n",
        "#   print(f\"Response:\\n{response}\")\n",
        "#   print(\"===============\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RrE_Rvaz_8CV",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a28124aa",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv-py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
