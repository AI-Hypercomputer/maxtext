diff --git a/MaxText/layers/attentions.py b/MaxText/layers/attentions.py
index 61f11637..73597629 100644
--- a/MaxText/layers/attentions.py
+++ b/MaxText/layers/attentions.py
@@ -24,7 +24,6 @@ from jax import lax
 from jax import random
 from jax.ad_checkpoint import checkpoint_name
 from jax.experimental import shard_map
-from jax.experimental.pallas.ops import attention as pallas_attention
 from jax.experimental.pallas.ops.tpu.splash_attention import splash_attention_mask
 from jax.experimental.pallas.ops.tpu.splash_attention import splash_attention_kernel
 import jax.numpy as jnp
diff --git a/MaxText/max_utils.py b/MaxText/max_utils.py
index af3db1e5..f3f2a325 100644
--- a/MaxText/max_utils.py
+++ b/MaxText/max_utils.py
@@ -227,8 +227,9 @@ def initialize_jax_for_gpu():
     coordinator_port = str(os.getenv("JAX_COORDINATOR_PORT"))
     jax.distributed.initialize(
         coordinator_address=f"{coordinator_ip}:{coordinator_port}",
-        num_processes=int(os.getenv("NNODES")),
-        process_id=int(os.getenv("NODE_RANK")),
+        num_processes=int(os.getenv("JAX_NUM_PROCESSES")),
+        process_id=int(os.getenv("PROCESS_ID")),
+        local_device_ids=int(os.getenv("LOCAL_DEVICE_ID")),
     )
     max_logging.log(f"JAX global devices: {jax.devices()}")
 
diff --git a/MaxText/train.py b/MaxText/train.py
index bfe5a320..fbe96f1b 100644
--- a/MaxText/train.py
+++ b/MaxText/train.py
@@ -20,6 +20,8 @@ limitations under the License.
 # Calling jax.device_count here prevents a "TPU platform already registered" error.
 # See github.com/google/maxtext/issues/20 for more
 
+from cuda_api import cudaProfilerStart, cudaProfilerStop
+
 import datetime
 import os
 import sys
@@ -492,6 +494,12 @@ def train_loop(config, state=None):
     if step == first_profiling_step:
       max_utils.activate_profiler(config)
 
+    if step == 6 and jax.process_index() == 0:
+      cudaProfilerStart()
+
+    if step == 8 and jax.process_index() == 0:
+      cudaProfilerStop()
+
     with jax.profiler.StepTraceAnnotation("train", step_num=step):
       example_batch = load_next_batch(data_iterator, example_batch, config)
       check_example_batch(config, example_batch=example_batch)
diff --git a/docker_build_dependency_image.sh b/docker_build_dependency_image.sh
index 95f7d893..5b9a26d8 100644
--- a/docker_build_dependency_image.sh
+++ b/docker_build_dependency_image.sh
@@ -59,7 +59,7 @@ if [[ -z ${LIBTPU_GCS_PATH+x} ]] ; then
     if [[ ${MODE} == "pinned" ]]; then
       export BASEIMAGE=ghcr.io/nvidia/jax:base-2024-03-13
     else
-      export BASEIMAGE=ghcr.io/nvidia/jax:base
+      export BASEIMAGE=ghcr.io/nvidia/jax:base-2024-05-31
     fi
     docker build --network host --build-arg MODE=${MODE} --build-arg JAX_VERSION=$JAX_VERSION --build-arg DEVICE=$DEVICE --build-arg BASEIMAGE=$BASEIMAGE -f ./maxtext_gpu_dependencies.Dockerfile -t ${LOCAL_IMAGE_NAME} .
   else
diff --git a/gpu_multi_process_run.sh b/gpu_multi_process_run.sh
index 72438d8e..09cb06e0 100644
--- a/gpu_multi_process_run.sh
+++ b/gpu_multi_process_run.sh
@@ -14,6 +14,7 @@ set -o pipefail
 export GPUS_PER_NODE=$GPUS_PER_NODE
 export JAX_COORDINATOR_PORT=$JAX_COORDINATOR_PORT
 export JAX_COORDINATOR_ADDRESS=$JAX_COORDINATOR_ADDRESS
+export JAX_NUM_PROCESSES=$((NNODES * GPUS_PER_NODE))
 
 set_nccl_gpudirect_tcpx_specific_configuration() {
   if [[ "$USE_GPUDIRECT" == "tcpx" ]]; then
@@ -142,9 +143,13 @@ resolve_coordinator_ip
 set -e
 
 PIDS=()
-eval ${COMMAND} &
-PID=$!
-PIDS+=($PID)
+for ((LOCAL_DEVICE_ID=0; LOCAL_DEVICE_ID <= $((GPUS_PER_NODE - 1)); LOCAL_DEVICE_ID++)); do
+  PROCESS_ID=$(($GPUS_PER_NODE*$NODE_RANK + $LOCAL_DEVICE_ID))
+  LOCAL_DEVICE_ID=$LOCAL_DEVICE_ID PROCESS_ID=$PROCESS_ID ${COMMAND} &
+  PID=$!
+  PIDS+=($PID)
+  echo "Launched MaxText/train.py for local_device_id: $LOCAL_DEVICE_ID process_id: $PROCESS_ID and PID $PID"
+done
 
 wait_all_success_or_exit "${PIDS[@]}"
 
diff --git a/requirements.txt b/requirements.txt
index 2bbaee0b..8f7112c5 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -21,7 +21,7 @@ pre-commit
 pytype
 sentencepiece==0.1.97
 tensorflow-text>=2.13.0
-tensorflow>=2.13.0
+tensorflow==2.13.0
 tensorflow-datasets
 tensorboardx
 tensorboard-plugin-profile
diff --git a/setup.sh b/setup.sh
index 0dd79c88..304aa296 100644
--- a/setup.sh
+++ b/setup.sh
@@ -155,8 +155,7 @@ elif [[ $MODE == "nightly" ]]; then
         # Install jaxlib-nightly
         pip3 install -U --pre jaxlib -f https://storage.googleapis.com/jax-releases/jaxlib_nightly_cuda12_releases.html
         # Install prebuilt Transformer Engine for GPU builds.
-        pip3 install "transformer-engine==1.5.0+297459b" \
-          --extra-index-url https://us-python.pkg.dev/gce-ai-infra/maxtext-build-support-packages/simple/
+        pip3 install git+https://github.com/NVIDIA/TransformerEngine.git@stable
     elif [[ $DEVICE == "tpu" ]]; then
         echo "Installing jax-nightly, jaxlib-nightly"
         # Install jax-nightly
