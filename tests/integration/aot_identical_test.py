# Copyright 2023â€“2026 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
These tests verify that the HLO graphs and Jaxpr generated by AOT compilation
(using train_compile.py) are identical to those generated from a real
training run (using train.py).
"""

import tempfile
import unittest
import pytest
import os
import shutil
import hashlib
import re
import jax
from tests.utils.test_helpers import get_test_config_path
from maxtext.trainers.pre_train import train_compile
from maxtext.trainers.pre_train import train


class AotBaseTest(unittest.TestCase):
  """Base class for AOT identity tests providing shared utilities."""

  def setUp(self):
    # Disable cache to ensure compilation occurs every time
    jax.config.update("jax_enable_compilation_cache", False)

  def get_device_user_facing_name(self):
    """Gets TPU device user facing name to generate correct AOT arguments."""
    devices = jax.devices()
    if not devices or "tpu" not in devices[0].platform.lower():
      pytest.skip("This test requires a TPU environment.")

    num_devices = len(devices)
    device_kind = devices[0].device_kind
    device_info = {
        "TPU v4": ("v4", 2 * num_devices),
        "TPU v5 lite": ("v5e", num_devices),
        "TPU v5p": ("v5p", 2 * num_devices),
        "TPU v6": ("v6e", num_devices),
    }

    prefix, topology_devices = next((v for k, v in device_info.items() if k in device_kind), (None, None))
    if prefix is None:
      raise ValueError(f"Unsupported TPU device kind for AOT test: {device_kind}")

    return f"{prefix}-{topology_devices}"

  def delete_dir(self, *directories):
    """Recursively deletes specified directories."""
    for directory in directories:
      if os.path.exists(directory):
        shutil.rmtree(directory)

  def check_large_files_equal(self, file_path1, file_path2):
    """Asserts that two text files have identical content via SHA256 hashing."""
    h1, h2 = hashlib.sha256(), hashlib.sha256()

    with open(file_path1, "rb") as f1:
      for chunk in iter(lambda: f1.read(8192), b""):
        h1.update(chunk)

    with open(file_path2, "rb") as f2:
      for chunk in iter(lambda: f2.read(8192), b""):
        h2.update(chunk)

    return h1.hexdigest() == h2.hexdigest()


class AotHloIdenticalTest(AotBaseTest):
  """Tests for Ahead of Time Compilation HLO Graph Verification."""

  def find_HLO_files(self, compile_dump_dir, real_dump_dir):
    """Locates the optimized HLO text files in the dump directories."""
    pattern = re.compile(r"^.*\.jit_train_step\..*\.after_optimizations_after_buffer_assignment\.txt$")
    compile_hlo = next((f for f in os.listdir(compile_dump_dir) if pattern.search(f)), None)
    real_hlo = next((f for f in os.listdir(real_dump_dir) if pattern.search(f)), None)
    return compile_hlo, real_hlo

  def assert_compile_and_real_match_hlo(self, test_name, *extra_args):
    """Assert real train and compile HLO are identical."""
    temp_dir = tempfile.gettempdir()
    train_dump_dir = os.path.join(temp_dir, "hlo_test_results", test_name, "real")
    compile_dump_dir = os.path.join(temp_dir, "hlo_test_results", test_name, "aot")
    # landing folder for MaxText's internal dump mechanism
    local_landing_dir = os.path.join(temp_dir, "hlo_aot_dump")

    hlo_dump_args = [
        "dump_hlo=True",
        f"dump_hlo_local_dir={local_landing_dir}",
        "dump_hlo_delete_local_after=False",
    ]

    shared_args = [
        "base_output_directory=gs://runner-maxtext-logs",
        "dataset_type=synthetic",
        "steps=1",
        "enable_checkpointing=False",
        "base_num_decoder_layers=1",
        "max_target_length=512",
        "base_emb_dim=256",
        "base_mlp_dim=256",
    ] + hlo_dump_args
    if extra_args:
      shared_args.extend(extra_args)

    self.delete_dir(local_landing_dir, compile_dump_dir, train_dump_dir)

    # Generate train.py HLO
    # xla flag only sets once for train.main
    os.makedirs(local_landing_dir, exist_ok=True)
    train_argv = (
        (None, get_test_config_path())
        + tuple(shared_args)
        + (
            f"dump_hlo_xla_flags=--xla_dump_to={local_landing_dir} "
            "--xla_dump_hlo_as_text "
            "--xla_dump_hlo_module_re=jit_train_step",
        )
    )
    train.main(train_argv)
    shutil.move(local_landing_dir, train_dump_dir)
    jax.clear_caches()

    # Generate train_compile.py HLO
    os.makedirs(local_landing_dir, exist_ok=True)
    topology = self.get_device_user_facing_name()
    aot_args = [f"compile_topology={topology}", "compile_topology_num_slices=1"]
    compile_argv = (None, get_test_config_path()) + tuple(shared_args) + tuple(aot_args)
    train_compile.main(compile_argv)
    shutil.move(local_landing_dir, compile_dump_dir)
    jax.clear_caches()

    # Compare
    compile_hlo, real_hlo = self.find_HLO_files(compile_dump_dir, train_dump_dir)
    self.assertTrue(compile_hlo and real_hlo, "Optimized HLO files were not found!")
    self.assertTrue(
        self.check_large_files_equal(os.path.join(compile_dump_dir, compile_hlo), os.path.join(train_dump_dir, real_hlo)),
        f"HLO file is not identical for test {test_name}!",
    )

  @pytest.mark.tpu_only
  @pytest.mark.skip(reason="Optimized HLO files were not found! Skipped until fixing b/463839714.")
  def test_default_hlo_match(self):
    self.assert_compile_and_real_match_hlo("default_run")


class AotJaxprIdenticalTest(AotBaseTest):
  """Tests for Ahead of Time Compilation Jaxpr Verification."""

  def find_jaxpr_file(self, dump_dir):
    """Locates the dumped jaxpr file."""
    jaxpr_path = os.path.join(dump_dir, "train_step.jaxpr")
    return jaxpr_path if os.path.exists(jaxpr_path) else None

  def assert_compile_and_real_match_jaxpr(self, test_name, *extra_args):
    """Assert real train and compile jaxpr are identical."""
    temp_dir = tempfile.gettempdir()
    train_dump_dir = os.path.join(temp_dir, "jaxpr_test_results", test_name, "real")
    compile_dump_dir = os.path.join(temp_dir, "jaxpr_test_results", test_name, "aot")

    shared_args = [
        "base_output_directory=gs://runner-maxtext-logs",
        "dataset_type=synthetic",
        "steps=1",
        "enable_checkpointing=False",
        "dump_jaxpr=True",
        "dump_jaxpr_delete_local_after=False",
        "skip_first_n_steps_for_profiler=0",
    ]
    if extra_args:
      shared_args.extend(extra_args)

    self.delete_dir(train_dump_dir, compile_dump_dir)
    # Ensure directories exist before running to avoid FileNotFoundError
    os.makedirs(train_dump_dir, exist_ok=True)
    os.makedirs(compile_dump_dir, exist_ok=True)

    # Run train.py and dump jaxpr
    train_argv = (
        None,
        get_test_config_path(),
        f"dump_jaxpr_local_dir={train_dump_dir}",
    ) + tuple(shared_args)
    train.main(train_argv)
    jax.clear_caches()

    # Run train_compile.py and dump jaxpr
    topology = self.get_device_user_facing_name()
    compile_argv = (
        None,
        get_test_config_path(),
        f"dump_jaxpr_local_dir={compile_dump_dir}",
        f"compile_topology={topology}",
        "compile_topology_num_slices=1",
    ) + tuple(shared_args)
    train_compile.main(compile_argv)
    jax.clear_caches()

    # Compare results
    train_file = self.find_jaxpr_file(train_dump_dir)
    compile_file = self.find_jaxpr_file(compile_dump_dir)
    self.assertTrue(train_file and compile_file, "Jaxpr files were not dumped!")
    self.assertTrue(
        self.check_large_files_equal(compile_file, train_file), f"Jaxpr file is not identical for test {test_name}!"
    )

  @pytest.mark.tpu_only
  def test_default_jaxpr_match(self):
    self.assert_compile_and_real_match_jaxpr("default_run")
