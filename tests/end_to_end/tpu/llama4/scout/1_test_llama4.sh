#!/bin/bash

# This file is documentation for how to get started with Llama4 Scout.

# This file runs Step 1 on CPU (on a daily basis).
# 1. Convert the HuggingFace checkpoint (bf16) to MaxText-compatible checkpoint (bf16): 
#    Scanned format is better for training; unscanned format is better for decoding.
# 2. Run logit check, pre-training, fine-tuning, and decoding.

set -ex

export MODEL_NAME='llama4-17b-16e'
export TOKENIZER_PATH='meta-llama/Llama-4-Scout-17B-16E'

# Installing torch for checkpoint conversion and forward_pass_logit_checker.py
python3 -m pip install torch --index-url https://download.pytorch.org/whl/cpu

if [ -z "${BASE_OUTPUT_PATH}" ]; then
  # Non-Googlers please remember to point `BASE_OUTPUT_PATH` to GCS buckets that you own, this script uses internal buckets for testing.
  # this bucket will store all the files generated by MaxText during a run
  export BASE_OUTPUT_PATH=gs://runner-maxtext-logs/$(date +%Y-%m-%d-%H-%M)
  echo "BASE_OUTPUT_PATH is not set"
fi
BASE_OUTPUT_PATH=${BASE_OUTPUT_PATH%/}
echo using BASE_OUTPUT_PATH = ${BASE_OUTPUT_PATH}

# Step 1: Checkpoint conversion
# You can use the HuggingFace checkpoint at TODO
# Assume HF checkpoints are uploaded to GCS bucket at CKPT_BUCKET 
# Non-Googlers please remember to point `CKPT_BUCKET` to GCS buckets that you own
# Copying the HF checkpoint into a local directory `/tmp` -- you are free to use a different directory
if [ -z "${CKPT_DISK_LOCATION}" ]; then
  export CKPT_BUCKET=gs://maxtext-llama/llama4-17b-16e/hf-checkpoint
  gcloud storage cp -r ${CKPT_BUCKET} /tmp
  export CKPT_DISK_LOCATION=/tmp/hf-checkpoint
fi

# Convert checkpoint to `unscanned` format, more suitable for decoding
JAX_PLATFORMS=cpu python3 -m maxtext.checkpoint_conversion.standalone_scripts.llama4_ckpt_unscanned --base-model-path ${CKPT_DISK_LOCATION} --maxtext-model-path ${BASE_OUTPUT_PATH}/unscanned --model-size ${MODEL_NAME} --huggingface-checkpoint True
echo "Wrote MaxText compatible unscanned checkpoint to ${BASE_OUTPUT_PATH}/unscanned/0/items"