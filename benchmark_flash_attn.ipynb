{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple script to benchmark flash and dot product attention kernels\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from MaxText.layers.attentions import AttentionOp\n",
    "from MaxText import pyconfig\n",
    "from MaxText import maxtext_utils\n",
    "from MaxText.layers import quantizations\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "model_args = [\"\", \"MaxText/configs/base.yml\", \"weight_dtype=bfloat16\", \"quantization=int8\", \"quantize_kvcache=True\", \"checkpoint_is_quantized=True\", \"ici_tensor_parallelism=1\", \"ici_fsdp_parallelism=1\", \"ici_context_parallelism=8\", \"context_parallel_load_balance=False\"]\n",
    "\n",
    "config = pyconfig.initialize(model_args)\n",
    "\n",
    "devices_array = maxtext_utils.create_device_mesh(config=config)\n",
    "mesh = jax.sharding.Mesh(devices_array, config.mesh_axes)\n",
    "quant = quantizations.configure_quantization(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "kernel = \"flash\" # For now, this won't matter because we'll call the actual kernel manually (i.e. `AttentionOp.apply_dot_product` / `AttentionOp.apply_flash_attention`)\n",
    "MAX_TARGET_LEN = 8192\n",
    "NUM_QUERY_HEADS = 64\n",
    "NUM_KV_HEADS = 8\n",
    "HEAD_DIM = 128\n",
    "attention_op = AttentionOp(\n",
    "  config, mesh, kernel, MAX_TARGET_LEN, NUM_QUERY_HEADS, NUM_KV_HEADS, compute_axis_order=(0,1,2,3), quant=quant, kv_quant=quantizations.configure_kv_quant(config)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://source.corp.google.com/piper///depot/google3/experimental/users/tohaowu/pallas/flash_tuning.py\n",
    "original_key = jax.random.PRNGKey(0)\n",
    "qkv_keys = jax.random.split(original_key, 1)\n",
    "qkv_key_idx = 0\n",
    "qkv_key = qkv_keys[qkv_key_idx]\n",
    "qkv_key_idx += 1\n",
    "batch_size = 1\n",
    "q_seq_len = MAX_TARGET_LEN\n",
    "kv_seq_len = MAX_TARGET_LEN\n",
    "model_mode = \"autoregressive\"\n",
    "\n",
    "\n",
    "q = jax.random.normal(qkv_key, (batch_size, q_seq_len, NUM_QUERY_HEADS, HEAD_DIM), dtype=jnp.bfloat16)\n",
    "k = jax.random.normal(qkv_key, (batch_size, kv_seq_len, NUM_KV_HEADS, HEAD_DIM), dtype=jnp.bfloat16)\n",
    "v = jax.random.normal(qkv_key, (batch_size, kv_seq_len, NUM_KV_HEADS, HEAD_DIM), dtype=jnp.bfloat16)\n",
    "\n",
    "q = jax.device_put(q)\n",
    "k = jax.device_put(k)\n",
    "v = jax.device_put(v)\n",
    "\n",
    "decoder_segment_ids = jnp.zeros((batch_size, kv_seq_len), dtype=jnp.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import timeit\n",
    "\n",
    "# WARMUP_ITERS = 10\n",
    "# TIME_IT_NUMBER = 10\n",
    "# TIME_IT_REPEAT = 5\n",
    "\n",
    "\n",
    "# print(\"Starting warmup...\")\n",
    "# global_block_q = 512\n",
    "# global_block_kv = 512\n",
    "# global_block_kv_compute = 512\n",
    "# global_block_q_dkv = 512\n",
    "# global_block_kv_dkv = 512\n",
    "# global_block_kv_dkv_compute = 512\n",
    "# global_block_q_dq = 512\n",
    "# global_block_kv_dq = 512\n",
    "# global_q_layout = \"qkv\"\n",
    "# global_k_layout = \"qkv\"\n",
    "# global_v_layout = \"qkv\" # HEAD_DIM_MINOR, SEQ_MINOR\n",
    "\n",
    "# for _ in range(WARMUP_ITERS):\n",
    "#   jax.block_until_ready(attention_op.apply_attention_dot(q,k,v,decoder_segment_ids, model_mode))\n",
    "#   jax.block_until_ready(attention_op.tpu_flash_attention(q,k,v,decoder_segment_ids, model_mode=model_mode, global_block_q=global_block_q,\n",
    "#       global_block_kv=global_block_kv,\n",
    "#       global_block_kv_compute=  global_block_kv_compute,\n",
    "#       global_block_q_dkv=global_block_q_dkv,\n",
    "#       global_block_kv_dkv=global_block_kv_dkv,\n",
    "#       global_block_kv_dkv_compute=global_block_kv_dkv_compute,\n",
    "#       global_block_q_dq=global_block_q_dq,\n",
    "#       global_block_kv_dq=global_block_kv_dq,\n",
    "#       global_q_layout=global_q_layout,\n",
    "#       global_k_layout=global_k_layout,\n",
    "#       global_v_layout=global_v_layout))\n",
    "\n",
    "\n",
    "# # # with jax.profiler.StepTraceAnnotation(\"gqa_pallas\", step_num=k_seq_len):\n",
    "# print(\"Starting benchmark for dot product attention...\")\n",
    "# times_dot_product = timeit.repeat(lambda: jax.block_until_ready(attention_op.apply_attention_dot(q,k,v,decoder_segment_ids, model_mode)), repeat=TIME_IT_REPEAT, number=TIME_IT_NUMBER)\n",
    "# times_dot_product = min(times_dot_product) / TIME_IT_NUMBER\n",
    "\n",
    "# # with jax.profiler.StepTraceAnnotation(\"gqa_reference\", step_num=k_seq_len):\n",
    "# print(\"Starting benchmark for Splash attention...\")\n",
    "# times_flash = timeit.repeat(lambda: jax.block_until_ready(attention_op.tpu_flash_attention(q,k,v,decoder_segment_ids, model_mode=model_mode)), repeat=TIME_IT_REPEAT, number=TIME_IT_NUMBER)\n",
    "# times_flash = min(times_flash) / TIME_IT_NUMBER\n",
    "\n",
    "\n",
    "# print(f\"    Dot Product Attention:        {times_dot_product:.6f} s\")\n",
    "# print(f\"    Flash Attention: {times_flash:.6f} s\")\n",
    "# print(f\"    Speedup: {times_dot_product / times_flash if times_dot_product > 0 else float('inf'):.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying block size 256 and layout HEAD_DIM_MINOR\n",
      "Starting warmup...\n",
      "Starting benchmark for Splash attention...\n",
      "    Flash Attention: 0.067719 s\n",
      "Trying block size 256 and layout SEQ_MINOR\n",
      "Starting warmup...\n",
      "Starting benchmark for Splash attention...\n",
      "    Flash Attention: 0.065657 s\n",
      "Trying block size 512 and layout HEAD_DIM_MINOR\n",
      "Starting warmup...\n",
      "Starting benchmark for Splash attention...\n",
      "    Flash Attention: 0.047557 s\n",
      "Trying block size 512 and layout SEQ_MINOR\n",
      "Starting warmup...\n",
      "Starting benchmark for Splash attention...\n",
      "    Flash Attention: 0.047107 s\n",
      "Trying block size 1024 and layout HEAD_DIM_MINOR\n",
      "Starting warmup...\n",
      "Starting benchmark for Splash attention...\n",
      "    Flash Attention: 0.044642 s\n",
      "Trying block size 1024 and layout SEQ_MINOR\n",
      "Starting warmup...\n",
      "Starting benchmark for Splash attention...\n",
      "    Flash Attention: 0.044642 s\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "WARMUP_ITERS = 20\n",
    "TIME_IT_NUMBER = 15\n",
    "TIME_IT_REPEAT = 10\n",
    "\n",
    "for block_size in [256, 512, 1024]:\n",
    "  for layout in [\"HEAD_DIM_MINOR\", \"SEQ_MINOR\"]:\n",
    "    print(f\"Trying block size {block_size} and layout {layout}\")\n",
    "    print(\"Starting warmup...\")\n",
    "    global_block_q = block_size\n",
    "    global_block_kv = block_size\n",
    "    global_block_kv_compute = block_size\n",
    "    global_block_q_dkv = block_size\n",
    "    global_block_kv_dkv = block_size\n",
    "    global_block_kv_dkv_compute = block_size\n",
    "    global_block_q_dq = block_size\n",
    "    global_block_kv_dq = block_size\n",
    "    global_q_layout = layout\n",
    "    global_k_layout = layout\n",
    "    global_v_layout = layout\n",
    "\n",
    "    for _ in range(WARMUP_ITERS):\n",
    "      jax.block_until_ready(attention_op.apply_attention_dot(q,k,v,decoder_segment_ids, model_mode))\n",
    "      jax.block_until_ready(attention_op.tpu_flash_attention(q,k,v,decoder_segment_ids, model_mode=model_mode, global_block_q=global_block_q,\n",
    "          global_block_kv=global_block_kv,\n",
    "          global_block_kv_compute=  global_block_kv_compute,\n",
    "          global_block_q_dkv=global_block_q_dkv,\n",
    "          global_block_kv_dkv=global_block_kv_dkv,\n",
    "          global_block_kv_dkv_compute=global_block_kv_dkv_compute,\n",
    "          global_block_q_dq=global_block_q_dq,\n",
    "          global_block_kv_dq=global_block_kv_dq,\n",
    "          global_q_layout=global_q_layout,\n",
    "          global_k_layout=global_k_layout,\n",
    "          global_v_layout=global_v_layout))\n",
    "\n",
    "\n",
    "    # with jax.profiler.StepTraceAnnotation(\"gqa_reference\", step_num=k_seq_len):\n",
    "    print(\"Starting benchmark for Splash attention...\")\n",
    "    times_flash = timeit.repeat(lambda: jax.block_until_ready(attention_op.tpu_flash_attention(q,k,v,decoder_segment_ids, model_mode=model_mode, global_block_q=global_block_q,\n",
    "          global_block_kv=global_block_kv,\n",
    "          global_block_kv_compute=  global_block_kv_compute,\n",
    "          global_block_q_dkv=global_block_q_dkv,\n",
    "          global_block_kv_dkv=global_block_kv_dkv,\n",
    "          global_block_kv_dkv_compute=global_block_kv_dkv_compute,\n",
    "          global_block_q_dq=global_block_q_dq,\n",
    "          global_block_kv_dq=global_block_kv_dq,\n",
    "          global_q_layout=global_q_layout,\n",
    "          global_k_layout=global_k_layout,\n",
    "          global_v_layout=global_v_layout)), repeat=TIME_IT_REPEAT, number=TIME_IT_NUMBER)\n",
    "    times_flash = min(times_flash) / TIME_IT_NUMBER\n",
    "    print(f\"    Flash Attention: {times_flash:.6f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maxtext-310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
