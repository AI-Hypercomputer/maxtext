#  Copyright 2023 Google LLC
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#       https://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.


"""Check if the logits generated by a model's MaxText implementation matches hf logits for the same inputs"""
import argparse
import sys
import os

current_dir = os.path.dirname(os.path.abspath(__file__))
maxtext_parent_dir = os.path.dirname(current_dir)
sys.path.append(maxtext_parent_dir)

import max_logging

max_logging.log(f"Added parent directory = {maxtext_parent_dir}")

import common_types
import jax
import jax.numpy as jnp
import numpy as np
import pyconfig
import train
from tabulate import tabulate


prompt_text = """# Activation dtypes.
dtype: "bfloat16"
# Used to configure quantization in the transformer layers, defaults to null implying bf16.
# Possible alternative settings are as follows:
# 'int8' for dynamic range quantization using 8-bits
# 'int8w' for weights only quantization using 8-bits
# 'int4w' for weights only quantization using 4-bits
# 'intmp' for mixed precision weight only quantization based on config file
# 'fp8' for 8-bit floating-point GeMMs on NVIDIA GPUs.
quantization: ""
# Choose one of default, high, and highest.
# https://kolonist26-jax-kr.readthedocs.io/en/latest/jax.lax.html#jax.lax.Precision
matmul_precision: "default"
activations_in_float32: False # Sets activations to float32 before nonlinearity it true, else dtype
# Path to file with quantization config - only used for mixed precision.
# Example configs in ../Maxtext/configs/quantization
# Allows us to configure different bits, tiling and scale for quantizing selected weights.
# Bits represents number of bits to quantize to,
# tile-size represents the tiling sized used in AQT tiled_dot_general,
# Value of scale is used to scale the abs_max value used for AQT quantization
# Defaults values are 8 bits, tile-size=-1 (no tiling) and scale=1.
quant_cfg_path: ""
quantize_kvcache: False # Set to True to quantize KV Cache values, defaults to False
# Valid kv_quant_axis values:
#   - "" is valid only when quantize_kvcache is False
#   - "dkv" indicates quantize kv cache over the cache_kv, i.e. kv dimension axis
#   - "heads_and_dkv" indicates quantize kv cache over cache_heads and cache_kv axes
# Default to "heads_and_dkv" for faster compution, kv_quant_axis is not used when quantize_kvcache is False
#   - "dkv" is expected with better accuracy but degraded computation
kv_quant_axis: "heads_and_dkv"
kv_quant_dtype: "int8"
checkpoint_is_quantized: False # Set to True if reading from a saved aqt quantized checkpoint
# Saves params quantized on fly at following path
save_quantized_params_path: ""
# Shard the range finding operation for quantization. By default this is set to number of slices.
quantization_local_shard_count: -1
decoder_block: "llama2" # which style of DecoderBlock to use.
# Global parameter scale needs to be a power of 2. If you want finer grained control of the model sizes
# then you should explicitly set base_embed_dim, base_num_query_heads, base_num_kv_heads,
# base_mlp_dim, base_num_decoder_layers and/or head_dim.
weight_dtype: float32
global_parameter_scale: 1
base_emb_dim: 2048
base_num_query_heads: 16
base_num_kv_heads: 16
base_mlp_dim: 7168
base_num_decoder_layers: 16
head_dim: 128
mlp_activations: ["silu", "linear"]
dropout_rate: 0
logits_via_embedding: False
normalize_embedding_logits: True  # whether to normlize pre-softmax logits if logits_via_embedding is true
logits_dot_in_fp32: True  # whether to use fp32 in logits_dense or shared_embedding dot product for stability"""


def get_data_hf(config, prompt_text, hf_token, model_id="meta-llama/Llama-2-7b-hf"):
  from huggingface_hub import login
  from transformers import AutoTokenizer, AutoModelForCausalLM
  import jax.numpy as jnp
  import numpy as np
  import torch
  """Get data (tokens, logits) from a Hugging Face model for the test at hf_data_index"""
  login(token=hf_token)
  
  tokenizer = AutoTokenizer.from_pretrained(model_id)
  hf_model = AutoModelForCausalLM.from_pretrained(
      model_id,
      torch_dtype=getattr(torch, config.dtype.name)
  )
  
  input_ids = tokenizer.encode(prompt_text, return_tensors="pt")[:, :config.max_target_length]

  # Get the logits for the prompt + completion using Hugging Face model
  with torch.no_grad():
      outputs = hf_model(input_ids)
      logits = outputs.logits.cpu().numpy().astype("float32")

  # Prepare tokens and logits
  tokens = input_ids.tolist()[0]
  
  max_logging.log(f"config.global_batch_size_to_train_on={config.global_batch_size_to_train_on}")
  
  ids = np.asarray(tokens, dtype=np.int32)

  # Use Hugging Face logits
  logits = np.asarray(logits[0], dtype=np.float32) 

  max_logging.log(f"prompt=\"{prompt_text}\" raw ids={ids}, logits.shape = {logits.shape}")

  # Create decoder segment ids and positions
  # decoder_segment_ids = jax.numpy.zeros(s) + common_types.DECODING_ACTIVE_SEQUENCE_INDICATOR
  decoder_segment_ids = jax.numpy.ones((config.global_batch_size_to_train_on, config.max_target_length)) * common_types.DECODING_ACTIVE_SEQUENCE_INDICATOR
  decoder_positions = jnp.stack(
      [jnp.arange(config.max_target_length, dtype=jnp.int32) for _ in range(config.global_batch_size_to_train_on)]
  )

  ids = jnp.stack([ids for _ in range(config.global_batch_size_to_train_on)])
  max_logging.log(f"ids={ids}, decoder_segment_ids = {decoder_segment_ids}, decoder_positions= {decoder_positions}")

  return ids, decoder_segment_ids, decoder_positions, logits


def main(config):
  # run forward pass on hf llama2 model add hf token
  ids, decoder_segment_ids, decoder_positions, logits_hf = get_data_hf(config, prompt_text, hf_token="")

  # initialize the model with weights from reference ckpt
  (
      init_rng,
      _,
      _,
      _,
      model,
      _,
      _,
      _,
      _,
      state,
  ) = train.setup_train_loop(config)
  
  full_train_logits = model.apply(
      state.params,
      ids,
      decoder_positions,
      decoder_segment_ids,
      enable_dropout=False,
      rngs={"aqt": init_rng},
  )
  full_train_logits = jax.experimental.multihost_utils.process_allgather(full_train_logits)

  token_size = logits_hf.shape[0]
  max_logging.log(f"{token_size=}")
  logits_maxtext = full_train_logits[0, :token_size, :]
  logits_hf = logits_hf[:token_size, :]
  max_logging.log(f"Max Numerical Difference {np.abs(logits_hf - logits_maxtext).max()}")

  max_logging.log(f"{logits_maxtext=}")
  max_logging.log(f"{logits_hf=}")
  
  maxtext_probabilities = jax.nn.softmax(logits_maxtext, axis=-1)
  hf_probabilities = jax.nn.softmax(logits_hf, axis=-1)
  
  max_logging.log(f"{maxtext_probabilities}=")
  max_logging.log(f"{hf_probabilities}=")

  kl_div = jax.numpy.sum(jax.scipy.special.kl_div(hf_probabilities, maxtext_probabilities), axis=-1)
  max_logging.log(f"KL divergence = {kl_div}, max KL divergence = {jax.numpy.max(kl_div)}")

  metrics = {}
  metrics["max_kl_div"] = jax.numpy.max(kl_div)
  metrics["l1_diff"] = np.abs(logits_hf - logits_maxtext).sum()
  metrics["abs_diff"] = np.abs(logits_hf - logits_maxtext).max()

  ranking_maxtext = np.argsort(logits_maxtext, axis=1)[:, -5:]
  ranking_hf = np.argsort(logits_hf, axis=1)[:, -5:]

  # for each token, 1 if there is a disagreement in the top 5 of next tokens, 0 otherwise
  metrics["disagreement_top5"] = np.mean((np.abs(ranking_hf - ranking_maxtext) > 0).sum(axis=1) > 0)

  table = [[key, value] for key, value in metrics.items()]
  print(tabulate(table, headers=["Metric", "Value"], tablefmt="orgtbl"))


if __name__ == "__main__":
  jax.config.update("jax_default_prng_impl", "unsafe_rbg")
  os.environ["TF_CPP_MIN_LOG_LEVEL"] = "0"

  parser = argparse.ArgumentParser()
  test_args, _ = parser.parse_known_args()


  pyconfig.initialize(sys.argv)
  cfg = pyconfig.config
  main(cfg)