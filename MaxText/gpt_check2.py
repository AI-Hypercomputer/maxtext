"""Check if the logits generated by a model's MaxText implementation matches hf logits for the same inputs"""
import argparse
import sys
import os
from absl import app

current_dir = os.path.dirname(os.path.abspath(__file__))
maxtext_parent_dir = os.path.dirname(current_dir)
sys.path.append(maxtext_parent_dir)

import max_logging

max_logging.log(f"Added parent directory = {maxtext_parent_dir}")

import common_types
import jax
import jax.numpy as jnp
import numpy as np
import pyconfig
from tabulate import tabulate
from MaxText import maxtext_utils
from MaxText.layers import quantizations
from MaxText.layers import models
from MaxText.common_types import MODEL_MODE_TRAIN



prompt_text = "I love to sleep"


def get_data_hf(config, prompt_text, hf_token, model_id="openai/gpt-oss-20b"):
  from huggingface_hub import login
  from transformers import AutoTokenizer, AutoModelForCausalLM
  import jax.numpy as jnp
  import numpy as np
  import torch
  """Get data (tokens, logits) from a Hugging Face model for the test at hf_data_index"""
  login(token=hf_token)
  
  tokenizer = AutoTokenizer.from_pretrained(model_id)
  hf_model_path = "/home/ranran_google_com/gpt_20b/gpt-oss-20b-bf16-v2"
  hf_model = AutoModelForCausalLM.from_pretrained(
      hf_model_path,
      trust_remote_code=True,
      torch_dtype=getattr(torch, config.dtype.name)
  )
  
  input_ids = tokenizer.encode(prompt_text, return_tensors="pt")[:, :config.max_target_length]

  # Get the logits for the prompt + completion using Hugging Face model
  with torch.no_grad():
      outputs = hf_model(input_ids)
      logits = outputs.logits.cpu().numpy().astype("float32")

  # Prepare tokens and logits
  tokens = input_ids.tolist()[0]
  print(f"tokens: {tokens}")
  
  max_logging.log(f"config.global_batch_size_to_train_on={config.global_batch_size_to_train_on}")
  
  ids = np.asarray(tokens, dtype=np.int32)

  # Use Hugging Face logits
  logits = np.asarray(logits[0], dtype=np.float32) 

  max_logging.log(f"prompt=\"{prompt_text}\" raw ids={ids}, logits.shape = {logits.shape}")

  # Create decoder segment ids and positions
  # decoder_segment_ids = jax.numpy.zeros(s) + common_types.DECODING_ACTIVE_SEQUENCE_INDICATOR
  decoder_segment_ids = jax.numpy.ones((config.global_batch_size_to_train_on, config.max_target_length)) * common_types.DECODING_ACTIVE_SEQUENCE_INDICATOR
  decoder_positions = jnp.stack(
      [jnp.arange(config.max_target_length, dtype=jnp.int32) for _ in range(config.global_batch_size_to_train_on)]
  )

  ids = jnp.stack([ids for _ in range(config.global_batch_size_to_train_on)])
  max_logging.log(f"ids={ids}, decoder_segment_ids = {decoder_segment_ids}, decoder_positions= {decoder_positions}")

  return ids, decoder_segment_ids, decoder_positions, logits


def main(argv):
  jax.config.update("jax_default_prng_impl", "unsafe_rbg")
  os.environ["TF_CPP_MIN_LOG_LEVEL"] = "0"

  config = pyconfig.initialize(argv)
  init_rng = jax.random.PRNGKey(config.init_weights_seed)
  init_rng, rng1 = jax.random.split(init_rng)
  devices_array = maxtext_utils.create_device_mesh(config)
  mesh = jax.sharding.Mesh(devices_array, config.mesh_axes)
  quant = quantizations.configure_quantization(config)
  model = models.transformer_as_linen(config, mesh=mesh, quant=quant, model_mode=MODEL_MODE_TRAIN)
  state, _ = maxtext_utils.setup_decode_state(model, config, rng1, mesh, None)

  ids, decoder_segment_ids, decoder_positions, logits_hf = get_data_hf(config, prompt_text, hf_token="")
  
  full_train_logits = model.apply(
      state.params,
      ids,
      decoder_positions,
      decoder_segment_ids,
      enable_dropout=False,
      rngs={"aqt": init_rng},
  )
  full_train_logits = jax.experimental.multihost_utils.process_allgather(full_train_logits)

  token_size = logits_hf.shape[0]
  max_logging.log(f"{token_size=}")
  logits_maxtext = full_train_logits[0, :token_size, :]
  logits_hf = logits_hf[:token_size, :]
  max_logging.log(f"Max Numerical Difference {np.abs(logits_hf - logits_maxtext).max()}")

  max_logging.log(f"{logits_maxtext=}")
  max_logging.log(f"{logits_hf=}")
  
  maxtext_probabilities = jax.nn.softmax(logits_maxtext, axis=-1)
  hf_probabilities = jax.nn.softmax(logits_hf, axis=-1)
  
  max_logging.log(f"{maxtext_probabilities}=")
  max_logging.log(f"{hf_probabilities}=")

  kl_div = jax.numpy.sum(jax.scipy.special.kl_div(hf_probabilities, maxtext_probabilities), axis=-1)
  max_logging.log(f"KL divergence = {kl_div}, max KL divergence = {jax.numpy.max(kl_div)}")

  metrics = {}
  metrics["max_kl_div"] = jax.numpy.max(kl_div)
  metrics["l1_diff"] = np.abs(logits_hf - logits_maxtext).sum()
  metrics["abs_diff"] = np.abs(logits_hf - logits_maxtext).max()

  ranking_maxtext = np.argsort(logits_maxtext, axis=1)[:, -5:]
  ranking_hf = np.argsort(logits_hf, axis=1)[:, -5:]

  # for each token, 1 if there is a disagreement in the top 5 of next tokens, 0 otherwise
  metrics["disagreement_top5"] = np.mean((np.abs(ranking_hf - ranking_maxtext) > 0).sum(axis=1) > 0)

  table = [[key, value] for key, value in metrics.items()]
  print(tabulate(table, headers=["Metric", "Value"], tablefmt="orgtbl"))


if __name__ == "__main__":
  # jax.config.update("jax_default_prng_impl", "unsafe_rbg")
  # os.environ["TF_CPP_MIN_LOG_LEVEL"] = "0"

  # parser = argparse.ArgumentParser()
  # test_args, _ = parser.parse_known_args()
  # config = pyconfig.initialize(argv)

  # pyconfig.initialize(sys.argv)
  # cfg = pyconfig._config
  app.run(main)