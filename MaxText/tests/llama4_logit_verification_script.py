#  Copyright 2025 Google LLC
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#       https://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.


# This file compares the logits generated by MaxText/HF reference implementation for some input prompts for Llama4
# against a given set of golden logits.

# NOTE: please run this script from root, using the following commands for each backend type:

# Llama4 JAX cmd:
# JAX_PLATFORMS=cuda,cpu python -m MaxText.tests.llama4_logit_verification_script jax [scout/maverick] MaxText/configs/base.yml model_name=llama4-17b-[16e/128e]
# scan_layers=false base_output_directory=llama4 run_name=temp-testing-only hardware=gpu force_unroll=false weight_dtype=float32 max_target_length=16
# max_prefill_predict_length=4 per_device_batch_size=1 dtype=float32
# load_parameters_path=...

# Llama4 HF cmd:
# python -m MaxText.tests.llama4_logit_verification_script hf [scout/maverick]


import argparse
import sys

import jax
import jsonlines
import numpy as np
import torch
from transformers import AutoModelForCausalLM

from MaxText import maxtext_utils, pyconfig
from MaxText.layers import models, quantizations
from MaxText.tests.forward_pass_logit_checker import get_data

LOGIT_CHECKING_RTOL, LOGIT_CHECKING_ATOL = 8.5e-2, 8.5e-2


class MockConfigForForwardPass:
  """
  Mock config to set some parameters normally set by a MaxText config file.
  """

  def __init__(self):
    self.global_batch_size_to_train_on = 1
    self.max_target_length = 4


def setup_golden_data(model_size: str):
  """
  Sets up the data to run the forward pass on.

  Args:
    model_size (str): type of Llama4 model (i.e. Scout (16E) or Maverick (128E))

  Returns:
    ids: tokenized ids to feed as input to the forward pass
    decoder_segment_ids: segment ids to feed as input to the forward pass (used for MaxText / JAX only)
    decoder_positions: position ids to feed as input to the forward pass (used for MaxText / JAX only)
    golden_logits: pre-computed logits from the forward pass to serve as reference
  """
  model_size = model_size.lower()
  llama_4_data = "llama4-17b-16e-instruct" if model_size == "scout" else "llama4-17b-128e"
  input_golden_data_path = "MaxText/test_assets/golden_data_" + llama_4_data + ".jsonl"
  with jsonlines.open(input_golden_data_path, "r") as f:
    golden_data = list(f)
  assert len(golden_data) == 1
  golden_data_index = 0
  ids, decoder_segment_ids, decoder_positions, golden_logits = get_data(
      golden_data, golden_data_index, MockConfigForForwardPass()
  )
  golden_logits = np.array(golden_logits).astype(np.float32)
  print(
      "Golden token ids have values:",
      ids,
      "Golden decoder_positions have values:",
      decoder_positions,
      "Golden decoder_segment_ids have values:",
      decoder_segment_ids,
  )
  return ids, decoder_segment_ids, decoder_positions, golden_logits


def run_l4_hf_forward_pass(model_size: str):
  """
  Run the Llama4 HF forward pass on the golden data.

  Args:
    model_size (str): type of Llama4 model (i.e. Scout (16E) or Maverick (128E))

  Returns:
    full_train_logits (numpy array): logits from the forward pass
      of shape [batch_size, seq_len, vocab_size]
  """
  model_id = "meta-llama/Llama-4-Scout-17B-16E-Instruct" if model_size == "scout" else "meta-llama/Llama-4-Maverick-17B-128E"

  hf_model = AutoModelForCausalLM.from_pretrained(
      model_id,
      trust_remote_code=True,
      torch_dtype="float32",
  )

  ids, _, _, golden_logits = setup_golden_data(model_size)
  with torch.inference_mode():
    with torch.no_grad():
      ids = torch.tensor(ids.tolist())
      outputs = hf_model(ids, use_cache=False)
      logits = outputs.logits
  print(logits)
  print(
      "Do HF logits match golden logits? ",
      np.allclose(logits, golden_logits, rtol=LOGIT_CHECKING_RTOL, atol=LOGIT_CHECKING_ATOL),
  )


def run_l4_jax_forward_pass(config, model_size):
  """
  Run the Llama4 JAX forward pass on the golden data.

  Args:
    config: Typical MaxText config.
    model_size (str): type of Llama4 model (i.e. Scout (16E) or Maverick (128E))

  Returns:
    full_train_logits (numpy array): logits from the forward pass
      of shape [batch_size, seq_len, vocab_size]
  """
  init_rng = jax.random.PRNGKey(config.init_weights_seed)
  init_rng, rng1 = jax.random.split(init_rng)
  devices_array = maxtext_utils.create_device_mesh(config)
  mesh = jax.sharding.Mesh(devices_array, config.mesh_axes)
  quant = quantizations.configure_quantization(config)
  model = models.Transformer(config, mesh=mesh, quant=quant)
  state, _ = maxtext_utils.setup_decode_state(model, config, rng1, mesh, None)

  ids, decoder_segment_ids, decoder_positions, golden_logits = setup_golden_data(model_size)
  full_train_logits = model.apply(
      state.params,
      ids,
      decoder_positions,
      decoder_segment_ids,
      enable_dropout=False,
      rngs={"aqt": init_rng},
  )
  print(full_train_logits)
  print(
      "Do JAX logits match golden logits? ",
      np.allclose(full_train_logits, golden_logits, rtol=LOGIT_CHECKING_RTOL, atol=LOGIT_CHECKING_ATOL),
  )


if __name__ == "__main__":
  parser = argparse.ArgumentParser()
  parser.add_argument("backend_type", type=str, choices=["jax", "hf"])
  parser.add_argument("model_size", type=str, choices=["scout", "maverick"])
  test_args, _ = parser.parse_known_args()

  model_args = sys.argv
  backend_type = model_args.pop(1).lower()
  model_size = model_args.pop(1).lower()
  if backend_type == "jax":
    cfg = pyconfig.initialize(model_args)
    run_l4_jax_forward_pass(cfg, model_size)
  elif backend_type == "hf":
    run_l4_hf_forward_pass(model_size)
  else:
    raise NotImplementedError("Currently, we only support jax and hf as backend types but got " + backend_type)
