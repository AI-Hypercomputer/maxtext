#  Copyright 2023 Google LLC
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#       https://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.


# This forward_pass_logit_checker.py file compares the logits generated by MaxText implementation for some input prompts
# with the golden logits for those input prompts for a particular model. This forward_pass_logit_checker.py is generic that
# it can work with different models and expects an input file called golden_data_<model_name>.jsonl to be present
# under MaxText/test_assets
# For e.g., MaxText/test_assets/golden_data_llama2-7b.jsonl
# The golden jsonl file is a simple jsonlines file with each line is in the format of a dictionary containing the following
# required keys:
# 1. prompt: A string representing the prompt, for e.g., "I love to",
# 2. tokens: token ids after tokenizing the prompt,
# 3. logits: golden logits meaning the ideal logits generated by the model in question when fed with the prompt in #1
# There can be multiple such test cases in the jsonl file, each test case is a new line in the jsonl file
# This forward_pass_logit_checker.py runs the forward pass with the input tokens and asserts that the logits generated by the
# MaxText implementation of the same model matches the golden logits closely
# Users could use a script similar to MaxText/scratch_code/golden_llama2-7b_export.ipynb to create this jsonl file

"""Check if the logits generated by a model's MaxText implementation matches golden logits for the same inputs"""
import sys
import os

current_dir = os.path.dirname(os.path.abspath(__file__))
maxtext_parent_dir = os.path.dirname(current_dir)
sys.path.append(maxtext_parent_dir)

import max_logging
max_logging.log(f"Added parent directory = {maxtext_parent_dir}")

import common_types
import jax
import jax.numpy as jnp
import numpy as np
import pyconfig
import jsonlines
import train


def get_data(golden_data, golden_data_index, config):
  """ Get the golden data for the test indexed at golden_data_index"""

  # max_logging.log(f"Comparing forward pass for golden data index = {golden_data_index} ")
  # max_logging.log(f"config.global_batch_size_to_train_on={config.global_batch_size_to_train_on}")
  s = (config.global_batch_size_to_train_on, config.max_target_length)
  ids = np.asarray(golden_data[golden_data_index]['tokens'], dtype=np.int32)

  logits = np.asarray(golden_data[golden_data_index]['logits'], dtype=np.float32)
  # max_logging.log(f" prompt=\"{golden_data[golden_data_index]['prompt']}\" raw ids={ids}, logits.shape = {logits.shape}")


  decoder_segment_ids = jax.numpy.zeros(s) + common_types.DECODING_ACTIVE_SEQUENCE_INDICATOR
  decoder_positions = jnp.stack(
      [jnp.arange(config.max_target_length, dtype=jnp.int32) for _ in range(config.global_batch_size_to_train_on)]
  )

  ids = jnp.stack(
      [ids for _ in range(config.global_batch_size_to_train_on)]
  )
  # max_logging.log(f"ids={ids}, decoder_segment_ids = {decoder_segment_ids}, decoder_positions= {decoder_positions}")

  return ids, decoder_segment_ids, decoder_positions, logits

def main(config):
  """Test the Whole Model of model_name"""

  # initialize the Llama2-7b model with weights from Meta
  (
      _,
      _,
      _,
      _,
      model_loop,
      _,
      _,
      _,
      _,
      state_loop,
    ) = train.setup_train_loop(config)

  config.__setattr__("megablox", True)
  (
      _,
      _,
      _,
      _,
      model,
      _,
      _,
      _,
      _,
      state,
    ) = train.setup_train_loop(config)
  # print(f"state_loop: {state_loop.params}")
  # print(jax.tree_util.tree_structure(state_loop))
  
  for i in range(config.num_decoder_layers):
    state.params['params']['decoder'][f'layers_{i}']['MoeBlock_0'] = {'gate':{}}
    state.params['params']['decoder'][f'layers_{i}']['MoeBlock_0']['gate']['kernel'] = state_loop.params['params']['decoder'][f'layers_{i}']['gate']['kernel']
    
    wi_0 = []
    wi_1 = []
    wo = []
    for k in range(config.num_experts):
      wi_0.append(state_loop.params['params']['decoder'][f'layers_{i}'][f"mlp_{k}"]["wi_0"]["kernel"])
      wi_1.append(state_loop.params['params']['decoder'][f'layers_{i}'][f"mlp_{k}"]["wi_1"]["kernel"])
      wo.append(state_loop.params['params']['decoder'][f'layers_{i}'][f"mlp_{k}"]["wo"]["kernel"])
    
    state.params['params']['decoder'][f'layers_{i}']['MoeBlock_0']['wi_0'] = np.array(wi_0)
    state.params['params']['decoder'][f'layers_{i}']['MoeBlock_0']['wi_1'] = np.array(wi_1)
    state.params['params']['decoder'][f'layers_{i}']['MoeBlock_0']['wo'] = np.array(wo)

    # print(f"np.array(wi_0): {np.array(wi_0).shape}")
    # print(f"np.array(wi_1): {np.array(wi_1).shape}")
    # print(f"np.array(wo): {np.array(wo).shape}")
    # print(f"state: {state.params}")
    # print(jax.tree_util.tree_structure(state))

  # print(f"state:")
  # print(jax.tree_util.tree_structure(state))

  input_golden_data_path = "MaxText/test_assets/golden_data_mixtral-8x7b.jsonl"
  with jsonlines.open(input_golden_data_path, 'r') as f:
    golden_data = list(f)
  
  from jax import random
  init_rng = random.PRNGKey(42)

  all_atol = []
  all_rtol = []
  for golden_data_index in range(len(golden_data)):
    print(f"=============== {golden_data_index} ===========")
    # print(f"golden_data: {golden_data}")
    ids, decoder_segment_ids, decoder_positions, golden_logits = get_data(golden_data,golden_data_index,config)
    config.__setattr__("megablox", False)
    full_train_logits_loop = model_loop.apply(
        state_loop.params,
        ids,
        decoder_positions,
        decoder_segment_ids,
        enable_dropout=False,
        rngs={"aqt": init_rng},
    )
    config.__setattr__("megablox", True)
    full_train_logits = model.apply(
        state.params,
        ids,
        decoder_positions,
        decoder_segment_ids,
        enable_dropout=False,
        rngs={"aqt": init_rng},
    )

    print(f"full_train_logits_loop.shape: {full_train_logits_loop.shape}")
    print(f"full_train_logits.shape: {full_train_logits.shape}")
    print(f"full_train_logits_loop: {full_train_logits_loop[0, :, :10]}")
    print(f"full_train_logits: {full_train_logits[0, :, :10]}")
    print(f".....")

    loop_0 = full_train_logits_loop[0, :, :]
    matmul_0 = full_train_logits[0, :, :]
    norm_loop = np.linalg.norm(loop_0)
    norm_matmul = np.linalg.norm(matmul_0)
    norm_abs = np.linalg.norm(np.absolute(loop_0 - matmul_0))

    atol = norm_abs
    rtol = norm_abs/(norm_loop + norm_matmul)
    all_atol.append(atol)
    all_rtol.append(rtol)
    print(f"atol: {atol}")
    print(f"rtol: {rtol}")

    # assert jax.numpy.allclose(full_train_logits[0,:,:], full_train_logits_loop[0,:,:], atol=100, rtol=100, equal_nan=False)
  print(f"all_atol: {all_atol}")
  print(f"all_rtol: {all_rtol}")
  print(f"layer: {config.__getattr__('base_num_decoder_layers')}")
  print("comparison is successful")

if __name__ == "__main__":
  jax.config.update("jax_default_prng_impl", "unsafe_rbg")
  jax.config.update("jax_platform_name", "cpu")
  os.environ["TF_CPP_MIN_LOG_LEVEL"] = "0"
  pyconfig.initialize(sys.argv)
  cfg = pyconfig.config
  main(cfg)
