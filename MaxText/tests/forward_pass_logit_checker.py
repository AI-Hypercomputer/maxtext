# Copyright 2023–2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# This script, forward_pass_logits_checker.py, verifies the correctness of logits generated by the MaxText or HuggingFace
# implementation of a model by comparing them against a reference "golden" logits file for a set of input prompts.
#
# It supports multiple models and expects a reference file in JSON Lines (.jsonl) format, specified via the
# --golden_logits_path argument. If not provided, it defaults to:
#   MaxText/test_assets/golden_data_<model_name>.jsonl
# For example:
#   MaxText/test_assets/golden_data_llama2-7b.jsonl
#
# Each line in the golden .jsonl file should be a dictionary with the following keys:
#   1. prompt: a string prompt, e.g., "I love to"
#   2. tokens: the token IDs resulting from tokenizing the prompt
#   3. logits: the expected (golden) logits output by the model for the given prompt
#
# The script runs a forward pass using the MaxText implementation and compares the resulting logits against the golden
# ones, asserting that they match within a tolerance.
#
# Utilities and Colab notebooks used to generate the golden logits are available in MaxText/scratch_code — for example:
#   MaxText/scratch_code/golden_llama2-7b_export.ipynb

"""Check if the logits generated by a model's MaxText/HF implementation matches golden logits for the same inputs"""

import argparse
import os
import sys
import numpy as np
import jax
import jax.numpy as jnp
import jsonlines
import torch.nn.functional as F
from google.cloud import storage
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

from MaxText.utils.ckpt_conversion.utils.hf_utils import (
    convert_jax_weight_to_torch,
)
from MaxText import max_logging
from MaxText import maxtext_utils
from MaxText import pyconfig
from MaxText.common_types import DECODING_ACTIVE_SEQUENCE_INDICATOR, MODEL_MODE_TRAIN
from MaxText.globals import PKG_DIR
from MaxText.layers import models
from MaxText.layers import quantizations
import sys
import numpy

# numpy.set_printoptions(threshold=sys.maxsize)


def upload_blob(bucket_name, source_file_name, destination_blob_name):
  """Uploads a file to the bucket."""
  storage_client = storage.Client()
  bucket = storage_client.get_bucket(bucket_name)
  blob = bucket.blob(destination_blob_name)
  blob.upload_from_filename(source_file_name)


def get_top_k_tokens_scores(logits_tensor, tokenizer_instance, k=10, description=""):
  """Get the top-k tokens and their scores from a given logits tensor."""
  max_logging.log(f"\n--- {description} top {k} tokens ---")
  collected_tokens = []
  tokens = []
  topk_results = torch.topk(logits_tensor[0], k=k)
  for i in range(k):
    tok_id = topk_results.indices[i].item()
    score = topk_results.values[i].item()
    tok = tokenizer_instance.decode(tok_id)
    collected_tokens.append({"id": int(tok_id), "token": tok.strip(), "score": float(score)})
    tokens.append({"id": int(tok_id), "token": tok.strip(), "score": float(score)})

  # Prepare data for logging
  table_str = f"| {'Token ID':<10} | {'Token':<20} | {'Score':<10} |\n"
  table_str += f"|{'-'*12}|{'-'*22}|{'-'*12}|\n"
  for d in collected_tokens:
    table_str += f"| {d['id']:<10} | {d['token']:<20} | {d['score']:<10.4f} |\n"
  max_logging.log(table_str)
  return tokens


def compare_top_tokens(converted_tokens, golden_tokens):
  """
  Compares two lists of top tokens and calculates similarity metrics.

  Args:
      converted_tokens: top tokens from the converted model.
      golden_tokens:  top tokens from the golden model.
  """
  # Extract the sets of token IDs for comparison
  converted_ids = {token["id"] for token in converted_tokens}
  golden_ids = {token["id"] for token in golden_tokens}

  # --- Metric 1: Overlap Count & Jaccard Similarity ---
  intersection = converted_ids.intersection(golden_ids)
  union = converted_ids.union(golden_ids)

  overlap_count = len(intersection)
  jaccard_similarity = overlap_count / len(union) if union else 0.0

  # --- Metric 2: Rank Agreement ---
  rank_matches = 0
  min_len = min(len(converted_tokens), len(golden_tokens))
  for i in range(min_len):
    if converted_tokens[i]["id"] == golden_tokens[i]["id"]:
      rank_matches += 1

  rank_agreement = (rank_matches / min_len) * 100 if min_len > 0 else 0.0

  metrics = {
      "overlap_count": f"{overlap_count}/{min_len}",
      "jaccard_similarity": jaccard_similarity,
      "rank_agreement_percentage": rank_agreement,
  }

  max_logging.log("\n--- Similarity Metrics of Top Tokens ---")
  table_str = f"| {'Metric':<30} | {'Value':<20} |\n"
  table_str += f"|{'-'*32}|{'-'*22}|\n"
  for key, value in metrics.items():
    table_str += f"| {key:<30} | {str(value):<20} |\n"
  max_logging.log(table_str)


def check_kl_divergence(model_logits, golden_logits, atol=0.02):
  """
  Calculates KL divergence D_KL(P_golden || Q_model) over a batch of sequences.

  Args:
      model_logits: Logits from the converted model (Batch, SeqLen, VocabSize).
      golden_logits: Logits from the golden model (Batch, SeqLen, VocabSize).
      token_size: The number of vocabulary entries to consider for the comparison.
                  (Effectively vocab_size_to_compare).
  """
  # 1. Select the relevant vocabulary slice from the logits.
  token_size = min(model_logits.shape[2], golden_logits.shape[2])
  model_logits_sliced = model_logits[..., :token_size]
  golden_logits_sliced = golden_logits[..., :token_size]

  # 2. Reshape
  b, s, v = model_logits_sliced.shape
  model_logits_reshaped = model_logits_sliced.view(b * s, v)
  golden_logits_reshaped = golden_logits_sliced.view(b * s, v)

  # 3. Get the probability distributions.
  golden_probabilities = F.softmax(golden_logits_reshaped, dim=-1)
  model_log_probabilities = F.log_softmax(model_logits_reshaped, dim=-1)

  # 4. Calculate avg KL divergence for all token distributions.
  # use 'batchmean'; the sum of the KL divergences for each token in the batch
  # and then divides by the number of tokens (b * s)
  kl_div_value = F.kl_div(
      input=model_log_probabilities,
      target=golden_probabilities,
      reduction="batchmean",  # Use 'batchmean' for the average KL per token.
      log_target=False,
  )

  max_logging.log(f"\nAverage KL divergence per token (D_KL(P_golden || Q_model)): {kl_div_value.item():.6f}")

  # To find the max KL divergence for any single token in the set
  # use reduction='none'.
  kl_divs_per_token = F.kl_div(
      input=model_log_probabilities, target=golden_probabilities, reduction="none", log_target=False
  ).sum(
      dim=-1
  )  # Sum over the vocab dim to get a single KL value per token

  max_kl_div = kl_divs_per_token.max()
  max_logging.log(f"\nMax KL divergence for a single token in the set: {max_kl_div.item():.6f}")

  assert max_kl_div < atol, f"KL divergence values {max_kl_div.item():.6f} exceed the threshold {atol}"


def get_data(golden_data_point, config, global_batch_size_to_train_on=1):
  """Get the golden data for the test indexed at golden_data_index"""

  max_logging.log(f"global_batch_size_to_train_on={global_batch_size_to_train_on}")
  if config.use_multimodal:
    assert "pixel_values" in golden_data_point, "no image found in golden data while use_multimodal=True"
    pixel_values = np.asarray(golden_data_point["pixel_values"], dtype=np.float32)
    max_logging.log(f"pixel_values.shape = {pixel_values.shape}")
    model_prefix = config.model_name.split("-")[0]
    if model_prefix in ["gemma3"]:
      pixel_values = np.transpose(pixel_values, (1, 2, 0))
    elif model_prefix in ["llama4"]:
      pixel_values = pixel_values[None, :]
    pixel_values = np.stack([pixel_values for _ in range(global_batch_size_to_train_on)])
  else:
    pixel_values = None

  original_ids = np.asarray(golden_data_point["tokens"], dtype=np.int32)
  seq_len = len(original_ids)

  if seq_len > config.max_target_length:
    raise ValueError(
        f"Golden data sequence length ({seq_len}) is greater than max_target_length ({config.max_target_length})"
    )

  s = (global_batch_size_to_train_on, config.max_target_length)

  # Pad ids to max_target_length. MaxText expects 0 for padding.
  padded_ids = np.pad(original_ids, (0, config.max_target_length - seq_len), "constant", constant_values=0)
  ids = np.stack([padded_ids for _ in range(global_batch_size_to_train_on)])

  logits = np.asarray(golden_data_point["logits"], dtype=np.float32)
  if "formatted_prompt" in golden_data_point:
    prompt = golden_data_point["formatted_prompt"]
  else:
    prompt = golden_data_point["prompt"]
  max_logging.log(f' prompt="{prompt}" raw ids={original_ids}, logits.shape = {logits.shape}')

  decoder_segment_ids = np.zeros(s, dtype=np.int32)
  decoder_segment_ids[:, :seq_len] = DECODING_ACTIVE_SEQUENCE_INDICATOR
  decoder_positions = np.stack(
      [np.arange(config.max_target_length, dtype=np.int32) for _ in range(global_batch_size_to_train_on)]
  )
  return ids, decoder_segment_ids, decoder_positions, logits, seq_len, pixel_values


def main(config, test_args):  # pylint: disable=W0621
  """Test the Whole Model of model_name"""

  if test_args.hf_model_path == "":
    raise ValueError
  # hf_model = AutoModelForCausalLM.from_pretrained(test_args.hf_model_path, torch_dtype=torch.bfloat16)

  hf_model = AutoModelForCausalLM.from_pretrained(
      test_args.hf_model_path,
      torch_dtype=torch.float32,
      trust_remote_code=True,
  )

  tokenizer = AutoTokenizer.from_pretrained(test_args.model_id)
  if "Llama-3.1" in test_args.hf_model_path:
    tokenizer.pad_token = tokenizer.eos_token

  # prompts = ["I love to"]
  # prompts = [config.prompt]
  # prompts = [test_args.prompts]
  # prompts = [
  #     "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
  # ]
  prompts = [test_args.prompts]
  golden_data = []
  for input_text in prompts:
    max_logging.log(f"\n--- Prompt: {input_text} ---")
    # Tokenize for HF
    inputs = tokenizer(
        input_text, return_tensors="pt", padding="max_length", max_length=config.max_target_length, truncation=True
    )
    print(inputs)
    print(inputs["attention_mask"].sum())
    # actual_seq_len = inputs["input_ids"].shape[1]
    print("huggingface")
    with torch.no_grad():
      hf_logits_torch = hf_model(**inputs).logits

    data = {
        "prompt": input_text,
        "tokens": inputs["input_ids"].tolist()[0],
        "logits": hf_logits_torch.cpu().numpy().astype("float32").tolist()[0],
    }
    print(data["prompt"], data["tokens"])
    golden_data.append(data)

  """Comparing maxtext/huggingface model with pre-loaded golden logitis"""
  max_logging.log("Initializing MaxText model")
  init_rng = jax.random.PRNGKey(config.init_weights_seed)
  init_rng, rng1 = jax.random.split(init_rng)
  devices_array = maxtext_utils.create_device_mesh(config)
  mesh = jax.sharding.Mesh(devices_array, config.mesh_axes)
  quant = quantizations.configure_quantization(config)
  model = models.transformer_as_linen(config, mesh=mesh, quant=quant, model_mode=MODEL_MODE_TRAIN)
  state, _ = maxtext_utils.setup_decode_state(model, config, rng1, mesh, None)

  for golden_data_index, golden_data_point in enumerate(golden_data):
    max_logging.log(f"\n--- Comparing forward pass for golden data index: {golden_data_index} ---")
    ids, decoder_segment_ids, decoder_positions, golden_logits, seq_len, images = get_data(golden_data_point, config)
    print("ids", ids)
    print("decoder_segment_id", decoder_segment_ids)
    print("decoder_positions", decoder_positions)
    print(seq_len)
    max_logging.log("maxtext forward pass")
    full_train_logits = model.apply(
        state.params,
        ids,
        decoder_positions,
        decoder_segment_ids,
        encoder_images=images,
        enable_dropout=False,
        rngs={"aqt": init_rng},
    )

    full_train_logits = jax.experimental.multihost_utils.process_allgather(full_train_logits)
    # if full_train_logits shape is [num_hosts, batch_size, seq_len, vocab_size]
    if full_train_logits.ndim == 4:
      full_train_logits = jnp.reshape(full_train_logits, (-1, config.max_target_length, config.vocab_size))
    # Slice to original sequence length
    full_train_logits = full_train_logits[:, :seq_len, :]

    token_size = int(test_args.token_size) if test_args.token_size else seq_len
    if full_train_logits.shape[-1] != golden_logits.shape[-1]:
      max_logging.log(
          f"Vocab size mismatch: train logits vocab size {full_train_logits.shape[-1]}, "
          f"golden logits vocab size {golden_logits.shape[-1]}. "
          "Comparing up to the smaller vocab size."
      )
    min_vocab_size = min(full_train_logits.shape[-1], golden_logits.shape[-1])
    train_logits_slice = full_train_logits[0, :token_size, :min_vocab_size]
    golden_logits_slice = golden_logits[:token_size, :min_vocab_size]
    max_logging.log(f"{golden_logits_slice[2]=}")
    max_logging.log(f"{train_logits_slice[2]=}")

    # Calculate absolute and relative differences for detailed reporting
    abs_diff = jnp.abs(train_logits_slice - golden_logits_slice)

    # To avoid division by zero, add a small epsilon where golden_logits_slice is zero
    safe_golden_logits = jnp.where(golden_logits_slice == 0, 1e-8, golden_logits_slice)
    rel_diff = abs_diff / jnp.abs(safe_golden_logits)

    max_abs_diff_idx = jnp.unravel_index(jnp.argmax(abs_diff), abs_diff.shape)
    max_rel_diff_idx = jnp.unravel_index(jnp.argmax(rel_diff), rel_diff.shape)

    max_abs_diff_val = abs_diff[max_abs_diff_idx]
    max_rel_diff_val = rel_diff[max_rel_diff_idx]
    msg = (
        f"Max absolute difference: {max_abs_diff_val:.6f} at index {max_abs_diff_idx}\n"
        f"  (Train: {train_logits_slice[max_abs_diff_idx]:.6f}, Golden: {golden_logits_slice[max_abs_diff_idx]:.6f})\n"
        f"Max relative difference: {max_rel_diff_val:.6f} at index {max_rel_diff_idx}\n"
        f"  (Train: {train_logits_slice[max_rel_diff_idx]:.6f}, Golden: {golden_logits_slice[max_rel_diff_idx]:.6f})"
    )
    max_logging.log(msg)

    model_probabilities = jax.nn.softmax(train_logits_slice, axis=-1)
    golden_probabilities = jax.nn.softmax(golden_logits_slice, axis=-1)

    max_logging.log(f"{golden_probabilities[1]=}")
    max_logging.log(f"{model_probabilities[1]=}")

    kl_div = jax.numpy.sum(jax.scipy.special.kl_div(golden_probabilities, model_probabilities), axis=-1)
    max_logging.log(f"KL divergence = {kl_div}, max KL divergence = {jax.numpy.max(kl_div)}")

    if test_args.max_kl_div is not None:
      max_logging.log(
          f"Checking KL Divergence between train distribution and golden distribution against theshold {test_args.max_kl_div}."
      )
      assert jax.numpy.all(
          kl_div < test_args.max_kl_div,
      ), f"KL divergence values exceed the specified threshold of {test_args.max_kl_div}. Max divergence: {jax.numpy.max(kl_div)}"

    max_logging.log(
        f"Checking Numerical Differences between train logits and golden logits against atol={test_args.rtol} rtol={test_args.atol}."
    )
    rtol_val = float(test_args.rtol)
    atol_val = float(test_args.atol)
    assert jax.numpy.allclose(
        train_logits_slice, golden_logits_slice, rtol=rtol_val, atol=atol_val, equal_nan=False
    ), f"Logits do not match closely enough. Required rtol={test_args.rtol}, atol={test_args.atol}."


if __name__ == "__main__":
  jax.config.update("jax_default_prng_impl", "unsafe_rbg")
  os.environ["TF_CPP_MIN_LOG_LEVEL"] = "0"

  parser = argparse.ArgumentParser()
  parser.add_argument("--atol", type=float, required=False, default=0.1)
  parser.add_argument("--rtol", type=float, required=False, default=0.1)
  parser.add_argument("--token_size", type=int, required=False)
  parser.add_argument("--max_kl_div", type=float, required=False, default=None)
  parser.add_argument("--golden_logits_path", type=str, required=False, default="")
  parser.add_argument("--hf_model_path", type=str, required=False, default="")
  parser.add_argument("--model_id", type=str, required=False, default="")
  parser.add_argument("--run_hf_model", type=bool, required=False, default=False)
  parser.add_argument("--output_logits_path", type=str, required=False, default="")
  parser.add_argument("--gcs_output_logits_path", type=str, required=False, default="")
  parser.add_argument("--prompts", type=str, required=False, help="A single prompt")
  test_args, _ = parser.parse_known_args()

  # Remove args defined in this test file to avoid error from pyconfig
  model_args = sys.argv
  to_remove_args = [
      "--atol",
      "--rtol",
      "--token_size",
      "--max_kl_div",
      "--golden_logits_path",
      "--hf_model_path",
      "--run_hf_model",
      "--output_logits_path",
      "--gcs_output_logits_path",
      "--prompts",
      "--model_id",
  ]
  for arg in to_remove_args:
    model_args = [s for s in model_args if not s.startswith(arg)]

  cfg = pyconfig.initialize(model_args)

  if cfg.use_multimodal:
    assert (
        not test_args.run_hf_model
    ), "Multimodal does not support running hf model on-the-fly, please generate hf golden logits using generate_hf_golden_logits.py"
  main(cfg, test_args)
