# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Implementation of Engine API for MaxText"""
import copy as cp
import functools
from typing import Any, List, Optional, Tuple, Callable
from collections import defaultdict

import flax
from flax import linen as nn
from flax.linen import partitioning as nn_partitioning

from layers import models, quantizations

import jax
import jax.numpy as jnp
from jax.experimental import host_callback
from jax.sharding import PartitionSpec as P

import common_types
from jetstream.core import config_lib
from jetstream.engine import engine_api
from jetstream.engine import tokenizer_pb2
from jetstream.engine import tokenizer_api
from jetstream.engine import token_utils

import max_utils
import inference_utils
import pyconfig


import logging

# Configure logging at the beginning of your main script or globally
logging.basicConfig(filename='maxengine_debug.log', level=logging.INFO, 
                    format='%(asctime)s - %(levelname)s - %(message)s')

import warnings

warnings.simplefilter("ignore", category=FutureWarning)
DecodeState = Any
Prefix = Any
PackedPrefix = Any
Params = Any
PRNGKeyType = Any


class MaxEngineConfig:
  """Engine specific config class to allow using multiple MaxEngine instances in an inference run.
  The default pyconfig.config is a global param shared across multiple instances and doesn't
  allow using different config for each MaxEngine instance.
  """

  def __init__(self, keys):
    # self.keys = keys
    self.__dict__["keys"] = keys

  def __getattr__(self, attr):
    if attr not in self.keys:
      raise ValueError(f"Requested key {attr}, not in config")
    return self.keys[attr]

  def __setattr__(self, attr, value):
    raise ValueError

  def get_keys(self):
    return self.keys


class MaxEngine(engine_api.Engine):
  """The computational core of the generative model server.

  Engine defines an API that models must adhere to as they plug into the
  JetStream efficient serving infrastructure.
  """

  def __init__(self, config: Any, devices: config_lib.Devices | None = None):
    self.config = config

    # Mesh definition
    devices_array = max_utils.create_device_mesh(config=config, devices=devices)
    self._mesh = jax.sharding.Mesh(devices_array, config.mesh_axes)

    # Model and Optimizer definition
    quant = quantizations.configure_quantization(config)
    self.model = models.Transformer(config, mesh=self._mesh, quant=quant)
    self.replicated_sharding = jax.sharding.NamedSharding(self._mesh, P(None))

    self.abstract_params = None
    self.prefill_kv_cache_annotations = None
    self.kv_cache_annotations = None
    self.kv_cache_annotations_named = None
    self.prefill_kv_cache_shardings = None
    self.kv_cache_shardings = None
    self.state_mesh_annotations = None

  def load_params(self, *args, rng: Optional[PRNGKeyType] = None, **kwargs) -> Params:
    """Load Parameters, typically from GCS"""
    # pylint: disable=unused-argument

    if rng is None:
      rng = jax.random.PRNGKey(0)

    if self.model.quant and self.config.checkpoint_is_quantized:
      print("Loading from the quantized checkpoint...")
      self.model.quant.quant_mode = quantizations.get_quant_mode("serve")

    rng1, rng2, rng3 = jax.random.split(rng, 3)
    state, self.state_mesh_annotations = max_utils.setup_decode_state(self.model, self.config, rng1, self._mesh, None)
    # pylint: disable=isinstance-second-argument-not-valid-type
    self.abstract_params = jax.tree_util.tree_map(
        lambda x: jax.ShapeDtypeStruct(shape=x.shape, dtype=x.dtype, sharding=x.sharding)
        if isinstance(x, jax.Array)
        else None,
        state.params,
    )

    self.prefill_kv_cache_annotations = max_utils.get_prefill_kv_cache_annotations(self.model, self.config, rng2, self._mesh)
    self.prefill_kv_cache_shardings = jax.tree_util.tree_map(
        lambda x: jax.sharding.NamedSharding(self._mesh, x),
        self.prefill_kv_cache_annotations,
    )

    if self.config.stack_prefill_result_cache:
      # Add extra axis for the axis generated by the stack.
      self.prefill_kv_cache_shardings = jax.tree_util.tree_map(
          lambda x: jax.sharding.NamedSharding(self._mesh, jax.sharding.PartitionSpec(None, *x.spec)),
          self.prefill_kv_cache_shardings,
      )
      self.prefill_kv_cache_shardings = self.prefill_kv_cache_shardings["decoder"]["layers_0"]

    self.kv_cache_annotations = max_utils.get_kv_cache_annotations(self.model, self.config, rng2, self._mesh)
    self.kv_cache_shardings = jax.tree_util.tree_map(
        lambda x: jax.sharding.NamedSharding(self._mesh, x),
        self.kv_cache_annotations,
    )

    if self.model.quant and not self.config.checkpoint_is_quantized:
      params = self.quantize_params(state, rng3)
    else:
      params = state.params
    max_utils.print_mem_stats("After load_params")
    return params

  def quantize_params(self, state, rng: Optional[PRNGKeyType] = None):
    """Forward pass to quantize decode params."""
    if rng is None:
      rng = jax.random.PRNGKey(0)

    self.model.quant.quant_mode = quantizations.get_quant_mode("convert")

    @jax.jit
    def model_apply(_p, _rng):
      return self.model.apply(
          _p | {"aqt": {}},
          jnp.ones((1, self.config.max_prefill_predict_length), dtype=jnp.int32),
          jnp.ones((1, self.config.max_prefill_predict_length), dtype=jnp.int32),
          decoder_segment_ids=jnp.zeros((1, self.config.max_prefill_predict_length), dtype=jnp.int32),
          enable_dropout=False,
          model_mode=common_types.MODEL_MODE_PREFILL,
          rngs={"params": _rng},
          mutable=True,
      )

    _, new_vars = model_apply(state.params, rng)
    # Remove param values which have corresponding qtensors in aqt to save memory.
    params = {}
    params["aqt"] = new_vars["aqt"]
    params["params"] = quantizations.remove_quantized_params(state.params["params"], new_vars["aqt"])
    self.abstract_params = jax.tree_util.tree_map(
        lambda x: jax.ShapeDtypeStruct(shape=x.shape, dtype=x.dtype, sharding=x.sharding),
        params,
    )
    max_utils.save_quantized_checkpoint_if_configured(self.config, params)
    self.model.quant.quant_mode = quantizations.get_quant_mode("serve")
    return params

  def _maybe_stack_prefill_result_cache(self, cache):
    """Stack the caches across the layers."""
    if not self.config.stack_prefill_result_cache:
      return cache

    layer_keys = []
    for i in range(self.config.num_decoder_layers):
      layer_keys.append(f"layers_{i}")

    layer_cache = [cache["decoder"][layer_key] for layer_key in layer_keys]

    return jax.tree.map(lambda *c: jnp.stack(c), *layer_cache)

  def _maybe_unstack_prefill_result_cache(self, cache):
    """Unstack the caches across the layers."""
    if not self.config.stack_prefill_result_cache:
      return cache

    flat_cache, treedef = jax.tree.flatten(cache)
    layer_cache = [jax.tree.unflatten(treedef, flat_cache_vars) for flat_cache_vars in zip(*flat_cache, strict=True)]
    res_cache = {"decoder": {}}

    for i in range(self.config.num_decoder_layers):
      res_cache["decoder"][f"layers_{i}"] = layer_cache[i]

    return res_cache

  @functools.partial(jax.jit, static_argnums=(0,))
  def prefill(
      self,
      *,
      params: Params,
      existing_prefix: Optional[jax.Array] = None,
      padded_tokens: jax.Array,
      true_length: int,
      sampler: Optional[Callable[[Any], Any]] = None,  # pylint: disable=unused-argument
      rng: Optional[PRNGKeyType] = None,
      slot: int = 0,
  ) -> Tuple[Prefix, engine_api.ResultTokens]:
    """Computes a kv-cache for a new generate request.

    Args:
      params: Scalar multiplier.
      existing_prefix: If provided, represents a prefix that has already been
        processed by the underlying model.
      padded_tokens: Logically appended tokens to any existing prefix, this is
        what we compute prefill on.
      true_length: The real length of the tokens, pre-pad.
    Returns:
      kv_cache: For the resulting text.
    """
    # jax.debug.print("maxengine.prefill - START")
    if existing_prefix:
      raise ValueError("We don't know what to do with existing_prefix")

    if rng is None:
      rng = jax.random.PRNGKey(0)

    input_tokens = jnp.expand_dims(padded_tokens, 0)  # [BATCH, SEQUENCE]
    positions = jnp.expand_dims(jnp.arange(0, input_tokens.shape[1]), 0)

    zero_to_n = jnp.arange(0, padded_tokens.shape[0])
    ones_to_keep = zero_to_n < true_length
    one_d_output = ones_to_keep * common_types.DECODING_ACTIVE_SEQUENCE_INDICATOR
    sequence_indicator = jnp.expand_dims(one_d_output, 0)

    rng, new_rng = jax.random.split(rng)

    # jax.debug.print("maxengine.prefill - before apply")
    with self._mesh, nn_partitioning.axis_rules(self.config.logical_axis_rules):
      flat_logits, new_vars = self.model.apply(
          params,
          input_tokens,
          positions,
          decoder_segment_ids=sequence_indicator,
          enable_dropout=False,
          model_mode=common_types.MODEL_MODE_PREFILL,
          rngs={"params": new_rng},
          mutable=["cache"],
          slot=slot,
          true_length=true_length,
      )

    next_pos = jnp.full((1, 1), true_length, dtype=jnp.int32)
    generated_tokens = jnp.zeros((1, 1), dtype=jnp.int32)
    selected_logits = jax.lax.dynamic_slice(
        flat_logits,
        (0, true_length - 1, 0),
        (flat_logits.shape[0], 1, flat_logits.shape[2]),
    )
    selected_logits = jax.lax.with_sharding_constraint(selected_logits, self.replicated_sharding)

    # sampling first token
    first_generated_token = inference_utils.sampling(
        selected_logits,
        rng,
        self.config.decode_sampling_strategy,
        topk=self.config.decode_sampling_top_k,
        nucleus_topp=self.config.decode_sampling_nucleus_p,
        temperature=self.config.decode_sampling_temperature,
    )

    all_valid = jnp.ones(first_generated_token.shape, dtype=jnp.int8)
    result = engine_api.ResultTokens(
        data=jnp.concatenate((first_generated_token, all_valid, generated_tokens), axis=1),
        # Tokens are shape [batch, speculations], so when we concatenate
        # tokens, validity and length along their index 1 dimension then they
        # occupy 0:speculations.
        tokens_idx=(0, 1),
        # Validity occupies the same amount of space, but next in line.
        valid_idx=(1, 2),
        # And lengths is rank 1.
        length_idx=(2, 3),
        samples_per_slot=1,
    )

    cache = new_vars["cache"]
    cache = self._maybe_stack_prefill_result_cache(cache)
    # jax.debug.print("maxengine.prefill - EXIT")
    return {
        "logits": selected_logits,
        "cache": cache,
        "next_pos": next_pos,
        "generated_tokens": generated_tokens,
        "tokens": first_generated_token,
    }, result

  @functools.partial(jax.jit, static_argnums=(0,), static_argnames=("num_prompts",))
  def prefill_concat(
      self,
      *,
      params: Params,
      existing_prefix: Optional[jax.Array] = None,
      padded_tokens: jax.Array,
      decoder_positions: jax.Array,
      decoder_segment_ids: jax.Array,
      start_pos: jax.Array,
      true_lengths: jax.Array,
      num_prompts: int,
      sampler: Optional[Callable[[Any], Any]] = None,  # pylint: disable=unused-argument
      rng: Optional[PRNGKeyType] = None,
  ) -> Tuple[Any, PackedPrefix, List[engine_api.ResultTokens]]:
    """Computes a kv-cache for a new packed generate request, which is a
    concatenation of several shorter prompts. Experimentation shows that
    longer prefill sequences gives approximately 15% boost in time per prefilled
    token.

    Args:
      params: Scalar multiplier.
      existing_prefix: If provided, represents a prefix that has already been
        processed by the underlying model.
      padded_tokens: Logically appended tokens to any existing prefix, this is
        what we compute prefill on.
      decoder_positions: int values indicating the position of token in its
        original sequence.
      decoder_segment_ids: int values indicating which sequence the the token
        originally belong to.
      start_pos: Padded array indicating the start position of each of the prompts.
      true_length: Padded array indicating the true lengths of each of the prompts.
      num_prompts: the number of prompts packed in the entire sequence.
    Returns:
      kv_cache: For the resulting text.
    """
    if existing_prefix:
      raise ValueError("We don't know what to do with existing_prefix")

    if rng is None:
      rng = jax.random.PRNGKey(0)
    input_tokens = jnp.expand_dims(padded_tokens, 0)  # [BATCH, SEQUENCE]
    decoder_positions = jnp.expand_dims(decoder_positions, 0)
    decoder_segment_ids = jnp.expand_dims(decoder_segment_ids, 0)
    rng, new_rng = jax.random.split(rng)
    with self._mesh, nn_partitioning.axis_rules(self.config.logical_axis_rules):
      flat_logits, new_vars = self.model.apply(
          params,
          input_tokens,
          decoder_positions,
          decoder_segment_ids=decoder_segment_ids,
          enable_dropout=False,
          model_mode=common_types.MODEL_MODE_PREFILL,
          rngs={"params": new_rng},
          mutable=["cache"],
      )
    cache = new_vars["cache"]
    cache = self._maybe_stack_prefill_result_cache(cache)

    def process_packed_logits_and_caches(packed_flat_logits, idx):
      next_pos = jnp.full((1, 1), true_lengths[idx], dtype=jnp.int32)
      generated_tokens = jnp.zeros((1, 1), dtype=jnp.int32)
      selected_logits = jax.lax.dynamic_slice(
          packed_flat_logits,
          (0, start_pos[idx] + true_lengths[idx] - 1, 0),
          (packed_flat_logits.shape[0], 1, packed_flat_logits.shape[2]),
      )
      selected_logits = jax.lax.with_sharding_constraint(selected_logits, self.replicated_sharding)
      first_generated_token = inference_utils.sampling(
          selected_logits,
          rng,
          self.config.decode_sampling_strategy,
          topk=self.config.decode_sampling_top_k,
          nucleus_topp=self.config.decode_sampling_nucleus_p,
          temperature=self.config.decode_sampling_temperature,
      )
      all_valid = jnp.ones(first_generated_token.shape, dtype=jnp.int8)
      result = engine_api.ResultTokens(
          data=jnp.concatenate((first_generated_token, all_valid, generated_tokens), axis=1),
          # Tokens are shape [batch, speculations], so when we concatenate
          # tokens, validity and length along their index 1 dimension then they
          # occupy 0:speculations.
          tokens_idx=(0, 1),
          # Validity occupies the same amount of space, but next in line.
          valid_idx=(1, 2),
          # And lengths is rank 1.
          length_idx=(2, 3),
          samples_per_slot=1,
      )
      return {
          "logits": selected_logits,
          "next_pos": next_pos,
          "generated_tokens": generated_tokens,
          "tokens": first_generated_token,
      }, result

    prefill_results = defaultdict(list)
    first_tokens = []
    for idx in range(num_prompts):
      prefill_result, first_token = process_packed_logits_and_caches(flat_logits, idx)
      for k, v in prefill_result.items():
        prefill_results[k].append(v)
      first_tokens.append(first_token)
    prefill_results = {k: jnp.stack(v) for k, v in prefill_results.items()}
    return cache, prefill_results, first_tokens

  @functools.partial(jax.jit, static_argnums=(0,), donate_argnums=(2,))
  def generate(
      self,
      params: Params,
      decode_state: DecodeState,
      sampler: Optional[Callable[[Any], Any]] = None,  # pylint: disable=unused-argument
      rng: Optional[PRNGKeyType] = None,
      slot: int = -1,
  ) -> Tuple[DecodeState, engine_api.ResultTokens]:
    """Run one generate step"""
    if rng is None:
        rng = jax.random.PRNGKey(0)

    previous_token = decode_state["tokens"]
    
    # Get sequence length for this slot
    sequence_length = decode_state["cache"]["sequence_lengths"][slot] if slot >= 0 else None

    jax.debug.print("engine.generate - START - slot: {}, sequence_length: {}", 
                   slot, sequence_length)
    
    rng, new_rng = jax.random.split(rng)
    
    with self._mesh, nn_partitioning.axis_rules(self.config.logical_axis_rules):
        out_logits, new_vars = self.model.apply(
            params | {"cache": decode_state["cache"]},
            previous_token,
            decode_state["next_pos"],
            enable_dropout=False,
            model_mode=common_types.MODEL_MODE_AUTOREGRESSIVE,
            rngs={"params": new_rng},
            mutable=["cache"],
            slot=slot,
            true_length=sequence_length,  # Pass through the sequence length
        )

    out_logits = jax.lax.with_sharding_constraint(out_logits, self.replicated_sharding)
    new_cache = jax.lax.with_sharding_constraint(new_vars["cache"], self.kv_cache_shardings)

    # sampling tokens
    new_token = inference_utils.sampling(
        out_logits,
        rng,
        self.config.decode_sampling_strategy,
        topk=self.config.decode_sampling_top_k,
        nucleus_topp=self.config.decode_sampling_nucleus_p,
        temperature=self.config.decode_sampling_temperature,
    )

    all_valid = jnp.ones(new_token.shape, dtype=jnp.int8)
    result = engine_api.ResultTokens(
        data=jnp.concatenate((new_token, all_valid, decode_state["generated_tokens"]), axis=1),
        # Tokens are shape [batch, speculations], so when we concatenate
        # tokens, validity and length along their index 1 dimension then they
        # occupy 0:speculations.
        tokens_idx=(0, 1),
        # Validity occupies the same amount of space, but next in line.
        valid_idx=(1, 2),
        # And lengths is rank 1.
        length_idx=(2, 3),
        samples_per_slot=1,
    )

    return {
        "logits": out_logits,
        "cache": new_cache,
        "next_pos": decode_state["next_pos"] + 1,
        "generated_tokens": decode_state["generated_tokens"] + 1,
        "tokens": new_token,
    }, result

  @functools.partial(
      jax.jit,
      static_argnums=(0,),
      donate_argnums=(
          1,
          2,
      ),
  )
  def insert(
      self,
      prefix: Prefix,
      decode_state: DecodeState,
      slot: int,
  ) -> DecodeState:
    """Insert a single computed prefill cache into KV cache."""
    unboxed_prefix = max_utils.unbox_logicallypartioned(prefix)

    unboxed_prefix["cache"] = self._maybe_unstack_prefill_result_cache(unboxed_prefix["cache"])

    def copy(path, partial_cache, full_cache, annotations):
      path_key = path[-1].key
      if path_key in [
          "cache_ar_index",
          "cached_ar_key",
          "cached_ar_value",
          "cached_ar_key_scale",
          "cached_ar_value_scale",
      ]:
        return full_cache  # we don't even zero these out because we can mask them out.

      batch_idx = -1
      if "cache_batch" in annotations:
        batch_idx = annotations.index("cache_batch")
      elif "cache_scale_batch" in annotations:
        batch_idx = annotations.index("cache_scale_batch")

      if batch_idx < 0:
        raise ValueError(f"Batch index {batch_idx=} shouldn't be less than zero for {path_key}, got {annotations=}")

      if path_key == "cache_ar_segment_id":
        ### goal: zero this out in case there is existing data
        s = list(full_cache.shape)
        s[batch_idx] = 1
        zeros = jnp.zeros(tuple(s), dtype=jnp.int32)
        return jax.lax.dynamic_update_index_in_dim(full_cache, zeros, slot, batch_idx)
      elif path_key == "cache_prefill_segment_id":
        s = list(full_cache.shape)
        s[batch_idx] = 1
        zeros = jnp.zeros(tuple(s), dtype=jnp.int32)
        ## zero out in case prefill cache is too small to cover
        full_cache = jax.lax.dynamic_update_index_in_dim(full_cache, zeros, slot, batch_idx)
        ## copy prefill cachce
        full_cache = jax.lax.dynamic_update_index_in_dim(full_cache, partial_cache, slot, batch_idx)
        return full_cache
      elif path_key == "cached_ar_lengths":
        return full_cache.at[slot].set(0)
      elif path_key in [
          "cached_prefill_key",
          "cached_prefill_value",
          "cached_prefill_key_scale",
          "cached_prefill_value_scale",
      ]:
        return jax.lax.dynamic_update_index_in_dim(full_cache, partial_cache, slot, batch_idx)
      else:
        raise ValueError(f"We don't have a strategy for inserting {path_key}")

    if self.config.attention == "paged":

      def copy_paged(path, prefix_cache, decode_state_cache):
        """Handle paged cache copying with comprehensive component support.
        
        Args:
            path: A tuple representing the path in the tree
            prefix_cache: Source cache data
            decode_state_cache: Destination cache data
            
        Returns:
            Updated cache data based on component type
        """
        # Safe path key extraction
        if isinstance(path, tuple) and path:
            last_component = path[-1]
            path_key = getattr(last_component, 'key', str(last_component))
        else:
            path_key = str(path)

        # Page manager special case
        if path_key == "page_manager":
            return prefix_cache

        # Complete component categorization
        ar_components = {
            # Core AR cache components
            "cache_ar_index",
            "cache_ar_segment_id",
            "cached_ar_key",
            "cached_ar_value",
            # AR scaling components
            "cached_ar_key_scale",
            "cached_ar_value_scale",
            # AR metadata
            "cached_ar_lengths"
        }

        prefill_components = {
            # Core prefill cache components
            "cache_prefill_segment_id",  # Added missing component
            "cached_prefill_key",
            "cached_prefill_value",
            # Prefill scaling components
            "cached_prefill_key_scale",
            "cached_prefill_value_scale"
        }

        page_components = {
            # Core page storage
            "key_pages",
            "value_pages",
            # Page management
            "page_status",
            "page_map",
            # Sequence tracking
            "sequence_lengths",
            "num_pages_used",
            # Position tracking
            "current_page",
            "current_page_position"
        }

        # Component handling logic
        if path_key in ar_components:
            # AR components maintain their state
            return decode_state_cache
        
        if path_key in prefill_components:
            # Prefill components get updated
            # Special handling for segment_id to maintain proper boundaries
            if path_key == "cache_prefill_segment_id":
                # Copy segment ID while preserving structure
                return jax.lax.dynamic_update_slice_in_dim(
                    decode_state_cache,
                    prefix_cache,
                    0,  # Start index
                    axis=0  # Assuming batch dimension
                )
            return prefix_cache

        if path_key in ["key_pages", "value_pages"]:
            def _update_pages(prefix_page_idx, state):
                """Update page content while maintaining cache coherency."""
                decode_state_pages, prefix_pages, page_map = state

                # Extract source page
                prefix_page = jax.lax.dynamic_index_in_dim(
                    prefix_pages, prefix_page_idx, axis=1, keepdims=False
                )
                # Get target location
                page_index = jax.lax.dynamic_index_in_dim(
                    page_map, prefix_page_idx, axis=0, keepdims=True
                )

                # Update state pages
                decode_state_pages = jax.lax.dynamic_update_slice_in_dim(
                    decode_state_pages, prefix_page, page_index[0], axis=1
                )
                return decode_state_pages, prefix_pages, page_map

            # Get page count for processing
            num_pages = jax.lax.dynamic_index_in_dim(
                prefix["cache"]["page_manager"]["num_pages_used"].value,
                slot,
                axis=0,
                keepdims=False
            )

            # Process all pages
            decode_state_cache, _, _ = jax.lax.fori_loop(
                0,
                num_pages,
                _update_pages,
                (
                    decode_state_cache,
                    prefix_cache,
                    prefix["cache"]["page_manager"]["page_map"].value[slot]
                )
            )
            return decode_state_cache

        if path_key in page_components:
            # Maintain page management state
            return decode_state_cache

        # Comprehensive error for unknown components
        raise ValueError(
            f"Unhandled cache component: {path_key}\n"
            f"AR components: {sorted(ar_components)}\n"
            f"Prefill components: {sorted(prefill_components)}\n"
            f"Page components: {sorted(page_components)}\n"
            "Please update component handling for this cache element.\n"
            f"Component categories:\n"
            f"- AR: Autoregressive cache state\n"
            f"- Prefill: Prefill cache state\n"
            f"- Page: Page management state"
        )

      inserted_cache = jax.tree_util.tree_map_with_path(
          copy_paged,
          unboxed_prefix["cache"],
          decode_state["cache"],
      )
    else:
      inserted_cache = jax.tree_util.tree_map_with_path(
          copy,
          unboxed_prefix["cache"],
          decode_state["cache"],
          self.kv_cache_annotations_named,
      )

    # Use dynamic_update_index_in_dim for all updates.
    inserted_logits = jax.lax.dynamic_update_index_in_dim(decode_state["logits"], unboxed_prefix["logits"], slot, 0)
    inserted_next_pos = jax.lax.dynamic_update_index_in_dim(decode_state["next_pos"], unboxed_prefix["next_pos"], slot, 0)
    inserted_generated_tokens = jax.lax.dynamic_update_index_in_dim(
        decode_state["generated_tokens"],
        unboxed_prefix["generated_tokens"],
        slot,
        0,
    )
    inserted_tokens = jax.lax.dynamic_update_index_in_dim(decode_state["tokens"], unboxed_prefix["tokens"], slot, 0)

    inserted_logits = jax.lax.with_sharding_constraint(inserted_logits, self.replicated_sharding)
    inserted_generated_tokens = jax.lax.with_sharding_constraint(inserted_generated_tokens, self.replicated_sharding)
    inserted_next_pos = jax.lax.with_sharding_constraint(inserted_next_pos, self.replicated_sharding)
    inserted_tokens = jax.lax.with_sharding_constraint(inserted_tokens, self.replicated_sharding)
    inserted_cache = jax.lax.with_sharding_constraint(inserted_cache, self.kv_cache_shardings)

    return {
        "logits": inserted_logits,
        "cache": inserted_cache,
        "next_pos": inserted_next_pos,
        "generated_tokens": inserted_generated_tokens,
        "tokens": inserted_tokens,
    }

  @functools.partial(
      jax.jit,
      static_argnums=(0,),
      static_argnames=(
          "num_prompts",
          "seq_len",
      ),
      donate_argnums=(
          1,
          2,
      ),
  )
  def insert_partial(
      self,
      prefix: PackedPrefix,
      decode_state: DecodeState,
      cache: Any,
      slots: jax.Array,
      *,
      start_indices: jax.Array,
      num_prompts: int,
      seq_len: int,
  ) -> DecodeState:
    """Insert into KV cache"""
    unboxed_prefix = max_utils.unbox_logicallypartioned(prefix)
    cache_unboxed = max_utils.unbox_logicallypartioned(cache)
    cache_unboxed = self._maybe_unstack_prefill_result_cache(cache_unboxed)
    start_idx = 0
    slot = slots[0]

    def copy(path, partial_cache, full_cache, annotations):
      path_key = path[-1].key
      if path_key in [
          "cache_ar_index",
          "cached_ar_key",
          "cached_ar_value",
          "cached_ar_key_scale",
          "cached_ar_value_scale",
      ]:
        return full_cache  # we don't even zero these out because we can mask them out.

      batch_idx = -1
      if "cache_batch" in annotations:
        batch_idx = annotations.index("cache_batch")
      elif "cache_scale_batch" in annotations:
        batch_idx = annotations.index("cache_scale_batch")

      if batch_idx < 0:
        raise ValueError(f"Batch index {batch_idx=} shouldn't be less than zero for {path_key}, got {annotations=}")

      if path_key == "cache_ar_segment_id":
        ### goal: zero this out in case there is existing data
        zeros = jnp.zeros((1, self.config.max_target_length - self.config.max_prefill_predict_length), dtype=jnp.int32)
        return jax.lax.dynamic_update_index_in_dim(full_cache, zeros, slot, batch_idx)
      elif path_key == "cache_prefill_segment_id":
        zeros = jnp.zeros((1, self.config.max_prefill_predict_length), dtype=jnp.int32)
        ## zero out in case prefill cache is too small to cover
        full_cache = jax.lax.dynamic_update_index_in_dim(full_cache, zeros, slot, batch_idx)
        ## copy prefill cache
        partial_cache = jax.lax.dynamic_slice(partial_cache, (0, start_idx), (1, seq_len))
        partial_cache = (partial_cache == partial_cache[0, 0]).astype(int)
        full_cache = jax.lax.dynamic_update_index_in_dim(full_cache, partial_cache, slot, batch_idx)
        return full_cache
      elif path_key == "cached_ar_lengths":
        return full_cache.at[slot].set(0)
      elif path_key in [
          "cached_prefill_key",
          "cached_prefill_value",
          "cached_prefill_key_scale",
          "cached_prefill_value_scale",
      ]:
        seqlen_index = self.config.prefill_cache_axis_order.split(",").index("1")
        start_indices = [0, 0, 0, 0]
        start_indices[seqlen_index] = start_idx
        slice_size = list(partial_cache.shape)
        slice_size[seqlen_index] = seq_len

        slice_size = tuple(slice_size)
        partial_cache = jax.lax.dynamic_slice(partial_cache, start_indices, slice_size)
        # jax.debug.print("start_indices: {}, slice_size: {}", start_indices, slice_size)

        return jax.lax.dynamic_update_index_in_dim(full_cache, partial_cache, slot, batch_idx)
      else:
        raise ValueError(f"We don't have a strategy for inserting {path_key}")

    inserted_cache = decode_state["cache"]
    inserted_logits = decode_state["logits"]
    inserted_next_pos = decode_state["next_pos"]
    inserted_generated_tokens = decode_state["generated_tokens"]
    inserted_tokens = decode_state["tokens"]

    for i in range(num_prompts):
      start_idx = start_indices[i]
      slot = slots[i]
      inserted_cache = jax.tree_util.tree_map_with_path(copy, cache_unboxed, inserted_cache, self.kv_cache_annotations_named)
      inserted_logits = jax.lax.dynamic_update_index_in_dim(inserted_logits, unboxed_prefix["logits"][i, ...], slot, 0)
      inserted_next_pos = jax.lax.dynamic_update_index_in_dim(inserted_next_pos, unboxed_prefix["next_pos"][i, ...], slot, 0)
      inserted_generated_tokens = jax.lax.dynamic_update_index_in_dim(
          inserted_generated_tokens,
          unboxed_prefix["generated_tokens"][i, ...],
          slot,
          0,
      )
      inserted_tokens = jax.lax.dynamic_update_index_in_dim(inserted_tokens, unboxed_prefix["tokens"][i, ...], slot, 0)

    inserted_logits = jax.lax.with_sharding_constraint(inserted_logits, self.replicated_sharding)
    inserted_generated_tokens = jax.lax.with_sharding_constraint(inserted_generated_tokens, self.replicated_sharding)
    inserted_next_pos = jax.lax.with_sharding_constraint(inserted_next_pos, self.replicated_sharding)
    inserted_tokens = jax.lax.with_sharding_constraint(inserted_tokens, self.replicated_sharding)
    inserted_cache = jax.lax.with_sharding_constraint(inserted_cache, self.kv_cache_shardings)

    return {
        "logits": inserted_logits,
        "cache": inserted_cache,
        "next_pos": inserted_next_pos,
        "generated_tokens": inserted_generated_tokens,
        "tokens": inserted_tokens,
    }

  def get_prefix_destination_sharding(self) -> Any:
    return {
        "logits": self.replicated_sharding,
        "cache": self.prefill_kv_cache_shardings,
        "next_pos": self.replicated_sharding,
        "generated_tokens": self.replicated_sharding,
        "tokens": self.replicated_sharding,
    }

  def get_tokenizer(self) -> tokenizer_pb2.TokenizerParameters:
    """Return a protobuf of tokenizer info, callable from Py or C++."""
    return tokenizer_pb2.TokenizerParameters(path=self.config.tokenizer_path, extra_ids=0)

  def build_tokenizer(self, metadata: tokenizer_pb2.TokenizerParameters) -> tokenizer_api.Tokenizer:
    """Return a tokenizer"""
    if "tiktoken" in metadata.path:
      return token_utils.TikToken(metadata)
    else:
      return token_utils.SentencePieceTokenizer(metadata)

  def init_decode_state(self, rng: Optional[PRNGKeyType] = None) -> DecodeState:
    """Initialize decode state with proper sharding support for paged attention.
    
    Args:
        rng: Optional PRNG key for initialization. If None, a default key is created.
        
    Returns:
        DecodeState: Initialized state for decoding with proper sharding and cache setup.
    """
    if rng is None:
        rng = jax.random.PRNGKey(0)

    def init(abstract_params):
        """Initialize the basic structure with placeholder values.
        
        This creates the initial arrays needed for decode state, including cache.
        """
        # Calculate batch size accounting for all devices
        batch_size = int(self.config.per_device_batch_size * jax.device_count())
        
        # Create dummy inputs for model.apply
        x = jnp.ones((batch_size, 1), dtype=jnp.int32)
        
        # Get initial cache structure through model application
        _, cache = self.model.apply(
            abstract_params,
            x,
            x,
            enable_dropout=False,
            model_mode=common_types.MODEL_MODE_AUTOREGRESSIVE,
            rngs={"params": rng},
            mutable=["cache"],
        )

        # Initialize position tracking arrays
        next_pos = jnp.zeros((batch_size, 1), dtype=jnp.int32)
        generated_tokens = jnp.zeros((batch_size, 1), dtype=jnp.int32)
        tokens = jnp.zeros((batch_size, 1), dtype=jnp.int32)

        # Return complete state dictionary
        return {
            "logits": jnp.zeros((batch_size, 1, self.config.vocab_size)),
            "cache": cache["cache"],
            "next_pos": next_pos,
            "generated_tokens": generated_tokens,
            "tokens": tokens,
        }

    # Get abstract shapes and partitioning specs
    with nn_partitioning.axis_rules(self.config.logical_axis_rules):
        abstract_outputs = jax.eval_shape(init, self.abstract_params)
    logical_annotations = nn.get_partition_spec(abstract_outputs)

    # Convert logical annotations to mesh annotations
    with self._mesh, nn_partitioning.axis_rules(self.config.logical_axis_rules):
        mesh_annotations = nn.logical_to_mesh(logical_annotations)

    # Create shardings from mesh annotations
    shardings = jax.tree_util.tree_map(
        lambda mesh_annotation: jax.sharding.NamedSharding(self._mesh, mesh_annotation),
        mesh_annotations,
    )

    # Initialize with proper sharding
    @functools.partial(jax.jit, out_shardings=shardings)
    def initialize():
        return jax.tree_util.tree_map(
            lambda x: jnp.zeros(x.shape, x.dtype), 
            abstract_outputs
        )

    # Get initial state and extract cache
    init_state = initialize()
    cache = init_state["cache"]

    def is_lp(k):
        """Check if a value is LogicallyPartitioned."""
        return isinstance(k, flax.linen.spmd.LogicallyPartitioned)

    def get_paged_attention_names(path_or_value, value):
        """Get sharding names for paged attention arrays.
        
        Args:
            path_or_value: Tuple representing path in the tree
            value: The actual value at this position
            
        Returns:
            tuple: Sharding names for this component
        """
        # Handle LogicallyPartitioned values
        if isinstance(value, flax.linen.spmd.LogicallyPartitioned):
            return tuple(value.names)
        
        # Handle path components
        if isinstance(path_or_value, tuple):
            if path_or_value:
                last_component = path_or_value[-1]
                component = getattr(last_component, 'key', str(last_component))
            else:
                return ()
        else:
            return ()

        # Define sharding rules for different components
        paged_attention_rules = {
            "key_pages": ("paged_kv_heads", "num_pages", "tokens_per_page", "paged_kv_head_dim_size"),
            "value_pages": ("paged_kv_heads", "num_pages", "tokens_per_page", "paged_kv_head_dim_size"),
            "page_status": ("num_pages",),
            "page_map": ("cache_batch", "num_pages"),
            "sequence_lengths": ("cache_batch",),
            "num_pages_used": ("cache_batch",),
            "current_page": ("cache_batch",),
            "current_page_position": ("cache_batch",)
        }
        
        return paged_attention_rules.get(str(component), ())

    def print_tree_structure(tree, prefix=""):
        """Print detailed information about a tree's structure.
        
        Args:
            tree: The tree to examine
            prefix: String prefix for output formatting
        """
        try:
            leaves = jax.tree_util.tree_leaves(tree)
            print(f"{prefix}Number of leaves: {len(leaves)}")
            print(f"{prefix}Leaf types: {[type(leaf) for leaf in leaves]}")
            print(f"{prefix}Tree structure: {jax.tree_util.tree_structure(tree)}")
        except Exception as e:
            print(f"{prefix}Error examining tree: {e}")

    def debug_path(path_or_value, value, depth=0):
        """Debug helper that prints path and value information before processing.
        
        Args:
            path_or_value: The path through the tree
            value: The value at this location
            depth: Current depth in the tree for indentation
        """
        indent = "  " * depth
        print(f"{indent}Path type: {type(path_or_value)}")
        print(f"{indent}Path value: {path_or_value}")
        print(f"{indent}Value type: {type(value)}")
        result = get_paged_attention_names(path_or_value, value)
        print(f"{indent}Returning sharding: {result}")
        return result

    # try:
    #     print("\nStarting cache tree analysis:")
    #     print_tree_structure(cache, "Cache tree: ")
        
    #     print("\nMapping tree with debug information:")
    #     self.kv_cache_annotations_named = jax.tree_util.tree_map_with_path(
    #         debug_path,
    #         cache,
    #         is_leaf=is_lp
    #     )
        
    #     print("\nFinal annotations structure:")
    #     print_tree_structure(
    #         self.kv_cache_annotations_named, 
    #         "Annotations tree: "
    #     )
            
    # except Exception as e:
    #     print(f"\nError during tree mapping: {e}")
    #     print(f"Cache type: {type(cache)}")
    #     print(f"Cache structure: {jax.tree_util.tree_structure(cache)}")
    #     raise

    # Return unboxed state
    zeroed = max_utils.unbox_logicallypartioned(init_state)
    return zeroed
  
  def free_slot(self, decode_state: dict, slot: int) -> None:
    """Release all pages and cached KV data for the given slot."""
    with self._mesh:
        cache = decode_state["cache"]
        if all(k in cache for k in ["page_status", "page_map"]):
            # Use dynamic_update_index_in_dim to set values
            num_pages = cache["num_pages_used"].value.shape[0]

            # Create temporary 2D arrays and update specific rows using dynamic_update_index_in_dim
            page_status_2d = jnp.zeros((num_pages, cache["page_status"].value.shape[0]), dtype=cache["page_status"].value.dtype)
            page_status_2d = jax.lax.dynamic_update_index_in_dim(page_status_2d, cache["page_status"].value, 0, axis=0)  
            page_status_2d = jax.lax.dynamic_update_index_in_dim(page_status_2d, jnp.zeros_like(cache["page_status"].value), slot, axis=0)
            cache["page_status"].value = page_status_2d[slot]  # Extract back to original shape

            page_map_2d = jnp.full((num_pages, *cache["page_map"].value.shape), -1, dtype=cache["page_map"].value.dtype)
            page_map_2d = jax.lax.dynamic_update_index_in_dim(page_map_2d, cache["page_map"].value, 0, axis=0)
            page_map_2d = jax.lax.dynamic_update_index_in_dim(page_map_2d, jnp.full_like(cache["page_map"].value, -1), slot, axis=0)
            cache["page_map"].value = page_map_2d[slot]

            sequence_lengths_2d = jnp.zeros((num_pages, cache["sequence_lengths"].value.shape[0]), dtype=cache["sequence_lengths"].value.dtype)
            sequence_lengths_2d = jax.lax.dynamic_update_index_in_dim(sequence_lengths_2d, cache["sequence_lengths"].value, 0, axis=0)
            sequence_lengths_2d = jax.lax.dynamic_update_index_in_dim(sequence_lengths_2d, jnp.zeros_like(cache["sequence_lengths"].value), slot, axis=0)
            cache["sequence_lengths"].value = sequence_lengths_2d[slot]

            num_pages_used_2d = jnp.zeros((num_pages, cache["num_pages_used"].value.shape[0]), dtype=cache["num_pages_used"].value.dtype)
            num_pages_used_2d = jax.lax.dynamic_update_index_in_dim(num_pages_used_2d, cache["num_pages_used"].value, 0, axis=0)
            num_pages_used_2d = jax.lax.dynamic_update_index_in_dim(num_pages_used_2d, jnp.zeros_like(cache["num_pages_used"].value), slot, axis=0)
            cache["num_pages_used"].value = num_pages_used_2d[slot]

            current_page_2d = jnp.full((num_pages, cache["current_page"].value.shape[0]), -1, dtype=cache["current_page"].value.dtype)
            current_page_2d = jax.lax.dynamic_update_index_in_dim(current_page_2d, cache["current_page"].value, 0, axis=0)
            current_page_2d = jax.lax.dynamic_update_index_in_dim(current_page_2d, jnp.full_like(cache["current_page"].value, -1), slot, axis=0)
            cache["current_page"].value = current_page_2d[slot]

            current_page_position_2d = jnp.zeros((num_pages, cache["current_page_position"].value.shape[0]), dtype=cache["current_page_position"].value.dtype)
            current_page_position_2d = jax.lax.dynamic_update_index_in_dim(current_page_position_2d, cache["current_page_position"].value, 0, axis=0)
            current_page_position_2d = jax.lax.dynamic_update_index_in_dim(current_page_position_2d, jnp.zeros_like(cache["current_page_position"].value), slot, axis=0)
            cache["current_page_position"].value = current_page_position_2d[slot]

  @property
  def max_concurrent_decodes(self) -> int:
    """Free slots."""
    return int(self.config.per_device_batch_size * jax.device_count())

  @property
  def max_prefill_length(self) -> int:
    """Maximum prefill length."""
    return int(self.config.max_prefill_predict_length)

  @property
  def samples_per_slot(self) -> int:
    """Number of samples per slot."""
    return 1

  @property
  def mesh(self) -> jax.sharding.Mesh:
    return self._mesh

  @property
  def colocated_cpus(self) -> None:
    """CPU devices colocated with the engine's accelerators."""
    raise NotImplementedError


def set_engine_vars_from_base_engine(
    engine: MaxEngine,
    base_engine: MaxEngine,
    rng: PRNGKeyType,
):
  """Set internal vars from base_engine, which has already loaded the checkpoint and has sharding,
  mesh, and kv cache related vars set.
  """
  engine.model.quant.quant_mode = base_engine.model.quant.quant_mode
  engine.state_mesh_annotations = base_engine.state_mesh_annotations
  engine.abstract_params = base_engine.abstract_params
  engine.kv_cache_annotations = max_utils.get_kv_cache_annotations(engine.model, engine.config, rng, engine.mesh)  # pylint: disable=protected-access
  engine.kv_cache_shardings = jax.tree_util.tree_map(
      lambda x: jax.sharding.NamedSharding(engine.mesh, x),
      engine.kv_cache_annotations,  # pylint: disable=protected-access
  )


def create_engine_from_config_flags(batch_size, max_prefill_predict_length, max_target_length, args_str):
  """Create new MaxEngine instance with given batch_size, prefill and target lengths, and any config
  params provided through `args_str`.
  """
  args = {}
  args["scan_layers"] = "false"
  args["async_checkpointing"] = "false"
  args["ici_fsdp_parallelism"] = "1"
  args["ici_autoregressive_parallelism"] = "1"
  args["ici_tensor_parallelism"] = "-1"
  args["weight_dtype"] = "bfloat16"
  args["attention"] = "dot_product"

  # batch and cache related
  args["max_prefill_predict_length"] = f"{max_prefill_predict_length}"
  args["max_target_length"] = f"{max_target_length}"
  args["per_device_batch_size"] = f"{batch_size}"
  print(f"Command line args: {args_str}")
  cmd_args = args_str.split(" ")
  for cmd_arg in cmd_args:
    k, v = cmd_arg.split("=")
    args[k.strip()] = v.strip()
  assert "load_parameters_path" in args, "load_parameters_path must be defined"
  updated_args = ["MaxText/maxengine_server.py", "/mnt/disks/persist/maxtext/MaxText/configs/base.yml"]
  for k, v in args.items():
    option = f"{k}={v}"
    updated_args.append(option)
  print(f"Invoking maxengine with args:\n \t{updated_args}")
  pyconfig.initialize(updated_args)
  cfg = MaxEngineConfig(cp.deepcopy(pyconfig._config.keys))  # pylint: disable=protected-access
  engine = MaxEngine(cfg)
  return engine
