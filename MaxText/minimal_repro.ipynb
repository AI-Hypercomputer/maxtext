{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b95492-c81f-479d-bf13-632fce560914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 02:58:47.095580: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731725927.108842  284214 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731725927.112809  284214 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating keys from env and command line: ['run_name', 'model_name', 'enable_checkpointing', 'dtype', 'quantization', 'weight_dtype', 'megablox', 'scan_layers', 'attention', 'ici_fsdp_parallelism', 'ici_tensor_parallelism', 'ici_autoregressive_parallelism', 'tokenizer_path', 'per_device_batch_size', 'max_target_length', 'max_prefill_predict_length']\n",
      "Running Model: mixtral-8x7b\n",
      "Updating following parameters in config\n",
      "\n",
      "base_emb_dim: 4096\n",
      "base_num_query_heads: 32\n",
      "base_num_kv_heads: 8\n",
      "base_mlp_dim: 14336\n",
      "base_num_decoder_layers: 1\n",
      "head_dim: 128\n",
      "mlp_activations: ['silu', 'linear']\n",
      "vocab_size: 32000\n",
      "enable_dropout: False\n",
      "logits_via_embedding: False\n",
      "normalization_layer_epsilon: 1e-05\n",
      "num_experts: 8\n",
      "num_experts_per_tok: 2\n",
      "rope_max_timescale: 1000000\n",
      "decoder_block: mistral\n",
      "Updating keys from model: ['base_emb_dim', 'base_num_query_heads', 'base_num_kv_heads', 'base_mlp_dim', 'base_num_decoder_layers', 'head_dim', 'mlp_activations', 'vocab_size', 'enable_dropout', 'logits_via_embedding', 'normalization_layer_epsilon', 'num_experts', 'num_experts_per_tok', 'rope_max_timescale', 'decoder_block']\n",
      "Not using emergency checkpoint, ignoring local_checkpoint_directory and local_checkpoint_period\n",
      "dataset_type set to tfds, will use keys['dataset_path']='' and keys['dataset_name']='c4/en:3.0.1'\n",
      "Config param activations_in_float32: False\n",
      "Config param adam_b1: 0.9\n",
      "Config param adam_b2: 0.95\n",
      "Config param adam_eps: 1e-08\n",
      "Config param adam_eps_root: 0.0\n",
      "Config param adam_weight_decay: 0.1\n",
      "Config param add_bos: True\n",
      "Config param add_eos: True\n",
      "Config param allow_split_physical_axes: False\n",
      "Config param ar_cache_axis_order: 1,2,0,3\n",
      "Config param async_checkpointing: True\n",
      "Config param attention: dot_product\n",
      "Config param attention_type: global\n",
      "Config param attn_logits_soft_cap: None\n",
      "Config param autoregressive_decode_assert: \n",
      "Config param base_emb_dim: 4096\n",
      "Config param base_mlp_dim: 14336\n",
      "Config param base_num_decoder_layers: 1\n",
      "Config param base_num_kv_heads: 8\n",
      "Config param base_num_query_heads: 32\n",
      "Config param base_output_directory: \n",
      "Config param capacity_factor: -1.0\n",
      "Config param cast_logits_to_fp32: True\n",
      "Config param checkpoint_dir: moe_test/checkpoints/\n",
      "Config param checkpoint_is_quantized: False\n",
      "Config param checkpoint_period: 10000\n",
      "Config param checkpoint_storage_target_data_file_size_bytes: 2147483648\n",
      "Config param checkpoint_storage_use_ocdbt: True\n",
      "Config param checkpoint_storage_use_zarr3: True\n",
      "Config param collect_stack_trace: False\n",
      "Config param compile_topology: \n",
      "Config param compile_topology_num_slices: -1\n",
      "Config param compiled_trainstep_file: \n",
      "Config param compute_axis_order: 0,1,2,3\n",
      "Config param cosine_learning_rate_final_fraction: 0.1\n",
      "Config param custom_mesh: \n",
      "Config param data_sharding: (('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'tensor', 'expert', 'autoregressive'),)\n",
      "Config param data_shuffle_seed: 0\n",
      "Config param dataset_name: c4/en:3.0.1\n",
      "Config param dataset_path: \n",
      "Config param dataset_type: tfds\n",
      "Config param dcn_autoregressive_parallelism: 1\n",
      "Config param dcn_data_parallelism: -1\n",
      "Config param dcn_expert_parallelism: 1\n",
      "Config param dcn_fsdp_parallelism: 1\n",
      "Config param dcn_fsdp_transpose_parallelism: 1\n",
      "Config param dcn_pipeline_parallelism: 1\n",
      "Config param dcn_sequence_parallelism: 1\n",
      "Config param dcn_tensor_parallelism: 1\n",
      "Config param decode_sampling_nucleus_p: -1\n",
      "Config param decode_sampling_strategy: greedy\n",
      "Config param decode_sampling_temperature: 1.0\n",
      "Config param decode_sampling_top_k: 0\n",
      "Config param decoder_block: mistral\n",
      "Config param decoder_layer_input: device\n",
      "Config param dropout_rate: 0.0\n",
      "Config param dtype: bfloat16\n",
      "Config param emb_dim: 4096\n",
      "Config param enable_checkpoint_cloud_logger: False\n",
      "Config param enable_checkpoint_standard_logger: False\n",
      "Config param enable_checkpointing: False\n",
      "Config param enable_data_shuffling: True\n",
      "Config param enable_dropout: False\n",
      "Config param enable_emergency_checkpoint: False\n",
      "Config param enable_goodput_recording: True\n",
      "Config param enable_jax_profiler: False\n",
      "Config param enable_model_warmup: False\n",
      "Config param enable_single_controller: False\n",
      "Config param enable_single_replica_ckpt_restoring: False\n",
      "Config param eval_data_column: text\n",
      "Config param eval_dataset_name: c4/en:3.0.1\n",
      "Config param eval_interval: -1\n",
      "Config param eval_per_device_batch_size: 1.0\n",
      "Config param eval_split: validation\n",
      "Config param eval_steps: -1\n",
      "Config param expansion_factor_real_data: -1\n",
      "Config param final_logits_soft_cap: None\n",
      "Config param force_unroll: False\n",
      "Config param fused_mlp: False\n",
      "Config param fused_qkv: False\n",
      "Config param gcs_metrics: False\n",
      "Config param global_batch_size_to_eval_on: 4\n",
      "Config param global_batch_size_to_load: 4\n",
      "Config param global_batch_size_to_load_eval: 4\n",
      "Config param global_batch_size_to_train_on: 4\n",
      "Config param global_parameter_scale: 1\n",
      "Config param goodput_upload_interval_seconds: 60\n",
      "Config param gradient_accumulation_steps: 1\n",
      "Config param gradient_clipping_threshold: 1.0\n",
      "Config param grain_eval_files: \n",
      "Config param grain_train_files: \n",
      "Config param grain_worker_count: 1\n",
      "Config param hardware: tpu\n",
      "Config param head_dim: 128\n",
      "Config param hf_access_token: \n",
      "Config param hf_data_dir: \n",
      "Config param hf_eval_files: \n",
      "Config param hf_eval_split: \n",
      "Config param hf_path: \n",
      "Config param hf_train_files: \n",
      "Config param ici_autoregressive_parallelism: 1\n",
      "Config param ici_data_parallelism: 1\n",
      "Config param ici_expert_parallelism: 1\n",
      "Config param ici_fsdp_parallelism: 1\n",
      "Config param ici_fsdp_transpose_parallelism: 1\n",
      "Config param ici_pipeline_parallelism: 1\n",
      "Config param ici_sequence_parallelism: 1\n",
      "Config param ici_tensor_parallelism: 4\n",
      "Config param inference_metadata_file: \n",
      "Config param inference_microbenchmark_log_file_path: \n",
      "Config param inference_microbenchmark_loop_iters: 10\n",
      "Config param inference_microbenchmark_prefill_lengths: 64,128,256,512,1024\n",
      "Config param inference_microbenchmark_stages: prefill,generate\n",
      "Config param init_weights_seed: 0\n",
      "Config param jax_cache_dir: ~/jax_cache\n",
      "Config param jax_profiler_port: 9999\n",
      "Config param key_proj: remat\n",
      "Config param kv_quant_axis: heads_and_dkv\n",
      "Config param kv_quant_dtype: int8\n",
      "Config param learning_rate: 3e-05\n",
      "Config param learning_rate_schedule_steps: 150001\n",
      "Config param load_balance_loss_weight: 0.01\n",
      "Config param load_from_prefill_dir: False\n",
      "Config param load_full_state_path: \n",
      "Config param load_parameters_path: \n",
      "Config param local_checkpoint_directory: \n",
      "Config param local_checkpoint_period: 0\n",
      "Config param log_period: 100\n",
      "Config param logical_axis_rules: (('activation_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_batch_no_exp', ('data', 'fsdp', 'fsdp_transpose')), ('activation_embed_and_logits_batch', ('data', 'stage', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_heads', ('tensor', 'sequence')), ('activation_kv_heads', ('tensor', 'sequence')), ('activation_length', 'sequence'), ('activation_embed', 'tensor'), ('activation_mlp', 'tensor'), ('activation_kv', 'tensor'), ('activation_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_head_dim', 'tensor'), ('activation_vocab', ('tensor', 'sequence')), ('activation_vocab', 'tensor'), ('activation_vocab', 'sequence'), ('activation_stage', 'stage'), ('activation_exp', 'expert'), ('mlp', ('fsdp_transpose', 'tensor', 'autoregressive')), ('vocab', ('tensor', 'autoregressive')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'expert')), ('embed', ('fsdp', 'sequence', 'expert')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence')), ('embed_no_exp', ('fsdp', 'sequence')), ('norm', 'tensor'), ('heads', ('tensor', 'autoregressive')), ('layers', 'stage'), ('kv', ()), ('kv_heads', ('tensor', 'autoregressive')), ('kv_head_dim', ()), ('cache_batch', ()), ('cache_heads', ('autoregressive', 'tensor')), ('cache_kv', ()), ('cache_sequence', ()), ('exp', 'expert'))\n",
      "Config param logits_dot_in_fp32: False\n",
      "Config param logits_via_embedding: False\n",
      "Config param matmul_precision: default\n",
      "Config param max_checkify: False\n",
      "Config param max_corpus_chars: 10000000\n",
      "Config param max_prefill_predict_length: 1024\n",
      "Config param max_target_length: 2048\n",
      "Config param megablox: True\n",
      "Config param mesh_axes: ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'tensor', 'expert', 'autoregressive']\n",
      "Config param metrics_dir: moe_test/metrics/\n",
      "Config param metrics_file: \n",
      "Config param micro_batch_size_to_eval_on: 4\n",
      "Config param micro_batch_size_to_train_on: 4\n",
      "Config param mlp_activations: ['silu', 'linear']\n",
      "Config param mlp_dim: 14336\n",
      "Config param mlpwi: remat\n",
      "Config param mlpwi_0: remat\n",
      "Config param mlpwi_1: remat\n",
      "Config param mlpwo: remat\n",
      "Config param model_name: mixtral-8x7b\n",
      "Config param monitor_goodput: True\n",
      "Config param normalization_layer_epsilon: 1e-05\n",
      "Config param normalize_embedding_logits: True\n",
      "Config param num_decoder_layers: 1\n",
      "Config param num_experts: 8\n",
      "Config param num_experts_per_tok: 2\n",
      "Config param num_kv_heads: 8\n",
      "Config param num_layers_per_pipeline_stage: 1\n",
      "Config param num_pipeline_microbatches: -1\n",
      "Config param num_pipeline_repeats: -1\n",
      "Config param num_query_heads: 32\n",
      "Config param num_slices: 1\n",
      "Config param opt_type: adamw\n",
      "Config param out_proj: remat\n",
      "Config param param_scan_axis: 1\n",
      "Config param per_device_batch_size: 1.0\n",
      "Config param pipeline_delay_activation_forwarding: False\n",
      "Config param prefill_cache_axis_order: 1,2,0,3\n",
      "Config param prefill_cache_dir: \n",
      "Config param profile_cleanly: True\n",
      "Config param profiler: \n",
      "Config param profiler_steps: 5\n",
      "Config param prometheus_port: 0\n",
      "Config param prompt: I love to\n",
      "Config param quant_cfg_path: \n",
      "Config param quantization: int8\n",
      "Config param quantization_local_shard_count: 1\n",
      "Config param quantize_kvcache: False\n",
      "Config param query_proj: remat\n",
      "Config param ragged_block_size: 256\n",
      "Config param record_internal_nn_metrics: 0\n",
      "Config param remat_policy: full\n",
      "Config param reshape_q: False\n",
      "Config param reuse_example_batch: 0\n",
      "Config param rope_max_timescale: 1000000\n",
      "Config param rope_min_timescale: 1\n",
      "Config param run_name: moe_test\n",
      "Config param sa_block_q: 512\n",
      "Config param sa_block_q_dkv: 512\n",
      "Config param sa_block_q_dq: 512\n",
      "Config param save_config_to_gcs: False\n",
      "Config param save_quantized_params_path: \n",
      "Config param scan_layers: False\n",
      "Config param scan_pipeline_iterations: True\n",
      "Config param skip_first_n_steps_for_profiler: 1\n",
      "Config param sliding_window_size: 0\n",
      "Config param stack_trace_interval_seconds: 600\n",
      "Config param stack_trace_to_cloud: False\n",
      "Config param steps: 150001\n",
      "Config param target_eval_loss: 0.0\n",
      "Config param tensorboard_dir: moe_test/tensorboard/\n",
      "Config param tokenize_eval_data: True\n",
      "Config param tokenize_train_data: True\n",
      "Config param tokenizer_path: assets/tokenizer.mistral-v3\n",
      "Config param train_data_column: text\n",
      "Config param trainable_position_size: -1\n",
      "Config param upload_all_profiler_results: False\n",
      "Config param use_iota_embed: False\n",
      "Config param use_post_attn_norm: False\n",
      "Config param use_post_ffw_norm: False\n",
      "Config param use_ragged_attention: False\n",
      "Config param use_untrainable_positional_embedding: False\n",
      "Config param use_vertex_tensorboard: False\n",
      "Config param using_pipeline_parallelism: False\n",
      "Config param value_proj: remat\n",
      "Config param vertex_tensorboard_project: \n",
      "Config param vertex_tensorboard_region: \n",
      "Config param vocab_size: 32000\n",
      "Config param warmup_steps_fraction: 0.1\n",
      "Config param weight_dtype: bfloat16\n"
     ]
    }
   ],
   "source": [
    "import pyconfig\n",
    "\"\"\"\n",
    "  MaxText/configs/base.yml \\\n",
    "  tokenizer_path=${TOKENIZER_PATH} \\\n",
    "  max_prefill_predict_length=${MAX_PREFILL_PREDICT_LENGTH} \\\n",
    "  max_target_length=${MAX_TARGET_LENGTH} \\\n",
    "  model_name=${MODEL_NAME} \\\n",
    "  ici_fsdp_parallelism=${ICI_FSDP_PARALLELISM} \\\n",
    "  ici_autoregressive_parallelism=${ICI_AUTOREGRESSIVE_PARALLELISM} \\\n",
    "  ici_tensor_parallelism=${ICI_TENSOR_PARALLELISM} \\\n",
    "  scan_layers=${SCAN_LAYERS} \\\n",
    "  weight_dtype=${WEIGHT_DTYPE} \\\n",
    "  dtype=${DTYPE} \\\n",
    "  attention=dot_product \\\n",
    "  per_device_batch_size=${PER_DEVICE_BATCH_SIZE} megablox=${MEGABLOX} quantization=${QUANTIZATION}\n",
    "\"\"\"\n",
    "\n",
    "pyconfig.initialize(\n",
    "    [None, \"configs/base.yml\"],\n",
    "    run_name=\"moe_test\",\n",
    "    tokenizer_path=\"assets/tokenizer.mistral-v3\",\n",
    "    max_prefill_predict_length=1024,\n",
    "    max_target_length=2048,\n",
    "    ici_fsdp_parallelism=1,\n",
    "    ici_autoregressive_parallelism=1,\n",
    "    ici_tensor_parallelism=4,\n",
    "    scan_layers=False,\n",
    "    weight_dtype=\"bfloat16\",\n",
    "    enable_checkpointing=False,\n",
    "    model_name=\"mixtral-8x7b\",\n",
    "    dtype=\"bfloat16\",\n",
    "    attention=\"dot_product\",\n",
    "    megablox=True,  # won't be an issue without megablox\n",
    "    quantization=\"int8\",\n",
    "    per_device_batch_size=1,\n",
    ")\n",
    "config = pyconfig.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "809315d1-7e49-463e-b893-7317ce28c327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_devices: 4, shape (1, 1, 1, 1, 1, 4, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "from layers import models, quantizations\n",
    "import max_utils\n",
    "import jax\n",
    "\n",
    "devices_array = max_utils.create_device_mesh(config)\n",
    "quant = quantizations.configure_quantization(config)\n",
    "mesh = jax.sharding.Mesh(devices_array, config.mesh_axes)\n",
    "model = models.Transformer(config, mesh=mesh, quant=quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee5d5866-2cbf-43f8-8ddf-244d778d336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb00a5ea-4d86-4c58-9180-83483b1d618a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No decode checkpoint specified - generating random weights.\n",
      "Running MoE megablox implementation.\n",
      "No existing checkpoints found, not restoring checkpoint.\n",
      "Running MoE megablox implementation.\n"
     ]
    }
   ],
   "source": [
    "rng1, rng2, rng3 = jax.random.split(rng, 3)\n",
    "state, state_mesh_annotations = max_utils.setup_decode_state(model, config, rng1, mesh, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e79b83e8-6834-40e2-b347-308e904bbf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaxlib\n",
    "\n",
    "abstract_params = jax.tree_util.tree_map(\n",
    "        lambda x: jax.ShapeDtypeStruct(shape=x.shape, dtype=x.dtype, sharding=x.sharding)\n",
    "        if isinstance(x, jaxlib.xla_extension.ArrayImpl)\n",
    "        else None,\n",
    "        state.params,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c4faea6-e1d6-4302-a056-8d003438c669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MoE megablox implementation.\n"
     ]
    }
   ],
   "source": [
    "kv_cache_annotations = max_utils.get_kv_cache_annotations(model, config, rng2, mesh)\n",
    "kv_cache_shardings = jax.tree_util.tree_map(\n",
    "        lambda x: jax.sharding.NamedSharding(mesh, x), kv_cache_annotations\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d1490d7-554a-47a0-b61f-878496980485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import common_types\n",
    "\n",
    "@jax.jit\n",
    "def model_apply(_p, _rng):\n",
    "  return model.apply(\n",
    "      _p | {\"aqt\": {}},\n",
    "      jnp.ones((1, config.max_prefill_predict_length), dtype=jnp.int32),\n",
    "      jnp.ones((1, config.max_prefill_predict_length), dtype=jnp.int32),\n",
    "      decoder_segment_ids=jnp.zeros((1, config.max_prefill_predict_length), dtype=jnp.int32),\n",
    "      enable_dropout=False,\n",
    "      model_mode=common_types.MODEL_MODE_PREFILL,\n",
    "      rngs={\"params\": _rng},\n",
    "      mutable=True,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e00ab63-bd39-4421-911c-04dde874a0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MoE megablox implementation.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "add got incompatible shapes for broadcasting: (4, 256, 1024), (1, 1024, 4096).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _, new_vars \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m, in \u001b[0;36mmodel_apply\u001b[0;34m(_p, _rng)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;129m@jax\u001b[39m\u001b[38;5;241m.\u001b[39mjit\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_apply\u001b[39m(_p, _rng):\n\u001b[0;32m----> 6\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_p\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maqt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m      \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_prefill_predict_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m      \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_prefill_predict_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdecoder_segment_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_prefill_predict_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m      \u001b[49m\u001b[43menable_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommon_types\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL_MODE_PREFILL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m      \u001b[49m\u001b[43mrngs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_rng\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmutable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "File \u001b[0;32m~/maxtext/MaxText/layers/models.py:473\u001b[0m, in \u001b[0;36mTransformer.__call__\u001b[0;34m(self, decoder_input_tokens, decoder_positions, decoder_segment_ids, enable_dropout, model_mode)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder_segment_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m model_mode \u001b[38;5;241m==\u001b[39m common_types\u001b[38;5;241m.\u001b[39mMODEL_MODE_AUTOREGRESSIVE:\n\u001b[1;32m    468\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    469\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuring autoregressive decoding we assume the tokens are in the active sequence\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    470\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m which is always \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommon_types\u001b[38;5;241m.\u001b[39mDECODING_ACTIVE_SEQUENCE_INDICATOR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    471\u001b[0m   )\n\u001b[0;32m--> 473\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_positions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_positions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_segment_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_segment_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43menable_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/maxtext/MaxText/layers/models.py:385\u001b[0m, in \u001b[0;36mDecoder.__call__\u001b[0;34m(self, decoder_input_tokens, decoder_positions, decoder_segment_ids, deterministic, model_mode)\u001b[0m\n\u001b[1;32m    383\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m lyr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mnum_decoder_layers):\n\u001b[0;32m--> 385\u001b[0m       y \u001b[38;5;241m=\u001b[39m \u001b[43mRemattedBlockLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayers_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlyr\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m          \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m          \u001b[49m\u001b[43mdecoder_segment_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m          \u001b[49m\u001b[43mdecoder_positions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m          \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_norm_layer()(\n\u001b[1;32m    394\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    395\u001b[0m     weight_dtype\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mweight_dtype,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m     kernel_axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm\u001b[39m\u001b[38;5;124m\"\u001b[39m,),\n\u001b[1;32m    399\u001b[0m )(y)\n\u001b[1;32m    400\u001b[0m y \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(rate\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdropout_rate, broadcast_dims\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,))(y, deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "File \u001b[0;32m~/maxtext/MaxText/layers/mistral.py:153\u001b[0m, in \u001b[0;36mMistralDecoderLayer.__call__\u001b[0;34m(self, inputs, decoder_segment_ids, decoder_positions, deterministic, model_mode)\u001b[0m\n\u001b[1;32m    141\u001b[0m   mlp_lnx \u001b[38;5;241m=\u001b[39m linears\u001b[38;5;241m.\u001b[39mMlpBlock(\n\u001b[1;32m    142\u001b[0m       intermediate_dim\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mmlp_dim,\n\u001b[1;32m    143\u001b[0m       activations\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mmlp_activations,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m       quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant,\n\u001b[1;32m    150\u001b[0m   )(hidden_states, deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n\u001b[1;32m    151\u001b[0m   mlp_lnx \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mwith_logical_constraint(mlp_lnx, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivation_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivation_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivation_embed\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 153\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mmlp_lnx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mintermediate_inputs\u001b[49m\n\u001b[1;32m    154\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(rate\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdropout_rate, broadcast_dims\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,))(layer_output, deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n\u001b[1;32m    156\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mwith_logical_constraint(\n\u001b[1;32m    157\u001b[0m     layer_output,\n\u001b[1;32m    158\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivation_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivation_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivation_embed\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    159\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:1050\u001b[0m, in \u001b[0;36m_forward_operator_to_aval.<locals>.op\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mop\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m-> 1050\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:573\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    571\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 573\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/numpy/ufunc_api.py:177\u001b[0m, in \u001b[0;36mufunc.__call__\u001b[0;34m(self, out, where, *args)\u001b[0m\n\u001b[1;32m    175\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere argument of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    176\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__static_props[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_vectorized\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/numpy/ufuncs.py:1157\u001b[0m, in \u001b[0;36m_add\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add two arrays element-wise.\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m \n\u001b[1;32m   1133\u001b[0m \u001b[38;5;124;03mJAX implementation of :obj:`numpy.add`. This is a universal function,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;124;03m  Array([10, 11, 12, 13], dtype=int32)\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m x, y \u001b[38;5;241m=\u001b[39m promote_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m, x, y)\n\u001b[0;32m-> 1157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mbitwise_or(x, y)\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/lax/lax.py:2072\u001b[0m, in \u001b[0;36mbroadcasting_shape_rule\u001b[0;34m(name, *avals)\u001b[0m\n\u001b[1;32m   2070\u001b[0m       result_shape\u001b[38;5;241m.\u001b[39mappend(non_1s[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   2071\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2072\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m got incompatible shapes for broadcasting: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2073\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtuple\u001b[39m,\u001b[38;5;250m \u001b[39mshapes)))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   2075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(result_shape)\n",
      "\u001b[0;31mTypeError\u001b[0m: add got incompatible shapes for broadcasting: (4, 256, 1024), (1, 1024, 4096)."
     ]
    }
   ],
   "source": [
    "_, new_vars = model_apply(state.params, rng)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
