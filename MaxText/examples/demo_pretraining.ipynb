{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bef36305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in /home/mazumdera_google_com/.venv-py312/lib/python3.12/site-packages (1.6.0)\n"
     ]
    }
   ],
   "source": [
    "# Use nest_asyncio to allow nested event loops in notebooks\n",
    "!pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e986cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added '/home/mazumdera_google_com/maxtext' to sys.path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "# This will be '.../maxtext/MaxText/scratch_code'\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Navigate two levels up to get to the project root 'maxtext'\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..', '..'))\n",
    "\n",
    "# Add the project root to the system path if it's not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added '{project_root}' to sys.path\")\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f53124",
   "metadata": {},
   "source": [
    "Sanity test to ensure we have TPU devices accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a545acd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
       " TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
       " TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
       " TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
       " TpuDevice(id=4, process_index=0, coords=(0,2,0), core_on_chip=0),\n",
       " TpuDevice(id=5, process_index=0, coords=(1,2,0), core_on_chip=0),\n",
       " TpuDevice(id=6, process_index=0, coords=(0,3,0), core_on_chip=0),\n",
       " TpuDevice(id=7, process_index=0, coords=(1,3,0), core_on_chip=0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "jax.distributed.initialize() # distributed.initialize should only be called once.\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2836f120",
   "metadata": {},
   "source": [
    "Import MaxText and its components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab2e1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 23:56:47.230922: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756252607.244333 1781995 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756252607.248280 1781995 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756252607.260243 1781995 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756252607.260256 1781995 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756252607.260258 1781995 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756252607.260260 1781995 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "/home/mazumdera_google_com/.venv-py312/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-08-26 23:56:50.023550: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/mazumdera_google_com/.venv-py312/lib/python3.12/site-packages/google/cloud/resourcemanager_v3/services/folders/async_client.py:27: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import MaxText as mt\n",
    "from MaxText import pyconfig\n",
    "from MaxText import maxtext_utils\n",
    "import numpy as np\n",
    "from MaxText.input_pipeline import _input_pipeline_utils\n",
    "import os\n",
    "import datetime\n",
    "from MaxText.globals import PKG_DIR\n",
    "from MaxText import max_logging\n",
    "from MaxText import common_types\n",
    "import jax\n",
    "from MaxText import train_utils\n",
    "from MaxText.train import train_step, eval_step\n",
    "from MaxText.data_loader import DataLoader\n",
    "from MaxText import checkpointing\n",
    "from flax import linen as nn\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d2de93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating keys from env and command line: ['run_name', 'model_name', 'load_parameters_path', 'async_checkpointing', 'tokenizer_path', 'tokenizer_type', 'per_device_batch_size', 'dataset_type', 'hf_access_token', 'max_target_length', 'max_prefill_predict_length']\n",
      "Running Model: llama3.1-8b\n",
      "Updating following parameters in config\n",
      "\n",
      "base_emb_dim: 4096\n",
      "base_num_query_heads: 32\n",
      "base_num_kv_heads: 8\n",
      "base_num_decoder_layers: 32\n",
      "base_mlp_dim: 14336\n",
      "head_dim: 128\n",
      "mlp_activations: ['silu', 'linear']\n",
      "vocab_size: 128256\n",
      "enable_dropout: False\n",
      "logits_via_embedding: False\n",
      "normalization_layer_epsilon: 1e-05\n",
      "rope_max_timescale: 500000\n",
      "decoder_block: llama2\n",
      "Updating keys from model: ['base_emb_dim', 'base_num_query_heads', 'base_num_kv_heads', 'base_num_decoder_layers', 'base_mlp_dim', 'head_dim', 'mlp_activations', 'vocab_size', 'enable_dropout', 'logits_via_embedding', 'normalization_layer_epsilon', 'rope_max_timescale', 'decoder_block']\n",
      "Jax distributed system is already initialized.\n",
      "Not using emergency checkpoint, ignoring local_checkpoint_directory, local_checkpoint_period, use_replicator_service and replicator_backup_interval_minutes\n",
      "dataset_type set to hf, will use keys['hf_path']='', keys['hf_data_dir']='' and keys['hf_train_files']='' to read data\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "hf_path can't be empty when dataset_type=hf",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Replace path to your Llama3.1-8b checkpoint for the `load_parameters_path` argument.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m config = \u001b[43mpyconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../configs/base.yml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_target_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_prefill_predict_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtiktoken\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43massets/tokenizer_llama3.tiktoken/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_parameters_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgs://maxtext-model-checkpoints/llama3.1-8b/2025-01-23-19-04/scanned/0/items\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mllama3.1-8b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43masync_checkpointing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhf_access_token\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhf_QrrySpgzbXIYpouWPPjQXzlEvrtdrpqZeC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     15\u001b[39m \n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m model = mt.from_config(config)\n\u001b[32m     19\u001b[39m mesh = model.mesh\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/maxtext/MaxText/pyconfig.py:1195\u001b[39m, in \u001b[36minitialize\u001b[39m\u001b[34m(argv, **kwargs)\u001b[39m\n\u001b[32m   1194\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minitialize\u001b[39m(argv, **kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m1195\u001b[39m   _config = \u001b[43m_HyperParameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1196\u001b[39m   config = HyperParameters(_config)\n\u001b[32m   1197\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/maxtext/MaxText/pyconfig.py:544\u001b[39m, in \u001b[36m_HyperParameters.__init__\u001b[39m\u001b[34m(self, argv, **kwargs)\u001b[39m\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m raw_keys[\u001b[33m\"\u001b[39m\u001b[33mjax_cache_dir\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    542\u001b[39m   compilation_cache.set_cache_dir(os.path.expanduser(raw_keys[\u001b[33m\"\u001b[39m\u001b[33mjax_cache_dir\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m \u001b[43m_HyperParameters\u001b[49m\u001b[43m.\u001b[49m\u001b[43muser_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m raw_keys[\u001b[33m\"\u001b[39m\u001b[33mdataset_type\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mc4_mlperf\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m raw_keys[\u001b[33m\"\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mgpt3-175b\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    546\u001b[39m   _HyperParameters.configure_gpt3_task(raw_keys)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/maxtext/MaxText/pyconfig.py:661\u001b[39m, in \u001b[36m_HyperParameters.user_init\u001b[39m\u001b[34m(raw_keys)\u001b[39m\n\u001b[32m    659\u001b[39m validate_keys(raw_keys)\n\u001b[32m    660\u001b[39m validate_tokenizer(raw_keys)\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m \u001b[43mvalidate_data_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    662\u001b[39m validate_constant_bound(raw_keys)\n\u001b[32m    663\u001b[39m validate_quantization_methods(raw_keys)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/maxtext/MaxText/pyconfig.py:267\u001b[39m, in \u001b[36mvalidate_data_input\u001b[39m\u001b[34m(keys)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keys[\u001b[33m\"\u001b[39m\u001b[33mdataset_type\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mhf\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    264\u001b[39m   max_logging.log(\n\u001b[32m    265\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdataset_type set to hf, will use \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeys[\u001b[33m'\u001b[39m\u001b[33mhf_path\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeys[\u001b[33m'\u001b[39m\u001b[33mhf_data_dir\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeys[\u001b[33m'\u001b[39m\u001b[33mhf_train_files\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m to read data\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    266\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m   \u001b[38;5;28;01massert\u001b[39;00m keys[\u001b[33m\"\u001b[39m\u001b[33mhf_path\u001b[39m\u001b[33m\"\u001b[39m] != \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mhf_path can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be empty when dataset_type=hf\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    268\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m keys[\u001b[33m\"\u001b[39m\u001b[33mhf_train_files\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    269\u001b[39m     keys[\u001b[33m\"\u001b[39m\u001b[33mhf_train_files\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mAssertionError\u001b[39m: hf_path can't be empty when dataset_type=hf"
     ]
    }
   ],
   "source": [
    "# Replace path to your Llama3.1-8b checkpoint for the `load_parameters_path` argument.\n",
    "config = pyconfig.initialize(\n",
    "    [\"\", \"../configs/base.yml\"], \n",
    "    per_device_batch_size=1.0,\n",
    "    run_name=\"test\",\n",
    "    max_target_length=4,\n",
    "    max_prefill_predict_length=4,\n",
    "    tokenizer_type=\"tiktoken\",\n",
    "    tokenizer_path=\"assets/tokenizer_llama3.tiktoken/\",\n",
    "    load_parameters_path=\"gs://maxtext-model-checkpoints/llama3.1-8b/2025-01-23-19-04/scanned/0/items\",\n",
    "    model_name=\"llama3.1-8b\",\n",
    "    async_checkpointing=False,\n",
    "    dataset_type=\"hf\",\n",
    "    hf_access_token=\"${HF_ACCESS_TOKEN}\",\n",
    "\n",
    ")\n",
    "\n",
    "model = mt.from_config(config)\n",
    "mesh = model.mesh\n",
    "# init_rng = jax.random.PRNGKey(config.init_weights_seed)\n",
    "# state, _ = maxtext_utils.setup_decode_state(model, config, init_rng, mesh, None)\n",
    "\n",
    "\n",
    "(\n",
    "      init_rng,\n",
    "      checkpoint_manager,\n",
    "      state_mesh_shardings,\n",
    "      model,\n",
    "      mesh,\n",
    "      learning_rate_schedule,\n",
    "      data_iterator,\n",
    "      eval_data_iterator,\n",
    "      state,\n",
    "  ) = train_utils.setup_train_loop(config, None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8af010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train_step, p_eval_step = train_utils.jit_train_and_eval_step(\n",
    "      config, model, mesh, state, state_mesh_shardings, train_step, eval_step, eval_data_iterator\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf62f1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert eval_data_iterator\n",
    "\n",
    "def evaluate(eval_data_iterator, state, rng):\n",
    "    eval_step_count = 0\n",
    "    # pylint: disable=not-callable\n",
    "    for eval_batch in eval_data_iterator:\n",
    "        if config.eval_steps > 0 and eval_step_count >= config.eval_steps:\n",
    "            break\n",
    "        with mesh, nn_partitioning.axis_rules(config.logical_axis_rules):\n",
    "            eval_metrics = p_eval_step(state, eval_batch, rng)\n",
    "            print(f\"evaluation/total_loss = {eval_metrics[\"scalar\"].get(\"evaluation/total_loss\", 0.0)} at eval step = {eval_step_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3c0d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_step = int(state.step)  # this is the start_step for training\n",
    "data_loader = DataLoader(config, mesh, data_iterator, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d996ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "last_step_completion = datetime.datetime.now()\n",
    "nextrng = jax.jit(jax.random.fold_in)(init_rng, start_step)\n",
    "\n",
    "evaluate(eval_data_iterator, state, nextrng)\n",
    "\n",
    "for step in tqdm(np.arange(start_step, config.steps)):\n",
    "\n",
    "    example_batch = data_loader.load_next_batch()\n",
    "    with mesh, nn_partitioning.axis_rules(config.logical_axis_rules):\n",
    "        state, metrics = p_train_step(state, example_batch, nextrng)\n",
    "\n",
    "    step_time_delta = datetime.datetime.now() - last_step_completion\n",
    "    last_step_completion = datetime.datetime.now()\n",
    "    nextrng = jax.jit(jax.random.fold_in)(init_rng, step)\n",
    "\n",
    "checkpointing.maybe_save_checkpoint(checkpoint_manager, state, config, data_iterator, step)\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "evaluate(eval_data_iterator, state, nextrng)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d2d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = source_tokenizer.encode(config.prompt)\n",
    "ids = np.asarray(input_ids, dtype=np.int32)\n",
    "s = (config.global_batch_size_to_train_on, config.max_target_length)\n",
    "decoder_segment_ids = np.zeros(s) + common_types.DECODING_ACTIVE_SEQUENCE_INDICATOR\n",
    "decoder_positions = np.stack(\n",
    "    [np.arange(config.max_target_length, dtype=np.int32) for _ in range(config.global_batch_size_to_train_on)]\n",
    ")\n",
    "\n",
    "\n",
    "ids = np.stack([ids for _ in range(config.global_batch_size_to_train_on)])\n",
    "max_logging.log(f\"input_ids={input_ids}, \\nids={ids}, \\ndecoder_segment_ids = {decoder_segment_ids}, \\ndecoder_positions= {decoder_positions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647018c1",
   "metadata": {},
   "source": [
    "Run a forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7436751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_logits = model.apply(\n",
    "          state.params,\n",
    "          ids,\n",
    "          decoder_positions,\n",
    "          decoder_segment_ids,\n",
    "          enable_dropout=False,\n",
    "          rngs={\"aqt\": init_rng},\n",
    "      )\n",
    "full_train_logits = jax.experimental.multihost_utils.process_allgather(full_train_logits)\n",
    "max_logging.log(f\"{full_train_logits[0, 0, :]=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5640ab55",
   "metadata": {},
   "source": [
    "Check the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb06c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_logits = jax.lax.dynamic_slice(\n",
    "        full_train_logits,\n",
    "        (0, 0, full_train_logits.shape[2]-1, 0),\n",
    "        (1, 1, 1, full_train_logits.shape[3])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f2a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider the greedily sampled token\n",
    "init_rng, new_rng = jax.random.split(init_rng)\n",
    "first_generated_token = inference_utils.sampling(\n",
    "        selected_logits,\n",
    "        new_rng,\n",
    "        config.decode_sampling_strategy, #\"greedy\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32555a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_generated_token.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de52746",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_tokenizer.decode([first_generated_token.item()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
