{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaxText Training Demo\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "- Set up MaxText for training\n",
    "- Configure model parameters\n",
    "- Prepare synthetic and real datasets\n",
    "- Run training with monitoring\n",
    "- Save and manage checkpoints\n",
    "- Evaluate model performance\n",
    "\n",
    "This demo showcases MaxText's capabilities for training large language models on TPUs and GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install MaxText and dependencies\n",
    "# !git clone https://github.com/AI-Hypercomputer/maxtext.git\n",
    "# %cd maxtext\n",
    "# !bash setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install additional Python dependencies\n",
    "# !pip install -q jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
    "# !pip install -q flax optax orbax-checkpoint grain-python tensorflow-datasets\n",
    "# !pip install -q sentencepiece transformers datasets\n",
    "# !pip install -q tensorboardX matplotlib seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "\n",
    "# Add MaxText to path\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "\n",
    "from MaxText import pyconfig\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "print(f\"Number of devices: {jax.device_count()}\")\n",
    "print(f\"Devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nest_asyncio\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "# This will be '.../maxtext/MaxText/scratch_code'\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Navigate two levels up to get to the project root 'maxtext'\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..', '..'))\n",
    "\n",
    "# Add the project root to the system path if it's not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added '{project_root}' to sys.path\")\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Google Cloud Storage bucket (replace with your bucket)\n",
    "GCS_BUCKET = \"go/my-buckets-tpu-prod-env-one-vm\" #\"gs://your-maxtext-bucket\"  # Replace with your bucket\n",
    "BASE_OUTPUT_DIR = f\"{GCS_BUCKET}/training_outputs\"\n",
    "DATASET_PATH = f\"{GCS_BUCKET}/datasets\"\n",
    "\n",
    "# Create a unique run name\n",
    "RUN_NAME = f\"training_demo_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "print(f\"Run name: {RUN_NAME}\")\n",
    "print(f\"Output directory: {BASE_OUTPUT_DIR}/{RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = pyconfig.initialize(\n",
    "    [\"\", \"../configs/base.yml\"], \n",
    "    per_device_batch_size=1.0,\n",
    "    run_name=\"test\",\n",
    "    max_target_length=4,\n",
    "    max_prefill_predict_length=4,\n",
    "    tokenizer_type=\"tiktoken\",\n",
    "    tokenizer_path=\"assets/tokenizer_llama3.tiktoken/\",\n",
    "    load_parameters_path=\"path/to/your/llama3.1-8b/checkpoint\",  # Replace with your checkpoint path\n",
    "    model_name=\"llama3.1-8b\",\n",
    "    async_checkpointing=False,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create synthetic dataset for testing\n",
    "def create_synthetic_dataset(batch_size, seq_length, vocab_size, num_batches=10):\n",
    "    \"\"\"Create synthetic training data for testing.\"\"\"\n",
    "    key = random.PRNGKey(42)\n",
    "    \n",
    "    datasets = []\n",
    "    for i in range(num_batches):\n",
    "        key, subkey = random.split(key)\n",
    "        \n",
    "        # Create random token sequences\n",
    "        inputs = random.randint(subkey, (batch_size, seq_length), 0, vocab_size)\n",
    "        \n",
    "        # Targets are shifted inputs (for autoregressive training)\n",
    "        targets = jnp.concatenate([\n",
    "            inputs[:, 1:],\n",
    "            random.randint(subkey, (batch_size, 1), 0, vocab_size)\n",
    "        ], axis=1)\n",
    "        \n",
    "        # Create attention masks (all ones for synthetic data)\n",
    "        inputs_segmentation = jnp.ones_like(inputs)\n",
    "        targets_segmentation = jnp.ones_like(targets)\n",
    "        \n",
    "        # Position indices\n",
    "        inputs_position = jnp.tile(\n",
    "            jnp.arange(seq_length)[None, :],\n",
    "            (batch_size, 1)\n",
    "        )\n",
    "        \n",
    "        batch_data = {\n",
    "            'inputs': inputs,\n",
    "            'targets': targets,\n",
    "            'inputs_segmentation': inputs_segmentation,\n",
    "            'targets_segmentation': targets_segmentation,\n",
    "            'inputs_position': inputs_position,\n",
    "        }\n",
    "        datasets.append(batch_data)\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Create synthetic dataset\n",
    "synthetic_data = create_synthetic_dataset(\n",
    "    batch_size=config.per_device_batch_size,\n",
    "    seq_length=config.max_target_length,\n",
    "    vocab_size=config.vocab_size,\n",
    "    num_batches=100\n",
    ")\n",
    "\n",
    "print(f\"Created {len(synthetic_data)} synthetic batches\")\n",
    "print(f\"Batch shape: {synthetic_data[0]['inputs'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Load HuggingFace dataset\n",
    "def load_huggingface_dataset(dataset_name='wikitext', subset='wikitext-2-raw-v1'):\n",
    "    \"\"\"Load and preprocess a HuggingFace dataset.\"\"\"\n",
    "    from datasets import load_dataset\n",
    "    from transformers import AutoTokenizer\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = load_dataset(dataset_name, subset, split='train')\n",
    "    \n",
    "    # Load tokenizer (using GPT-2 as example)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Tokenize function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=config_params['max_seq_length'],\n",
    "            return_tensors='np'\n",
    "        )\n",
    "    \n",
    "    # Tokenize dataset\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset\n",
    "\n",
    "# Uncomment to load real dataset\n",
    "# real_dataset = load_huggingface_dataset()\n",
    "# print(f\"Loaded dataset with {len(real_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training with synthetic data\n",
    "import subprocess\n",
    "\n",
    "# Construct training command\n",
    "train_command = f\"\"\"\n",
    "python3 -m MaxText.train MaxText/configs/base.yml \\\n",
    "    run_name={RUN_NAME} \\\n",
    "    base_output_directory={BASE_OUTPUT_DIR} \\\n",
    "    dataset_type=synthetic \\\n",
    "    steps=100 \\\n",
    "    per_device_batch_size={training_config['per_device_batch_size']} \\\n",
    "    learning_rate={training_config['learning_rate']} \\\n",
    "    warmup_steps={training_config['warmup_steps']} \\\n",
    "    log_period={training_config['log_period']} \\\n",
    "    checkpoint_period={training_config['checkpoint_period']} \\\n",
    "    dtype={training_config['dtype']} \\\n",
    "    enable_checkpointing=True\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"Training command:\")\n",
    "print(train_command)\n",
    "print(\"\\nStarting training...\")\n",
    "\n",
    "# Run training (uncomment to execute)\n",
    "# result = subprocess.run(train_command, shell=True, capture_output=True, text=True)\n",
    "# print(result.stdout)\n",
    "# if result.stderr:\n",
    "#     print(\"Errors:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Training loop implementation for demonstration\n",
    "from typing import Dict, Any\n",
    "import time\n",
    "\n",
    "class TrainingMetrics:\n",
    "    \"\"\"Track training metrics.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.learning_rates = []\n",
    "        self.grad_norms = []\n",
    "        self.throughput = []\n",
    "        self.steps = []\n",
    "    \n",
    "    def update(self, step, loss, lr, grad_norm, tokens_per_second):\n",
    "        self.steps.append(step)\n",
    "        self.losses.append(loss)\n",
    "        self.learning_rates.append(lr)\n",
    "        self.grad_norms.append(grad_norm)\n",
    "        self.throughput.append(tokens_per_second)\n",
    "    \n",
    "    def plot(self):\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        \n",
    "        # Loss curve\n",
    "        axes[0, 0].plot(self.steps, self.losses)\n",
    "        axes[0, 0].set_xlabel('Step')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].set_title('Training Loss')\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Learning rate schedule\n",
    "        axes[0, 1].plot(self.steps, self.learning_rates)\n",
    "        axes[0, 1].set_xlabel('Step')\n",
    "        axes[0, 1].set_ylabel('Learning Rate')\n",
    "        axes[0, 1].set_title('Learning Rate Schedule')\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # Gradient norm\n",
    "        axes[1, 0].plot(self.steps, self.grad_norms)\n",
    "        axes[1, 0].set_xlabel('Step')\n",
    "        axes[1, 0].set_ylabel('Gradient Norm')\n",
    "        axes[1, 0].set_title('Gradient Norm')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Throughput\n",
    "        axes[1, 1].plot(self.steps, self.throughput)\n",
    "        axes[1, 1].set_xlabel('Step')\n",
    "        axes[1, 1].set_ylabel('Tokens/Second')\n",
    "        axes[1, 1].set_title('Training Throughput')\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "# Simulate training loop\n",
    "def simulate_training_loop(num_steps=100):\n",
    "    \"\"\"Simulate a training loop with synthetic metrics.\"\"\"\n",
    "    metrics = TrainingMetrics()\n",
    "    \n",
    "    print(\"Starting simulated training...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for step in range(1, num_steps + 1):\n",
    "        # Simulate metrics\n",
    "        loss = 10.0 * np.exp(-step / 50) + np.random.normal(0, 0.1)\n",
    "        lr = training_config['learning_rate'] * min(1.0, step / training_config['warmup_steps'])\n",
    "        grad_norm = 2.0 * np.exp(-step / 100) + np.random.normal(0, 0.05)\n",
    "        tokens_per_second = 50000 + np.random.normal(0, 1000)\n",
    "        \n",
    "        # Update metrics\n",
    "        metrics.update(step, loss, lr, grad_norm, tokens_per_second)\n",
    "        \n",
    "        # Print progress\n",
    "        if step % training_config['log_period'] == 0:\n",
    "            print(f\"Step {step:4d} | Loss: {loss:.4f} | LR: {lr:.6f} | \"\n",
    "                  f\"Grad Norm: {grad_norm:.3f} | Throughput: {tokens_per_second:.0f} tok/s\")\n",
    "        \n",
    "        # Simulate checkpoint saving\n",
    "        if step % training_config['checkpoint_period'] == 0:\n",
    "            print(f\"  → Saving checkpoint at step {step}\")\n",
    "        \n",
    "        time.sleep(0.01)  # Simulate computation time\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Run simulated training\n",
    "metrics = simulate_training_loop(num_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Monitoring and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "fig = metrics.plot()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig.savefig(f'training_metrics_{RUN_NAME}.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Metrics plot saved to: training_metrics_{RUN_NAME}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display training statistics\n",
    "stats = {\n",
    "    'Final Loss': metrics.losses[-1],\n",
    "    'Average Loss': np.mean(metrics.losses),\n",
    "    'Loss Reduction': metrics.losses[0] - metrics.losses[-1],\n",
    "    'Average Throughput': np.mean(metrics.throughput),\n",
    "    'Peak Throughput': np.max(metrics.throughput),\n",
    "    'Average Gradient Norm': np.mean(metrics.grad_norms),\n",
    "    'Total Training Time (simulated)': len(metrics.steps) * 0.01,\n",
    "}\n",
    "\n",
    "print(\"Training Statistics:\")\n",
    "print(\"=\" * 40)\n",
    "for key, value in stats.items():\n",
    "    if 'Throughput' in key:\n",
    "        print(f\"{key:.<30} {value:,.0f} tokens/sec\")\n",
    "    elif 'Time' in key:\n",
    "        print(f\"{key:.<30} {value:.2f} seconds\")\n",
    "    else:\n",
    "        print(f\"{key:.<30} {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint management utilities\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "def list_checkpoints(output_dir):\n",
    "    \"\"\"List available checkpoints.\"\"\"\n",
    "    checkpoint_dir = f\"{output_dir}/checkpoints\"\n",
    "    \n",
    "    # Simulate checkpoint listing\n",
    "    checkpoints = [\n",
    "        {'step': 100, 'loss': 5.234, 'size_mb': 450},\n",
    "        {'step': 200, 'loss': 3.876, 'size_mb': 450},\n",
    "        {'step': 300, 'loss': 2.945, 'size_mb': 450},\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(checkpoints)\n",
    "    return df\n",
    "\n",
    "# List checkpoints\n",
    "checkpoints_df = list_checkpoints(BASE_OUTPUT_DIR)\n",
    "print(\"Available Checkpoints:\")\n",
    "print(checkpoints_df.to_string(index=False))\n",
    "\n",
    "# Plot checkpoint losses\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(checkpoints_df['step'], checkpoints_df['loss'], 'o-')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Checkpoint Losses')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load checkpoint for inference or continued training\n",
    "def load_checkpoint_command(checkpoint_step):\n",
    "    \"\"\"Generate command to load a checkpoint.\"\"\"\n",
    "    checkpoint_path = f\"{BASE_OUTPUT_DIR}/checkpoints/checkpoint_{checkpoint_step}\"\n",
    "    \n",
    "    command = f\"\"\"\n",
    "python3 -m MaxText.train MaxText/configs/base.yml \\\n",
    "    run_name={RUN_NAME}_continued \\\n",
    "    base_output_directory={BASE_OUTPUT_DIR} \\\n",
    "    load_parameters_path={checkpoint_path} \\\n",
    "    dataset_type=synthetic \\\n",
    "    steps=200\n",
    "\"\"\".strip()\n",
    "    \n",
    "    return command\n",
    "\n",
    "# Example: Continue training from checkpoint\n",
    "continue_command = load_checkpoint_command(checkpoint_step=100)\n",
    "print(\"Command to continue training from checkpoint:\")\n",
    "print(continue_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Training Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed precision training configuration\n",
    "mixed_precision_config = {\n",
    "    'dtype': 'bfloat16',  # or 'float32', 'float16'\n",
    "    'weight_dtype': 'float32',  # Keep weights in full precision\n",
    "    'gradient_accumulation_steps': 4,  # Accumulate gradients\n",
    "    'use_gradient_checkpointing': True,  # Save memory\n",
    "}\n",
    "\n",
    "print(\"Mixed Precision Configuration:\")\n",
    "for key, value in mixed_precision_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed training configuration for multi-host\n",
    "distributed_config = {\n",
    "    'ici_fsdp_parallelism': 8,  # Fully Sharded Data Parallelism\n",
    "    'ici_tensor_parallelism': 1,  # Tensor parallelism\n",
    "    'dcn_data_parallelism': 1,   # Data parallelism across data center network\n",
    "    'compile_topology': 'v5e-256',  # TPU topology\n",
    "    'compile_topology_num_slices': 2,  # Number of TPU slices\n",
    "}\n",
    "\n",
    "print(\"Distributed Training Configuration:\")\n",
    "for key, value in distributed_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "    \n",
    "# Calculate total parallelism\n",
    "total_parallelism = (\n",
    "    distributed_config['ici_fsdp_parallelism'] * \n",
    "    distributed_config['ici_tensor_parallelism'] * \n",
    "    distributed_config['dcn_data_parallelism']\n",
    ")\n",
    "print(f\"\\nTotal parallelism: {total_parallelism}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate command for training specific models\n",
    "def generate_model_training_command(model_name='llama2-7b', dataset='c4'):\n",
    "    \"\"\"Generate training command for specific models.\"\"\"\n",
    "    \n",
    "    model_configs = {\n",
    "        'llama2-7b': {\n",
    "            'config': 'MaxText/configs/models/llama2-7b.yml',\n",
    "            'batch_size': 4,\n",
    "            'learning_rate': 3e-4,\n",
    "            'steps': 10000,\n",
    "        },\n",
    "        'gemma-2b': {\n",
    "            'config': 'MaxText/configs/models/gemma-2b.yml',\n",
    "            'batch_size': 8,\n",
    "            'learning_rate': 5e-4,\n",
    "            'steps': 5000,\n",
    "        },\n",
    "        'mixtral-8x7b': {\n",
    "            'config': 'MaxText/configs/models/mixtral-8x7b.yml',\n",
    "            'batch_size': 2,\n",
    "            'learning_rate': 1e-4,\n",
    "            'steps': 15000,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    if model_name not in model_configs:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "    config = model_configs[model_name]\n",
    "    \n",
    "    command = f\"\"\"\n",
    "python3 -m MaxText.train {config['config']} \\\n",
    "    run_name={model_name}_training \\\n",
    "    base_output_directory={BASE_OUTPUT_DIR} \\\n",
    "    dataset_path={DATASET_PATH}/{dataset} \\\n",
    "    dataset_type=c4 \\\n",
    "    per_device_batch_size={config['batch_size']} \\\n",
    "    learning_rate={config['learning_rate']} \\\n",
    "    steps={config['steps']} \\\n",
    "    enable_checkpointing=True \\\n",
    "    checkpoint_period=1000\n",
    "\"\"\".strip()\n",
    "    \n",
    "    return command\n",
    "\n",
    "# Generate commands for different models\n",
    "for model in ['llama2-7b', 'gemma-2b', 'mixtral-8x7b']:\n",
    "    print(f\"\\n{model} Training Command:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(generate_model_training_command(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Model FLOPs Utilization (MFU)\n",
    "def calculate_mfu(model_params, hardware_flops, achieved_tflops):\n",
    "    \"\"\"Calculate Model FLOPs Utilization.\"\"\"\n",
    "    # Approximate FLOPs per token for transformer\n",
    "    # 6 * model_params for forward and backward pass\n",
    "    model_flops_per_token = 6 * model_params\n",
    "    \n",
    "    # MFU calculation\n",
    "    mfu = achieved_tflops / hardware_flops\n",
    "    \n",
    "    return mfu\n",
    "\n",
    "# Hardware specifications\n",
    "hardware_specs = {\n",
    "    'v5e-256': {'tflops': 197, 'memory_gb': 16},\n",
    "    'v5p-128': {'tflops': 459, 'memory_gb': 95},\n",
    "    'a100-80gb': {'tflops': 312, 'memory_gb': 80},\n",
    "    'h100-80gb': {'tflops': 989, 'memory_gb': 80},\n",
    "}\n",
    "\n",
    "# Example performance calculation\n",
    "model_size_b = 7  # 7B parameters\n",
    "hardware = 'v5p-128'\n",
    "achieved_tflops = 320  # Example achieved performance\n",
    "\n",
    "mfu = calculate_mfu(\n",
    "    model_params=model_size_b * 1e9,\n",
    "    hardware_flops=hardware_specs[hardware]['tflops'],\n",
    "    achieved_tflops=achieved_tflops\n",
    ")\n",
    "\n",
    "print(f\"Performance Analysis for {model_size_b}B model on {hardware}:\")\n",
    "print(f\"  Hardware Peak: {hardware_specs[hardware]['tflops']} TFLOPS\")\n",
    "print(f\"  Achieved: {achieved_tflops} TFLOPS\")\n",
    "print(f\"  MFU: {mfu:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory usage estimation\n",
    "def estimate_memory_usage(model_params_b, batch_size, seq_length, dtype='bfloat16'):\n",
    "    \"\"\"Estimate memory usage for training.\"\"\"\n",
    "    \n",
    "    bytes_per_param = {\n",
    "        'float32': 4,\n",
    "        'bfloat16': 2,\n",
    "        'float16': 2,\n",
    "        'int8': 1,\n",
    "    }\n",
    "    \n",
    "    param_bytes = bytes_per_param[dtype]\n",
    "    \n",
    "    # Model parameters\n",
    "    model_memory_gb = (model_params_b * 1e9 * param_bytes) / (1024**3)\n",
    "    \n",
    "    # Optimizer states (Adam uses 2x model size for momentum terms)\n",
    "    optimizer_memory_gb = model_memory_gb * 2\n",
    "    \n",
    "    # Activations (rough estimate)\n",
    "    activation_memory_gb = (\n",
    "        batch_size * seq_length * model_params_b * 0.1 * param_bytes\n",
    "    ) / (1024**3)\n",
    "    \n",
    "    # Gradients\n",
    "    gradient_memory_gb = model_memory_gb\n",
    "    \n",
    "    total_memory_gb = (\n",
    "        model_memory_gb + optimizer_memory_gb + \n",
    "        activation_memory_gb + gradient_memory_gb\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'model': model_memory_gb,\n",
    "        'optimizer': optimizer_memory_gb,\n",
    "        'activations': activation_memory_gb,\n",
    "        'gradients': gradient_memory_gb,\n",
    "        'total': total_memory_gb,\n",
    "    }\n",
    "\n",
    "# Estimate memory for different model sizes\n",
    "model_sizes = [1, 7, 13, 30, 70]\n",
    "memory_estimates = []\n",
    "\n",
    "for size in model_sizes:\n",
    "    mem = estimate_memory_usage(\n",
    "        model_params_b=size,\n",
    "        batch_size=8,\n",
    "        seq_length=2048,\n",
    "        dtype='bfloat16'\n",
    "    )\n",
    "    memory_estimates.append({\n",
    "        'Model Size (B)': size,\n",
    "        'Total Memory (GB)': mem['total'],\n",
    "        'Model (GB)': mem['model'],\n",
    "        'Optimizer (GB)': mem['optimizer'],\n",
    "    })\n",
    "\n",
    "memory_df = pd.DataFrame(memory_estimates)\n",
    "print(\"Memory Requirements by Model Size:\")\n",
    "print(memory_df.to_string(index=False, float_format='%.1f'))\n",
    "\n",
    "# Plot memory requirements\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(memory_df['Model Size (B)'].astype(str), memory_df['Total Memory (GB)'])\n",
    "plt.xlabel('Model Size (B parameters)')\n",
    "plt.ylabel('Memory (GB)')\n",
    "plt.title('Total Memory Requirements')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "categories = ['Model', 'Optimizer']\n",
    "for idx, row in memory_df.iterrows():\n",
    "    values = [row['Model (GB)'], row['Optimizer (GB)']]\n",
    "    bottom = 0\n",
    "    for cat, val in zip(categories, values):\n",
    "        plt.bar(str(row['Model Size (B)']), val, bottom=bottom, \n",
    "                label=cat if idx == 0 else \"\")\n",
    "        bottom += val\n",
    "\n",
    "plt.xlabel('Model Size (B parameters)')\n",
    "plt.ylabel('Memory (GB)')\n",
    "plt.title('Memory Breakdown')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices checklist\n",
    "best_practices = [\n",
    "    {\n",
    "        'category': 'Data',\n",
    "        'practices': [\n",
    "            'Use efficient data formats (TFRecord, ArrayRecord)',\n",
    "            'Implement data sharding for distributed training',\n",
    "            'Pre-tokenize datasets for faster loading',\n",
    "            'Use data validation to catch issues early',\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'category': 'Performance',\n",
    "        'practices': [\n",
    "            'Enable mixed precision training (bfloat16)',\n",
    "            'Use gradient accumulation for larger effective batch sizes',\n",
    "            'Enable XLA compilation flags',\n",
    "            'Profile code to identify bottlenecks',\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'category': 'Stability',\n",
    "        'practices': [\n",
    "            'Implement gradient clipping',\n",
    "            'Use learning rate warmup',\n",
    "            'Monitor gradient norms and loss spikes',\n",
    "            'Save checkpoints frequently',\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'category': 'Debugging',\n",
    "        'practices': [\n",
    "            'Enable stack trace collection',\n",
    "            'Use small datasets for quick iteration',\n",
    "            'Implement comprehensive logging',\n",
    "            'Test with synthetic data first',\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"MaxText Training Best Practices:\")\n",
    "print(\"=\" * 50)\n",
    "for section in best_practices:\n",
    "    print(f\"\\n{section['category']}:\")\n",
    "    for practice in section['practices']:\n",
    "        print(f\"  ✓ {practice}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variables for optimization\n",
    "optimization_env_vars = {\n",
    "    'XLA_FLAGS': '--xla_gpu_enable_async_collectives=true',\n",
    "    'LIBTPU_INIT_ARGS': '--xla_enable_async_all_gather=true',\n",
    "    'JAX_TRACEBACK_FILTERING': 'off',  # For debugging\n",
    "    'JAX_ENABLE_X64': 'false',  # Use 32-bit by default\n",
    "    'TF_CPP_MIN_LOG_LEVEL': '0',  # Show all logs\n",
    "}\n",
    "\n",
    "print(\"Recommended Environment Variables:\")\n",
    "print(\"-\" * 40)\n",
    "for var, value in optimization_env_vars.items():\n",
    "    print(f\"export {var}='{value}'\")\n",
    "\n",
    "# Set environment variables in notebook\n",
    "for var, value in optimization_env_vars.items():\n",
    "    os.environ[var] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training summary\n",
    "print(\"Training Demo Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Run Name: {RUN_NAME}\")\n",
    "print(f\"Model Configuration: {config_params['model_size']}B parameters\")\n",
    "print(f\"Training Steps: {training_config['steps']}\")\n",
    "print(f\"Batch Size: {training_config['per_device_batch_size']}\")\n",
    "print(f\"Learning Rate: {training_config['learning_rate']}\")\n",
    "print(f\"Hardware: {training_config['hardware']}\")\n",
    "print(f\"Output Directory: {BASE_OUTPUT_DIR}/{RUN_NAME}\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Scale up to larger models (7B, 13B, 70B)\")\n",
    "print(\"2. Use real datasets (C4, Wikipedia, custom data)\")\n",
    "print(\"3. Implement fine-tuning for specific tasks\")\n",
    "print(\"4. Deploy model for inference using MaxText decode\")\n",
    "print(\"5. Optimize for multi-host distributed training\")\n",
    "print(\"6. Integrate with monitoring tools (TensorBoard, Weights & Biases)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate complete training script\n",
    "complete_script = f\"\"\"\n",
    "#!/bin/bash\n",
    "# Complete MaxText Training Script\n",
    "# Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "# Set environment variables\n",
    "export XLA_FLAGS='--xla_gpu_enable_async_collectives=true'\n",
    "export LIBTPU_INIT_ARGS='--xla_enable_async_all_gather=true'\n",
    "\n",
    "# Configuration\n",
    "RUN_NAME={RUN_NAME}\n",
    "BASE_OUTPUT_DIR={BASE_OUTPUT_DIR}\n",
    "DATASET_PATH={DATASET_PATH}\n",
    "\n",
    "# Run training\n",
    "python3 -m MaxText.train MaxText/configs/base.yml \\\\\n",
    "    run_name=$RUN_NAME \\\\\n",
    "    base_output_directory=$BASE_OUTPUT_DIR \\\\\n",
    "    dataset_path=$DATASET_PATH \\\\\n",
    "    dataset_type=c4 \\\\\n",
    "    steps={training_config['steps']} \\\\\n",
    "    per_device_batch_size={training_config['per_device_batch_size']} \\\\\n",
    "    learning_rate={training_config['learning_rate']} \\\\\n",
    "    warmup_steps={training_config['warmup_steps']} \\\\\n",
    "    enable_checkpointing=true \\\\\n",
    "    checkpoint_period={training_config['checkpoint_period']} \\\\\n",
    "    eval_interval={training_config['eval_interval']} \\\\\n",
    "    dtype={training_config['dtype']}\n",
    "\n",
    "echo \"Training completed!\"\n",
    "\"\"\"\n",
    "\n",
    "# Save script\n",
    "script_path = f\"train_{RUN_NAME}.sh\"\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(complete_script.strip())\n",
    "\n",
    "print(f\"Complete training script saved to: {script_path}\")\n",
    "print(\"\\nTo run the training:\")\n",
    "print(f\"  chmod +x {script_path}\")\n",
    "print(f\"  ./{script_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
