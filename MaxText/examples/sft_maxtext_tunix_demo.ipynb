{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaxText + Tunix SFT Integration Demo\n",
    "\n",
    "This notebook demonstrates how to run Supervised Fine-Tuning (SFT) using MaxText models with the Tunix trainer. The workflow follows the steps implemented in `MaxText/sft/sft_trainer.py`.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The integration consists of several key components:\n",
    "1. **MaxText Model Loading**: Using `mt.from_pretrained()` to load pre-trained models\n",
    "2. **Tunix Adapter**: `TunixMaxTextLlama` wrapper that bridges MaxText models with Tunix trainer\n",
    "3. **Tunix Trainer**: Handles the training loop, optimization, and data management\n",
    "4. **Data Hooks**: Custom hooks for MaxText-specific data processing\n",
    "5. **Training Hooks**: Custom hooks for MaxText-specific training logic\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have the following packages installed:\n",
    "- `MaxText`\n",
    "- `tunix`\n",
    "- `jax`\n",
    "- `flax`\n",
    "- `orbax-checkpoint`\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's set up the environment and imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install tunix orbax-checkpoint\n",
    "\n",
    "# Set environment variables for TPU/GPU compatibility\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"0\"\n",
    "if \"xla_tpu_spmd_rng_bit_generator_unsafe\" not in os.environ.get(\"LIBTPU_INIT_ARGS\", \"\"):\n",
    "    os.environ[\"LIBTPU_INIT_ARGS\"] = os.environ.get(\"LIBTPU_INIT_ARGS\", \"\") + \" --xla_tpu_spmd_rng_bit_generator_unsafe=true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "# MaxText imports\n",
    "import MaxText as mt\n",
    "from MaxText import pyconfig\n",
    "from MaxText import max_utils\n",
    "from MaxText import maxtext_utils\n",
    "from MaxText import optimizers\n",
    "from MaxText.integration.tunix.tunix_adaptor import TunixMaxTextLlama\n",
    "from MaxText.sft import hooks\n",
    "\n",
    "# Tunix imports\n",
    "from tunix.sft import peft_trainer, profiler\n",
    "\n",
    "# Checkpointing\n",
    "from orbax import checkpoint as ocp\n",
    "\n",
    "# Set JAX configuration\n",
    "jax.config.update(\"jax_default_prng_impl\", \"unsafe_rbg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Setup\n",
    "\n",
    "Let's create a configuration object similar to what's used in the SFT trainer. We'll use a simplified configuration for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple configuration class for demonstration\n",
    "class SimpleConfig:\n",
    "    def __init__(self):\n",
    "        # Model configuration\n",
    "        self.model_name = \"llama2-7b\"\n",
    "        self.load_parameters_path = None  # Set to checkpoint path if loading from checkpoint\n",
    "        \n",
    "        # Training configuration\n",
    "        self.steps = 100\n",
    "        self.eval_interval = 10\n",
    "        self.eval_steps = 5\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        \n",
    "        # Checkpointing\n",
    "        self.checkpoint_period = 50\n",
    "        self.checkpoint_dir = \"./checkpoints\"\n",
    "        self.async_checkpointing = True\n",
    "        \n",
    "        # Profiling and logging\n",
    "        self.profiler = False\n",
    "        self.tensorboard_dir = \"./tensorboard\"\n",
    "        self.skip_first_n_steps_for_profiler = 0\n",
    "        self.profiler_steps = 10\n",
    "        \n",
    "        # Model-specific\n",
    "        self.logical_axis_rules = ()\n",
    "        \n",
    "        # Data configuration\n",
    "        self.dataset_type = \"hf\"\n",
    "        self.hf_path = \"HuggingFaceH4/ultrachat_200k\"\n",
    "        self.train_split = \"train_sft\"\n",
    "        self.hf_eval_split = \"test_sft\"\n",
    "        self.train_data_columns = [\"messages\"]\n",
    "        self.eval_data_columns = [\"messages\"]\n",
    "        \n",
    "        # Training parameters\n",
    "        self.learning_rate = 2e-5\n",
    "        self.weight_dtype = \"bfloat16\"\n",
    "        self.per_device_batch_size = 1\n",
    "        self.max_target_length = 1024\n",
    "        \n",
    "        # SFT specific\n",
    "        self.use_sft = True\n",
    "        self.sft_train_on_completion_only = True\n",
    "        self.packing = True\n",
    "\n",
    "# Create config instance\n",
    "config = SimpleConfig()\n",
    "\n",
    "print(\"Configuration created:\")\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Steps: {config.steps}\")\n",
    "print(f\"Learning rate: {config.learning_rate}\")\n",
    "print(f\"Dataset: {config.hf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunix Configuration Setup\n",
    "\n",
    "This function creates the Tunix training configuration from MaxText config, following the pattern in `sft_trainer.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tunix_config(mt_config):\n",
    "    \"\"\"Create Tunix training configuration from MaxText config.\"\"\"\n",
    "    # Checkpointing configurations\n",
    "    checkpointing_options = ocp.CheckpointManagerOptions(\n",
    "        save_interval_steps=mt_config.checkpoint_period,\n",
    "        enable_async_checkpointing=mt_config.async_checkpointing,\n",
    "    )\n",
    "\n",
    "    # Metrics configurations\n",
    "    metrics_logging_options = peft_trainer.metrics_logger.MetricsLoggerOptions(\n",
    "        log_dir=mt_config.tensorboard_dir\n",
    "    )\n",
    "\n",
    "    # Profiler configurations\n",
    "    profiler_options = None\n",
    "    if mt_config.profiler:\n",
    "        profiler_options = profiler.ProfilerOptions(\n",
    "            log_dir=mt_config.tensorboard_dir,\n",
    "            skip_first_n_steps=mt_config.skip_first_n_steps_for_profiler,\n",
    "            profiler_steps=mt_config.profiler_steps,\n",
    "        )\n",
    "\n",
    "    return peft_trainer.TrainingConfig(\n",
    "        eval_every_n_steps=mt_config.eval_interval,\n",
    "        max_steps=mt_config.steps,\n",
    "        gradient_accumulation_steps=mt_config.gradient_accumulation_steps,\n",
    "        checkpoint_root_directory=mt_config.checkpoint_dir,\n",
    "        checkpointing_options=checkpointing_options,\n",
    "        metrics_logging_options=metrics_logging_options,\n",
    "        profiler_options=profiler_options,\n",
    "    )\n",
    "\n",
    "# Create Tunix config\n",
    "tunix_config = get_tunix_config(config)\n",
    "print(\"Tunix configuration created:\")\n",
    "print(f\"Max steps: {tunix_config.max_steps}\")\n",
    "print(f\"Eval every: {tunix_config.eval_every_n_steps} steps\")\n",
    "print(f\"Checkpoint dir: {tunix_config.checkpoint_root_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxText Model Loading with Tunix Integration\n",
    "\n",
    "This function demonstrates how to load a MaxText model and wrap it with the Tunix adapter, following the pattern in `sft_trainer.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maxtext_model(config, default_loss_function=True):\n",
    "    \"\"\"Load MaxText model and wrap with Tunix adapter.\"\"\"\n",
    "    \n",
    "    def create_model():\n",
    "        # Load the model using MaxText's from_pretrained\n",
    "        # Note: In a real scenario, you would need proper config and devices\n",
    "        # For demo purposes, we'll create a placeholder\n",
    "        print(\"Creating MaxText model...\")\n",
    "        \n",
    "        # This is a simplified version - in practice you'd use:\n",
    "        # model = mt.from_pretrained(config, rngs=nnx.Rngs(params=0, dropout=1))\n",
    "        \n",
    "        # For demo, we'll create a mock model structure\n",
    "        class MockMaxTextModel:\n",
    "            def __init__(self):\n",
    "                self.mesh = None\n",
    "                self.enable_dropout = True\n",
    "            \n",
    "            def __call__(self, decoder_input_tokens, decoder_positions, decoder_segment_ids=None):\n",
    "                # Mock forward pass\n",
    "                batch_size, seq_len = decoder_input_tokens.shape\n",
    "                vocab_size = 32000  # Mock vocab size\n",
    "                return jnp.zeros((batch_size, seq_len, vocab_size))\n",
    "        \n",
    "        return MockMaxTextModel()\n",
    "    \n",
    "    # Create the model\n",
    "    model = create_model()\n",
    "    \n",
    "    # Create a mock mesh for demonstration\n",
    "    # In practice, this would come from the actual model\n",
    "    mesh = None\n",
    "    \n",
    "    # Load checkpoint if specified\n",
    "    if config.load_parameters_path:\n",
    "        print(f\"Loading checkpoint from {config.load_parameters_path}\")\n",
    "        # In practice, you would use:\n",
    "        # checkpoint = mt.checkpointing.load_params_from_path(...)\n",
    "        # if checkpoint:\n",
    "        #     nnx.update(model, checkpoint)\n",
    "    \n",
    "    # Wrap with Tunix adapter\n",
    "    if default_loss_function:\n",
    "        print(\"Using Tunix default loss function\")\n",
    "        tunix_model = TunixMaxTextLlama(\n",
    "            base_model=model,\n",
    "            use_attention_mask=False,  # trust Tunix loss masking\n",
    "        )\n",
    "    else:\n",
    "        print(\"Using MaxText loss function\")\n",
    "        tunix_model = model\n",
    "    \n",
    "    return tunix_model, mesh\n",
    "\n",
    "# Load the model\n",
    "print(\"Loading MaxText model with Tunix integration...\")\n",
    "model, mesh = get_maxtext_model(config, default_loss_function=True)\n",
    "print(f\"Model loaded: {type(model)}\")\n",
    "print(f\"Mesh: {mesh}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function Configuration\n",
    "\n",
    "Configure whether to use Tunix's default loss function or MaxText's custom loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_tunix_default_loss_function(trainer):\n",
    "    \"\"\"Configure trainer to use Tunix default loss function.\"\"\"\n",
    "    def gen_model_input_fn(x):\n",
    "        return {\n",
    "            \"input_tokens\": x[\"inputs\"],\n",
    "            \"positions\": x[\"inputs_position\"],\n",
    "            \"input_mask\": x[\"inputs_segmentation\"],\n",
    "            \"attention_mask\": x[\"inputs_segmentation\"],\n",
    "        }\n",
    "\n",
    "    trainer = trainer.with_gen_model_input_fn(gen_model_input_fn)\n",
    "    return trainer\n",
    "\n",
    "def use_maxtext_loss_function(trainer, mt_config):\n",
    "    \"\"\"Configure trainer to use MaxText custom loss function.\"\"\"\n",
    "    def loss_fn(model, inputs, inputs_position, inputs_segmentation,\n",
    "                targets, targets_position, targets_segmentation):\n",
    "        # In practice, you would import and use MaxText's loss function:\n",
    "        # from MaxText.train import loss_fn\n",
    "        # data = {\n",
    "        #     \"inputs\": inputs,\n",
    "        #     \"inputs_position\": inputs_position,\n",
    "        #     \"inputs_segmentation\": inputs_segmentation,\n",
    "        #     \"targets\": targets,\n",
    "        #     \"targets_position\": targets_position,\n",
    "        #     \"targets_segmentation\": targets_segmentation,\n",
    "        # }\n",
    "        # return loss_fn(model, mt_config, data, dropout_rng=None, params=None, is_train=True)\n",
    "        \n",
    "        # For demo, return a mock loss\n",
    "        return jnp.mean((inputs - targets) ** 2), {}\n",
    "\n",
    "    trainer = trainer.with_loss_fn(loss_fn, has_aux=True)\n",
    "    return trainer\n",
    "\n",
    "print(\"Loss function configuration functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "Now let's set up the training components including the optimizer, learning rate schedule, and hooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training_components(config, mesh):\n",
    "    \"\"\"Set up training components: optimizer, learning rate schedule, and hooks.\"\"\"\n",
    "    \n",
    "    # Create learning rate schedule\n",
    "    # In practice, you would use:\n",
    "    # learning_rate_schedule = maxtext_utils.create_learning_rate_schedule(config)\n",
    "    # For demo, create a simple constant schedule\n",
    "    def simple_lr_schedule(step):\n",
    "        return config.learning_rate\n",
    "    \n",
    "    learning_rate_schedule = simple_lr_schedule\n",
    "    \n",
    "    # Create optimizer\n",
    "    # In practice, you would use:\n",
    "    # optimizer = optimizers.get_optimizer(config, learning_rate_schedule)\n",
    "    # For demo, create a simple SGD optimizer\n",
    "    from optax import sgd\n",
    "    optimizer = sgd(learning_rate=config.learning_rate)\n",
    "    \n",
    "    # Create training hooks\n",
    "    # In practice, you would use:\n",
    "    # training_hooks = hooks.SFTTrainingHooks(config, mesh, learning_rate_schedule, goodput_recorder)\n",
    "    # For demo, create mock hooks\n",
    "    class MockTrainingHooks:\n",
    "        def __init__(self, config, mesh, lr_schedule):\n",
    "            self.config = config\n",
    "            self.mesh = mesh\n",
    "            self.lr_schedule = lr_schedule\n",
    "        \n",
    "        def on_step_begin(self, step, state):\n",
    "            print(f\"Training step {step} beginning\")\n",
    "        \n",
    "        def on_step_end(self, step, state, metrics):\n",
    "            print(f\"Training step {step} completed with loss: {metrics.get('loss', 'N/A')}\")\n",
    "    \n",
    "    training_hooks = MockTrainingHooks(config, mesh, learning_rate_schedule)\n",
    "    \n",
    "    # Create data hooks\n",
    "    # In practice, you would use:\n",
    "    # data_hooks = hooks.SFTDataHooks(config, mesh, goodput_recorder)\n",
    "    # For demo, create mock hooks\n",
    "    class MockDataHooks:\n",
    "        def __init__(self, config, mesh):\n",
    "            self.config = config\n",
    "            self.mesh = mesh\n",
    "        \n",
    "        def get_train_data_iterator(self):\n",
    "            # Mock data iterator\n",
    "            def mock_iterator():\n",
    "                for i in range(100):\n",
    "                    yield {\n",
    "                        \"inputs\": jnp.ones((1, 128), dtype=jnp.int32),\n",
    "                        \"inputs_position\": jnp.arange(128).reshape(1, -1),\n",
    "                        \"inputs_segmentation\": jnp.ones((1, 128), dtype=jnp.int32),\n",
    "                        \"targets\": jnp.ones((1, 128), dtype=jnp.int32),\n",
    "                        \"targets_position\": jnp.arange(128).reshape(1, -1),\n",
    "                        \"targets_segmentation\": jnp.ones((1, 128), dtype=jnp.int32),\n",
    "                    }\n",
    "            return mock_iterator()\n",
    "        \n",
    "        def get_eval_data_iterator(self):\n",
    "            # Mock eval data iterator\n",
    "            def mock_eval_iterator():\n",
    "                for i in range(10):\n",
    "                    yield {\n",
    "                        \"inputs\": jnp.ones((1, 128), dtype=jnp.int32),\n",
    "                        \"inputs_position\": jnp.arange(128).reshape(1, -1),\n",
    "                        \"inputs_segmentation\": jnp.ones((1, 128), dtype=jnp.int32),\n",
    "                        \"targets\": jnp.ones((1, 128), dtype=jnp.int32),\n",
    "                        \"targets_position\": jnp.arange(128).reshape(1, -1),\n",
    "                        \"targets_segmentation\": jnp.ones((1, 128), dtype=jnp.int32),\n",
    "                    }\n",
    "            return mock_eval_iterator()\n",
    "    \n",
    "    data_hooks = MockDataHooks(config, mesh)\n",
    "    \n",
    "    return optimizer, learning_rate_schedule, training_hooks, data_hooks\n",
    "\n",
    "# Set up training components\n",
    "print(\"Setting up training components...\")\n",
    "optimizer, lr_schedule, training_hooks, data_hooks = setup_training_components(config, mesh)\n",
    "print(\"Training components set up successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Configuration\n",
    "\n",
    "Now let's configure the Tunix trainer with all the components we've set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_trainer(model, optimizer, tunix_config, training_hooks, data_hooks, config):\n",
    "    \"\"\"Configure the Tunix trainer with all components.\"\"\"\n",
    "    \n",
    "    # Create the base trainer\n",
    "    trainer = peft_trainer.PeftTrainer(model, optimizer, tunix_config)\n",
    "    \n",
    "    # Add training hooks\n",
    "    trainer = trainer.with_training_hooks(training_hooks)\n",
    "    \n",
    "    # Add data hooks\n",
    "    trainer = trainer.with_data_hooks(data_hooks)\n",
    "    \n",
    "    # Configure loss function\n",
    "    if hasattr(config, 'use_tunix_loss') and config.use_tunix_loss:\n",
    "        print(\"Using Tunix default loss function\")\n",
    "        trainer = use_tunix_default_loss_function(trainer)\n",
    "    else:\n",
    "        print(\"Using MaxText loss function\")\n",
    "        trainer = use_maxtext_loss_function(trainer, config)\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "# Configure the trainer\n",
    "print(\"Configuring Tunix trainer...\")\n",
    "trainer = configure_trainer(model, optimizer, tunix_config, training_hooks, data_hooks, config)\n",
    "print(f\"Trainer configured: {type(trainer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Execution\n",
    "\n",
    "Now let's execute the training loop. In a real scenario, this would run the actual training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_training(trainer, data_hooks, mesh, config):\n",
    "    \"\"\"Execute the training loop.\"\"\"\n",
    "    \n",
    "    print(\"Starting training execution...\")\n",
    "    print(f\"Training for {config.steps} steps\")\n",
    "    print(f\"Evaluation every {config.eval_interval} steps\")\n",
    "    \n",
    "    # In practice, you would run:\n",
    "    # with mesh:\n",
    "    #     trainer.train(data_hooks.get_train_data_iterator(), data_hooks.get_eval_data_iterator())\n",
    "    \n",
    "    # For demo purposes, we'll simulate the training process\n",
    "    print(\"\\nSimulating training process...\")\n",
    "    \n",
    "    for step in range(min(10, config.steps)):  # Show first 10 steps for demo\n",
    "        # Simulate training step\n",
    "        print(f\"Step {step + 1}: Training...\")\n",
    "        \n",
    "        # Simulate evaluation\n",
    "        if (step + 1) % config.eval_interval == 0:\n",
    "            print(f\"  Step {step + 1}: Evaluating...\")\n",
    "        \n",
    "        # Simulate checkpointing\n",
    "        if (step + 1) % config.checkpoint_period == 0:\n",
    "            print(f\"  Step {step + 1}: Saving checkpoint...\")\n",
    "    \n",
    "    print(\"\\nTraining simulation completed!\")\n",
    "    print(\"\\nIn a real scenario, this would:\")\n",
    "    print(\"1. Load actual training data from HuggingFace\")\n",
    "    print(\"2. Run actual training steps with gradient updates\")\n",
    "    print(\"3. Perform real evaluation on validation data\")\n",
    "    print(\"4. Save actual model checkpoints\")\n",
    "    print(\"5. Log metrics to TensorBoard\")\n",
    "\n",
    "# Execute training (simulation)\n",
    "execute_training(trainer, data_hooks, mesh, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Training Function\n",
    "\n",
    "Here's the complete training function that puts everything together, following the pattern in `sft_trainer.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mt_config, default_loss_function=True):\n",
    "    \"\"\"Complete training function following sft_trainer.py pattern.\"\"\"\n",
    "    \n",
    "    print(\"=== Starting MaxText + Tunix SFT Training ===\")\n",
    "    \n",
    "    # Step 1: Get Tunix configuration\n",
    "    print(\"\\n1. Setting up Tunix configuration...\")\n",
    "    tunix_config = get_tunix_config(mt_config)\n",
    "    \n",
    "    # Step 2: Load MaxText model with Tunix integration\n",
    "    print(\"\\n2. Loading MaxText model...\")\n",
    "    model, mesh = get_maxtext_model(mt_config, default_loss_function)\n",
    "    \n",
    "    # Step 3: Set up training components\n",
    "    print(\"\\n3. Setting up training components...\")\n",
    "    optimizer, learning_rate_schedule, training_hooks, data_hooks = setup_training_components(mt_config, mesh)\n",
    "    \n",
    "    # Step 4: Configure trainer\n",
    "    print(\"\\n4. Configuring Tunix trainer...\")\n",
    "    trainer = configure_trainer(model, optimizer, tunix_config, training_hooks, data_hooks, mt_config)\n",
    "    \n",
    "    # Step 5: Execute training\n",
    "    print(\"\\n5. Executing training...\")\n",
    "    execute_training(trainer, data_hooks, mesh, mt_config)\n",
    "    \n",
    "    print(\"\\n=== Training Setup Complete ===\")\n",
    "    print(\"\\nTo run actual training, you would need:\")\n",
    "    print(\"1. Real model checkpoint or pretrained weights\")\n",
    "    print(\"2. Proper TPU/GPU configuration\")\n",
    "    print(\"3. Real training data\")\n",
    "    print(\"4. Proper environment setup\")\n",
    "    \n",
    "    return trainer, model, mesh\n",
    "\n",
    "# Run the complete training setup\n",
    "print(\"Running complete training setup...\")\n",
    "trainer, model, mesh = train(config, default_loss_function=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Examples\n",
    "\n",
    "Here are some examples of how to use this integration in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Training with Tunix default loss\n",
    "print(\"=== Example 1: Tunix Default Loss ===\")\n",
    "config.use_tunix_loss = True\n",
    "trainer1, _, _ = train(config, default_loss_function=True)\n",
    "\n",
    "# Example 2: Training with MaxText custom loss\n",
    "print(\"\\n=== Example 2: MaxText Custom Loss ===\")\n",
    "config.use_tunix_loss = False\n",
    "trainer2, _, _ = train(config, default_loss_function=False)\n",
    "\n",
    "# Example 3: Different model configurations\n",
    "print(\"\\n=== Example 3: Different Model Configs ===\")\n",
    "configs = [\n",
    "    {\"model_name\": \"llama2-7b\", \"learning_rate\": 1e-5},\n",
    "    {\"model_name\": \"llama2-13b\", \"learning_rate\": 5e-6},\n",
    "    {\"model_name\": \"gemma-7b\", \"learning_rate\": 2e-5}\n",
    "]\n",
    "\n",
    "for cfg in configs:\n",
    "    print(f\"\\nConfig: {cfg}\")\n",
    "    config.model_name = cfg[\"model_name\"]\n",
    "    config.learning_rate = cfg[\"learning_rate\"]\n",
    "    # In practice, you would run actual training here\n",
    "    print(f\"  Would train {cfg['model_name']} with lr={cfg['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with Real MaxText\n",
    "\n",
    "To integrate with real MaxText models, you would need to:\n",
    "\n",
    "1. **Install MaxText properly**: `pip install -e .`\n",
    "2. **Set up proper TPU/GPU configuration**\n",
    "3. **Use real model checkpoints**\n",
    "4. **Configure proper data pipelines**\n",
    "\n",
    "## Key Benefits of This Integration\n",
    "\n",
    "1. **Leverages Tunix's optimized training infrastructure**\n",
    "2. **Maintains MaxText's model architecture and capabilities**\n",
    "3. **Provides flexible loss function options**\n",
    "4. **Supports both training and evaluation workflows**\n",
    "5. **Integrates with MaxText's checkpointing and logging systems**\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Set up proper TPU/GPU environment**\n",
    "2. **Download real model checkpoints**\n",
    "3. **Configure real training data**\n",
    "4. **Run actual training experiments**\n",
    "5. **Monitor training progress and metrics**\n",
    "\n",
    "This notebook provides the foundation for integrating MaxText with Tunix for SFT training. The actual training execution would require proper hardware setup and real data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
