{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a921093",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U \"jax[cpu]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e80b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/google-deepmind/gemma.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4050cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(repo_id=\"google/gemma-2b-flax\", local_dir=\"/local/path/gemma-2b-flax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be8907dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "VARIANT = '2b' # @param ['2b', '2b-it', '7b', '7b-it'] {type:\"string\"}\n",
    "\n",
    "\n",
    "ckpt_path = '/home/zhaoyuec/workdir/gemma2-2b/ckpt/'\n",
    "vocab_path = '/home/zhaoyuec/workdir/gemma2-2b/tokenizer.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd6a2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters\n",
    "from gemma import params as params_lib\n",
    "params = params_lib.load_and_format_params(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6908204c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.Load(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "954b1e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma2 2b\n"
     ]
    }
   ],
   "source": [
    "# We use the `transformer_lib.TransformerConfig.from_params` function to\n",
    "# automatically load the correct configuration from a checkpoint. Note that the\n",
    "# vocabulary size is smaller than the number of input embeddings due to unused\n",
    "# tokens in this release.\n",
    "\n",
    "from gemma import transformer as transformer_lib\n",
    "\n",
    "config_2b = transformer_lib.TransformerConfig.from_params(\n",
    "    params,\n",
    "    cache_size=30  # Number of time steps in the transformer's cache\n",
    ")\n",
    "model_2b = transformer_lib.Transformer(config=config_2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d45d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gemma import sampler as sampler_lib\n",
    "# Create a sampler with the right param shapes.\n",
    "sampler = sampler_lib.Sampler(\n",
    "    transformer=model_2b,\n",
    "    vocab=vocab,\n",
    "    params=params['transformer'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34ffb3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "I love to\n",
      "Output:\n",
      "<unused62>\n",
      "\n",
      "##########\n",
      "Prompt:\n",
      "Today is a\n",
      "Output:\n",
      "<unused60>\n",
      "\n",
      "##########\n",
      "Prompt:\n",
      "What is the\n",
      "Output:\n",
      "\n",
      "\n",
      "\n",
      "##########\n"
     ]
    }
   ],
   "source": [
    "prompt_texts = [\"I love to\", \"Today is a\", \"What is the\"  ]\n",
    "\n",
    "out_data = sampler(\n",
    "    input_strings=prompt_texts,\n",
    "    total_generation_steps=6,  # number of steps performed when generating\n",
    "  )\n",
    "\n",
    "for input_string, out_string in zip(prompt_texts, out_data.text):\n",
    "  print(f\"Prompt:\\n{input_string}\\nOutput:\\n{out_string}\")\n",
    "  print()\n",
    "  print(10*'#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b649f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "def get_attention_mask_and_positions(example: jax.Array,\n",
    "                                     pad_id : int,\n",
    "                                     )-> tuple[jax.Array, jax.Array]:\n",
    "    \"\"\"Builds the position and attention mask vectors from the given tokens.\"\"\"\n",
    "\n",
    "    pad_mask = example != pad_id\n",
    "\n",
    "    current_token_position = transformer_lib.build_positions_from_mask(pad_mask)\n",
    "    attention_mask = transformer_lib.make_causal_attn_mask(pad_mask)\n",
    "    return current_token_position, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "647ea726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expanded_one_sample_input=Array([[     2, 235285,   2182,    577]], dtype=int32), positions=Array([0, 1, 2, 3], dtype=int32), attention_mask=Array([[[ True, False, False, False],\n",
      "        [ True,  True, False, False],\n",
      "        [ True,  True,  True, False],\n",
      "        [ True,  True,  True,  True]]], dtype=bool)\n",
      "logits=Array([[[-24.5, -8.8125, -7.25, ..., -23.75, -23.875, -23.75],\n",
      "        [-17.125, -1.54688, -0.138672, ..., -9, -9.625, -9.9375],\n",
      "        [-21, -10.5, 5.65625, ..., -14.4375, -14.5, -14.4375],\n",
      "        [-24.5, -7.40625, -15.9375, ..., -18.75, -19.25, -19.125]]],      dtype=bfloat16)\n",
      "(1, 4, 256128)\n",
      "expanded_one_sample_input=Array([[    2, 15528,   603,   476]], dtype=int32), positions=Array([0, 1, 2, 3], dtype=int32), attention_mask=Array([[[ True, False, False, False],\n",
      "        [ True,  True, False, False],\n",
      "        [ True,  True,  True, False],\n",
      "        [ True,  True,  True,  True]]], dtype=bool)\n",
      "logits=Array([[[-24.5, -8.8125, -7.25, ..., -23.75, -23.875, -23.75],\n",
      "        [-18.875, -11.5, 5.84375, ..., -11.875, -12.4375, -12.5625],\n",
      "        [-15.5, -4.5, -7.34375, ..., -12.125, -12.75, -11.875],\n",
      "        [-21.5, -4.9375, -6.0625, ..., -15.9375, -17, -16.25]]],      dtype=bfloat16)\n",
      "(1, 4, 256128)\n",
      "expanded_one_sample_input=Array([[   2, 1841,  603,  573]], dtype=int32), positions=Array([0, 1, 2, 3], dtype=int32), attention_mask=Array([[[ True, False, False, False],\n",
      "        [ True,  True, False, False],\n",
      "        [ True,  True,  True, False],\n",
      "        [ True,  True,  True,  True]]], dtype=bool)\n",
      "logits=Array([[[-24.5, -8.8125, -7.25, ..., -23.75, -23.875, -23.75],\n",
      "        [-17.875, -0.894531, -13.375, ..., -12.0625, -11, -12.125],\n",
      "        [-15, -1.86719, -1.85156, ..., -12.75, -13.875, -12.8125],\n",
      "        [-15.125, -15.25, -9.0625, ..., -13.9375, -14.5, -14.875]]],      dtype=bfloat16)\n",
      "(1, 4, 256128)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from gemma import transformer as transformer_lib\n",
    "import jsonlines\n",
    "\n",
    "params = params_lib.load_and_format_params(ckpt_path)\n",
    "\n",
    "output_path = \"golden_data_gemma2-2b.jsonl\"  \n",
    "all_data_to_save = []\n",
    "\n",
    "for prompt_index in range(len(prompt_texts)):\n",
    "    prompt_text = prompt_texts[prompt_index]\n",
    "    one_sample_input = np.array([2]+vocab.encode(prompt_text))\n",
    "    expanded_one_sample_input = jnp.expand_dims(one_sample_input, axis=0)\n",
    "    pad_id = vocab.pad_id\n",
    "    get_attention_mask_and_positions(one_sample_input, pad_id)\n",
    "    # Build the position and attention mask vectors.\n",
    "    positions, attention_mask = get_attention_mask_and_positions(one_sample_input, pad_id)\n",
    "    print(f\"{expanded_one_sample_input=}, {positions=}, {attention_mask=}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Foward pass on the input data.\n",
    "    # No attention cache is needed here.\n",
    "\n",
    "    logits, _ = model_2b.apply(\n",
    "    #     params,\n",
    "        {'params': params['transformer']},\n",
    "        expanded_one_sample_input,\n",
    "        positions,\n",
    "        None,              # Attention cache is None.\n",
    "        attention_mask,\n",
    "    )\n",
    "    print(f\"{logits=}\")\n",
    "    print(logits.shape)\n",
    "    # Prepare data to be saved  \n",
    "    data_to_save = {  \n",
    "        \"prompt\": prompt_texts[prompt_index],  \n",
    "        \"completion\": out_data.text[prompt_index],  \n",
    "        \"tokens\": [2]+vocab.encode(prompt_texts[prompt_index]),  \n",
    "        \"logits\": logits[0].tolist() #remove the batch dim and then tolist() for json serialization\n",
    "    }  \n",
    "    all_data_to_save.append(data_to_save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f4b01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to golden_data_gemma-2b.jsonl\n"
     ]
    }
   ],
   "source": [
    "with jsonlines.open(output_path,'w') as f:    \n",
    "    f.write_all(all_data_to_save)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Data saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
