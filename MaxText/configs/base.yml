# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This sentinel is a reminder to choose a real run name.
# If there is already a checkpoint under this run, that checkpoint will auto-resume.
run_name: ""

# At most one of load_parameters_path and load_from_other_directory should be set.
#   * load_parameters_path is used when you only care about the parameters, not the optimizer state or
#      the step count
#   * load_from_other_directory is used when you want to use a saved checkpoint from a different directory
#        (corresponding to a different run_name), and save new metrics and checkpoints to a new directory.
#   * If neither is provided then the most recent checkpoint saved under the current run_name is loaded if
#        any exists, else the model initializes randomly.
# If we aren't resuming from an existing checkpoint, load parameters from this path if provided.
load_parameters_path: ""
# Loads a full checkpoint including optimizer state and step count from a directory other than [run_name]
# e.g. gs://my-base-output-directory/my-previous-run-name/checkpoints
load_from_other_directory: ""
# Which step to load from other directory, defaults to latest step
load_from_other_directory_step: -1

reuse_example_batch: 0 # for testing TPU performance, this options repeated uses the same batch.

metrics_file: "" # for testing, local file that stores scalar metrics. If empty, no metrics are written.
# If true save metrics such as loss and TFLOPS to GCS in {base_output_directory}/{run_name}/metrics/
gcs_metrics: False

# Activation dtypes.
dtype: "bfloat16"
int8_training: True
# The following five flags will only have effect when int8_training is True
use_dqdg: False
fwd_int8: True
dlhs_int8: True
drhs_int8: True

fwd_int8_qk: False
dlhs_int8_qk: False
drhs_int8_qk: False

fwd_int8_pv: False
dlhs_int8_pv: False
drhs_int8_pv: False

quantize_logits: True

# There is no quality here, use just to measure performance headroom.
aqt_use_dummy_static_bound: False
aqt_rng_type: "jax.uniform"

# Global parameter scale needs to be a power of 2. If you want finer grained control of the model sizes
# then you should explicitly set base_embed_dim, base_num_heads, base_mlp_dim, base_num_decoder_layers and/or head_dim.
global_parameter_scale: 0 # has to be set explicitely.
global_parameter_scale_mlp_bonus: 0
base_emb_dim: 2048
base_num_heads: 8
base_mlp_dim: 8192
base_num_decoder_layers: 20
head_dim: 256
# activation functions are .
mlp_activations: ["relu"]
dropout_rate: 0
logits_via_embedding: True # NOTE: this is True just for testing.
# proj, minimal, full, or none
remat_policy: "full"
scan_layers: True
param_scan_axis: 1

record_internal_nn_metrics: 0

# Output directory
# Create a GCS bucket, e.g. my-maxtext-outputs and set this to "gs://my-maxtext-outputs/"
base_output_directory: "gs://maxtext-experiments-multipod/"

# Parallelism
mesh_axes: ["data", "fsdp", "tensor"]
logical_axis_rules:
  [
    ["activation_batch", ["data", "fsdp"]],
    ["activation_length", ["data", "fsdp"]],
    ["activation_embed", "tensor"],
    ["activation_mlp", "tensor"],
    ["activation_heads", "tensor"],
    ["activation_kv", "tensor"],
    ["activation_vocab", "tensor"],
    ["mlp", "tensor"],
    ["vocab", "tensor"],
    ["embed", "fsdp"],
    ["heads", "tensor"],
  ]
data_sharding: [["data", "fsdp", "tensor"]]

# One axis for each parallelism type may hold a placeholder (-1)
# value to auto-shard based on available slices and devices.
# By default, product of the DCN axes should equal number of slices
# and product of the ICI axes should equal number of devices per slice.
dcn_data_parallelism: -1 # recommended DCN axis to be auto-sharded
dcn_fsdp_parallelism: 1
dcn_tensor_parallelism: 1
ici_data_parallelism: 1
ici_fsdp_parallelism: -1 # recommended ICI axis to be auto-sharded
ici_tensor_parallelism: 1
num_slice: 1 # used by run-sweep.py . will be copied to the other yml file.

# Dataset
# Replace with your path given as argument in download_dataset.sh, e.g. "gs://my-maxtext-dataset/"
# dataset_path: "gs://max-datasets-rogue/"
dataset_path: "gs://max-datasets-rogue-useast/"

vocab_size: 32_768 # powers of 2 for sharding
vocab_relative_path: "vocabs" # Assumes we're allowed
dataset_name: "c4/en:3.0.1"
eval_dataset_name: "c4/en:3.0.1"
eval_split: "validation"
per_device_batch_size: 4
eval_per_device_batch_size: 0
max_corpus_chars: 10_000_000

# Training loop
steps: -1
log_period: 100
save_period: 500
learning_rate: 0.001
learning_rate_schedule_steps: -1 # length of learning rate schedule, should be at least steps, defaults to steps.
clip_by_global_norm: 0.0 # no clipping default
clip_by_block_rms: 0.0 # no clipping default
clip_by_ucb: 0.0 # no clipping default , exact value is ignored if above 0.0

# Maximum length cutoff for training examples.
max_target_length: 1024
# Used to calculate cosine LR schedule length and the lenght of the training in run-sweep.py
# New runs should set it to about 0.8
fill_ratio: 1.0
# Maximum length cutoff for held-out evaluation examples.
max_eval_target_length: 512

# Maximum length cutoff for predicted tokens.
max_predict_length: 50
# Sampling temperature for language model inference.
sampling_temperature: 0.6
# Top k cutoff for logit sampling. If 0 then no top-k cutoff is used.
sampling_top_k: 20
eos_id: 2 # sentencepiece default
# Prompt for language model sampling.
prompt: "I love to "

# Train loop profiler saves only for 2 steps, so no problem with the default.
enable_profiler: True
# If enable_checkpointing is true, an asynchronous checkpointer will be used if
# async_checkpointing is true, else a synchronous one is used. If you have
# problems with the checkpointer we recommend trying the sychronous one.
enable_checkpointing: True
async_checkpointing: True

# When dropout is false the model is a deterministic function of the
# data_shuffle_seed and init_weights_seed (i.e. reproducible losses)
enable_dropout: False
enable_data_shuffling: True
data_shuffle_seed: 0
init_weights_seed: 0

# Adam optimizer parameters
adam_b1: 0.9 # Exponential decay rate to track the first moment of past gradients.
adam_b2: 0.95 # Exponential decay rate to track the second moment of past gradients.
adam_eps: 1.e-8 # A small constant applied to denominator outside of the square root.
adam_eps_root: 0. # A small constant applied to denominator inside the square root.

# Stack trace parameters
collect_stack_trace: True
stack_trace_to_cloud: False # Uploads to cloud logging if True, else to the console if False.
stack_trace_interval_seconds: 600 # Stack trace collection frequency in seconds.
