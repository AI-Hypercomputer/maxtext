# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This sentinel is a reminder to choose a real run name.
# If there is already a checkpoint under this run, that checkpoint will auto-resume.
run_name: ""

# At most one of load_parameters_path and load_from_other_directory should be set.
#   * load_parameters_path is used when you only care about the parameters, not the optimizer state or
#      the step count
#   * load_from_other_directory is used when you want to use a saved checkpoint from a different directory
#        (corresponding to a different run_name), and save new metrics and checkpoints to a new directory.
#   * If neither is provided then the most recent checkpoint saved under the current run_name is loaded if
#        any exists, else the model initializes randomly.
# If we aren't resuming from an existing checkpoint, load parameters from this path if provided.
load_parameters_path: ""
# Loads a full checkpoint including optimizer state and step count from a directory other than [run_name]
# e.g. gs://my-base-output-directory/my-previous-run-name/checkpoints
load_from_other_directory: ""
# Which step to load from other directory, defaults to latest step
load_from_other_directory_step: -1

reuse_example_batch: 0 # for testing TPU performance, this options repeated uses the same batch.

metrics_file: "" # for testing, local file that stores scalar metrics. If empty, no metrics are written.
# If true save metrics such as loss and TFLOPS to GCS in {base_output_directory}/{run_name}/metrics/
gcs_metrics: False

# Activation dtypes.
dtype: "bfloat16"
int8_training: False
# The following flags will only have effect when int8_training is True
use_dqdg: False
fwd_int8: True
dlhs_int8: True
drhs_int8: False
fwd_int8_qk: False
dlhs_int8_qk: False
drhs_int8_qk: False
fwd_int8_pv: True
dlhs_int8_pv: True
drhs_int8_pv: False
aqt_use_dummy_static_bound: False
aqt_use_fwd_quant: False
aqt_rng_type: "jax.uniform"

# Global parameter scale needs to be a power of 2. If you want finer grained control of the model sizes 
# then you should explicitly set base_embed_dim, base_num_heads, base_mlp_dim, base_num_decoder_layers and/or head_dim.
global_parameter_scale: 1
base_emb_dim: 2560
base_num_heads: 8
base_mlp_dim: 8192
base_num_decoder_layers: 2
head_dim: 256
# activation functions are .
mlp_activations: ["relu"]
dropout_rate: 0
logits_via_embedding: True  # NOTE: this is True just for testing.
# proj, minimal, full, or none
remat_policy: 'full'
scan_layers: True
param_scan_axis: 1
enable_flash_attention: False # WARNING: Does not support attention_bias, requires JAX>=0.4.16

record_internal_nn_metrics: 0

# Output directory
# Create a GCS bucket, e.g. my-maxtext-outputs and set this to "gs://my-maxtext-outputs/"
base_output_directory: ""

# Parallelism
mesh_axes: ['data', 'fsdp', 'tensor']
logical_axis_rules: [
                      ['activation_batch', ['data', 'fsdp']],
                      ['activation_length' , ['data', 'fsdp']],
                      ['activation_embed', 'tensor'],
                      ['activation_mlp', 'tensor'],
                      ['activation_heads', 'tensor'],
                      ['activation_kv', 'tensor'],
                      ['activation_vocab', 'tensor'],
                      ['mlp', 'tensor'],
                      ['vocab', 'tensor'],
                      ['embed', 'fsdp'],
                      ['heads', 'tensor'],
                    ]
data_sharding: [['data', 'fsdp', 'tensor']]

# One axis for each parallelism type may hold a placeholder (-1)
# value to auto-shard based on available slices and devices.
# By default, product of the DCN axes should equal number of slices
# and product of the ICI axes should equal number of devices per slice.
dcn_data_parallelism: -1  # recommended DCN axis to be auto-sharded
dcn_fsdp_parallelism: 1
dcn_tensor_parallelism: 1
ici_data_parallelism: 1
ici_fsdp_parallelism: -1  # recommended ICI axis to be auto-sharded
ici_tensor_parallelism: 1


# Dataset
# Replace with your path given as argument in download_dataset.sh, e.g. "gs://my-maxtext-dataset/"
dataset_path: ""
vocab_size: 32_768 # powers of 2 for sharding
assets_path: "assets"
vocab_relative_path: "tokenizer"  # Assumes we're allowed
# dataset_name: 'c4/en:3.0.1'
# eval_dataset_name: 'c4/en:3.0.1'
# dataset_name: 'array-record/c4/en/3.0.1'
# eval_dataset_name: 'array-record/c4/en/3.0.1'
dataset_name: 'lm1b/1.1.0'
eval_dataset_name: 'lm1b/1.1.0'
eval_split: 'test'
per_device_batch_size: 12.0
eval_per_device_batch_size: 0
max_corpus_chars: 10_000_000
dataset_type: array_record # must be c4, array_record or synthetic
# dataset_type: c4

# Training loop
steps: 150_001 # If set to -1 then will inherit value from learning_rate_schedule_steps
log_period: 100
save_period: 10_000
# We take inspiration from Llama2's learning rate (LR) schedule, see https://arxiv.org/pdf/2307.09288.pdf section 2.2
# Learning rate schedule has either two or three parts:
# 1) Linear warmup from 0 to [learning_rate] over steps 0 to [learning_rate_schedule_steps * warmup_steps_fraction]
# 2) Cosine decay from [learning_rate] to [learning_rate * cosine_learning_rate_final_fraction] from warmup to learning_rate_schedule_steps
# 3) Constant learning rate of 0 from learning_rate_schedule_steps to steps.
# The zero learning rate section can be used to more accurately measure the fully trained model's performance. 
learning_rate: 3.e-5
cosine_learning_rate_final_fraction: 0.1 
warmup_steps_fraction: 0.1
learning_rate_schedule_steps: -1 # By default the length of the schedule is set to the number of steps.
# However you may choose a longer schedule (learning_rate_schedule_steps > steps), in which case the training will end before 
# dropping fully down. Or you may choose a shorter schedule, where the unspecified steps will have a learning rate of 0.

# Maximum length cutoff for training examples.
max_target_length: 1024
# Maximum length cutoff for held-out evaluation examples.
max_eval_target_length: 1024

# Maximum length cutoff for predicted tokens.
max_predict_length: 64
# Sampling temperature for language model inference.
sampling_temperature: 0.6
# Top k cutoff for logit sampling. If 0 then no top-k cutoff is used.
sampling_top_k: 20
eos_id: 2  # sentencepiece default
# Prompt for language model sampling.
prompt: "I love to "

enable_profiler: False
# If enable_checkpointing is true, an asynchronous checkpointer will be used if
# async_checkpointing is true, else a synchronous one is used. If you have
# problems with the checkpointer we recommend trying the sychronous one.
enable_checkpointing: True
async_checkpointing: True

# When dropout is false the model is a deterministic function of the 
# data_shuffle_seed and init_weights_seed (i.e. reproducible losses)
enable_dropout: True
enable_data_shuffling: True
data_shuffle_seed: 0
init_weights_seed: 0

# You may disable clipping by setting gradient_clipping_threshold to zero.
gradient_clipping_threshold: 1.0

# AdamW optimizer parameters
# We use AdamW following Llama2's training details, see https://arxiv.org/pdf/2307.09288.pdf section 2.2
adam_b1: 0.9 # Exponential decay rate to track the first moment of past gradients.
adam_b2: 0.95 # Exponential decay rate to track the second moment of past gradients.
adam_eps: 1.e-8 # A small constant applied to denominator outside of the square root.
adam_eps_root: 0. # A small constant applied to denominator inside the square root.
adam_weight_decay: 0.1 # AdamW Weight decay

# Stack trace parameters
collect_stack_trace: False
stack_trace_to_cloud: False  # Uploads to cloud logging if True, else to the console if False.
stack_trace_interval_seconds: 600  # Stack trace collection frequency in seconds.

# Use iota operator in Embed
use_iota_embed: False
