base_emb_dim: 6144
base_mlp_dim: 6144
base_num_decoder_layers: 48
base_num_kv_heads: 8
base_num_query_heads: 96
decoder_block: "mistral"
enable_dropout: False
head_dim: 64
logits_via_embedding: False
mlp_activations: ["silu", "linear"]  # linear not in below and not much used
normalization_layer_epsilon: 1.0e-5
num_experts: 16
num_experts_per_tok: 2
rope_max_timescale: 10_000_000
vocab_size: 151872
