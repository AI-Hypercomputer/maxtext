# Model config for Qwen3 MoE
base_emb_dim: 2048
base_num_query_heads: 32
base_num_kv_heads: 4
base_mlp_dim: 6144
base_num_decoder_layers: 24
head_dim: 64
mlp_activations: ["silu","linear"]
vocab_size: 151936
enable_dropout: False
logits_via_embedding: False
normalization_layer_epsilon: 1.0e-6
num_experts: 128
num_experts_per_tok: 8
rope_max_timescale: 10000
decoder_block: "qwen3_moe"
