# Use Static provisioning to define a PV for Cloud Storage buckets.
# https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/cloud-storage-fuse-csi-driver#provision-static.
apiVersion: v1
kind: PersistentVolume
metadata:
  name: gcs-checkpointing-pv
spec:
  accessModes:
  - ReadWriteMany
  capacity:
    storage: 64Gi
  persistentVolumeReclaimPolicy: Retain
  storageClassName: gcsfuse-sc # dummy storage class
  claimRef:
    name: ckpt-test-claim
    # Need to create the namespace in GKE. Or you can use "default" for testing.
    namespace: test-ns
  mountOptions:
  - debug_fuse
  - implicit-dirs #avoid if possible
  - max-conns-per-host=0
  # Needed for uplift renaming limit in checkpointing tests.
  - rename-dir-limit=200000
  - metadata-cache:ttl-secs:-1
  - metadata-cache:stat-cache-max-size-mb:-1
  - metadata-cache:type-cache-max-size-mb:-1
# - file-system:kernel-list-cache-ttl-secs:-1
  - file-cache:max-size-mb:-1
  - file-cache:cache-file-for-range-read:true
  # Experiment with this flag on and off.
  # - file-cache:enable-parallel-downloads:true
  csi:
    driver: gcsfuse.csi.storage.gke.io
    volumeHandle: xai-scale-testing-checkpoint
    volumeAttributes:
      enableMetrics: "true"
      skipCSIBucketAccessCheck: "true"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ckpt-test-claim
  # Need to create the namespace in GKE. Or you can use "default" for testing.
  namespace: test-ns
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 64Gi
  volumeName: gcs-checkpointing-pv
  storageClassName: gcsfuse-sc # dummy storage class
---
apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  name: chkpt-test
  labels:
    kueue.x-k8s.io/queue-name: multislice-queue  # Name of the LocalQueue
    xpk.google.com/workload: chkpt-test
  annotations:
    alpha.jobset.sigs.k8s.io/exclusive-topology: cloud.google.com/gke-nodepool # 1:1 job replica to node pool assignment
spec:
  failurePolicy:
    maxRestarts: 0
  replicatedJobs:
    - name: slice-job
      replicas: 1
      template:
        spec:
          # Modify the following two values if you intend to run the workload in smaller scale.
          parallelism: 1024    # Equal to the number of VMs per slice
          completions: 1024    # Same as the above.
          backoffLimit: 0   # When any pod fails, the job is failed
          template:
            metadata:
              labels:
                xpk.google.com/workload: chkpt-test
              # Required for GCSFuse.
              # For other storage solutions, please modify this section.
              annotations:
                gke-gcsfuse/volumes: "true"
                gke-gcsfuse/cpu-limit: "0"
                gke-gcsfuse/memory-limit: "0"
                gke-gcsfuse/ephemeral-storage-limit: "0"
            spec:
              schedulerName: default-scheduler
              restartPolicy: Never
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                    - matchExpressions:
                      - key: cloud.google.com/gke-nodepool
                        operator: NotIn
                        values:
                        - default-pool

              # For GCSFuse: to make sure that the pods are running on nodes with L-SSDs.
              # For other storage solutions that do not need L-SSDs, please remove this line.
              nodeSelector:
                cloud.google.com/gke-ephemeral-storage-local-ssd: "true"

              priorityClassName: high
              hostNetwork: true
              dnsPolicy: ClusterFirstWithHostNet
              terminationGracePeriodSeconds: 30
              # For GCSFuse: the setup of K8S SA is needed. https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/cloud-storage-fuse-csi-driver#authentication
              # For other storage solutions that do not need the K8S SA, please remove this line.
              serviceAccountName: bernardhan-benchmark
              containers:
              - name: jax-cpu
                image: gcr.io/gcs-tess/distributed_maxtext_checkpointing_benchmark

                env: 
                - name: REPLICATED_JOB_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.annotations['jobset.sigs.k8s.io/replicatedjob-name']
                - name: JOB_INDEX
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.annotations['jobset.sigs.k8s.io/job-index']
                - name: JOB_COMPLETION_INDEX
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
                # Modify the following two values too, if you intend to run the workload in smaller scale.
                - name: PROCESSES_IN_JOB
                  value: "1024"
                - name: JAX_PROCESS_COUNT
                  value: "1024"
                - name: JOBSET_NAME
                  value: "chkpt-test"
                - name: JAX_COORDINATOR_ADDRESS
                  value: "$(JOBSET_NAME)-$(REPLICATED_JOB_NAME)-0-0.$(JOBSET_NAME)"
  
                ports:
                - containerPort: 8471
                - containerPort: 8080
                - containerPort: 1234
                securityContext:
                  privileged: true
                command:
                - bash
                - -c
                - |
                  # Modify the following values for the checkpoint workload.
                  export BENCHMARK_RUN_FLAGS="RUN_NAME=bernardhan-ckpt-test STEPS=60 CHECKPOINT_PERIOD=50 OUTPUT_PATH=/mnt/gcsfuse/test-output PREVIOUS_STATE=/mnt/gcsfuse/maxtext-output/64b-real/checkpoints/0/items GCS_METRICS_BUCKET=distributed-checkpointing-metrics"

                  echo XPK Start: $(date) ; _sigterm() ( kill -SIGTERM $! 2>/dev/null;); trap _sigterm SIGTERM;(bash MaxText/configs/v5e/64b.sh ${BENCHMARK_RUN_FLAGS} EXECUTABLE=standalone_checkpointer.py HARDWARE='cpu' PLATFORM=gke) & PID=$!; while kill -0 $PID 2>/dev/null; do sleep 5; done; wait $PID; EXIT_CODE=$? ;  echo XPK End: $(date); echo EXIT_CODE=$EXIT_CODE;   

                resources:
                  requests:
                    # Requesting 20 CPU cores as the node machine is n2-32, this is to
                    # ensure that one pod is scheduled per node.
                    cpu: 20000m

                volumeMounts:
                # For other storage solutions, please modify the mount path and modify the PATH variables.
                - mountPath: /dev/shm
                  name: dshm-2
                - mountPath: /mnt/gcsfuse
                  name: gcs-pvc
                  readOnly: false
  
              volumes:
              - emptyDir:
                  medium: Memory
                name: dshm-2
              - name: gcs-pvc
                persistentVolumeClaim:
                  claimName: ckpt-test-claim