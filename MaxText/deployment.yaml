# Use Static provisioning to define a PV for Cloud Storage buckets.
# https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/cloud-storage-fuse-csi-driver#provision-static.
apiVersion: v1
kind: PersistentVolume
metadata:
  name: gcs-distributed-training-pv
spec:
  accessModes:
  - ReadWriteMany
  capacity:
    storage: 64Gi
  persistentVolumeReclaimPolicy: Retain
  storageClassName: gcsfuse-sc # dummy storage class
  claimRef:
    name: training-test-claim
    namespace: default
  mountOptions:
  - debug_fuse
  - implicit-dirs #avoid if possible
  - max-conns-per-host=0
  - metadata-cache:ttl-secs:-1
  - metadata-cache:stat-cache-max-size-mb:-1
  - metadata-cache:type-cache-max-size-mb:-1
#  - file-system:kernel-list-cache-ttl-secs:-1
  - file-cache:max-size-mb:-1
  - file-cache:cache-file-for-range-read:true
  - file-cache:enable-parallel-downloads:true
  csi:
    driver: gcsfuse.csi.storage.gke.io
    volumeHandle: xai-hf-dataset-parquet-10g # unique bucket name. xai-hf-dataset-parquet-10g for the 10G * 12K dataset.
    volumeAttributes:
      skipCSIBucketAccessCheck: "true"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: training-test-claim
  namespace: default
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 64Gi
  volumeName: gcs-distributed-training-pv
  storageClassName: gcsfuse-sc # dummy storage class
---
apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  name: xpk-test-workload
  labels:
    kueue.x-k8s.io/queue-name: multislice-queue  # Name of the LocalQueue
    xpk.google.com/workload: xpk-test-workload
  annotations:
    alpha.jobset.sigs.k8s.io/exclusive-topology: cloud.google.com/gke-nodepool # 1:1 job replica to node pool assignment
spec:
  failurePolicy:
    maxRestarts: 0
  replicatedJobs:
    - name: slice-job
      replicas: 1
      template:
        spec:
          parallelism: 256    # Equal to the number of VMs per slice
          completions: 256    # Same as the above.
          backoffLimit: 0   # When any pod fails, the job is failed
          template:
            metadata:
              labels:
                xpk.google.com/workload: xpk-test-workload
              annotations: # required for GCSFuse
                gke-gcsfuse/volumes: "true"
                gke-gcsfuse/cpu-limit: "0"
                gke-gcsfuse/memory-limit: "0"
                gke-gcsfuse/ephemeral-storage-limit: "0"
            
            spec:
              initContainers:
              # Metadata Prefetch native sidecar.
              - name: metadata-prefetch-container
                image: alpine:3.20
                restartPolicy: Always
                command:
                - "/bin/sh"
                - "-c"
                - |
                  _term() { 
                    echo "Caught SIGTERM signal: Terminating..." 
                    kill -TERM "$child" 2>/dev/null
                    exit 0
                  }

                  trap _term SIGTERM

                  echo "Starting ls on the bucket..."
                  # Redirect output to /dev/null to prevent storage of output.
                  ls -R /mnt/gcsfuse > /dev/null && \
                  echo "Metadata prefetch complete. Going to sleep..." && \
                  sleep infinite &

                  child=$! 
                  wait "$child"
                securityContext:
                  allowPrivilegeEscalation: false
                  capabilities:
                    drop:
                    - ALL
                  readOnlyRootFilesystem: true
                  runAsGroup: 65534
                  runAsNonRoot: true
                  runAsUser: 65534
                  seccompProfile:
                    type: RuntimeDefault
                volumeMounts:
                - name: gcs-pvc
                  mountPath: /mnt/gcsfuse

              schedulerName: default-scheduler
              restartPolicy: Never
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                    - matchExpressions:
                      - key: cloud.google.com/gke-nodepool
                        operator: NotIn
                        values:
                        - default-pool

              nodeSelector:
                cloud.google.com/gke-ephemeral-storage-local-ssd: "true"
                
              priorityClassName: medium
              hostNetwork: true
              dnsPolicy: ClusterFirstWithHostNet
              terminationGracePeriodSeconds: 30
              serviceAccountName: bernardhan-benchmark
              containers:
              - name: jax-cpu
                image: gcr.io/gcs-tess/bernardhan_runner_test
                
                env: 
                - name: REPLICATED_JOB_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.annotations['jobset.sigs.k8s.io/replicatedjob-name']
                - name: JOB_INDEX
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.annotations['jobset.sigs.k8s.io/job-index']
                - name: JOB_COMPLETION_INDEX
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
                - name: PROCESSES_IN_JOB
                  value: "256"
                - name: JAX_PROCESS_COUNT
                  value: "256"
                
                - name: JOBSET_NAME
                  value: "xpk-test-workload"
                - name: JAX_COORDINATOR_ADDRESS
                  value: "$(JOBSET_NAME)-$(REPLICATED_JOB_NAME)-0-0.$(JOBSET_NAME)"
  
                ports:
                - containerPort: 8471
                - containerPort: 8080
                - containerPort: 1234
                securityContext:
                  privileged: true
                command:
                - bash
                - -c
                - |
                  export COMMON_RUN_FLAGS="enable_checkpointing=False ici_data_parallelism=8 ici_fsdp_parallelism=32 hardware=cpu";
                  export BENCHMARK_RUN_FLAGS="dataset_bucket=xai-hf-dataset-parquet-10g epochs=2 local_batch_size=32 prefetch_factor=2 data_loader_num_workers=2 per_step_computation_time=0";
                  echo XPK Start: $(date) ; _sigterm() ( kill -SIGTERM $! 2>/dev/null;); trap _sigterm SIGTERM;(JAX_PLATFORMS=cpu python3 MaxText/standalone_dataloader.py MaxText/configs/base.yml ${BENCHMARK_RUN_FLAGS} ${COMMON_RUN_FLAGS}) & PID=$!; while kill -0 $PID 2>/dev/null; do sleep 5; done; wait $PID; EXIT_CODE=$? ;  echo XPK End: $(date); echo EXIT_CODE=$EXIT_CODE;

                volumeMounts:
                - mountPath: /mnt/gcsfuse
                  name: gcs-pvc
                  readOnly: true
  
              volumes:
              - name: gcs-pvc
                persistentVolumeClaim:
                  claimName: training-test-claim