{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c258fbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping qwix as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 22:17:54.582047: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754173074.594491   34124 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754173074.598170   34124 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754173074.608649   34124 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754173074.608662   34124 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754173074.608664   34124 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754173074.608665   34124 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# add the parent directory (one level up) to sys.path\n",
    "# sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../../maxtext')))\n",
    "\n",
    "# ! pip install -r ../../maxtext/requirements.txt\n",
    "\n",
    "!pip uninstall -q -y qwix\n",
    "!pip install -q git+https://github.com/google/qwix@24294e2e1816fffaee70987023f8f710a27966c2\n",
    "\n",
    "import MaxText as mt\n",
    "from MaxText import pyconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d45ea825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from MaxText.integration.tunix.tunix_adaptor import TunixMaxTextLlama\n",
    "# from flax import linen as nn\n",
    "\n",
    "def get_ref_maxtext_model():\n",
    "\n",
    "  #python3 -m MaxText.train MaxText/configs/base.yml base_output_directory=${BASE_OUTPUT_DIRECTORY} dataset_path=${DATASET_PATH} tokenizer_path=assets/tokenizer.gemma load_parameters_path=${CONVERTED_CHECKPOINT} per_device_batch_size=1 run_name=${FINETUNE_RUN_NAME} max_target_length=8192 steps=10 async_checkpointing=false model_name=gemma-2b checkpoint_period=5\n",
    "\n",
    "  #TODO: @mazumdera: change this to use Gemma2-2b-it\n",
    "  config = pyconfig.initialize(\n",
    "      [\"\", \"MaxText/configs/base.yml\"], #TODO: @mazumdera: why decode.py?\n",
    "      base_output_directory=\"gs://dummy_output_dir\",  # This is not used in Tunix.\n",
    "      run_name=\"test-tunix-maxtext-llama3-8b\",\n",
    "      # dataset_path=we use Tunix's dataset\n",
    "      # load_parameters_path=\"gs://maxtext-gemma/2b/\", #TODO: @mazumdera: change this to use checkpoint\n",
    "      # tokenizer_type=\"tiktoken\",\n",
    "      # tokenizer_path=\"assets/tokenizer_llama3.tiktoken\",\n",
    "      tokenizer_path=\"../maxtext/assets/tokenizer.gemma\",\n",
    "      per_device_batch_size=8,\n",
    "      max_target_length=8192,\n",
    "      steps=10,\n",
    "      async_checkpointing=\"false\",\n",
    "      # model_name=\"llama3.1-8b\",\n",
    "      model_name=\"gemma-2b\",\n",
    "      checkpoint_period=5,\n",
    "      skip_jax_distributed_system=\"true\",\n",
    "      weight_dtype=\"bfloat16\",\n",
    "      attention=\"dot_product\"\n",
    "\n",
    "  )\n",
    "  model = mt.from_pretrained(\n",
    "      config)\n",
    "  \n",
    "  mesh = model.mesh\n",
    "  \n",
    "  return model, mesh, config\n",
    "#   # checkpoint = mt.checkpointing.load_params_from_path(\n",
    "#   #     load_parameters_from_path=\"gs://maxtext-gemma/2b/\",\n",
    "#   #     abstract_unboxed_params=None,\n",
    "#   #     checkpoint_storage_concurrent_gb=None,\n",
    "#   # )\n",
    "#   checkpoint = {}\n",
    "\n",
    "#   def create_model(config):\n",
    "#     return mt.from_pretrained(config, rngs=nnx.Rngs(params=0, dropout=1))\n",
    "\n",
    "#   model = nnx.eval_shape(create_model, config=config)\n",
    "\n",
    "#   @nnx.jit\n",
    "#   def partial_init(checkpoint, config):\n",
    "#     model = create_model(config)\n",
    "#     nnx.update(model, checkpoint)\n",
    "#     # shard model\n",
    "#     state = nnx.state(model)\n",
    "#     specs = nnx.get_partition_spec(state)\n",
    "#     state = jax.lax.with_sharding_constraint(state, specs)\n",
    "#     nnx.update(model, state)\n",
    "#     return model\n",
    "\n",
    "#   with jax.sharding.use_mesh(model.mesh), nn.logical_axis_rules(config.logical_axis_rules):\n",
    "#     model = partial_init(checkpoint, config)\n",
    "#   print(model)\n",
    "\n",
    "  \n",
    "#   tunix_model = TunixMaxTextLlama(\n",
    "#         base_model=model,\n",
    "#         use_attention_mask=False,  # trust Tunix loss masking\n",
    "#     )\n",
    "#   mesh  = tunix_model.base.mesh\n",
    "  \n",
    "#   #TODO: @mazumdera: change this to use llama3.1-8b\n",
    "#   # model_config = None\n",
    "#   # We can continue to use Tunix's model_config\n",
    "#   model_config = gemma_lib.TransformerConfig.gemma2_2b()\n",
    "\n",
    "#   # Add these lines to properly get the graph definition and state\n",
    "#   graphdef, state = nnx.split(tunix_model)\n",
    "#   tunix_model = nnx.merge(graphdef, state)  # Recreate model in proper NNX format\n",
    "    \n",
    "  \n",
    "#   return tunix_model, mesh, model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c303e6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating keys from env and command line: ['run_name', 'model_name', 'async_checkpointing', 'checkpoint_period', 'weight_dtype', 'attention', 'base_output_directory', 'tokenizer_path', 'per_device_batch_size', 'steps', 'skip_jax_distributed_system', 'max_target_length']\n",
      "Running Model: gemma-2b\n",
      "Updating following parameters in config\n",
      "\n",
      "base_emb_dim: 2048\n",
      "base_num_query_heads: 8\n",
      "base_num_kv_heads: 1\n",
      "base_mlp_dim: 16384\n",
      "base_num_decoder_layers: 18\n",
      "head_dim: 256\n",
      "mlp_activations: ['gelu', 'linear']\n",
      "vocab_size: 256128\n",
      "decoder_block: gemma\n",
      "normalization_layer_epsilon: 1e-06\n",
      "logits_via_embedding: True\n",
      "Updating keys from model: ['base_emb_dim', 'base_num_query_heads', 'base_num_kv_heads', 'base_mlp_dim', 'base_num_decoder_layers', 'head_dim', 'mlp_activations', 'vocab_size', 'decoder_block', 'normalization_layer_epsilon', 'logits_via_embedding']\n",
      "Skipping jax distributed system due to skip_jax_distributed_system=True flag.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using emergency checkpoint, ignoring local_checkpoint_directory, local_checkpoint_period, use_replicator_service and replicator_backup_interval_minutes\n",
      "dataset_type set to tfds, will use keys['dataset_path']='' and keys['dataset_name']='c4/en:3.0.1'\n",
      "Config param activations_in_float32: False\n",
      "Config param adam_b1: 0.9\n",
      "Config param adam_b2: 0.95\n",
      "Config param adam_eps: 1e-08\n",
      "Config param adam_eps_root: 0.0\n",
      "Config param adam_weight_decay: 0.1\n",
      "Config param add_bos: True\n",
      "Config param add_eos: True\n",
      "Config param allow_split_physical_axes: False\n",
      "Config param ar_cache_axis_order: 1,2,0,3\n",
      "Config param async_checkpointing: False\n",
      "Config param attention: dot_product\n",
      "Config param attention_type: global\n",
      "Config param attn_logits_soft_cap: None\n",
      "Config param autoregressive_decode_assert: \n",
      "Config param base_emb_dim: 2048\n",
      "Config param base_mlp_dim: 16384\n",
      "Config param base_moe_mlp_dim: 7168\n",
      "Config param base_num_decoder_layers: 18\n",
      "Config param base_num_kv_heads: 1\n",
      "Config param base_num_query_heads: 8\n",
      "Config param base_output_directory: gs://dummy_output_dir\n",
      "Config param beta_fast: 32\n",
      "Config param beta_slow: 1\n",
      "Config param capacity_factor: -1.0\n",
      "Config param cast_logits_to_fp32: True\n",
      "Config param checkpoint_conversion_fn: None\n",
      "Config param checkpoint_dir: gs://dummy_output_dir/test-tunix-maxtext-llama3-8b/checkpoints/\n",
      "Config param checkpoint_is_quantized: False\n",
      "Config param checkpoint_period: 5\n",
      "Config param checkpoint_storage_concurrent_gb: 96\n",
      "Config param checkpoint_storage_target_data_file_size_bytes: 2147483648\n",
      "Config param checkpoint_storage_use_ocdbt: True\n",
      "Config param checkpoint_storage_use_zarr3: True\n",
      "Config param chunk_attn_window_size: 0\n",
      "Config param collect_stack_trace: False\n",
      "Config param colocated_python_data_input: False\n",
      "Config param compile_topology: \n",
      "Config param compile_topology_num_slices: -1\n",
      "Config param compiled_trainstep_file: \n",
      "Config param compute_axis_order: 0,1,2,3\n",
      "Config param constant_bound_config: []\n",
      "Config param context: remat\n",
      "Config param context_parallel_load_balance: True\n",
      "Config param cosine_learning_rate_final_fraction: 0.1\n",
      "Config param custom_mesh: \n",
      "Config param data_sharding: (('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'),)\n",
      "Config param data_shuffle_seed: 0\n",
      "Config param dataset_name: c4/en:3.0.1\n",
      "Config param dataset_path: \n",
      "Config param dataset_type: tfds\n",
      "Config param dcn_autoregressive_parallelism: 1\n",
      "Config param dcn_context_autoregressive_parallelism: 1\n",
      "Config param dcn_context_parallelism: 1\n",
      "Config param dcn_data_parallelism: -1\n",
      "Config param dcn_expert_parallelism: 1\n",
      "Config param dcn_fsdp_parallelism: 1\n",
      "Config param dcn_fsdp_transpose_parallelism: 1\n",
      "Config param dcn_parallelism: [-1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Config param dcn_pipeline_parallelism: 1\n",
      "Config param dcn_sequence_parallelism: 1\n",
      "Config param dcn_tensor_parallelism: 1\n",
      "Config param dcn_tensor_sequence_parallelism: 1\n",
      "Config param dcn_tensor_transpose_parallelism: 1\n",
      "Config param decode_sampling_nucleus_p: -1\n",
      "Config param decode_sampling_strategy: greedy\n",
      "Config param decode_sampling_temperature: 1.0\n",
      "Config param decode_sampling_top_k: 0\n",
      "Config param decoder_block: DecoderBlockType.GEMMA\n",
      "Config param decoder_layer_input: device\n",
      "Config param dpo_beta: 0.1\n",
      "Config param dpo_label_smoothing: 0.0\n",
      "Config param dropout_rate: 0.0\n",
      "Config param dtype: bfloat16\n",
      "Config param dtype_mm: float32\n",
      "Config param dump_hlo: False\n",
      "Config param dump_hlo_delete_local_after: True\n",
      "Config param dump_hlo_gcs_dir: \n",
      "Config param dump_hlo_local_dir: /tmp/xla_dump/\n",
      "Config param dump_hlo_module_name: jit_train_step\n",
      "Config param dump_hlo_upload_all: False\n",
      "Config param dump_hlo_xla_flags: \n",
      "Config param dump_step: -1\n",
      "Config param emb_dim: 2048\n",
      "Config param enable_checkpoint_cloud_logger: False\n",
      "Config param enable_checkpointing: True\n",
      "Config param enable_data_shuffling: True\n",
      "Config param enable_dropout: True\n",
      "Config param enable_emergency_checkpoint: False\n",
      "Config param enable_gcp_goodput_metrics: True\n",
      "Config param enable_gcp_step_deviation_metrics: True\n",
      "Config param enable_goodput_recording: False\n",
      "Config param enable_jax_profiler: False\n",
      "Config param enable_llm_inference_pool: False\n",
      "Config param enable_model_warmup: False\n",
      "Config param enable_orbax_v1: False\n",
      "Config param enable_padding_causal_mask: True\n",
      "Config param enable_pathways_goodput: False\n",
      "Config param enable_prefix_caching: False\n",
      "Config param enable_single_controller: False\n",
      "Config param enable_single_replica_ckpt_restoring: False\n",
      "Config param enable_tensorboard: True\n",
      "Config param eval_data_columns: ['text']\n",
      "Config param eval_dataset_name: c4/en:3.0.1\n",
      "Config param eval_image_column: image\n",
      "Config param eval_interval: -1\n",
      "Config param eval_per_device_batch_size: 8.0\n",
      "Config param eval_split: validation\n",
      "Config param eval_steps: -1\n",
      "Config param expansion_factor_real_data: -1\n",
      "Config param final_logits_soft_cap: None\n",
      "Config param first_num_dense_layers: 0\n",
      "Config param float32_logits: False\n",
      "Config param float32_qk_product: False\n",
      "Config param force_unroll: False\n",
      "Config param freeze_vision_encoder_params: True\n",
      "Config param fused_mlp: False\n",
      "Config param fused_qkv: False\n",
      "Config param gcs_metrics: False\n",
      "Config param generate_slice: v5e-16\n",
      "Config param global_batch_size_to_eval_on: 64\n",
      "Config param global_batch_size_to_load: 64\n",
      "Config param global_batch_size_to_load_eval: 64\n",
      "Config param global_batch_size_to_train_on: 64\n",
      "Config param global_parameter_scale: 1\n",
      "Config param goodput_upload_interval_seconds: 30\n",
      "Config param gradient_accumulation_steps: 1\n",
      "Config param gradient_clipping_threshold: 1.0\n",
      "Config param grain_eval_files: \n",
      "Config param grain_file_type: arrayrecord\n",
      "Config param grain_train_files: \n",
      "Config param grain_worker_count: 1\n",
      "Config param grain_worker_count_eval: 1\n",
      "Config param hardware: tpu\n",
      "Config param head_dim: 256\n",
      "Config param heartbeat_reporting_interval_in_seconds: 5\n",
      "Config param hf_data_dir: \n",
      "Config param hf_eval_files: \n",
      "Config param hf_eval_split: \n",
      "Config param hf_path: \n",
      "Config param hf_train_files: \n",
      "Config param hidden_size_for_vit: 1408\n",
      "Config param ici_autoregressive_parallelism: 1\n",
      "Config param ici_context_autoregressive_parallelism: 1\n",
      "Config param ici_context_parallelism: 1\n",
      "Config param ici_data_parallelism: 1\n",
      "Config param ici_expert_parallelism: 1\n",
      "Config param ici_fsdp_parallelism: -1\n",
      "Config param ici_fsdp_transpose_parallelism: 1\n",
      "Config param ici_parallelism: [1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Config param ici_pipeline_parallelism: 1\n",
      "Config param ici_sequence_parallelism: 1\n",
      "Config param ici_tensor_parallelism: 1\n",
      "Config param ici_tensor_sequence_parallelism: 1\n",
      "Config param ici_tensor_transpose_parallelism: 1\n",
      "Config param image_path: \n",
      "Config param image_placeholder: <|image|>\n",
      "Config param image_size_for_vit: 896\n",
      "Config param inference_benchmark_test: False\n",
      "Config param inference_metadata_file: \n",
      "Config param inference_microbenchmark_log_file_path: \n",
      "Config param inference_microbenchmark_loop_iters: 10\n",
      "Config param inference_microbenchmark_num_samples: [1, 2, 3, 4, 5]\n",
      "Config param inference_microbenchmark_prefill_lengths: 64,128,256,512,1024\n",
      "Config param inference_microbenchmark_stages: prefill,generate\n",
      "Config param inference_server: MaxtextInterleavedServer\n",
      "Config param inhomogeneous_layer_cycle_interval: 1\n",
      "Config param init_weights_seed: 0\n",
      "Config param input_data_sharding_logical_axes: ['activation_embed_and_logits_batch', 'activation_norm_length']\n",
      "Config param interleave_moe_layer_step: 1\n",
      "Config param intermediate_size_for_vit: 5632\n",
      "Config param jax_cache_dir: ~/jax_cache\n",
      "Config param jax_debug_log_modules: \n",
      "Config param jax_distributed_initialization_timeout: 300\n",
      "Config param jax_profiler_port: 9999\n",
      "Config param key_proj: remat\n",
      "Config param kv_lora_rank: 512\n",
      "Config param kv_quant_axis: heads_and_dkv\n",
      "Config param kv_quant_dtype: int8\n",
      "Config param learning_rate: 3e-05\n",
      "Config param learning_rate_schedule_steps: 10\n",
      "Config param load_balance_loss_weight: 0.01\n",
      "Config param load_from_prefill_dir: False\n",
      "Config param load_full_state_path: \n",
      "Config param load_parameters_path: \n",
      "Config param local_checkpoint_directory: \n",
      "Config param local_checkpoint_period: 0\n",
      "Config param local_rope_max_timescale: -1\n",
      "Config param log_config: True\n",
      "Config param log_period: 100\n",
      "Config param logical_axis_rules: (('activation_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_batch_no_exp', ('data', 'fsdp', 'fsdp_transpose')), ('activation_embed_and_logits_batch', ('data', 'stage', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence', 'autoregressive')), ('activation_kv_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_length', ('sequence', 'context')), ('prefill_activation_length', ('sequence', 'context')), ('activation_length', ('context',)), ('activation_norm_length', ('tensor_sequence', 'context', 'sequence')), ('prefill_activation_norm_length', ('tensor_sequence', 'context', 'sequence')), ('activation_q_length', ('context',)), ('activation_kv_length', ()), ('activation_embed', ('tensor', 'tensor_transpose')), ('activation_mlp', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_kv', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_prefill_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_head_dim', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose')), ('activation_vocab', 'tensor_sequence'), ('activation_vocab', ('sequence', 'context')), ('activation_stage', 'stage'), ('activation_exp', ('expert',)), ('decode_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('decode_length', ('sequence',)), ('mlp', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), ('vocab', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('q_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('kv_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'context', 'expert')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'context')), ('embed_tensor_transpose', ('tensor_transpose',)), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'expert')), ('norm', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('layers', 'stage'), ('kv', ()), ('kv_head_dim', ()), ('cache_batch_prefill', ()), ('cache_batch', ()), ('cache_heads_none', ()), ('cache_heads', ('autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence')), ('cache_heads', ('autoregressive', 'tensor', 'tensor_sequence')), ('cache_kv', ()), ('cache_sequence', ()), ('exp', 'expert'), ('paged_kv_heads', ('tensor',)), ('num_pages', ()), ('tokens_per_page', ()), ('paged_kv_head_dim_size', ()))\n",
      "Config param logits_dot_in_fp32: False\n",
      "Config param logits_via_embedding: True\n",
      "Config param lora_input_adapters_path: \n",
      "Config param matmul_precision: default\n",
      "Config param max_checkify: False\n",
      "Config param max_corpus_chars: 10000000\n",
      "Config param max_position_embeddings: 163840\n",
      "Config param max_prefill_predict_length: 64\n",
      "Config param max_target_length: 8192\n",
      "Config param megablox: True\n",
      "Config param mesh_axes: ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive']\n",
      "Config param metrics_dir: gs://dummy_output_dir/test-tunix-maxtext-llama3-8b/metrics/\n",
      "Config param metrics_file: \n",
      "Config param micro_batch_size_to_eval_on: 64\n",
      "Config param micro_batch_size_to_train_on: 64\n",
      "Config param mla_naive_kvcache: True\n",
      "Config param mlp_activations: ['gelu', 'linear']\n",
      "Config param mlp_dim: 16384\n",
      "Config param mlpwi: remat\n",
      "Config param mlpwi_0: remat\n",
      "Config param mlpwi_1: remat\n",
      "Config param mlpwo: remat\n",
      "Config param model_call_mode: \n",
      "Config param model_fsdp_ag_once: False\n",
      "Config param model_name: gemma-2b\n",
      "Config param moe_mlp_dim: 7168\n",
      "Config param monitor_goodput: False\n",
      "Config param monitor_step_time_deviation: True\n",
      "Config param mscale: 1.0\n",
      "Config param mtp_eval_target_module: 0\n",
      "Config param mtp_loss_scaling_factor: 0.1\n",
      "Config param mtp_num_layers: 0\n",
      "Config param mu_dtype: bfloat16\n",
      "Config param multi_sampling: False\n",
      "Config param n_routing_groups: -1\n",
      "Config param nope_layer_interval: -1\n",
      "Config param normalization_layer_epsilon: 1e-06\n",
      "Config param normalize_embedding_logits: True\n",
      "Config param num_attention_heads_for_vit: 16\n",
      "Config param num_channels_for_vit: 3\n",
      "Config param num_decoder_layers: 18\n",
      "Config param num_epoch: 1\n",
      "Config param num_experts: 1\n",
      "Config param num_experts_per_tok: 1\n",
      "Config param num_hidden_layers_for_vit: 34\n",
      "Config param num_kv_heads: 1\n",
      "Config param num_layers_per_pipeline_stage: 1\n",
      "Config param num_pipeline_microbatches: -1\n",
      "Config param num_pipeline_repeats: -1\n",
      "Config param num_query_heads: 8\n",
      "Config param num_slices: 1\n",
      "Config param opt_type: adamw\n",
      "Config param optimize_mesh_for_tpu_v6e: False\n",
      "Config param optimizer_memory_host_offload: False\n",
      "Config param original_max_position_embeddings: 4096\n",
      "Config param out_proj: remat\n",
      "Config param override_model_config: False\n",
      "Config param packing: True\n",
      "Config param pagedattn_head_dim_alignment: 128\n",
      "Config param pagedattn_max_pages_per_group: 256\n",
      "Config param pagedattn_num_pages: 64\n",
      "Config param pagedattn_pages_per_compute_block: 4\n",
      "Config param pagedattn_tokens_per_page: 32\n",
      "Config param param_scan_axis: 1\n",
      "Config param parameter_memory_host_offload: False\n",
      "Config param patch_size_for_vit: 14\n",
      "Config param per_device_batch_size: 8.0\n",
      "Config param pipeline_delay_activation_forwarding: False\n",
      "Config param pipeline_fsdp_ag_once: False\n",
      "Config param pipeline_parallel_layers: -1\n",
      "Config param pixel_shuffle_ratio_for_vit: 0.5\n",
      "Config param prefill_cache_axis_order: 1,2,0,3\n",
      "Config param prefill_cache_dir: \n",
      "Config param prefill_chunk_size: 256\n",
      "Config param prefill_slice: v5e-16\n",
      "Config param prefix_caching_dram_byte: 100000000000\n",
      "Config param prefix_caching_hbm_byte: 10000000000\n",
      "Config param profile_cleanly: True\n",
      "Config param profile_periodically_period: -1\n",
      "Config param profiler: \n",
      "Config param profiler_steps: 5\n",
      "Config param projector_dropout_for_vit: 0.0\n",
      "Config param projector_input_dim_for_vit: 4096\n",
      "Config param projector_output_dim_for_vit: 4096\n",
      "Config param prometheus_port: 0\n",
      "Config param prompt: I love to\n",
      "Config param q_lora_rank: 0\n",
      "Config param qk_nope_head_dim: 128\n",
      "Config param qk_rope_head_dim: 64\n",
      "Config param qkv_proj: remat\n",
      "Config param quant_cfg_path: \n",
      "Config param quantization: \n",
      "Config param quantization_calibration_method: absmax\n",
      "Config param quantization_local_shard_count: 1\n",
      "Config param quantize_kvcache: False\n",
      "Config param query_proj: remat\n",
      "Config param ragged_block_size: 256\n",
      "Config param record_internal_nn_metrics: 0\n",
      "Config param remat_policy: full\n",
      "Config param remat_policy_for_vit: minimal\n",
      "Config param replicate_quant_scale: False\n",
      "Config param replicator_backup_interval_minutes: 0\n",
      "Config param report_heartbeat_metric_for_gcp_monitoring: False\n",
      "Config param report_performance_metric_for_gcp_monitoring: False\n",
      "Config param reshape_q: False\n",
      "Config param return_log_prob: False\n",
      "Config param reuse_example_batch: 0\n",
      "Config param rope_factor: 40\n",
      "Config param rope_max_timescale: 10000\n",
      "Config param rope_min_timescale: 1\n",
      "Config param rope_theta_for_vit: 10000\n",
      "Config param rope_type: default\n",
      "Config param rope_use_scale: True\n",
      "Config param routed_bias: False\n",
      "Config param routed_scaling_factor: 1.0\n",
      "Config param routed_score_func: \n",
      "Config param run_name: test-tunix-maxtext-llama3-8b\n",
      "Config param sa_block_kv: 512\n",
      "Config param sa_block_kv_compute: 512\n",
      "Config param sa_block_kv_dkv: 512\n",
      "Config param sa_block_kv_dkv_compute: 512\n",
      "Config param sa_block_kv_dq: 512\n",
      "Config param sa_block_q: 512\n",
      "Config param sa_block_q_dkv: 512\n",
      "Config param sa_block_q_dq: 512\n",
      "Config param sa_k_layout: HEAD_DIM_MINOR\n",
      "Config param sa_q_layout: HEAD_DIM_MINOR\n",
      "Config param sa_use_fused_bwd_kernel: False\n",
      "Config param sa_v_layout: HEAD_DIM_MINOR\n",
      "Config param save_config_to_gcs: False\n",
      "Config param save_quantized_params_path: \n",
      "Config param scan_layers: True\n",
      "Config param scan_layers_per_stage: False\n",
      "Config param scan_pipeline_iterations: True\n",
      "Config param set_remat_policy_on_layers_per_stage: False\n",
      "Config param set_remat_policy_on_pipeline_iterations: True\n",
      "Config param sft_train_on_completion_only: False\n",
      "Config param sharding_tolerance: 0.02\n",
      "Config param shardy: True\n",
      "Config param shared_experts: 1\n",
      "Config param skip_first_n_steps_for_profiler: 1\n",
      "Config param skip_jax_distributed_system: True\n",
      "Config param sliding_window_size: 0\n",
      "Config param source_checkpoint_layout: orbax\n",
      "Config param sparse_matmul: True\n",
      "Config param stack_prefill_result_cache: False\n",
      "Config param stack_trace_interval_seconds: 600\n",
      "Config param stack_trace_to_cloud: False\n",
      "Config param step_deviation_interval_seconds: 30\n",
      "Config param steps: 10\n",
      "Config param subslice_shape: \n",
      "Config param target_eval_loss: 0.0\n",
      "Config param temperature_tuning: False\n",
      "Config param tensorboard_dir: gs://dummy_output_dir/test-tunix-maxtext-llama3-8b/tensorboard/\n",
      "Config param tile_activation_dim: 1024\n",
      "Config param tile_batch_seq: 512\n",
      "Config param tile_size_for_vit: 336\n",
      "Config param tile_weight_dim: 1024\n",
      "Config param tokenize_eval_data: True\n",
      "Config param tokenize_train_data: True\n",
      "Config param tokenizer_path: ../maxtext/assets/tokenizer.gemma\n",
      "Config param tokenizer_type: sentencepiece\n",
      "Config param topk_routing_group: -1\n",
      "Config param train_data_columns: ['text']\n",
      "Config param train_image_column: image\n",
      "Config param train_split: train\n",
      "Config param trainable_position_size: -1\n",
      "Config param upload_all_profiler_results: False\n",
      "Config param use_chat_template: False\n",
      "Config param use_chunked_prefill: False\n",
      "Config param use_dpo: False\n",
      "Config param use_iota_embed: False\n",
      "Config param use_multimodal: False\n",
      "Config param use_post_attn_norm: False\n",
      "Config param use_post_ffw_norm: False\n",
      "Config param use_qk_norm: False\n",
      "Config param use_qwix_quantization: False\n",
      "Config param use_ragged_attention: False\n",
      "Config param use_random_routing: False\n",
      "Config param use_replicator_service: False\n",
      "Config param use_sft: False\n",
      "Config param use_untrainable_positional_embedding: False\n",
      "Config param use_vertex_tensorboard: False\n",
      "Config param using_pipeline_parallelism: False\n",
      "Config param v_head_dim: 128\n",
      "Config param value_proj: remat\n",
      "Config param vertex_tensorboard_project: \n",
      "Config param vertex_tensorboard_region: \n",
      "Config param vision_output_dim_for_vit: 4096\n",
      "Config param vocab_size: 256128\n",
      "Config param warmup_steps_fraction: 0.1\n",
      "Config param weight_dtype: bfloat16\n",
      "Num_devices: 8, shape (1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "gemma, mesh, config = get_ref_maxtext_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a99797d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "    # attributes\n",
       "    config = <MaxText.pyconfig.HyperParameters object at 0x78494c0af8b0>\n",
       "    mesh = Mesh(device_ids=array([[[[[[[[[[[[0]]]]]]]]],\n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "             [[[[[[[[[1]]]]]]]]],\n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "             [[[[[[[[[2]]]]]]]]],\n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "             [[[[[[[[[3]]]]]]]]],\n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "             [[[[[[[[[7]]]]]]]]],\n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "             [[[[[[[[[6]]]]]]]]],\n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "             [[[[[[[[[5]]]]]]]]],\n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "             [[[[[[[[[4]]]]]]]]]]]]), axis_names=('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'), axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto))\n",
       "    quant = None\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca7f392a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mazumdera_google_com/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No decode checkpoint specified - generating random weights.\n",
      "No existing checkpoints found, not restoring checkpoint.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_train_logits[0, 0, :]=array([[-0.94921875,  0.61328125,  9.5625    , ...,  0.74609375,\n",
      "         0.33789062,  0.11328125],\n",
      "       [-0.7109375 ,  0.2734375 ,  0.90625   , ..., -1.21875   ,\n",
      "         0.6171875 , -0.9375    ],\n",
      "       [ 0.12695312,  0.47851562,  0.86328125, ..., -1.1640625 ,\n",
      "         0.12158203, -0.9296875 ],\n",
      "       ...,\n",
      "       [-0.33203125,  0.921875  ,  0.91796875, ..., -1.390625  ,\n",
      "         0.8046875 ,  0.09716797],\n",
      "       [ 0.05932617,  0.328125  , -0.05517578, ..., -0.91796875,\n",
      "        -0.01245117, -0.41210938],\n",
      "       [ 0.5625    , -0.04663086, -0.42773438, ..., -1.46875   ,\n",
      "         0.37304688,  0.38476562]], dtype=float32)\n",
      "Successfully ran the model!\n",
      "Tokens: [2, 49688, 736, 1280, 6987, 235292, 108, 2299, 235303, 235256, 573, 8957, 3646, 235336, 108]\n",
      "positions: [[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      " [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      " [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      " [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      " [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      " [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      " [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      " [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import os\n",
    "import MaxText as mt\n",
    "from MaxText import pyconfig\n",
    "from MaxText import maxtext_utils\n",
    "import numpy as np\n",
    "from MaxText.input_pipeline import _input_pipeline_utils\n",
    "import os\n",
    "from MaxText.globals import PKG_DIR\n",
    "from MaxText import max_logging\n",
    "from MaxText import common_types\n",
    "import jax\n",
    "from MaxText import inference_utils\n",
    "\n",
    "TEMP_BATCH_SIZE = 8\n",
    "\n",
    "tokens = [2,  49688 ,   736 ,  1280 ,  6987, 235292  ,  108 ,  2299, 235303, 235256,\n",
    "    573 ,  8957 ,  3646, 235336  ,  108]\n",
    "repeated_tokens = jnp.repeat(jnp.array(tokens)[None, :], TEMP_BATCH_SIZE, axis=0)\n",
    "positions = jnp.repeat(jnp.arange(0, len(tokens))[None, :], TEMP_BATCH_SIZE, axis=0)\n",
    "\n",
    "init_rng = jax.random.PRNGKey(config.init_weights_seed)\n",
    "state, _ = maxtext_utils.setup_decode_state(gemma, config, init_rng, mesh, None)\n",
    "\n",
    "full_train_logits = gemma.apply(\n",
    "          state.params,\n",
    "            decoder_input_tokens=repeated_tokens,\n",
    "            decoder_positions=positions,\n",
    "          decoder_segment_ids = None,\n",
    "          enable_dropout=False,\n",
    "          rngs={\"aqt\": init_rng},\n",
    "      )\n",
    "full_train_logits = jax.experimental.multihost_utils.process_allgather(full_train_logits)\n",
    "max_logging.log(f\"{full_train_logits[0, 0, :]=}\")\n",
    "\n",
    "\n",
    "print(\"Successfully ran the model!\")\n",
    "\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"positions: {positions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59101dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8, 15, 256128)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c1f0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted next token ID: [     2  49688    736   1280   6987 235292    108   2299  78327 235256\n",
      "    573   8957   3646 235336    108]\n"
     ]
    }
   ],
   "source": [
    "last_token_logits = full_train_logits[:,-1, :, :]\n",
    "predicted_token_id = jnp.argmax(last_token_logits, axis=-1)\n",
    "# Decode the token ID to see the predicted word.\n",
    "# Since TEMP_BATCH_SIZE is 1, we can just grab the first element.\n",
    "next_token_id = predicted_token_id[0]\n",
    "# predicted_token_text = gemma_tokenizer.decode([int(next_token_id)])\n",
    "\n",
    "print(f\"\\nPredicted next token ID: {next_token_id}\")   \n",
    "# print(f\"Predicted next token: '{predicted_token_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97664d37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
