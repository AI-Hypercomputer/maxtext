export TOKENIZER_PATH=assets/tokenizer.llama2
export LOAD_PARAMETERS_PATH=gs://msingh-bkt/checkpoints/quant_llama2-7b-chat/20241120034012/int8_
export MAX_PREFILL_PREDICT_LENGTH=1024
export MAX_TARGET_LENGTH=2048
export MODEL_NAME=llama2-7b
export ICI_FSDP_PARALLELISM=1
export ICI_AUTOREGRESSIVE_PARALLELISM=1
export ICI_TENSOR_PARALLELISM=4
export SCAN_LAYERS=false
export WEIGHT_DTYPE=bfloat16
export PER_DEVICE_BATCH_SIZE=4
export INFERENCE_SERVER=MaxtextInterleavedServer
python -m benchmarks.lm_eval.decode_multi_test \
  MaxText/configs/base.yml \
  tokenizer_path=${TOKENIZER_PATH} \
  load_parameters_path=${LOAD_PARAMETERS_PATH} \
  max_prefill_predict_length=${MAX_PREFILL_PREDICT_LENGTH} \
  max_target_length=${MAX_TARGET_LENGTH} \
  model_name=${MODEL_NAME} \
  ici_fsdp_parallelism=${ICI_FSDP_PARALLELISM} \
  ici_autoregressive_parallelism=${ICI_AUTOREGRESSIVE_PARALLELISM} \
  ici_tensor_parallelism=${ICI_TENSOR_PARALLELISM} \
  scan_layers=${SCAN_LAYERS} \
  weight_dtype=${WEIGHT_DTYPE} \
  per_device_batch_size=${PER_DEVICE_BATCH_SIZE} \
  checkpoint_is_quantized=True \
  quantization=int8 \
  quantize_kvcache=False \
  compute_axis_order=0,2,1,3 \
  ar_cache_axis_order=0,2,1,3 \
  enable_jax_profiler=True \
  stack_prefill_result_cache=False \
  inference_benchmark_test=True \
  inference_server=${INFERENCE_SERVER} \
  attention=paged \
  pagedattn_num_pages=128 \
  pagedattn_tokens_per_page=32 \
  pagedattn_pages_per_compute_block=4