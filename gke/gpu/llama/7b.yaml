workload:
  nodes: 32
  image: "gcr.io/tpu-prod-env-multipod/maxtext_gpu_jax_stable:2024-04-29"
  xla_flags: "--xla_gpu_enable_latency_hiding_scheduler=true --xla_gpu_enable_async_all_gather=true --xla_gpu_enable_async_reduce_scatter=true --xla_gpu_enable_triton_gemm=false --xla_gpu_simplify_all_fp_conversions --xla_gpu_graph_level=0 --xla_gpu_enable_async_all_reduce=true --xla_gpu_enable_highest_priority_async_stream=true --xla_gpu_all_reduce_combine_threshold_bytes=1073741824 --xla_gpu_all_gather_combine_threshold_bytes=134217728 --xla_gpu_reduce_scatter_combine_threshold_bytes=134217728 --xla_gpu_enable_pipelined_all_gather=true --xla_gpu_enable_pipelined_reduce_scatter=true --xla_gpu_enable_pipelined_all_reduce=true --xla_gpu_enable_while_loop_double_buffering=true --xla_gpu_enable_triton_softmax_fusion=false --xla_gpu_enable_all_gather_combine_by_dim=false --xla_gpu_enable_reduce_scatter_combine_by_dim=false --xla_disable_hlo_passes=rematerialization"
  train_command: "python MaxText/train.py MaxText/configs/base.yml hardware=gpu run_name=maxtext-llama2-7b steps=30 dcn_data_parallelism=32 ici_fsdp_parallelism=8 num_slices=32 per_device_batch_size=4 max_target_length=4096 model_name=llama2-7b enable_checkpointing=false attention=cudnn_flash_te remat_policy=minimal use_iota_embed=true scan_layers=false dataset_type=synthetic async_checkpointing=false base_output_directory=gs://runner-maxtext-logs enable_profiler=true logits_dot_in_fp32=false"
