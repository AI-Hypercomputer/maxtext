diff --git a/MaxText/inference/kvcache.py b/MaxText/inference/kvcache.py
index 1ac876e2..e18cd582 100644
--- a/MaxText/inference/kvcache.py
+++ b/MaxText/inference/kvcache.py
@@ -419,9 +419,18 @@ class KVCache(nnx.Module):
     cache_axis_names = transpose_tuple(cache_logical_axis_names, self.ar_cache_axis_order)
 
     cache_logical_shape = (self.batch, cache_length, self.key_heads, self.key_head_size)
+    #cache_logical_shape = (8, cache_length, self.key_heads, self.key_head_size)
+    #cache_logical_shape = (self.batch, cache_length, 16, self.key_head_size)
+    #cache_logical_shape = (8, cache_length, 8, self.key_head_size)
+    print('key cache_logical_shape: ' + str(cache_logical_shape))
     cache_shape_key = transpose_tuple(cache_logical_shape, self.ar_cache_axis_order)
 
     cache_logical_shape = (self.batch, cache_length, self.value_heads, self.value_head_size)
+    #cache_logical_shape = (8, cache_length, self.value_heads, self.value_head_size)
+    #cache_logical_shape = (self.batch, cache_length, 8, self.value_head_size)
+    #cache_logical_shape = (16, 16, 16, 16)
+    #cache_logical_shape = (8, cache_length, 8, self.value_head_size)
+    print('value cache_logical_shape: ' + str(cache_logical_shape))
     cache_shape_value = transpose_tuple(cache_logical_shape, self.ar_cache_axis_order)
 
     # TODO(b/339703100): investigate the issue why with_logical_partitioning doesn't enforce sharding
@@ -693,6 +702,15 @@ class KVCache(nnx.Module):
 
     else:
       one_hot_indices = one_hot_indices.astype(int)
+      print('cached_key.value: ' + str(cached_key.value))
+      print('one_token_key_shaped_for_cache: ' + str(one_token_key_shaped_for_cache))
+      print('ar_cache_update_idx: ' + str(ar_cache_update_idx))
+      print('ar_cache_update_axis: ' + str(ar_cache_update_axis))
+      print('cached_value.value: ' + str(cached_value.value))
+      print('one_token_value_shaped_for_cache: ' + str(one_token_value_shaped_for_cache))
+      print('ar_cache_update_idx: ' + str(ar_cache_update_idx))
+      print('ar_cache_update_axis: ' + str(ar_cache_update_axis))
+
       cached_key.value = jax.lax.dynamic_update_index_in_dim(
           cached_key.value, one_token_key_shaped_for_cache, ar_cache_update_idx, ar_cache_update_axis
       )
diff --git a/MaxText/layers/attentions.py b/MaxText/layers/attentions.py
index c444af51..059dc4e4 100644
--- a/MaxText/layers/attentions.py
+++ b/MaxText/layers/attentions.py
@@ -2126,8 +2126,11 @@ class Attention(nnx.Module):
     # and max_target_length, not the passed seq_len.
     # We can use a placeholder value. The correct fix might involve refactoring
     # KVCache.
+    #placeholder_seq_len = 1
     placeholder_seq_len = 1
 
+    #batch_size, placeholder_seq_len, emb_dim = inputs_kv_shape
+
     return kvcache.KVCache(
         max_prefill_length=self.max_prefill_predict_length,
         max_target_length=self.max_target_length,
diff --git a/MaxText/layers/decoders.py b/MaxText/layers/decoders.py
index 1e2cbda6..220ea097 100644
--- a/MaxText/layers/decoders.py
+++ b/MaxText/layers/decoders.py
@@ -341,7 +341,7 @@ class Decoder(nn.Module):
       case DecoderBlockType.DEFAULT:
         return [DecoderLayer]
       case DecoderBlockType.LLAMA2:
-        return [llama2.LlamaDecoderLayer]
+        return [llama2.LlamaDecoderLayerToLinen]
       case DecoderBlockType.MISTRAL:
         # TODO(ranran): update to Mistral with sliding window attention
         return [mistral.MistralDecoderLayer]
diff --git a/MaxText/layers/llama2.py b/MaxText/layers/llama2.py
index 51a09a37..bc503692 100644
--- a/MaxText/layers/llama2.py
+++ b/MaxText/layers/llama2.py
@@ -18,7 +18,7 @@ limitations under the License.
 # pylint: disable=arguments-differ
 # pylint: disable=no-name-in-module
 
-from typing import Optional
+from typing import Any, Optional
 
 import jax.numpy as jnp
 from jax.ad_checkpoint import checkpoint_name
@@ -26,15 +26,18 @@ from jax.sharding import Mesh
 # from jax.experimental.pallas.ops.tpu import flash_attention
 
 from flax import linen as nn
+from flax import nnx
 
 from MaxText.inference import page_manager
 from MaxText.common_types import Config
-from MaxText.layers.linears import mlp_block
+from MaxText.layers.linears import MlpBlock
+from MaxText.layers import initializers
+from MaxText.layers import nnx_wrappers
 from MaxText.layers import quantizations
-from MaxText.layers.attentions import attention_as_linen
+from MaxText.layers.attentions import Attention
 from MaxText.layers.quantizations import AqtQuantization as Quant
-from MaxText.layers.normalizations import rms_norm
-from MaxText.common_types import MODEL_MODE_PREFILL
+from MaxText.layers.normalizations import RMSNorm
+from MaxText.common_types import MODEL_MODE_PREFILL, MODEL_MODE_TRAIN, MODEL_MODE_AUTOREGRESSIVE
 
 
 # -----------------------------------------
@@ -42,15 +45,98 @@ from MaxText.common_types import MODEL_MODE_PREFILL
 # -----------------------------------------
 
 
-class LlamaDecoderLayer(nn.Module):
+class LlamaDecoderLayer(nnx.Module):
   """Transformer decoder layer that attends to the encoder."""
 
-  config: Config
-  mesh: Mesh
-  model_mode: str
-  quant: Optional[Quant] = None
+  def __init__(
+      self,
+      config: Config,
+      model_mode: str,
+      mesh: Mesh,
+      quant: Optional[Quant] = None,
+      rngs: Optional[nnx.Rngs] = None,
+      #**kwargs: Any,
+  ):
+
+    self.config = config
+    self.mesh = mesh
+    self.quant = quant
+    self.rngs = rngs
+
+    # We need a dummy input for initializing submodules.
+    # The batch size and feature dimension are important for KVCache and weights.
+    # TODO: probably just switch the 1 to max_target_length?
+    #dummy_inputs_shape = (int(config.per_device_batch_size), 1, int(config.emb_dim))
+    dummy_inputs_shape = (int(config.per_device_batch_size), config.max_target_length, int(config.emb_dim))
+
+    self.pre_self_attention_layer_norm = RMSNorm(
+        num_features=config.emb_dim,
+        dtype=config.dtype,
+        weight_dtype=config.weight_dtype,
+        kernel_axes=("norm",),
+        epsilon=config.normalization_layer_epsilon,
+        rngs=self.rngs,
+    )
+
+    self.self_attention = Attention(
+        config=config,
+        num_query_heads=config.num_query_heads,
+        num_kv_heads=config.num_kv_heads,
+        head_dim=config.head_dim,
+        max_target_length=config.max_target_length,
+        max_prefill_predict_length=config.max_prefill_predict_length,
+        attention_kernel=config.attention,
+        inputs_q_shape=dummy_inputs_shape,
+        inputs_kv_shape=dummy_inputs_shape,
+        mesh=mesh,
+        dtype=config.dtype,
+        weight_dtype=config.weight_dtype,
+        dropout_rate=config.dropout_rate,
+        float32_qk_product=config.float32_qk_product,
+        float32_logits=config.float32_logits,
+        quant=self.quant,
+        kv_quant=quantizations.configure_kv_quant(config),
+        prefill_cache_axis_order=tuple(map(int, config.prefill_cache_axis_order.split(","))),
+        ar_cache_axis_order=tuple(map(int, config.ar_cache_axis_order.split(","))),
+        compute_axis_order=tuple(map(int, config.compute_axis_order.split(","))),
+        reshape_q=config.reshape_q,
+        use_ragged_attention=config.use_ragged_attention,
+        ragged_block_size=config.ragged_block_size,
+        model_mode=model_mode,
+        #model_mode=MODEL_MODE_AUTOREGRESSIVE,
+        #model_mode=MODEL_MODE_TRAIN, # Only supporting training here
+        #model_mode=MODEL_MODE_PREFILL,  # This is just for initialization
+        rngs=self.rngs,
+    )
+
+    self.post_self_attention_layer_norm = RMSNorm(
+        num_features=config.emb_dim,
+        dtype=config.dtype,
+        weight_dtype=config.weight_dtype,
+        kernel_axes=("norm",),
+        epsilon=config.normalization_layer_epsilon,
+        rngs=self.rngs,
+    )
+
+    self.mlp = MlpBlock(
+        in_features=config.emb_dim,
+        intermediate_dim=config.mlp_dim,
+        activations=config.mlp_activations,
+        intermediate_dropout_rate=config.dropout_rate,
+        dtype=config.dtype,
+        weight_dtype=config.weight_dtype,
+        config=config,
+        quant=self.quant,
+        model_mode=model_mode,
+        #model_mode=MODEL_MODE_TRAIN, # Only supporting training here
+        #model_mode=MODEL_MODE_PREFILL, # This is just for initialization
+        rngs=self.rngs,
+    )
+
+    self.dropout = nnx.Dropout(rate=config.dropout_rate, broadcast_dims=(-2,), rngs=self.rngs)
+    #self.dropout = nnx.Dropout(rate=config.dropout_rate, broadcast_dims=(-2,))
+  
 
-  @nn.compact
   def __call__(
       self,
       inputs,
@@ -65,6 +151,8 @@ class LlamaDecoderLayer(nn.Module):
     cfg = self.config
     mesh = self.mesh
 
+    print('model_mode: ' + str(model_mode))
+
     if model_mode == MODEL_MODE_PREFILL:
       activation_axis_names = ("activation_batch", "prefill_activation_norm_length", "activation_embed")
     else:
@@ -72,48 +160,13 @@ class LlamaDecoderLayer(nn.Module):
 
     inputs = nn.with_logical_constraint(inputs, activation_axis_names)
     inputs = checkpoint_name(inputs, "decoder_layer_input")
-    lnx_rms = rms_norm(
-        num_features=inputs.shape[-1],
-        dtype=cfg.dtype,
-        weight_dtype=cfg.weight_dtype,
-        name="pre_self_attention_layer_norm",
-        kernel_axes=("norm",),
-        epsilon=cfg.normalization_layer_epsilon,
-    )
-    lnx = lnx_rms(inputs)
+    lnx = self.pre_self_attention_layer_norm(inputs)
 
     lnx = nn.with_logical_constraint(lnx, activation_axis_names)
 
-    # Self-attention block
-    attention_layer = attention_as_linen(
-        config=cfg,
-        num_query_heads=cfg.num_query_heads,
-        num_kv_heads=cfg.num_kv_heads,
-        head_dim=cfg.head_dim,
-        max_target_length=cfg.max_target_length,
-        max_prefill_predict_length=cfg.max_prefill_predict_length,
-        attention_kernel=cfg.attention,
-        inputs_q_shape=lnx.shape,
-        inputs_kv_shape=lnx.shape,
-        mesh=mesh,
-        dtype=cfg.dtype,
-        weight_dtype=cfg.weight_dtype,
-        dropout_rate=cfg.dropout_rate,
-        name="self_attention",
-        float32_qk_product=cfg.float32_qk_product,
-        float32_logits=cfg.float32_logits,
-        quant=self.quant,
-        kv_quant=quantizations.configure_kv_quant(cfg),
-        prefill_cache_axis_order=tuple(map(int, cfg.prefill_cache_axis_order.split(","))),
-        ar_cache_axis_order=tuple(map(int, cfg.ar_cache_axis_order.split(","))),
-        compute_axis_order=tuple(map(int, cfg.compute_axis_order.split(","))),
-        reshape_q=cfg.reshape_q,
-        use_ragged_attention=cfg.use_ragged_attention,
-        ragged_block_size=cfg.ragged_block_size,
-        model_mode=model_mode,
-    )
+    print('lnx.shape: ' + str(lnx.shape))
 
-    attention_lnx = attention_layer(
+    attention_lnx = self.self_attention(
         lnx,
         lnx,
         decoder_positions,
@@ -129,34 +182,18 @@ class LlamaDecoderLayer(nn.Module):
     intermediate_inputs = inputs + attention_lnx
 
     # Fully Connected
-    hidden_states = rms_norm(
-        num_features=intermediate_inputs.shape[-1],
-        dtype=cfg.dtype,
-        weight_dtype=cfg.weight_dtype,
-        name="post_self_attention_layer_norm",
-        kernel_axes=("norm",),
-        epsilon=cfg.normalization_layer_epsilon,
-    )(intermediate_inputs)
+    hidden_states = self.post_self_attention_layer_norm(intermediate_inputs)
     hidden_states = nn.with_logical_constraint(hidden_states, activation_axis_names)
 
     # MLP block.
-    mlp_lnx = mlp_block(
-        in_features=hidden_states.shape[-1],
-        intermediate_dim=cfg.mlp_dim,
-        activations=cfg.mlp_activations,
-        intermediate_dropout_rate=cfg.dropout_rate,
-        dtype=cfg.dtype,
-        weight_dtype=cfg.weight_dtype,
-        name="mlp",
-        config=cfg,
-        quant=self.quant,
-        model_mode=model_mode,
-    )(hidden_states, deterministic=deterministic)
+    mlp_lnx = self.mlp(hidden_states, deterministic=deterministic)
     mlp_lnx = nn.with_logical_constraint(mlp_lnx, activation_axis_names)
 
     layer_output = mlp_lnx + intermediate_inputs
 
-    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)
+    layer_output = self.dropout(layer_output, deterministic=deterministic)
+    #layer_output = self.dropout(layer_output, deterministic=deterministic, rngs=self.rngs)
+    #layer_output = nnx.Dropout(rate=self.config.dropout_rate, broadcast_dims=(-2,), rngs=self.rngs)(layer_output, deterministic=deterministic)
 
     layer_output = nn.with_logical_constraint(layer_output, activation_axis_names)
 
@@ -173,3 +210,8 @@ class LlamaDecoderLayer(nn.Module):
       return layer_output, None
     else:
       return layer_output
+
+LlamaDecoderLayerToLinen = nnx_wrappers.to_linen_class(
+    LlamaDecoderLayer,
+    base_metadata_fn=initializers.variable_to_logically_partitioned,
+)
diff --git a/MaxText/layers/nnx_wrappers.py b/MaxText/layers/nnx_wrappers.py
index 56380d33..27a912b7 100644
--- a/MaxText/layers/nnx_wrappers.py
+++ b/MaxText/layers/nnx_wrappers.py
@@ -478,6 +478,10 @@ class ToLinen(linen.Module):
         for k, v in collection_state.items():
           self.put_variable(collection, k, v)
 
+class _Missing:
+  ...
+
+_MISSING = _Missing()
 
 def to_linen(
   nnx_class: tp.Callable[..., Module],
@@ -499,3 +503,46 @@ def to_linen(
     skip_rng=skip_rng,
     name=name,
   )
+
+def to_linen_class(
+    base_nnx_class: type[M],
+    base_metadata_fn: (
+      tp.Callable[[variablelib.VariableState], tp.Any] | None
+    ) = to_linen_var,
+    base_skip_rng: bool = False,
+    **partial_kwargs: tp.Any,
+) -> type[ToLinen]:
+  """Dynamically wraps an NNX module class into a Flax Linen module class."""
+  class ToLinenPartial(ToLinen):
+
+    def __init_subclass__(cls, **kwargs):
+      super().__init_subclass__(**kwargs)
+
+      def __init__(self,
+                   args=None,
+                   kwargs=None,
+                   nnx_class=None,
+                   skip_rng=None,
+                   metadata_fn=None,
+                   name=_MISSING,
+                   parent=_MISSING,
+                   **other_kwargs,
+      ):
+        linen_kwargs = {}
+        if not isinstance(parent, _Missing):
+          linen_kwargs["parent"] = parent
+        if not isinstance(name, _Missing):
+          linen_kwargs["name"] = name
+        ToLinen.__init__(
+          self,
+          nnx_class=nnx_class or base_nnx_class,
+          args=args or (),
+          metadata_fn=metadata_fn or base_metadata_fn,
+          skip_rng=skip_rng or base_skip_rng,
+          kwargs=FrozenDict({**partial_kwargs, **(kwargs or {}), **other_kwargs}),
+          **linen_kwargs,
+        )
+
+      cls.__init__ = __init__
+
+  return ToLinenPartial
