{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_id = \"/mnt/disks/jacobplatin/models/llama4/maverick/4-layer-debug-hf/HF-4layers/\"\n",
    "# model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct-Original\"\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"float32\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "import MaxText.layers.models as models\n",
    "import MaxText.layers.quantizations as quantizations\n",
    "from MaxText import pyconfig\n",
    "from MaxText import max_utils\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "model_args = ['/mnt/disks/jacobplatin/code/maxtext/llama4_maverick_check_weight.py', 'MaxText/configs/base.yml', 'hardware=cpu', 'scan_layers=false', 'base_output_directory=llama4', 'run_name=temp-testing-only', 'model_name=llama4-17b-128e', 'skip_jax_distributed_system=true', 'load_parameters_path=/mnt/disks/jacobplatin/models/llama4/maverick/4-layer-unscanned/0/items/']\n",
    "config = pyconfig.initialize(model_args)\n",
    "\n",
    "init_rng = jax.random.PRNGKey(config.init_weights_seed)\n",
    "init_rng, rng1 = jax.random.split(init_rng)\n",
    "devices_array = max_utils.create_device_mesh(config)\n",
    "mesh = jax.sharding.Mesh(devices_array, config.mesh_axes)\n",
    "quant = quantizations.configure_quantization(config)\n",
    "model = models.Transformer(config, mesh=mesh, quant=quant)\n",
    "state, _ = max_utils.setup_decode_state(model, config, rng1, mesh, None)\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On key: model.layers.{layer_idx}.input_layernorm.weight\n",
      "On key: model.layers.{layer_idx}.post_attention_layernorm.weight\n",
      "On key: model.layers.{layer_idx}.self_attn.q_proj.weight\n",
      "On key: model.layers.{layer_idx}.self_attn.k_proj.weight\n",
      "On key: model.layers.{layer_idx}.self_attn.v_proj.weight\n",
      "On key: model.layers.{layer_idx}.self_attn.o_proj.weight\n"
     ]
    }
   ],
   "source": [
    "RTOL, ATOL = 1e-3, 1e-3\n",
    "\n",
    "\n",
    "def get_nested_robust(data, path: str, default = None, sep: str = '.'):\n",
    "  \"\"\"\n",
    "  Accesses nested dictionary/list elements using a dot-separated path string.\n",
    "  Handles dictionary keys and list/tuple indices.\n",
    "\n",
    "  Args:\n",
    "      data: The dictionary/list/tuple to traverse.\n",
    "      path: The dot-separated path string (e.g., 'a.b.c' or 'a.d.1').\n",
    "      default: The value to return if the path is not found or invalid. Defaults to None.\n",
    "      sep: The separator character used in the path string. Defaults to '.'.\n",
    "\n",
    "  Returns:\n",
    "      The value found at the specified path, or the default value if not found.\n",
    "  \"\"\"\n",
    "  keys = path.split(sep)\n",
    "  current_value = data\n",
    "  for key in keys:\n",
    "      if current_value is None: # Stop early if we hit None\n",
    "          return default\n",
    "      try:\n",
    "          if isinstance(current_value, dict):\n",
    "              current_value = current_value.get(key) # Safe dict access\n",
    "              if current_value is None and key not in current_value: # Distinguish missing key from value being None\n",
    "                  return default\n",
    "          elif isinstance(current_value, (list, tuple)):\n",
    "              try:\n",
    "                  index = int(key)\n",
    "                  current_value = current_value[index]\n",
    "              except (ValueError, IndexError): # Handle non-integer key or out-of-bounds\n",
    "                  return default\n",
    "          else:\n",
    "              # Cannot index further into this type\n",
    "              return default\n",
    "      except (KeyError, IndexError, TypeError): # Catch potential errors during access\n",
    "            return default\n",
    "\n",
    "  return current_value\n",
    "\n",
    "hf_to_maxtext_mapping = {\n",
    "  \"model.layers.{layer_idx}.input_layernorm.weight\": \"decoder.layers_{layer_idx}.pre_self_attention_layer_norm.scale\",\n",
    "  \"model.layers.{layer_idx}.post_attention_layernorm.weight\": \"decoder.layers_{layer_idx}.post_self_attention_layer_norm.scale\",\n",
    "  \"model.layers.{layer_idx}.self_attn.q_proj.weight\": \"decoder.layers_{layer_idx}.self_attention.query.kernel\",\n",
    "  \"model.layers.{layer_idx}.self_attn.k_proj.weight\": \"decoder.layers_{layer_idx}.self_attention.key.kernel\",\n",
    "  \"model.layers.{layer_idx}.self_attn.v_proj.weight\": \"decoder.layers_{layer_idx}.self_attention.value.kernel\",\n",
    "  \"model.layers.{layer_idx}.self_attn.o_proj.weight\": \"decoder.layers_{layer_idx}.self_attention.out.kernel\",\n",
    "}\n",
    "params = state.params[\"params\"]\n",
    "\n",
    "\n",
    "for hf_key, maxtext_key in hf_to_maxtext_mapping.items():\n",
    "    print(\"On key:\", hf_key)\n",
    "    for i in range(4):\n",
    "        is_dense_layer = i % 2 == 0\n",
    "        hf_key = hf_key.format(layer_idx=i)\n",
    "        maxtext_key = maxtext_key.format(layer_idx=i)\n",
    "        a = hf_model.state_dict()[hf_key].detach().numpy()\n",
    "        b = get_nested_robust(params, maxtext_key)\n",
    "        if \"self_attn.\" in hf_key:\n",
    "            if \"o_proj\" in hf_key:\n",
    "                b = b.reshape(5120, -1)\n",
    "            else:\n",
    "                b = b.reshape(5120, -1).transpose()\n",
    "        if not np.allclose(a, b, rtol=RTOL, atol=ATOL):\n",
    "            raise ValueError(f\"Failed on {hf_key} and {maxtext_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_assisted_decoding', '_auto_class', '_autoset_attn_implementation', '_backward_compatibility_gradient_checkpointing', '_backward_hooks', '_backward_pre_hooks', '_beam_search', '_beam_search_has_unfinished_sequences', '_buffers', '_cache_dependant_input_preparation', '_cache_dependant_input_preparation_exporting', '_call_impl', '_check_and_enable_flash_attn_2', '_check_and_enable_flex_attn', '_check_and_enable_sdpa', '_compiled_call_impl', '_constrained_beam_search', '_contrastive_search', '_convert_head_mask_to_5d', '_copy_lm_head_original_to_resized', '_create_repo', '_dispatch_accelerate_model', '_dola_decoding', '_expand_inputs_for_generation', '_fix_state_dict_key_on_load', '_fix_state_dict_key_on_save', '_fix_state_dict_keys_on_save', '_flatten_beam_dim', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_from_config', '_gather_beams', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_cache', '_get_candidate_generator', '_get_files_timestamps', '_get_initial_cache_position', '_get_key_renaming_mapping', '_get_layer_device_map_for_cache_init', '_get_logits_processor', '_get_name', '_get_no_split_modules', '_get_resized_embeddings', '_get_resized_lm_head', '_get_running_beams_for_next_iteration', '_get_stopping_criteria', '_get_top_k_continuations', '_group_beam_search', '_has_unfinished_sequences', '_hf_peft_config_loaded', '_hook_rss_memory_post_forward', '_hook_rss_memory_pre_forward', '_init_added_embeddings_weights_with_mean', '_init_added_lm_head_bias_with_mean', '_init_added_lm_head_weights_with_mean', '_init_weights', '_initialize_missing_keys', '_initialize_weights', '_is_full_backward_hook', '_is_hf_initialized', '_is_stateful', '_keep_in_fp32_modules', '_keep_in_fp32_modules', '_keys_to_ignore_on_load_missing', '_keys_to_ignore_on_load_unexpected', '_keys_to_ignore_on_save', '_load_from_flax', '_load_from_state_dict', '_load_from_tf', '_load_pretrained_model', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_initialize_input_ids_for_generation', '_maybe_warn_non_full_backward_hook', '_merge_criteria_processor_list', '_modules', '_move_missing_keys_from_meta_to_cpu', '_named_members', '_no_split_modules', '_no_split_modules', '_non_persistent_buffers_set', '_parameters', '_pp_plan', '_pp_plan', '_prefill_chunking', '_prepare_4d_causal_attention_mask_with_cache_position', '_prepare_attention_mask_for_generation', '_prepare_cache_for_generation', '_prepare_decoder_input_ids_for_generation', '_prepare_encoder_decoder_kwargs_for_generation', '_prepare_generated_length', '_prepare_generation_config', '_prepare_model_inputs', '_prepare_special_tokens', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_reorder_cache', '_replicate_for_data_parallel', '_resize_token_embeddings', '_sample', '_save_to_state_dict', '_set_default_torch_dtype', '_set_gradient_checkpointing', '_skip_keys_device_placement', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_supports_attention_backend', '_supports_cache_class', '_supports_default_dynamic_cache', '_supports_flash_attn_2', '_supports_flex_attn', '_supports_logits_to_keep', '_supports_quantized_cache', '_supports_sdpa', '_supports_static_cache', '_temporary_reorder_cache', '_tie_encoder_decoder_weights', '_tie_or_clone_weights', '_tied_weights_keys', '_tp_plan', '_tp_plan', '_unflatten_beam_dim', '_update_causal_mask', '_update_finished_beams', '_update_model_kwargs_for_generation', '_upload_modified_files', '_validate_assistant', '_validate_generated_length', '_validate_model_class', '_validate_model_kwargs', '_version', '_wrapped_call_impl', 'active_adapter', 'active_adapters', 'add_adapter', 'add_memory_hooks', 'add_model_tags', 'add_module', 'apply', 'base_model', 'base_model_prefix', 'bfloat16', 'buffers', 'call_super_init', 'can_generate', 'children', 'compile', 'compute_transition_scores', 'config', 'config_class', 'cpu', 'create_chunked_attention_mask', 'create_extended_attention_mask_for_decoder', 'cuda', 'delete_adapter', 'dequantize', 'device', 'disable_adapters', 'disable_input_require_grads', 'double', 'dtype', 'dummy_inputs', 'dump_patches', 'embed_tokens', 'enable_adapters', 'enable_input_require_grads', 'estimate_tokens', 'eval', 'extra_repr', 'float', 'floating_point_ops', 'forward', 'framework', 'from_pretrained', 'generate', 'generation_config', 'get_adapter_state_dict', 'get_buffer', 'get_compiled_call', 'get_extended_attention_mask', 'get_extra_state', 'get_head_mask', 'get_init_context', 'get_input_embeddings', 'get_memory_footprint', 'get_output_embeddings', 'get_parameter', 'get_parameter_or_buffer', 'get_position_embeddings', 'get_submodule', 'gradient_checkpointing', 'gradient_checkpointing_disable', 'gradient_checkpointing_enable', 'half', 'heal_tokens', 'init_weights', 'invert_attention_mask', 'ipu', 'is_backend_compatible', 'is_gradient_checkpointing', 'is_parallelizable', 'layers', 'load_adapter', 'load_state_dict', 'loss_function', 'loss_type', 'main_input_name', 'model_tags', 'modules', 'mtia', 'name_or_path', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'norm', 'num_parameters', 'padding_idx', 'parameters', 'post_init', 'prepare_inputs_for_generation', 'prune_heads', 'push_to_hub', 'register_backward_hook', 'register_buffer', 'register_for_auto_class', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'reset_memory_hooks_state', 'resize_position_embeddings', 'resize_token_embeddings', 'retrieve_modules_from_names', 'reverse_bettertransformer', 'rotary_emb', 'save_pretrained', 'set_adapter', 'set_extra_state', 'set_input_embeddings', 'set_submodule', 'share_memory', 'state_dict', 'supports_gradient_checkpointing', 'supports_pp_plan', 'supports_tp_plan', 'tie_weights', 'to', 'to_bettertransformer', 'to_empty', 'train', 'training', 'type', 'vocab_size', 'warn_if_padding_and_no_attention_mask', 'warnings_issued', 'xpu', 'zero_grad']\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m np\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39massert_allclose(a, b, rtol\u001b[38;5;241m=\u001b[39mRTOL, atol\u001b[38;5;241m=\u001b[39mATOL)\n\u001b[1;32m     24\u001b[0m RTOL, ATOL \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m, \u001b[38;5;241m1e-3\u001b[39m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_allclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogits_dense\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkernel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRTOL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mATOL\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama4-maxtedxt/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama4-maxtedxt/lib/python3.10/site-packages/numpy/testing/_private/utils.py:729\u001b[0m, in \u001b[0;36massert_array_compare.<locals>.func_assert_same_pos\u001b[0;34m(x, y, func, hasval)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Handling nan/inf.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \n\u001b[1;32m    723\u001b[0m \u001b[38;5;124;03mCombine results of running func on x and y, checking that they are True\u001b[39;00m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;124;03mat the same locations.\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \n\u001b[1;32m    726\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    727\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Hide traceback for py.test\u001b[39;00m\n\u001b[0;32m--> 729\u001b[0m x_id \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m y_id \u001b[38;5;241m=\u001b[39m func(y)\n\u001b[1;32m    731\u001b[0m \u001b[38;5;66;03m# We include work-arounds here to handle three types of slightly\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;66;03m# pathological ndarray subclasses:\u001b[39;00m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;66;03m# (1) all() on `masked` array scalars can return masked arrays, so we\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;66;03m# We are not committed to supporting such subclasses, but it's nice to\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;66;03m# support them if possible.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "RTOL, ATOL = 1e-3, 1e-3\n",
    "\n",
    "a = hf_model.model.layers[0].input_layernorm.weight.detach().numpy()\n",
    "b = state.params[\"params\"]['decoder']['layers_0'][\"pre_self_attention_layer_norm\"][\"scale\"]\n",
    "\n",
    "np.testing.assert_allclose(a, b, rtol=RTOL, atol=ATOL)\n",
    "\n",
    "\n",
    "a = hf_model.model.norm.weight.detach().numpy()\n",
    "b = state.params[\"params\"][\"decoder\"]['decoder_norm']['scale']\n",
    "np.testing.assert_allclose(a, b, rtol=RTOL, atol=ATOL)\n",
    "\n",
    "RTOL, ATOL = 1e-3, 1e-3\n",
    "np.testing.assert_allclose(hf_model.lm_head.weight.detach().cpu().numpy(), state.params[\"params\"][\"decoder\"]['logits_dense']['kernel'].transpose(), rtol=RTOL, atol=ATOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama4-maxtedxt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
